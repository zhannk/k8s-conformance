I0117 06:26:41.119506      23 e2e.go:116] Starting e2e run "dc5ce341-7d8d-4db7-8b68-67ab063698df" on Ginkgo node 1
Jan 17 06:26:41.163: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1673936800 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 17 06:26:41.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:26:41.346: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 17 06:26:41.407: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 17 06:26:41.496: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 17 06:26:41.496: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Jan 17 06:26:41.496: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 17 06:26:41.515: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Jan 17 06:26:41.515: INFO: e2e test version: v1.25.5
Jan 17 06:26:41.527: INFO: kube-apiserver version: v1.25.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 17 06:26:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:26:41.534: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.192 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 17 06:26:41.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:26:41.346: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 17 06:26:41.407: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 17 06:26:41.496: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 17 06:26:41.496: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Jan 17 06:26:41.496: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 17 06:26:41.515: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Jan 17 06:26:41.515: INFO: e2e test version: v1.25.5
    Jan 17 06:26:41.527: INFO: kube-apiserver version: v1.25.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 17 06:26:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:26:41.534: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:26:41.582
Jan 17 06:26:41.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 06:26:41.584
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:26:41.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:26:41.635
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/17/23 06:26:41.641
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:26:41.655
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:26:41.655
STEP: creating a pod to probe DNS 01/17/23 06:26:41.655
STEP: submitting the pod to kubernetes 01/17/23 06:26:41.655
Jan 17 06:26:41.675: INFO: Waiting up to 15m0s for pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0" in namespace "dns-4262" to be "running"
Jan 17 06:26:41.689: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.155663ms
Jan 17 06:26:43.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019659422s
Jan 17 06:26:45.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020093687s
Jan 17 06:26:47.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020060356s
Jan 17 06:26:49.701: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025151216s
Jan 17 06:26:51.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019137601s
Jan 17 06:26:53.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020642153s
Jan 17 06:26:55.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020073856s
Jan 17 06:26:57.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019011589s
Jan 17 06:26:59.699: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022959268s
Jan 17 06:27:01.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019157221s
Jan 17 06:27:03.697: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021868004s
Jan 17 06:27:05.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020004564s
Jan 17 06:27:07.697: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021389671s
Jan 17 06:27:09.707: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.031244588s
Jan 17 06:27:11.707: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.031502908s
Jan 17 06:27:13.694: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018467733s
Jan 17 06:27:15.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Running", Reason="", readiness=true. Elapsed: 34.018945528s
Jan 17 06:27:15.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:27:15.695
STEP: looking for the results for each expected name from probers 01/17/23 06:27:15.699
Jan 17 06:27:15.714: INFO: DNS probes using dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0 succeeded

STEP: deleting the pod 01/17/23 06:27:15.714
STEP: changing the externalName to bar.example.com 01/17/23 06:27:15.738
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:27:15.771
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:27:15.772
STEP: creating a second pod to probe DNS 01/17/23 06:27:15.772
STEP: submitting the pod to kubernetes 01/17/23 06:27:15.772
Jan 17 06:27:15.793: INFO: Waiting up to 15m0s for pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc" in namespace "dns-4262" to be "running"
Jan 17 06:27:15.800: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579668ms
Jan 17 06:27:17.808: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014909928s
Jan 17 06:27:19.806: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.013205928s
Jan 17 06:27:19.806: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:27:19.806
STEP: looking for the results for each expected name from probers 01/17/23 06:27:19.81
Jan 17 06:27:19.820: INFO: DNS probes using dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc succeeded

STEP: deleting the pod 01/17/23 06:27:19.82
STEP: changing the service to type=ClusterIP 01/17/23 06:27:19.848
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:27:19.874
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
 01/17/23 06:27:19.875
STEP: creating a third pod to probe DNS 01/17/23 06:27:19.875
STEP: submitting the pod to kubernetes 01/17/23 06:27:19.89
Jan 17 06:27:19.905: INFO: Waiting up to 15m0s for pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7" in namespace "dns-4262" to be "running"
Jan 17 06:27:19.919: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.07881ms
Jan 17 06:27:21.925: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.018309685s
Jan 17 06:27:21.925: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:27:21.925
STEP: looking for the results for each expected name from probers 01/17/23 06:27:21.928
Jan 17 06:27:21.939: INFO: DNS probes using dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7 succeeded

STEP: deleting the pod 01/17/23 06:27:21.939
STEP: deleting the test externalName service 01/17/23 06:27:21.961
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 06:27:22.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4262" for this suite. 01/17/23 06:27:22.013
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":1,"skipped":20,"failed":0}
------------------------------
• [SLOW TEST] [40.449 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:26:41.582
    Jan 17 06:26:41.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 06:26:41.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:26:41.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:26:41.635
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/17/23 06:26:41.641
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:26:41.655
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:26:41.655
    STEP: creating a pod to probe DNS 01/17/23 06:26:41.655
    STEP: submitting the pod to kubernetes 01/17/23 06:26:41.655
    Jan 17 06:26:41.675: INFO: Waiting up to 15m0s for pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0" in namespace "dns-4262" to be "running"
    Jan 17 06:26:41.689: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.155663ms
    Jan 17 06:26:43.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019659422s
    Jan 17 06:26:45.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020093687s
    Jan 17 06:26:47.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020060356s
    Jan 17 06:26:49.701: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025151216s
    Jan 17 06:26:51.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019137601s
    Jan 17 06:26:53.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020642153s
    Jan 17 06:26:55.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020073856s
    Jan 17 06:26:57.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019011589s
    Jan 17 06:26:59.699: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022959268s
    Jan 17 06:27:01.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019157221s
    Jan 17 06:27:03.697: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021868004s
    Jan 17 06:27:05.696: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020004564s
    Jan 17 06:27:07.697: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021389671s
    Jan 17 06:27:09.707: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.031244588s
    Jan 17 06:27:11.707: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.031502908s
    Jan 17 06:27:13.694: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018467733s
    Jan 17 06:27:15.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0": Phase="Running", Reason="", readiness=true. Elapsed: 34.018945528s
    Jan 17 06:27:15.695: INFO: Pod "dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:27:15.695
    STEP: looking for the results for each expected name from probers 01/17/23 06:27:15.699
    Jan 17 06:27:15.714: INFO: DNS probes using dns-test-c7bfb824-d0c3-497a-a5d2-e5429f1a5bb0 succeeded

    STEP: deleting the pod 01/17/23 06:27:15.714
    STEP: changing the externalName to bar.example.com 01/17/23 06:27:15.738
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:27:15.771
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:27:15.772
    STEP: creating a second pod to probe DNS 01/17/23 06:27:15.772
    STEP: submitting the pod to kubernetes 01/17/23 06:27:15.772
    Jan 17 06:27:15.793: INFO: Waiting up to 15m0s for pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc" in namespace "dns-4262" to be "running"
    Jan 17 06:27:15.800: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579668ms
    Jan 17 06:27:17.808: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014909928s
    Jan 17 06:27:19.806: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.013205928s
    Jan 17 06:27:19.806: INFO: Pod "dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:27:19.806
    STEP: looking for the results for each expected name from probers 01/17/23 06:27:19.81
    Jan 17 06:27:19.820: INFO: DNS probes using dns-test-9c205195-4061-4a05-b745-a0b9ee6601fc succeeded

    STEP: deleting the pod 01/17/23 06:27:19.82
    STEP: changing the service to type=ClusterIP 01/17/23 06:27:19.848
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:27:19.874
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4262.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4262.svc.cluster.local; sleep 1; done
     01/17/23 06:27:19.875
    STEP: creating a third pod to probe DNS 01/17/23 06:27:19.875
    STEP: submitting the pod to kubernetes 01/17/23 06:27:19.89
    Jan 17 06:27:19.905: INFO: Waiting up to 15m0s for pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7" in namespace "dns-4262" to be "running"
    Jan 17 06:27:19.919: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.07881ms
    Jan 17 06:27:21.925: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.018309685s
    Jan 17 06:27:21.925: INFO: Pod "dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:27:21.925
    STEP: looking for the results for each expected name from probers 01/17/23 06:27:21.928
    Jan 17 06:27:21.939: INFO: DNS probes using dns-test-93e9b0cc-50da-4446-b39e-c2896a7cbbd7 succeeded

    STEP: deleting the pod 01/17/23 06:27:21.939
    STEP: deleting the test externalName service 01/17/23 06:27:21.961
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 06:27:22.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4262" for this suite. 01/17/23 06:27:22.013
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:27:22.032
Jan 17 06:27:22.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 06:27:22.034
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:22.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:22.07
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 06:27:22.117
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:27:23.026
STEP: Deploying the webhook pod 01/17/23 06:27:23.039
STEP: Wait for the deployment to be ready 01/17/23 06:27:23.065
Jan 17 06:27:23.075: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 06:27:25.092
STEP: Verifying the service has paired with the endpoint 01/17/23 06:27:25.114
Jan 17 06:27:26.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/17/23 06:27:26.121
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/17/23 06:27:26.124
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 06:27:26.125
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/17/23 06:27:26.125
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/17/23 06:27:26.127
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 06:27:26.127
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 06:27:26.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:27:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2054" for this suite. 01/17/23 06:27:26.14
STEP: Destroying namespace "webhook-2054-markers" for this suite. 01/17/23 06:27:26.156
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":2,"skipped":22,"failed":0}
------------------------------
• [4.227 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:27:22.032
    Jan 17 06:27:22.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 06:27:22.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:22.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:22.07
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 06:27:22.117
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:27:23.026
    STEP: Deploying the webhook pod 01/17/23 06:27:23.039
    STEP: Wait for the deployment to be ready 01/17/23 06:27:23.065
    Jan 17 06:27:23.075: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 06:27:25.092
    STEP: Verifying the service has paired with the endpoint 01/17/23 06:27:25.114
    Jan 17 06:27:26.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/17/23 06:27:26.121
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/17/23 06:27:26.124
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 06:27:26.125
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/17/23 06:27:26.125
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/17/23 06:27:26.127
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 06:27:26.127
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 06:27:26.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:27:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2054" for this suite. 01/17/23 06:27:26.14
    STEP: Destroying namespace "webhook-2054-markers" for this suite. 01/17/23 06:27:26.156
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:27:26.26
Jan 17 06:27:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename csistoragecapacity 01/17/23 06:27:26.261
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.317
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/17/23 06:27:26.323
STEP: getting /apis/storage.k8s.io 01/17/23 06:27:26.329
STEP: getting /apis/storage.k8s.io/v1 01/17/23 06:27:26.332
STEP: creating 01/17/23 06:27:26.335
STEP: watching 01/17/23 06:27:26.367
Jan 17 06:27:26.367: INFO: starting watch
STEP: getting 01/17/23 06:27:26.382
STEP: listing in namespace 01/17/23 06:27:26.387
STEP: listing across namespaces 01/17/23 06:27:26.391
STEP: patching 01/17/23 06:27:26.396
STEP: updating 01/17/23 06:27:26.412
Jan 17 06:27:26.422: INFO: waiting for watch events with expected annotations in namespace
Jan 17 06:27:26.423: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/17/23 06:27:26.423
STEP: deleting a collection 01/17/23 06:27:26.444
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 17 06:27:26.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1206" for this suite. 01/17/23 06:27:26.475
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":3,"skipped":34,"failed":0}
------------------------------
• [0.233 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:27:26.26
    Jan 17 06:27:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename csistoragecapacity 01/17/23 06:27:26.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.317
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/17/23 06:27:26.323
    STEP: getting /apis/storage.k8s.io 01/17/23 06:27:26.329
    STEP: getting /apis/storage.k8s.io/v1 01/17/23 06:27:26.332
    STEP: creating 01/17/23 06:27:26.335
    STEP: watching 01/17/23 06:27:26.367
    Jan 17 06:27:26.367: INFO: starting watch
    STEP: getting 01/17/23 06:27:26.382
    STEP: listing in namespace 01/17/23 06:27:26.387
    STEP: listing across namespaces 01/17/23 06:27:26.391
    STEP: patching 01/17/23 06:27:26.396
    STEP: updating 01/17/23 06:27:26.412
    Jan 17 06:27:26.422: INFO: waiting for watch events with expected annotations in namespace
    Jan 17 06:27:26.423: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/17/23 06:27:26.423
    STEP: deleting a collection 01/17/23 06:27:26.444
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 17 06:27:26.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-1206" for this suite. 01/17/23 06:27:26.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:27:26.499
Jan 17 06:27:26.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename endpointslice 01/17/23 06:27:26.501
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.615
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/17/23 06:27:26.621
STEP: getting /apis/discovery.k8s.io 01/17/23 06:27:26.628
STEP: getting /apis/discovery.k8s.iov1 01/17/23 06:27:26.63
STEP: creating 01/17/23 06:27:26.632
STEP: getting 01/17/23 06:27:26.669
STEP: listing 01/17/23 06:27:26.674
STEP: watching 01/17/23 06:27:26.678
Jan 17 06:27:26.679: INFO: starting watch
STEP: cluster-wide listing 01/17/23 06:27:26.681
STEP: cluster-wide watching 01/17/23 06:27:26.687
Jan 17 06:27:26.687: INFO: starting watch
STEP: patching 01/17/23 06:27:26.689
STEP: updating 01/17/23 06:27:26.698
Jan 17 06:27:26.711: INFO: waiting for watch events with expected annotations
Jan 17 06:27:26.712: INFO: saw patched and updated annotations
STEP: deleting 01/17/23 06:27:26.712
STEP: deleting a collection 01/17/23 06:27:26.736
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 06:27:26.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6369" for this suite. 01/17/23 06:27:26.768
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":4,"skipped":57,"failed":0}
------------------------------
• [0.293 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:27:26.499
    Jan 17 06:27:26.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename endpointslice 01/17/23 06:27:26.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.615
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/17/23 06:27:26.621
    STEP: getting /apis/discovery.k8s.io 01/17/23 06:27:26.628
    STEP: getting /apis/discovery.k8s.iov1 01/17/23 06:27:26.63
    STEP: creating 01/17/23 06:27:26.632
    STEP: getting 01/17/23 06:27:26.669
    STEP: listing 01/17/23 06:27:26.674
    STEP: watching 01/17/23 06:27:26.678
    Jan 17 06:27:26.679: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 06:27:26.681
    STEP: cluster-wide watching 01/17/23 06:27:26.687
    Jan 17 06:27:26.687: INFO: starting watch
    STEP: patching 01/17/23 06:27:26.689
    STEP: updating 01/17/23 06:27:26.698
    Jan 17 06:27:26.711: INFO: waiting for watch events with expected annotations
    Jan 17 06:27:26.712: INFO: saw patched and updated annotations
    STEP: deleting 01/17/23 06:27:26.712
    STEP: deleting a collection 01/17/23 06:27:26.736
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 06:27:26.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6369" for this suite. 01/17/23 06:27:26.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:27:26.793
Jan 17 06:27:26.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:27:26.795
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.834
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/17/23 06:27:26.839
Jan 17 06:27:26.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:27:34.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:27:59.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1992" for this suite. 01/17/23 06:27:59.332
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":5,"skipped":63,"failed":0}
------------------------------
• [SLOW TEST] [32.552 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:27:26.793
    Jan 17 06:27:26.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:27:26.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:26.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:26.834
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/17/23 06:27:26.839
    Jan 17 06:27:26.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:27:34.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:27:59.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1992" for this suite. 01/17/23 06:27:59.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:27:59.346
Jan 17 06:27:59.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename watch 01/17/23 06:27:59.35
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:59.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:59.387
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/17/23 06:27:59.396
STEP: creating a new configmap 01/17/23 06:27:59.409
STEP: modifying the configmap once 01/17/23 06:27:59.426
STEP: changing the label value of the configmap 01/17/23 06:27:59.444
STEP: Expecting to observe a delete notification for the watched object 01/17/23 06:27:59.462
Jan 17 06:27:59.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3870 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:27:59.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3871 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:27:59.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3872 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/17/23 06:27:59.463
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/17/23 06:27:59.479
STEP: changing the label value of the configmap back 01/17/23 06:28:09.479
STEP: modifying the configmap a third time 01/17/23 06:28:09.507
STEP: deleting the configmap 01/17/23 06:28:09.523
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/17/23 06:28:09.577
Jan 17 06:28:09.577: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3901 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:28:09.577: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3903 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:28:09.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3904 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 06:28:09.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2994" for this suite. 01/17/23 06:28:09.584
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":6,"skipped":82,"failed":0}
------------------------------
• [SLOW TEST] [10.258 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:27:59.346
    Jan 17 06:27:59.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename watch 01/17/23 06:27:59.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:27:59.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:27:59.387
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/17/23 06:27:59.396
    STEP: creating a new configmap 01/17/23 06:27:59.409
    STEP: modifying the configmap once 01/17/23 06:27:59.426
    STEP: changing the label value of the configmap 01/17/23 06:27:59.444
    STEP: Expecting to observe a delete notification for the watched object 01/17/23 06:27:59.462
    Jan 17 06:27:59.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3870 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:27:59.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3871 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:27:59.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3872 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:27:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/17/23 06:27:59.463
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/17/23 06:27:59.479
    STEP: changing the label value of the configmap back 01/17/23 06:28:09.479
    STEP: modifying the configmap a third time 01/17/23 06:28:09.507
    STEP: deleting the configmap 01/17/23 06:28:09.523
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/17/23 06:28:09.577
    Jan 17 06:28:09.577: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3901 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:28:09.577: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3903 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:28:09.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2994  2135bfaf-0a87-4aef-a20f-341eaae9ef31 3904 0 2023-01-17 06:27:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 06:28:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 06:28:09.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2994" for this suite. 01/17/23 06:28:09.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:28:09.606
Jan 17 06:28:09.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:28:09.607
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:28:09.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:28:09.69
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1619 01/17/23 06:28:09.705
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 06:28:09.759
STEP: creating service externalsvc in namespace services-1619 01/17/23 06:28:09.759
STEP: creating replication controller externalsvc in namespace services-1619 01/17/23 06:28:09.816
I0117 06:28:09.843563      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1619, replica count: 2
I0117 06:28:12.894904      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 06:28:15.896389      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 06:28:18.896624      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 06:28:21.896847      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 06:28:24.897124      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/17/23 06:28:24.905
Jan 17 06:28:24.955: INFO: Creating new exec pod
Jan 17 06:28:24.983: INFO: Waiting up to 5m0s for pod "execpod7nbt2" in namespace "services-1619" to be "running"
Jan 17 06:28:24.992: INFO: Pod "execpod7nbt2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.746226ms
Jan 17 06:28:27.013: INFO: Pod "execpod7nbt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029157788s
Jan 17 06:28:27.013: INFO: Pod "execpod7nbt2" satisfied condition "running"
Jan 17 06:28:27.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1619 exec execpod7nbt2 -- /bin/sh -x -c nslookup nodeport-service.services-1619.svc.cluster.local'
Jan 17 06:28:27.496: INFO: stderr: "+ nslookup nodeport-service.services-1619.svc.cluster.local\n"
Jan 17 06:28:27.496: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-1619.svc.cluster.local\tcanonical name = externalsvc.services-1619.svc.cluster.local.\nName:\texternalsvc.services-1619.svc.cluster.local\nAddress: 10.254.149.239\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1619, will wait for the garbage collector to delete the pods 01/17/23 06:28:27.496
Jan 17 06:28:27.568: INFO: Deleting ReplicationController externalsvc took: 12.177832ms
Jan 17 06:28:27.669: INFO: Terminating ReplicationController externalsvc pods took: 100.947324ms
Jan 17 06:28:30.117: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:28:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1619" for this suite. 01/17/23 06:28:30.163
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":7,"skipped":95,"failed":0}
------------------------------
• [SLOW TEST] [20.572 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:28:09.606
    Jan 17 06:28:09.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:28:09.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:28:09.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:28:09.69
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1619 01/17/23 06:28:09.705
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 06:28:09.759
    STEP: creating service externalsvc in namespace services-1619 01/17/23 06:28:09.759
    STEP: creating replication controller externalsvc in namespace services-1619 01/17/23 06:28:09.816
    I0117 06:28:09.843563      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1619, replica count: 2
    I0117 06:28:12.894904      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 06:28:15.896389      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 06:28:18.896624      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 06:28:21.896847      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 06:28:24.897124      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/17/23 06:28:24.905
    Jan 17 06:28:24.955: INFO: Creating new exec pod
    Jan 17 06:28:24.983: INFO: Waiting up to 5m0s for pod "execpod7nbt2" in namespace "services-1619" to be "running"
    Jan 17 06:28:24.992: INFO: Pod "execpod7nbt2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.746226ms
    Jan 17 06:28:27.013: INFO: Pod "execpod7nbt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029157788s
    Jan 17 06:28:27.013: INFO: Pod "execpod7nbt2" satisfied condition "running"
    Jan 17 06:28:27.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1619 exec execpod7nbt2 -- /bin/sh -x -c nslookup nodeport-service.services-1619.svc.cluster.local'
    Jan 17 06:28:27.496: INFO: stderr: "+ nslookup nodeport-service.services-1619.svc.cluster.local\n"
    Jan 17 06:28:27.496: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-1619.svc.cluster.local\tcanonical name = externalsvc.services-1619.svc.cluster.local.\nName:\texternalsvc.services-1619.svc.cluster.local\nAddress: 10.254.149.239\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1619, will wait for the garbage collector to delete the pods 01/17/23 06:28:27.496
    Jan 17 06:28:27.568: INFO: Deleting ReplicationController externalsvc took: 12.177832ms
    Jan 17 06:28:27.669: INFO: Terminating ReplicationController externalsvc pods took: 100.947324ms
    Jan 17 06:28:30.117: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:28:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1619" for this suite. 01/17/23 06:28:30.163
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:28:30.18
Jan 17 06:28:30.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename taint-single-pod 01/17/23 06:28:30.181
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:28:30.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:28:30.229
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 17 06:28:30.242: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 06:29:30.317: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 17 06:29:30.325: INFO: Starting informer...
STEP: Starting pod... 01/17/23 06:29:30.326
Jan 17 06:29:30.559: INFO: Pod is running on cluster125-w73dz53kvqes-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/17/23 06:29:30.559
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 06:29:30.619
STEP: Waiting short time to make sure Pod is queued for deletion 01/17/23 06:29:30.661
Jan 17 06:29:30.661: INFO: Pod wasn't evicted. Proceeding
Jan 17 06:29:30.661: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 06:29:30.726
STEP: Waiting some time to make sure that toleration time passed. 01/17/23 06:29:30.769
Jan 17 06:30:45.773: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:30:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6658" for this suite. 01/17/23 06:30:45.782
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":8,"skipped":112,"failed":0}
------------------------------
• [SLOW TEST] [135.622 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:28:30.18
    Jan 17 06:28:30.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename taint-single-pod 01/17/23 06:28:30.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:28:30.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:28:30.229
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 17 06:28:30.242: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 06:29:30.317: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 17 06:29:30.325: INFO: Starting informer...
    STEP: Starting pod... 01/17/23 06:29:30.326
    Jan 17 06:29:30.559: INFO: Pod is running on cluster125-w73dz53kvqes-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/17/23 06:29:30.559
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 06:29:30.619
    STEP: Waiting short time to make sure Pod is queued for deletion 01/17/23 06:29:30.661
    Jan 17 06:29:30.661: INFO: Pod wasn't evicted. Proceeding
    Jan 17 06:29:30.661: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 06:29:30.726
    STEP: Waiting some time to make sure that toleration time passed. 01/17/23 06:29:30.769
    Jan 17 06:30:45.773: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:30:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-6658" for this suite. 01/17/23 06:30:45.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:30:45.802
Jan 17 06:30:45.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 06:30:45.804
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:30:45.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:30:45.848
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/17/23 06:30:45.862
Jan 17 06:30:45.887: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9864" to be "running and ready"
Jan 17 06:30:45.897: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.310799ms
Jan 17 06:30:45.898: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:47.905: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018288187s
Jan 17 06:30:47.905: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:49.904: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017004798s
Jan 17 06:30:49.904: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:51.924: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036680435s
Jan 17 06:30:51.924: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:53.903: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01580222s
Jan 17 06:30:53.903: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:55.907: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020015367s
Jan 17 06:30:55.907: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:57.903: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015805788s
Jan 17 06:30:57.903: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:30:59.904: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 14.017327264s
Jan 17 06:30:59.905: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 17 06:30:59.905: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/17/23 06:30:59.909
STEP: Then the orphan pod is adopted 01/17/23 06:30:59.928
STEP: When the matched label of one of its pods change 01/17/23 06:31:00.941
Jan 17 06:31:00.947: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/17/23 06:31:00.967
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 06:31:00.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9864" for this suite. 01/17/23 06:31:00.994
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":9,"skipped":119,"failed":0}
------------------------------
• [SLOW TEST] [15.211 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:30:45.802
    Jan 17 06:30:45.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 06:30:45.804
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:30:45.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:30:45.848
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/17/23 06:30:45.862
    Jan 17 06:30:45.887: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9864" to be "running and ready"
    Jan 17 06:30:45.897: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.310799ms
    Jan 17 06:30:45.898: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:47.905: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018288187s
    Jan 17 06:30:47.905: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:49.904: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017004798s
    Jan 17 06:30:49.904: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:51.924: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036680435s
    Jan 17 06:30:51.924: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:53.903: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01580222s
    Jan 17 06:30:53.903: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:55.907: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020015367s
    Jan 17 06:30:55.907: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:57.903: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015805788s
    Jan 17 06:30:57.903: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:30:59.904: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 14.017327264s
    Jan 17 06:30:59.905: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 17 06:30:59.905: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/17/23 06:30:59.909
    STEP: Then the orphan pod is adopted 01/17/23 06:30:59.928
    STEP: When the matched label of one of its pods change 01/17/23 06:31:00.941
    Jan 17 06:31:00.947: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/17/23 06:31:00.967
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 06:31:00.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9864" for this suite. 01/17/23 06:31:00.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:31:01.014
Jan 17 06:31:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 06:31:01.016
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:31:01.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:31:01.082
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-6333d0e9-739a-43e5-a3e6-0cf6ed207dc3 01/17/23 06:31:01.096
STEP: Creating secret with name s-test-opt-upd-5357c3c3-2e31-4b3c-a906-51f569170cf5 01/17/23 06:31:01.111
STEP: Creating the pod 01/17/23 06:31:01.122
Jan 17 06:31:01.142: INFO: Waiting up to 5m0s for pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6" in namespace "secrets-4667" to be "running and ready"
Jan 17 06:31:01.156: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.533057ms
Jan 17 06:31:01.157: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:31:03.165: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022662682s
Jan 17 06:31:03.165: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:31:05.163: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Running", Reason="", readiness=true. Elapsed: 4.021100764s
Jan 17 06:31:05.164: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Running (Ready = true)
Jan 17 06:31:05.164: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-6333d0e9-739a-43e5-a3e6-0cf6ed207dc3 01/17/23 06:31:05.269
STEP: Updating secret s-test-opt-upd-5357c3c3-2e31-4b3c-a906-51f569170cf5 01/17/23 06:31:05.282
STEP: Creating secret with name s-test-opt-create-2b533241-4081-4f4a-9710-8b5c41046d84 01/17/23 06:31:05.296
STEP: waiting to observe update in volume 01/17/23 06:31:05.308
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 06:32:38.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4667" for this suite. 01/17/23 06:32:38.103
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":10,"skipped":132,"failed":0}
------------------------------
• [SLOW TEST] [97.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:31:01.014
    Jan 17 06:31:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 06:31:01.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:31:01.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:31:01.082
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-6333d0e9-739a-43e5-a3e6-0cf6ed207dc3 01/17/23 06:31:01.096
    STEP: Creating secret with name s-test-opt-upd-5357c3c3-2e31-4b3c-a906-51f569170cf5 01/17/23 06:31:01.111
    STEP: Creating the pod 01/17/23 06:31:01.122
    Jan 17 06:31:01.142: INFO: Waiting up to 5m0s for pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6" in namespace "secrets-4667" to be "running and ready"
    Jan 17 06:31:01.156: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.533057ms
    Jan 17 06:31:01.157: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:31:03.165: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022662682s
    Jan 17 06:31:03.165: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:31:05.163: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6": Phase="Running", Reason="", readiness=true. Elapsed: 4.021100764s
    Jan 17 06:31:05.164: INFO: The phase of Pod pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6 is Running (Ready = true)
    Jan 17 06:31:05.164: INFO: Pod "pod-secrets-fdd14bee-b0ff-449f-b29c-956d047390b6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-6333d0e9-739a-43e5-a3e6-0cf6ed207dc3 01/17/23 06:31:05.269
    STEP: Updating secret s-test-opt-upd-5357c3c3-2e31-4b3c-a906-51f569170cf5 01/17/23 06:31:05.282
    STEP: Creating secret with name s-test-opt-create-2b533241-4081-4f4a-9710-8b5c41046d84 01/17/23 06:31:05.296
    STEP: waiting to observe update in volume 01/17/23 06:31:05.308
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 06:32:38.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4667" for this suite. 01/17/23 06:32:38.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:32:38.118
Jan 17 06:32:38.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename tables 01/17/23 06:32:38.12
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:38.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:38.166
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 17 06:32:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7848" for this suite. 01/17/23 06:32:38.186
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":11,"skipped":138,"failed":0}
------------------------------
• [0.081 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:32:38.118
    Jan 17 06:32:38.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename tables 01/17/23 06:32:38.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:38.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:38.166
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 17 06:32:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7848" for this suite. 01/17/23 06:32:38.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:32:38.201
Jan 17 06:32:38.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 06:32:38.202
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:38.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:38.26
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/17/23 06:32:38.266
Jan 17 06:32:38.286: INFO: Waiting up to 5m0s for pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71" in namespace "var-expansion-8412" to be "Succeeded or Failed"
Jan 17 06:32:38.295: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822075ms
Jan 17 06:32:40.302: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015626365s
Jan 17 06:32:42.306: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019847317s
Jan 17 06:32:44.303: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01721237s
Jan 17 06:32:46.301: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015381431s
Jan 17 06:32:48.301: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.01556535s
STEP: Saw pod success 01/17/23 06:32:48.301
Jan 17 06:32:48.302: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71" satisfied condition "Succeeded or Failed"
Jan 17 06:32:48.307: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 container dapi-container: <nil>
STEP: delete the pod 01/17/23 06:32:48.375
Jan 17 06:32:48.399: INFO: Waiting for pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 to disappear
Jan 17 06:32:48.405: INFO: Pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 06:32:48.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8412" for this suite. 01/17/23 06:32:48.411
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":12,"skipped":174,"failed":0}
------------------------------
• [SLOW TEST] [10.223 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:32:38.201
    Jan 17 06:32:38.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 06:32:38.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:38.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:38.26
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/17/23 06:32:38.266
    Jan 17 06:32:38.286: INFO: Waiting up to 5m0s for pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71" in namespace "var-expansion-8412" to be "Succeeded or Failed"
    Jan 17 06:32:38.295: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822075ms
    Jan 17 06:32:40.302: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015626365s
    Jan 17 06:32:42.306: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019847317s
    Jan 17 06:32:44.303: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01721237s
    Jan 17 06:32:46.301: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015381431s
    Jan 17 06:32:48.301: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.01556535s
    STEP: Saw pod success 01/17/23 06:32:48.301
    Jan 17 06:32:48.302: INFO: Pod "var-expansion-701b52eb-7d57-429c-b726-3759ad475e71" satisfied condition "Succeeded or Failed"
    Jan 17 06:32:48.307: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 06:32:48.375
    Jan 17 06:32:48.399: INFO: Waiting for pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 to disappear
    Jan 17 06:32:48.405: INFO: Pod var-expansion-701b52eb-7d57-429c-b726-3759ad475e71 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 06:32:48.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8412" for this suite. 01/17/23 06:32:48.411
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:32:48.424
Jan 17 06:32:48.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:32:48.425
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:48.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:48.472
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1299 01/17/23 06:32:48.48
STEP: creating replication controller nodeport-test in namespace services-1299 01/17/23 06:32:48.518
I0117 06:32:48.536935      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1299, replica count: 2
I0117 06:32:51.587932      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 06:32:51.588: INFO: Creating new exec pod
Jan 17 06:32:51.610: INFO: Waiting up to 5m0s for pod "execpodsftzd" in namespace "services-1299" to be "running"
Jan 17 06:32:51.616: INFO: Pod "execpodsftzd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585915ms
Jan 17 06:32:53.635: INFO: Pod "execpodsftzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.025458884s
Jan 17 06:32:53.635: INFO: Pod "execpodsftzd" satisfied condition "running"
Jan 17 06:32:54.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 17 06:32:54.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 17 06:32:54.943: INFO: stdout: ""
Jan 17 06:32:55.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 17 06:32:56.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 17 06:32:56.232: INFO: stdout: "nodeport-test-k29lk"
Jan 17 06:32:56.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
Jan 17 06:32:56.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
Jan 17 06:32:56.523: INFO: stdout: ""
Jan 17 06:32:57.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
Jan 17 06:32:57.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
Jan 17 06:32:57.811: INFO: stdout: ""
Jan 17 06:32:58.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
Jan 17 06:32:58.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
Jan 17 06:32:58.810: INFO: stdout: "nodeport-test-k29lk"
Jan 17 06:32:58.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 31872'
Jan 17 06:32:59.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 31872\nConnection to 10.0.0.16 31872 port [tcp/*] succeeded!\n"
Jan 17 06:32:59.085: INFO: stdout: "nodeport-test-pk87f"
Jan 17 06:32:59.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 31872'
Jan 17 06:32:59.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 31872\nConnection to 10.0.0.21 31872 port [tcp/*] succeeded!\n"
Jan 17 06:32:59.370: INFO: stdout: "nodeport-test-k29lk"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:32:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1299" for this suite. 01/17/23 06:32:59.379
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":13,"skipped":177,"failed":0}
------------------------------
• [SLOW TEST] [10.971 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:32:48.424
    Jan 17 06:32:48.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:32:48.425
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:48.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:48.472
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1299 01/17/23 06:32:48.48
    STEP: creating replication controller nodeport-test in namespace services-1299 01/17/23 06:32:48.518
    I0117 06:32:48.536935      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1299, replica count: 2
    I0117 06:32:51.587932      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 06:32:51.588: INFO: Creating new exec pod
    Jan 17 06:32:51.610: INFO: Waiting up to 5m0s for pod "execpodsftzd" in namespace "services-1299" to be "running"
    Jan 17 06:32:51.616: INFO: Pod "execpodsftzd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585915ms
    Jan 17 06:32:53.635: INFO: Pod "execpodsftzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.025458884s
    Jan 17 06:32:53.635: INFO: Pod "execpodsftzd" satisfied condition "running"
    Jan 17 06:32:54.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 17 06:32:54.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 17 06:32:54.943: INFO: stdout: ""
    Jan 17 06:32:55.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 17 06:32:56.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 17 06:32:56.232: INFO: stdout: "nodeport-test-k29lk"
    Jan 17 06:32:56.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
    Jan 17 06:32:56.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
    Jan 17 06:32:56.523: INFO: stdout: ""
    Jan 17 06:32:57.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
    Jan 17 06:32:57.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
    Jan 17 06:32:57.811: INFO: stdout: ""
    Jan 17 06:32:58.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.178.136 80'
    Jan 17 06:32:58.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.178.136 80\nConnection to 10.254.178.136 80 port [tcp/http] succeeded!\n"
    Jan 17 06:32:58.810: INFO: stdout: "nodeport-test-k29lk"
    Jan 17 06:32:58.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 31872'
    Jan 17 06:32:59.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 31872\nConnection to 10.0.0.16 31872 port [tcp/*] succeeded!\n"
    Jan 17 06:32:59.085: INFO: stdout: "nodeport-test-pk87f"
    Jan 17 06:32:59.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1299 exec execpodsftzd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 31872'
    Jan 17 06:32:59.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 31872\nConnection to 10.0.0.21 31872 port [tcp/*] succeeded!\n"
    Jan 17 06:32:59.370: INFO: stdout: "nodeport-test-k29lk"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:32:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1299" for this suite. 01/17/23 06:32:59.379
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:32:59.395
Jan 17 06:32:59.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replication-controller 01/17/23 06:32:59.397
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:59.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:59.437
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 17 06:32:59.450: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/17/23 06:32:59.47
STEP: Checking rc "condition-test" has the desired failure condition set 01/17/23 06:32:59.486
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/17/23 06:33:00.502
Jan 17 06:33:00.521: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/17/23 06:33:00.521
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 06:33:01.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6080" for this suite. 01/17/23 06:33:01.598
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":14,"skipped":179,"failed":0}
------------------------------
• [2.239 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:32:59.395
    Jan 17 06:32:59.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replication-controller 01/17/23 06:32:59.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:32:59.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:32:59.437
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 17 06:32:59.450: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/17/23 06:32:59.47
    STEP: Checking rc "condition-test" has the desired failure condition set 01/17/23 06:32:59.486
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/17/23 06:33:00.502
    Jan 17 06:33:00.521: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/17/23 06:33:00.521
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 06:33:01.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6080" for this suite. 01/17/23 06:33:01.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:33:01.637
Jan 17 06:33:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 06:33:01.638
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:01.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:01.708
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/17/23 06:33:01.718
STEP: Creating a ResourceQuota 01/17/23 06:33:06.724
STEP: Ensuring resource quota status is calculated 01/17/23 06:33:06.745
STEP: Creating a ReplicationController 01/17/23 06:33:08.758
STEP: Ensuring resource quota status captures replication controller creation 01/17/23 06:33:08.833
STEP: Deleting a ReplicationController 01/17/23 06:33:10.842
STEP: Ensuring resource quota status released usage 01/17/23 06:33:10.86
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 06:33:12.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7698" for this suite. 01/17/23 06:33:12.885
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":15,"skipped":222,"failed":0}
------------------------------
• [SLOW TEST] [11.263 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:33:01.637
    Jan 17 06:33:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 06:33:01.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:01.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:01.708
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/17/23 06:33:01.718
    STEP: Creating a ResourceQuota 01/17/23 06:33:06.724
    STEP: Ensuring resource quota status is calculated 01/17/23 06:33:06.745
    STEP: Creating a ReplicationController 01/17/23 06:33:08.758
    STEP: Ensuring resource quota status captures replication controller creation 01/17/23 06:33:08.833
    STEP: Deleting a ReplicationController 01/17/23 06:33:10.842
    STEP: Ensuring resource quota status released usage 01/17/23 06:33:10.86
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 06:33:12.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7698" for this suite. 01/17/23 06:33:12.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:33:12.904
Jan 17 06:33:12.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:33:12.906
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:12.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:12.953
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/17/23 06:33:12.959
Jan 17 06:33:12.981: INFO: Waiting up to 5m0s for pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa" in namespace "projected-4350" to be "running and ready"
Jan 17 06:33:12.989: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954978ms
Jan 17 06:33:12.989: INFO: The phase of Pod annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:33:14.998: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.016881495s
Jan 17 06:33:14.998: INFO: The phase of Pod annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa is Running (Ready = true)
Jan 17 06:33:14.998: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa" satisfied condition "running and ready"
Jan 17 06:33:15.571: INFO: Successfully updated pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 06:33:19.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4350" for this suite. 01/17/23 06:33:19.619
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":16,"skipped":282,"failed":0}
------------------------------
• [SLOW TEST] [6.735 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:33:12.904
    Jan 17 06:33:12.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:33:12.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:12.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:12.953
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/17/23 06:33:12.959
    Jan 17 06:33:12.981: INFO: Waiting up to 5m0s for pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa" in namespace "projected-4350" to be "running and ready"
    Jan 17 06:33:12.989: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954978ms
    Jan 17 06:33:12.989: INFO: The phase of Pod annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:33:14.998: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.016881495s
    Jan 17 06:33:14.998: INFO: The phase of Pod annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa is Running (Ready = true)
    Jan 17 06:33:14.998: INFO: Pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa" satisfied condition "running and ready"
    Jan 17 06:33:15.571: INFO: Successfully updated pod "annotationupdatefd1281a2-24c2-434a-93f9-eb558c94aeaa"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 06:33:19.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4350" for this suite. 01/17/23 06:33:19.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:33:19.647
Jan 17 06:33:19.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 06:33:19.649
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:19.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:19.695
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-9263f958-7d80-4eb4-9edc-37f8adffe7a1 01/17/23 06:33:19.709
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 06:33:19.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-123" for this suite. 01/17/23 06:33:19.733
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":17,"skipped":294,"failed":0}
------------------------------
• [0.110 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:33:19.647
    Jan 17 06:33:19.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 06:33:19.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:19.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:19.695
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-9263f958-7d80-4eb4-9edc-37f8adffe7a1 01/17/23 06:33:19.709
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 06:33:19.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-123" for this suite. 01/17/23 06:33:19.733
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:33:19.763
Jan 17 06:33:19.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 06:33:19.765
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:19.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:19.854
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-9bb8b58c-1da2-41b2-9be4-16779621e598 01/17/23 06:33:19.871
STEP: Creating configMap with name cm-test-opt-upd-60b009e9-4a9d-4ff9-b57c-c9f7ab6eecb0 01/17/23 06:33:19.888
STEP: Creating the pod 01/17/23 06:33:19.898
Jan 17 06:33:19.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976" in namespace "configmap-1126" to be "running and ready"
Jan 17 06:33:19.939: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Pending", Reason="", readiness=false. Elapsed: 18.364421ms
Jan 17 06:33:19.939: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:33:21.947: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026891819s
Jan 17 06:33:21.948: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:33:23.945: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Running", Reason="", readiness=true. Elapsed: 4.02483521s
Jan 17 06:33:23.945: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Running (Ready = true)
Jan 17 06:33:23.945: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9bb8b58c-1da2-41b2-9be4-16779621e598 01/17/23 06:33:24
STEP: Updating configmap cm-test-opt-upd-60b009e9-4a9d-4ff9-b57c-c9f7ab6eecb0 01/17/23 06:33:24.013
STEP: Creating configMap with name cm-test-opt-create-5c822309-2a5b-4dca-88aa-5282c2c0bacd 01/17/23 06:33:24.028
STEP: waiting to observe update in volume 01/17/23 06:33:24.041
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 06:34:42.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1126" for this suite. 01/17/23 06:34:42.619
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":18,"skipped":298,"failed":0}
------------------------------
• [SLOW TEST] [82.875 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:33:19.763
    Jan 17 06:33:19.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 06:33:19.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:33:19.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:33:19.854
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-9bb8b58c-1da2-41b2-9be4-16779621e598 01/17/23 06:33:19.871
    STEP: Creating configMap with name cm-test-opt-upd-60b009e9-4a9d-4ff9-b57c-c9f7ab6eecb0 01/17/23 06:33:19.888
    STEP: Creating the pod 01/17/23 06:33:19.898
    Jan 17 06:33:19.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976" in namespace "configmap-1126" to be "running and ready"
    Jan 17 06:33:19.939: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Pending", Reason="", readiness=false. Elapsed: 18.364421ms
    Jan 17 06:33:19.939: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:33:21.947: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026891819s
    Jan 17 06:33:21.948: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:33:23.945: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976": Phase="Running", Reason="", readiness=true. Elapsed: 4.02483521s
    Jan 17 06:33:23.945: INFO: The phase of Pod pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976 is Running (Ready = true)
    Jan 17 06:33:23.945: INFO: Pod "pod-configmaps-29effd68-1a8a-4644-bf08-54480ef06976" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9bb8b58c-1da2-41b2-9be4-16779621e598 01/17/23 06:33:24
    STEP: Updating configmap cm-test-opt-upd-60b009e9-4a9d-4ff9-b57c-c9f7ab6eecb0 01/17/23 06:33:24.013
    STEP: Creating configMap with name cm-test-opt-create-5c822309-2a5b-4dca-88aa-5282c2c0bacd 01/17/23 06:33:24.028
    STEP: waiting to observe update in volume 01/17/23 06:33:24.041
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 06:34:42.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1126" for this suite. 01/17/23 06:34:42.619
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:34:42.638
Jan 17 06:34:42.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-runtime 01/17/23 06:34:42.64
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:42.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:42.689
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/17/23 06:34:42.695
STEP: wait for the container to reach Failed 01/17/23 06:34:42.716
STEP: get the container status 01/17/23 06:34:46.755
STEP: the container should be terminated 01/17/23 06:34:46.762
STEP: the termination message should be set 01/17/23 06:34:46.762
Jan 17 06:34:46.762: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/17/23 06:34:46.762
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 06:34:46.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9939" for this suite. 01/17/23 06:34:46.805
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":19,"skipped":298,"failed":0}
------------------------------
• [4.182 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:34:42.638
    Jan 17 06:34:42.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-runtime 01/17/23 06:34:42.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:42.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:42.689
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/17/23 06:34:42.695
    STEP: wait for the container to reach Failed 01/17/23 06:34:42.716
    STEP: get the container status 01/17/23 06:34:46.755
    STEP: the container should be terminated 01/17/23 06:34:46.762
    STEP: the termination message should be set 01/17/23 06:34:46.762
    Jan 17 06:34:46.762: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/17/23 06:34:46.762
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 06:34:46.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9939" for this suite. 01/17/23 06:34:46.805
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:34:46.821
Jan 17 06:34:46.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename watch 01/17/23 06:34:46.823
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:46.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:46.876
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/17/23 06:34:46.885
STEP: creating a new configmap 01/17/23 06:34:46.89
STEP: modifying the configmap once 01/17/23 06:34:46.901
STEP: closing the watch once it receives two notifications 01/17/23 06:34:46.926
Jan 17 06:34:46.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5536 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:34:46.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5537 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/17/23 06:34:46.927
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/17/23 06:34:46.948
STEP: deleting the configmap 01/17/23 06:34:46.951
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/17/23 06:34:46.964
Jan 17 06:34:46.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5538 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 06:34:46.965: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5539 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 06:34:46.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8679" for this suite. 01/17/23 06:34:46.973
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":20,"skipped":302,"failed":0}
------------------------------
• [0.170 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:34:46.821
    Jan 17 06:34:46.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename watch 01/17/23 06:34:46.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:46.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:46.876
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/17/23 06:34:46.885
    STEP: creating a new configmap 01/17/23 06:34:46.89
    STEP: modifying the configmap once 01/17/23 06:34:46.901
    STEP: closing the watch once it receives two notifications 01/17/23 06:34:46.926
    Jan 17 06:34:46.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5536 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:34:46.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5537 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/17/23 06:34:46.927
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/17/23 06:34:46.948
    STEP: deleting the configmap 01/17/23 06:34:46.951
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/17/23 06:34:46.964
    Jan 17 06:34:46.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5538 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 06:34:46.965: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8679  c0b44431-270b-4941-913e-6557185e4ac2 5539 0 2023-01-17 06:34:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 06:34:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 06:34:46.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8679" for this suite. 01/17/23 06:34:46.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:34:46.992
Jan 17 06:34:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 06:34:46.993
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:47.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:47.041
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3682 01/17/23 06:34:47.048
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3682 01/17/23 06:34:47.069
Jan 17 06:34:47.108: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 17 06:34:57.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/17/23 06:34:57.129
STEP: updating a scale subresource 01/17/23 06:34:57.136
STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 06:34:57.149
STEP: Patch a scale subresource 01/17/23 06:34:57.155
STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 06:34:57.219
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 06:34:57.224: INFO: Deleting all statefulset in ns statefulset-3682
Jan 17 06:34:57.241: INFO: Scaling statefulset ss to 0
Jan 17 06:35:07.286: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 06:35:07.294: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 06:35:07.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3682" for this suite. 01/17/23 06:35:07.345
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":21,"skipped":326,"failed":0}
------------------------------
• [SLOW TEST] [20.374 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:34:46.992
    Jan 17 06:34:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 06:34:46.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:34:47.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:34:47.041
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3682 01/17/23 06:34:47.048
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3682 01/17/23 06:34:47.069
    Jan 17 06:34:47.108: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 17 06:34:57.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/17/23 06:34:57.129
    STEP: updating a scale subresource 01/17/23 06:34:57.136
    STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 06:34:57.149
    STEP: Patch a scale subresource 01/17/23 06:34:57.155
    STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 06:34:57.219
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 06:34:57.224: INFO: Deleting all statefulset in ns statefulset-3682
    Jan 17 06:34:57.241: INFO: Scaling statefulset ss to 0
    Jan 17 06:35:07.286: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 06:35:07.294: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 06:35:07.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3682" for this suite. 01/17/23 06:35:07.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:07.367
Jan 17 06:35:07.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 06:35:07.368
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:07.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:07.416
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 06:35:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1137" for this suite. 01/17/23 06:35:07.512
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":22,"skipped":358,"failed":0}
------------------------------
• [0.156 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:07.367
    Jan 17 06:35:07.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 06:35:07.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:07.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:07.416
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 06:35:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1137" for this suite. 01/17/23 06:35:07.512
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:07.527
Jan 17 06:35:07.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 06:35:07.529
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:07.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:07.576
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 17 06:35:07.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:35:10.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3949" for this suite. 01/17/23 06:35:10.861
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":23,"skipped":362,"failed":0}
------------------------------
• [3.351 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:07.527
    Jan 17 06:35:07.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 06:35:07.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:07.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:07.576
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 17 06:35:07.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:35:10.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3949" for this suite. 01/17/23 06:35:10.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:10.879
Jan 17 06:35:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 06:35:10.88
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:10.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:10.922
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/17/23 06:35:10.937
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_tcp@PTR;sleep 1; done
 01/17/23 06:35:10.978
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_tcp@PTR;sleep 1; done
 01/17/23 06:35:10.978
STEP: creating a pod to probe DNS 01/17/23 06:35:10.978
STEP: submitting the pod to kubernetes 01/17/23 06:35:10.979
Jan 17 06:35:11.011: INFO: Waiting up to 15m0s for pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31" in namespace "dns-5628" to be "running"
Jan 17 06:35:11.026: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31": Phase="Pending", Reason="", readiness=false. Elapsed: 14.567939ms
Jan 17 06:35:13.031: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31": Phase="Running", Reason="", readiness=true. Elapsed: 2.020070273s
Jan 17 06:35:13.031: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:35:13.032
STEP: looking for the results for each expected name from probers 01/17/23 06:35:13.035
Jan 17 06:35:13.047: INFO: Unable to read wheezy_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.063: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.098: INFO: Unable to read jessie_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.109: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.123: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:13.155: INFO: Lookups using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 failed for: [wheezy_udp@dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_udp@dns-test-service.dns-5628.svc.cluster.local jessie_tcp@dns-test-service.dns-5628.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local]

Jan 17 06:35:18.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.178: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.229: INFO: Unable to read jessie_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
Jan 17 06:35:18.287: INFO: Lookups using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 failed for: [wheezy_udp@dns-test-service.dns-5628.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_udp@dns-test-service.dns-5628.svc.cluster.local jessie_tcp@dns-test-service.dns-5628.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local]

Jan 17 06:35:23.250: INFO: DNS probes using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 succeeded

STEP: deleting the pod 01/17/23 06:35:23.25
STEP: deleting the test service 01/17/23 06:35:23.307
STEP: deleting the test headless service 01/17/23 06:35:23.372
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 06:35:23.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5628" for this suite. 01/17/23 06:35:23.415
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":24,"skipped":392,"failed":0}
------------------------------
• [SLOW TEST] [12.550 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:10.879
    Jan 17 06:35:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 06:35:10.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:10.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:10.922
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/17/23 06:35:10.937
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_tcp@PTR;sleep 1; done
     01/17/23 06:35:10.978
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5628.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5628.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5628.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_udp@PTR;check="$$(dig +tcp +noall +answer +search 157.224.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.224.157_tcp@PTR;sleep 1; done
     01/17/23 06:35:10.978
    STEP: creating a pod to probe DNS 01/17/23 06:35:10.978
    STEP: submitting the pod to kubernetes 01/17/23 06:35:10.979
    Jan 17 06:35:11.011: INFO: Waiting up to 15m0s for pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31" in namespace "dns-5628" to be "running"
    Jan 17 06:35:11.026: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31": Phase="Pending", Reason="", readiness=false. Elapsed: 14.567939ms
    Jan 17 06:35:13.031: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31": Phase="Running", Reason="", readiness=true. Elapsed: 2.020070273s
    Jan 17 06:35:13.031: INFO: Pod "dns-test-7e842d79-51ca-466f-bb14-f84079744e31" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:35:13.032
    STEP: looking for the results for each expected name from probers 01/17/23 06:35:13.035
    Jan 17 06:35:13.047: INFO: Unable to read wheezy_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.063: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.098: INFO: Unable to read jessie_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.109: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.123: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:13.155: INFO: Lookups using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 failed for: [wheezy_udp@dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@dns-test-service.dns-5628.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_udp@dns-test-service.dns-5628.svc.cluster.local jessie_tcp@dns-test-service.dns-5628.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local]

    Jan 17 06:35:18.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.178: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.229: INFO: Unable to read jessie_udp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local from pod dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31: the server could not find the requested resource (get pods dns-test-7e842d79-51ca-466f-bb14-f84079744e31)
    Jan 17 06:35:18.287: INFO: Lookups using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 failed for: [wheezy_udp@dns-test-service.dns-5628.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_udp@dns-test-service.dns-5628.svc.cluster.local jessie_tcp@dns-test-service.dns-5628.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5628.svc.cluster.local]

    Jan 17 06:35:23.250: INFO: DNS probes using dns-5628/dns-test-7e842d79-51ca-466f-bb14-f84079744e31 succeeded

    STEP: deleting the pod 01/17/23 06:35:23.25
    STEP: deleting the test service 01/17/23 06:35:23.307
    STEP: deleting the test headless service 01/17/23 06:35:23.372
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 06:35:23.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5628" for this suite. 01/17/23 06:35:23.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:23.43
Jan 17 06:35:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename ephemeral-containers-test 01/17/23 06:35:23.431
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:23.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:23.49
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/17/23 06:35:23.505
Jan 17 06:35:23.522: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2059" to be "running and ready"
Jan 17 06:35:23.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.560856ms
Jan 17 06:35:23.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:35:25.577: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054952834s
Jan 17 06:35:25.577: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:35:27.541: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018998743s
Jan 17 06:35:27.541: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:35:29.570: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047530709s
Jan 17 06:35:29.570: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:35:31.547: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.024556091s
Jan 17 06:35:31.547: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 17 06:35:31.547: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/17/23 06:35:31.564
Jan 17 06:35:31.614: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2059" to be "container debugger running"
Jan 17 06:35:31.621: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.188621ms
Jan 17 06:35:33.627: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012436671s
Jan 17 06:35:35.627: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012542793s
Jan 17 06:35:35.627: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/17/23 06:35:35.627
Jan 17 06:35:35.627: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2059 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 06:35:35.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:35:35.628: INFO: ExecWithOptions: Clientset creation
Jan 17 06:35:35.628: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-2059/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 17 06:35:35.776: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 06:35:35.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2059" for this suite. 01/17/23 06:35:35.801
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":25,"skipped":404,"failed":0}
------------------------------
• [SLOW TEST] [12.384 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:23.43
    Jan 17 06:35:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/17/23 06:35:23.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:23.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:23.49
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/17/23 06:35:23.505
    Jan 17 06:35:23.522: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2059" to be "running and ready"
    Jan 17 06:35:23.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.560856ms
    Jan 17 06:35:23.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:35:25.577: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054952834s
    Jan 17 06:35:25.577: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:35:27.541: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018998743s
    Jan 17 06:35:27.541: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:35:29.570: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047530709s
    Jan 17 06:35:29.570: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:35:31.547: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.024556091s
    Jan 17 06:35:31.547: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 17 06:35:31.547: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/17/23 06:35:31.564
    Jan 17 06:35:31.614: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2059" to be "container debugger running"
    Jan 17 06:35:31.621: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.188621ms
    Jan 17 06:35:33.627: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012436671s
    Jan 17 06:35:35.627: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012542793s
    Jan 17 06:35:35.627: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/17/23 06:35:35.627
    Jan 17 06:35:35.627: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2059 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 06:35:35.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:35:35.628: INFO: ExecWithOptions: Clientset creation
    Jan 17 06:35:35.628: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-2059/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 17 06:35:35.776: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 06:35:35.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2059" for this suite. 01/17/23 06:35:35.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:35.814
Jan 17 06:35:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:35:35.816
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:35.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:35.865
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:35:35.883
Jan 17 06:35:35.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73" in namespace "downward-api-8335" to be "Succeeded or Failed"
Jan 17 06:35:35.917: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351246ms
Jan 17 06:35:37.927: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019042407s
Jan 17 06:35:39.925: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017767831s
STEP: Saw pod success 01/17/23 06:35:39.925
Jan 17 06:35:39.926: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73" satisfied condition "Succeeded or Failed"
Jan 17 06:35:39.935: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 container client-container: <nil>
STEP: delete the pod 01/17/23 06:35:40.012
Jan 17 06:35:40.035: INFO: Waiting for pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 to disappear
Jan 17 06:35:40.040: INFO: Pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 06:35:40.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8335" for this suite. 01/17/23 06:35:40.053
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":26,"skipped":406,"failed":0}
------------------------------
• [4.265 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:35.814
    Jan 17 06:35:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:35:35.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:35.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:35.865
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:35:35.883
    Jan 17 06:35:35.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73" in namespace "downward-api-8335" to be "Succeeded or Failed"
    Jan 17 06:35:35.917: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351246ms
    Jan 17 06:35:37.927: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019042407s
    Jan 17 06:35:39.925: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017767831s
    STEP: Saw pod success 01/17/23 06:35:39.925
    Jan 17 06:35:39.926: INFO: Pod "downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73" satisfied condition "Succeeded or Failed"
    Jan 17 06:35:39.935: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 container client-container: <nil>
    STEP: delete the pod 01/17/23 06:35:40.012
    Jan 17 06:35:40.035: INFO: Waiting for pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 to disappear
    Jan 17 06:35:40.040: INFO: Pod downwardapi-volume-70e2b823-c5c7-43e6-9357-6881eaa2bf73 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 06:35:40.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8335" for this suite. 01/17/23 06:35:40.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:40.081
Jan 17 06:35:40.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 06:35:40.082
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:40.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:40.118
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 17 06:35:40.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:35:40.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1868" for this suite. 01/17/23 06:35:40.763
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":27,"skipped":415,"failed":0}
------------------------------
• [0.781 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:40.081
    Jan 17 06:35:40.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 06:35:40.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:40.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:40.118
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 17 06:35:40.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:35:40.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1868" for this suite. 01/17/23 06:35:40.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:40.864
Jan 17 06:35:40.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 06:35:40.866
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:40.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:40.929
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 17 06:35:41.021: INFO: Waiting up to 5m0s for pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3" in namespace "emptydir-wrapper-4864" to be "running and ready"
Jan 17 06:35:41.084: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3": Phase="Pending", Reason="", readiness=false. Elapsed: 63.29849ms
Jan 17 06:35:41.084: INFO: The phase of Pod pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:35:43.089: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.068467988s
Jan 17 06:35:43.089: INFO: The phase of Pod pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3 is Running (Ready = true)
Jan 17 06:35:43.089: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/17/23 06:35:43.094
STEP: Cleaning up the configmap 01/17/23 06:35:43.108
STEP: Cleaning up the pod 01/17/23 06:35:43.12
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 17 06:35:43.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4864" for this suite. 01/17/23 06:35:43.158
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":28,"skipped":424,"failed":0}
------------------------------
• [2.334 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:40.864
    Jan 17 06:35:40.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 06:35:40.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:40.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:40.929
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 17 06:35:41.021: INFO: Waiting up to 5m0s for pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3" in namespace "emptydir-wrapper-4864" to be "running and ready"
    Jan 17 06:35:41.084: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3": Phase="Pending", Reason="", readiness=false. Elapsed: 63.29849ms
    Jan 17 06:35:41.084: INFO: The phase of Pod pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:35:43.089: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.068467988s
    Jan 17 06:35:43.089: INFO: The phase of Pod pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3 is Running (Ready = true)
    Jan 17 06:35:43.089: INFO: Pod "pod-secrets-1b00b1b6-d7c0-427d-b719-6c067f6079f3" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/17/23 06:35:43.094
    STEP: Cleaning up the configmap 01/17/23 06:35:43.108
    STEP: Cleaning up the pod 01/17/23 06:35:43.12
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 17 06:35:43.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-4864" for this suite. 01/17/23 06:35:43.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:43.2
Jan 17 06:35:43.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:35:43.201
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:43.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:43.243
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-003b7cc1-1c10-4d19-be0f-dcc05f9e8bb4 01/17/23 06:35:43.252
STEP: Creating a pod to test consume configMaps 01/17/23 06:35:43.266
Jan 17 06:35:43.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0" in namespace "projected-5247" to be "Succeeded or Failed"
Jan 17 06:35:43.313: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.168766ms
Jan 17 06:35:45.319: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027073669s
Jan 17 06:35:47.320: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027851966s
STEP: Saw pod success 01/17/23 06:35:47.32
Jan 17 06:35:47.321: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0" satisfied condition "Succeeded or Failed"
Jan 17 06:35:47.326: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 06:35:47.338
Jan 17 06:35:47.367: INFO: Waiting for pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 to disappear
Jan 17 06:35:47.373: INFO: Pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 06:35:47.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5247" for this suite. 01/17/23 06:35:47.38
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":29,"skipped":472,"failed":0}
------------------------------
• [4.194 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:43.2
    Jan 17 06:35:43.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:35:43.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:43.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:43.243
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-003b7cc1-1c10-4d19-be0f-dcc05f9e8bb4 01/17/23 06:35:43.252
    STEP: Creating a pod to test consume configMaps 01/17/23 06:35:43.266
    Jan 17 06:35:43.292: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0" in namespace "projected-5247" to be "Succeeded or Failed"
    Jan 17 06:35:43.313: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.168766ms
    Jan 17 06:35:45.319: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027073669s
    Jan 17 06:35:47.320: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027851966s
    STEP: Saw pod success 01/17/23 06:35:47.32
    Jan 17 06:35:47.321: INFO: Pod "pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0" satisfied condition "Succeeded or Failed"
    Jan 17 06:35:47.326: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 06:35:47.338
    Jan 17 06:35:47.367: INFO: Waiting for pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 to disappear
    Jan 17 06:35:47.373: INFO: Pod pod-projected-configmaps-ab3d8620-ad72-4e15-9dec-0d67cf1626a0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 06:35:47.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5247" for this suite. 01/17/23 06:35:47.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:47.398
Jan 17 06:35:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 06:35:47.4
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:47.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:47.466
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/17/23 06:35:47.474
Jan 17 06:35:47.490: INFO: Waiting up to 5m0s for pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5" in namespace "emptydir-3662" to be "Succeeded or Failed"
Jan 17 06:35:47.497: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.157098ms
Jan 17 06:35:49.505: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014738742s
Jan 17 06:35:51.506: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016277291s
STEP: Saw pod success 01/17/23 06:35:51.506
Jan 17 06:35:51.506: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5" satisfied condition "Succeeded or Failed"
Jan 17 06:35:51.512: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 container test-container: <nil>
STEP: delete the pod 01/17/23 06:35:51.522
Jan 17 06:35:51.559: INFO: Waiting for pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 to disappear
Jan 17 06:35:51.571: INFO: Pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 06:35:51.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3662" for this suite. 01/17/23 06:35:51.579
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":30,"skipped":488,"failed":0}
------------------------------
• [4.208 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:47.398
    Jan 17 06:35:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 06:35:47.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:47.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:47.466
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/17/23 06:35:47.474
    Jan 17 06:35:47.490: INFO: Waiting up to 5m0s for pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5" in namespace "emptydir-3662" to be "Succeeded or Failed"
    Jan 17 06:35:47.497: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.157098ms
    Jan 17 06:35:49.505: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014738742s
    Jan 17 06:35:51.506: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016277291s
    STEP: Saw pod success 01/17/23 06:35:51.506
    Jan 17 06:35:51.506: INFO: Pod "pod-616dccf8-306c-4292-8626-2cbe5d982da5" satisfied condition "Succeeded or Failed"
    Jan 17 06:35:51.512: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 container test-container: <nil>
    STEP: delete the pod 01/17/23 06:35:51.522
    Jan 17 06:35:51.559: INFO: Waiting for pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 to disappear
    Jan 17 06:35:51.571: INFO: Pod pod-616dccf8-306c-4292-8626-2cbe5d982da5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 06:35:51.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3662" for this suite. 01/17/23 06:35:51.579
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:51.61
Jan 17 06:35:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:35:51.611
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:51.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:51.676
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:35:51.683
Jan 17 06:35:51.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3" in namespace "projected-4339" to be "Succeeded or Failed"
Jan 17 06:35:51.729: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.062821ms
Jan 17 06:35:53.736: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029087033s
Jan 17 06:35:55.736: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Running", Reason="", readiness=false. Elapsed: 4.02852627s
Jan 17 06:35:57.753: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045840735s
STEP: Saw pod success 01/17/23 06:35:57.753
Jan 17 06:35:57.753: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3" satisfied condition "Succeeded or Failed"
Jan 17 06:35:57.761: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 container client-container: <nil>
STEP: delete the pod 01/17/23 06:35:57.777
Jan 17 06:35:57.804: INFO: Waiting for pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 to disappear
Jan 17 06:35:57.809: INFO: Pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 06:35:57.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4339" for this suite. 01/17/23 06:35:57.818
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":31,"skipped":489,"failed":0}
------------------------------
• [SLOW TEST] [6.226 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:51.61
    Jan 17 06:35:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:35:51.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:51.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:51.676
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:35:51.683
    Jan 17 06:35:51.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3" in namespace "projected-4339" to be "Succeeded or Failed"
    Jan 17 06:35:51.729: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.062821ms
    Jan 17 06:35:53.736: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029087033s
    Jan 17 06:35:55.736: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Running", Reason="", readiness=false. Elapsed: 4.02852627s
    Jan 17 06:35:57.753: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045840735s
    STEP: Saw pod success 01/17/23 06:35:57.753
    Jan 17 06:35:57.753: INFO: Pod "downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3" satisfied condition "Succeeded or Failed"
    Jan 17 06:35:57.761: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 container client-container: <nil>
    STEP: delete the pod 01/17/23 06:35:57.777
    Jan 17 06:35:57.804: INFO: Waiting for pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 to disappear
    Jan 17 06:35:57.809: INFO: Pod downwardapi-volume-579a3ae1-f0e9-4b96-a6ea-5ad5f62ee2c3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 06:35:57.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4339" for this suite. 01/17/23 06:35:57.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:35:57.837
Jan 17 06:35:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:35:57.839
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:57.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:57.886
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/17/23 06:35:57.899
Jan 17 06:35:57.918: INFO: Waiting up to 5m0s for pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002" in namespace "downward-api-6506" to be "Succeeded or Failed"
Jan 17 06:35:57.931: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Pending", Reason="", readiness=false. Elapsed: 13.185749ms
Jan 17 06:35:59.937: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019797414s
Jan 17 06:36:01.938: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020409074s
STEP: Saw pod success 01/17/23 06:36:01.938
Jan 17 06:36:01.939: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002" satisfied condition "Succeeded or Failed"
Jan 17 06:36:01.944: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 container dapi-container: <nil>
STEP: delete the pod 01/17/23 06:36:01.981
Jan 17 06:36:02.021: INFO: Waiting for pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 to disappear
Jan 17 06:36:02.028: INFO: Pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 06:36:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6506" for this suite. 01/17/23 06:36:02.046
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":32,"skipped":508,"failed":0}
------------------------------
• [4.221 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:35:57.837
    Jan 17 06:35:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:35:57.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:35:57.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:35:57.886
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/17/23 06:35:57.899
    Jan 17 06:35:57.918: INFO: Waiting up to 5m0s for pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002" in namespace "downward-api-6506" to be "Succeeded or Failed"
    Jan 17 06:35:57.931: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Pending", Reason="", readiness=false. Elapsed: 13.185749ms
    Jan 17 06:35:59.937: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019797414s
    Jan 17 06:36:01.938: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020409074s
    STEP: Saw pod success 01/17/23 06:36:01.938
    Jan 17 06:36:01.939: INFO: Pod "downward-api-e0012298-1107-4c30-b562-3e829c6d0002" satisfied condition "Succeeded or Failed"
    Jan 17 06:36:01.944: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 06:36:01.981
    Jan 17 06:36:02.021: INFO: Waiting for pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 to disappear
    Jan 17 06:36:02.028: INFO: Pod downward-api-e0012298-1107-4c30-b562-3e829c6d0002 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 06:36:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6506" for this suite. 01/17/23 06:36:02.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:02.059
Jan 17 06:36:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename containers 01/17/23 06:36:02.06
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:02.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:02.097
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/17/23 06:36:02.121
Jan 17 06:36:02.144: INFO: Waiting up to 5m0s for pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f" in namespace "containers-9172" to be "Succeeded or Failed"
Jan 17 06:36:02.156: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.313423ms
Jan 17 06:36:04.163: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019529033s
Jan 17 06:36:06.163: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019450735s
STEP: Saw pod success 01/17/23 06:36:06.163
Jan 17 06:36:06.164: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f" satisfied condition "Succeeded or Failed"
Jan 17 06:36:06.168: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f container agnhost-container: <nil>
STEP: delete the pod 01/17/23 06:36:06.178
Jan 17 06:36:06.207: INFO: Waiting for pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f to disappear
Jan 17 06:36:06.213: INFO: Pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 06:36:06.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9172" for this suite. 01/17/23 06:36:06.22
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":33,"skipped":524,"failed":0}
------------------------------
• [4.178 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:02.059
    Jan 17 06:36:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename containers 01/17/23 06:36:02.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:02.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:02.097
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/17/23 06:36:02.121
    Jan 17 06:36:02.144: INFO: Waiting up to 5m0s for pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f" in namespace "containers-9172" to be "Succeeded or Failed"
    Jan 17 06:36:02.156: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.313423ms
    Jan 17 06:36:04.163: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019529033s
    Jan 17 06:36:06.163: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019450735s
    STEP: Saw pod success 01/17/23 06:36:06.163
    Jan 17 06:36:06.164: INFO: Pod "client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f" satisfied condition "Succeeded or Failed"
    Jan 17 06:36:06.168: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 06:36:06.178
    Jan 17 06:36:06.207: INFO: Waiting for pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f to disappear
    Jan 17 06:36:06.213: INFO: Pod client-containers-3b0f3b5b-0fad-4499-8279-112b3d43363f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 06:36:06.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9172" for this suite. 01/17/23 06:36:06.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:06.237
Jan 17 06:36:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 06:36:06.238
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:06.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:06.282
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 06:36:06.288
Jan 17 06:36:06.309: INFO: Waiting up to 5m0s for pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2" in namespace "emptydir-6077" to be "Succeeded or Failed"
Jan 17 06:36:06.316: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351675ms
Jan 17 06:36:08.322: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012510559s
Jan 17 06:36:10.323: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013784288s
STEP: Saw pod success 01/17/23 06:36:10.323
Jan 17 06:36:10.323: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2" satisfied condition "Succeeded or Failed"
Jan 17 06:36:10.333: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 container test-container: <nil>
STEP: delete the pod 01/17/23 06:36:10.348
Jan 17 06:36:10.378: INFO: Waiting for pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 to disappear
Jan 17 06:36:10.387: INFO: Pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 06:36:10.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6077" for this suite. 01/17/23 06:36:10.4
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":530,"failed":0}
------------------------------
• [4.188 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:06.237
    Jan 17 06:36:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 06:36:06.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:06.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:06.282
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 06:36:06.288
    Jan 17 06:36:06.309: INFO: Waiting up to 5m0s for pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2" in namespace "emptydir-6077" to be "Succeeded or Failed"
    Jan 17 06:36:06.316: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351675ms
    Jan 17 06:36:08.322: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012510559s
    Jan 17 06:36:10.323: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013784288s
    STEP: Saw pod success 01/17/23 06:36:10.323
    Jan 17 06:36:10.323: INFO: Pod "pod-824bb92a-5230-4471-a74c-a7b7b3597cc2" satisfied condition "Succeeded or Failed"
    Jan 17 06:36:10.333: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 container test-container: <nil>
    STEP: delete the pod 01/17/23 06:36:10.348
    Jan 17 06:36:10.378: INFO: Waiting for pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 to disappear
    Jan 17 06:36:10.387: INFO: Pod pod-824bb92a-5230-4471-a74c-a7b7b3597cc2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 06:36:10.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6077" for this suite. 01/17/23 06:36:10.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:10.428
Jan 17 06:36:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 06:36:10.43
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:10.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:10.467
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/17/23 06:36:10.482
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/17/23 06:36:10.5
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/17/23 06:36:10.501
STEP: creating a pod to probe DNS 01/17/23 06:36:10.501
STEP: submitting the pod to kubernetes 01/17/23 06:36:10.501
Jan 17 06:36:10.539: INFO: Waiting up to 15m0s for pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f" in namespace "dns-5115" to be "running"
Jan 17 06:36:10.561: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.882419ms
Jan 17 06:36:12.567: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027923208s
Jan 17 06:36:14.583: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.043439939s
Jan 17 06:36:14.583: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:36:14.583
STEP: looking for the results for each expected name from probers 01/17/23 06:36:14.597
Jan 17 06:36:14.639: INFO: DNS probes using dns-5115/dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f succeeded

STEP: deleting the pod 01/17/23 06:36:14.639
STEP: deleting the test headless service 01/17/23 06:36:14.696
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 06:36:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5115" for this suite. 01/17/23 06:36:14.751
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":35,"skipped":550,"failed":0}
------------------------------
• [4.340 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:10.428
    Jan 17 06:36:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 06:36:10.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:10.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:10.467
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/17/23 06:36:10.482
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/17/23 06:36:10.5
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5115.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/17/23 06:36:10.501
    STEP: creating a pod to probe DNS 01/17/23 06:36:10.501
    STEP: submitting the pod to kubernetes 01/17/23 06:36:10.501
    Jan 17 06:36:10.539: INFO: Waiting up to 15m0s for pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f" in namespace "dns-5115" to be "running"
    Jan 17 06:36:10.561: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.882419ms
    Jan 17 06:36:12.567: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027923208s
    Jan 17 06:36:14.583: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.043439939s
    Jan 17 06:36:14.583: INFO: Pod "dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:36:14.583
    STEP: looking for the results for each expected name from probers 01/17/23 06:36:14.597
    Jan 17 06:36:14.639: INFO: DNS probes using dns-5115/dns-test-d30f3350-0cb5-4082-abcb-51bfc597fa4f succeeded

    STEP: deleting the pod 01/17/23 06:36:14.639
    STEP: deleting the test headless service 01/17/23 06:36:14.696
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 06:36:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5115" for this suite. 01/17/23 06:36:14.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:14.771
Jan 17 06:36:14.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pod-network-test 01/17/23 06:36:14.772
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:14.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:14.819
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7508 01/17/23 06:36:14.836
STEP: creating a selector 01/17/23 06:36:14.836
STEP: Creating the service pods in kubernetes 01/17/23 06:36:14.837
Jan 17 06:36:14.837: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 06:36:14.931: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7508" to be "running and ready"
Jan 17 06:36:14.951: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.501595ms
Jan 17 06:36:14.951: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:16.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053856582s
Jan 17 06:36:16.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:18.972: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040224746s
Jan 17 06:36:18.972: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:20.966: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034623521s
Jan 17 06:36:20.966: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:22.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036953401s
Jan 17 06:36:22.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:24.957: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026133192s
Jan 17 06:36:24.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:26.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036302221s
Jan 17 06:36:26.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:28.959: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.027729185s
Jan 17 06:36:28.959: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:36:30.959: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027242232s
Jan 17 06:36:30.959: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:32.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026284197s
Jan 17 06:36:32.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:34.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.026722594s
Jan 17 06:36:34.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:36.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.026465415s
Jan 17 06:36:36.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:38.956: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.024577521s
Jan 17 06:36:38.956: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:40.959: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.02747702s
Jan 17 06:36:40.959: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:42.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.035365295s
Jan 17 06:36:42.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:44.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.0270178s
Jan 17 06:36:44.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 06:36:46.971: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.039166921s
Jan 17 06:36:46.971: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 06:36:46.971: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 06:36:46.978: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7508" to be "running and ready"
Jan 17 06:36:46.989: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.729129ms
Jan 17 06:36:46.989: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 06:36:46.989: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 06:36:46.997: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7508" to be "running and ready"
Jan 17 06:36:47.006: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.92981ms
Jan 17 06:36:47.006: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 06:36:47.006: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 06:36:47.024
Jan 17 06:36:47.070: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7508" to be "running"
Jan 17 06:36:47.079: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.110565ms
Jan 17 06:36:49.091: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020474831s
Jan 17 06:36:51.096: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025585928s
Jan 17 06:36:51.096: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 06:36:51.120: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7508" to be "running"
Jan 17 06:36:51.129: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.668434ms
Jan 17 06:36:51.129: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 17 06:36:51.133: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 06:36:51.133: INFO: Going to poll 10.100.206.7 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 06:36:51.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.206.7:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 06:36:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:36:51.139: INFO: ExecWithOptions: Clientset creation
Jan 17 06:36:51.139: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.206.7%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 06:36:51.284: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 06:36:51.284: INFO: Going to poll 10.100.135.24 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 06:36:51.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.135.24:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 06:36:51.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:36:51.295: INFO: ExecWithOptions: Clientset creation
Jan 17 06:36:51.295: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.135.24%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 06:36:51.448: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 06:36:51.448: INFO: Going to poll 10.100.168.78 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 06:36:51.456: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.168.78:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 06:36:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:36:51.457: INFO: ExecWithOptions: Clientset creation
Jan 17 06:36:51.457: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.168.78%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 06:36:51.603: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 06:36:51.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7508" for this suite. 01/17/23 06:36:51.612
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":36,"skipped":573,"failed":0}
------------------------------
• [SLOW TEST] [36.856 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:14.771
    Jan 17 06:36:14.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 06:36:14.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:14.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:14.819
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7508 01/17/23 06:36:14.836
    STEP: creating a selector 01/17/23 06:36:14.836
    STEP: Creating the service pods in kubernetes 01/17/23 06:36:14.837
    Jan 17 06:36:14.837: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 06:36:14.931: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7508" to be "running and ready"
    Jan 17 06:36:14.951: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.501595ms
    Jan 17 06:36:14.951: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:16.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053856582s
    Jan 17 06:36:16.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:18.972: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040224746s
    Jan 17 06:36:18.972: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:20.966: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034623521s
    Jan 17 06:36:20.966: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:22.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036953401s
    Jan 17 06:36:22.969: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:24.957: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026133192s
    Jan 17 06:36:24.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:26.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036302221s
    Jan 17 06:36:26.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:28.959: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.027729185s
    Jan 17 06:36:28.959: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:36:30.959: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027242232s
    Jan 17 06:36:30.959: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:32.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026284197s
    Jan 17 06:36:32.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:34.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.026722594s
    Jan 17 06:36:34.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:36.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.026465415s
    Jan 17 06:36:36.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:38.956: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.024577521s
    Jan 17 06:36:38.956: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:40.959: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.02747702s
    Jan 17 06:36:40.959: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:42.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.035365295s
    Jan 17 06:36:42.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:44.958: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.0270178s
    Jan 17 06:36:44.958: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 06:36:46.971: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.039166921s
    Jan 17 06:36:46.971: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 06:36:46.971: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 06:36:46.978: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7508" to be "running and ready"
    Jan 17 06:36:46.989: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.729129ms
    Jan 17 06:36:46.989: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 06:36:46.989: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 06:36:46.997: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7508" to be "running and ready"
    Jan 17 06:36:47.006: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.92981ms
    Jan 17 06:36:47.006: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 06:36:47.006: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 06:36:47.024
    Jan 17 06:36:47.070: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7508" to be "running"
    Jan 17 06:36:47.079: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.110565ms
    Jan 17 06:36:49.091: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020474831s
    Jan 17 06:36:51.096: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025585928s
    Jan 17 06:36:51.096: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 06:36:51.120: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7508" to be "running"
    Jan 17 06:36:51.129: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.668434ms
    Jan 17 06:36:51.129: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 17 06:36:51.133: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 06:36:51.133: INFO: Going to poll 10.100.206.7 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 06:36:51.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.206.7:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 06:36:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:36:51.139: INFO: ExecWithOptions: Clientset creation
    Jan 17 06:36:51.139: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.206.7%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 06:36:51.284: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 17 06:36:51.284: INFO: Going to poll 10.100.135.24 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 06:36:51.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.135.24:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 06:36:51.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:36:51.295: INFO: ExecWithOptions: Clientset creation
    Jan 17 06:36:51.295: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.135.24%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 06:36:51.448: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 17 06:36:51.448: INFO: Going to poll 10.100.168.78 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 06:36:51.456: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.168.78:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 06:36:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:36:51.457: INFO: ExecWithOptions: Clientset creation
    Jan 17 06:36:51.457: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7508/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.168.78%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 06:36:51.603: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 06:36:51.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7508" for this suite. 01/17/23 06:36:51.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:51.632
Jan 17 06:36:51.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:36:51.635
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:51.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:51.678
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:36:51.687
Jan 17 06:36:51.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e" in namespace "projected-5159" to be "Succeeded or Failed"
Jan 17 06:36:51.735: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.656862ms
Jan 17 06:36:53.741: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020938279s
Jan 17 06:36:55.742: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022716263s
STEP: Saw pod success 01/17/23 06:36:55.742
Jan 17 06:36:55.743: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e" satisfied condition "Succeeded or Failed"
Jan 17 06:36:55.748: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e container client-container: <nil>
STEP: delete the pod 01/17/23 06:36:55.765
Jan 17 06:36:55.800: INFO: Waiting for pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e to disappear
Jan 17 06:36:55.806: INFO: Pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 06:36:55.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5159" for this suite. 01/17/23 06:36:55.815
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":37,"skipped":632,"failed":0}
------------------------------
• [4.194 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:51.632
    Jan 17 06:36:51.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:36:51.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:51.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:51.678
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:36:51.687
    Jan 17 06:36:51.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e" in namespace "projected-5159" to be "Succeeded or Failed"
    Jan 17 06:36:51.735: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.656862ms
    Jan 17 06:36:53.741: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020938279s
    Jan 17 06:36:55.742: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022716263s
    STEP: Saw pod success 01/17/23 06:36:55.742
    Jan 17 06:36:55.743: INFO: Pod "downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e" satisfied condition "Succeeded or Failed"
    Jan 17 06:36:55.748: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e container client-container: <nil>
    STEP: delete the pod 01/17/23 06:36:55.765
    Jan 17 06:36:55.800: INFO: Waiting for pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e to disappear
    Jan 17 06:36:55.806: INFO: Pod downwardapi-volume-40536453-7d07-460f-8b21-fcd4a2f3b76e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 06:36:55.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5159" for this suite. 01/17/23 06:36:55.815
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:36:55.826
Jan 17 06:36:55.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 06:36:55.828
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:55.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:55.872
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/17/23 06:36:55.879
STEP: Counting existing ResourceQuota 01/17/23 06:37:00.885
STEP: Creating a ResourceQuota 01/17/23 06:37:05.891
STEP: Ensuring resource quota status is calculated 01/17/23 06:37:05.902
STEP: Creating a Secret 01/17/23 06:37:07.911
STEP: Ensuring resource quota status captures secret creation 01/17/23 06:37:07.937
STEP: Deleting a secret 01/17/23 06:37:09.958
STEP: Ensuring resource quota status released usage 01/17/23 06:37:09.97
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 06:37:11.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-828" for this suite. 01/17/23 06:37:11.987
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":38,"skipped":636,"failed":0}
------------------------------
• [SLOW TEST] [16.177 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:36:55.826
    Jan 17 06:36:55.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 06:36:55.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:36:55.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:36:55.872
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/17/23 06:36:55.879
    STEP: Counting existing ResourceQuota 01/17/23 06:37:00.885
    STEP: Creating a ResourceQuota 01/17/23 06:37:05.891
    STEP: Ensuring resource quota status is calculated 01/17/23 06:37:05.902
    STEP: Creating a Secret 01/17/23 06:37:07.911
    STEP: Ensuring resource quota status captures secret creation 01/17/23 06:37:07.937
    STEP: Deleting a secret 01/17/23 06:37:09.958
    STEP: Ensuring resource quota status released usage 01/17/23 06:37:09.97
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 06:37:11.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-828" for this suite. 01/17/23 06:37:11.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:37:12.015
Jan 17 06:37:12.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:37:12.016
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:12.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:12.058
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:37:12.068
Jan 17 06:37:12.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72" in namespace "projected-2495" to be "Succeeded or Failed"
Jan 17 06:37:12.132: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Pending", Reason="", readiness=false. Elapsed: 15.940128ms
Jan 17 06:37:14.140: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023064031s
Jan 17 06:37:16.145: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028638369s
STEP: Saw pod success 01/17/23 06:37:16.145
Jan 17 06:37:16.145: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72" satisfied condition "Succeeded or Failed"
Jan 17 06:37:16.151: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 container client-container: <nil>
STEP: delete the pod 01/17/23 06:37:16.167
Jan 17 06:37:16.193: INFO: Waiting for pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 to disappear
Jan 17 06:37:16.199: INFO: Pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 06:37:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2495" for this suite. 01/17/23 06:37:16.206
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":39,"skipped":642,"failed":0}
------------------------------
• [4.206 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:37:12.015
    Jan 17 06:37:12.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:37:12.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:12.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:12.058
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:37:12.068
    Jan 17 06:37:12.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72" in namespace "projected-2495" to be "Succeeded or Failed"
    Jan 17 06:37:12.132: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Pending", Reason="", readiness=false. Elapsed: 15.940128ms
    Jan 17 06:37:14.140: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023064031s
    Jan 17 06:37:16.145: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028638369s
    STEP: Saw pod success 01/17/23 06:37:16.145
    Jan 17 06:37:16.145: INFO: Pod "downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72" satisfied condition "Succeeded or Failed"
    Jan 17 06:37:16.151: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 container client-container: <nil>
    STEP: delete the pod 01/17/23 06:37:16.167
    Jan 17 06:37:16.193: INFO: Waiting for pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 to disappear
    Jan 17 06:37:16.199: INFO: Pod downwardapi-volume-29e9ce81-6bd9-4009-9185-b4f35cfbfb72 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 06:37:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2495" for this suite. 01/17/23 06:37:16.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:37:16.221
Jan 17 06:37:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:37:16.222
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:16.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:16.262
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:37:16.271
Jan 17 06:37:16.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b" in namespace "projected-6353" to be "Succeeded or Failed"
Jan 17 06:37:16.306: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.355303ms
Jan 17 06:37:18.332: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042493308s
Jan 17 06:37:20.321: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03177149s
Jan 17 06:37:22.312: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0223448s
STEP: Saw pod success 01/17/23 06:37:22.312
Jan 17 06:37:22.312: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b" satisfied condition "Succeeded or Failed"
Jan 17 06:37:22.317: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b container client-container: <nil>
STEP: delete the pod 01/17/23 06:37:22.33
Jan 17 06:37:22.360: INFO: Waiting for pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b to disappear
Jan 17 06:37:22.368: INFO: Pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 06:37:22.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6353" for this suite. 01/17/23 06:37:22.377
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":40,"skipped":648,"failed":0}
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:37:16.221
    Jan 17 06:37:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:37:16.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:16.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:16.262
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:37:16.271
    Jan 17 06:37:16.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b" in namespace "projected-6353" to be "Succeeded or Failed"
    Jan 17 06:37:16.306: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.355303ms
    Jan 17 06:37:18.332: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042493308s
    Jan 17 06:37:20.321: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03177149s
    Jan 17 06:37:22.312: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0223448s
    STEP: Saw pod success 01/17/23 06:37:22.312
    Jan 17 06:37:22.312: INFO: Pod "downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b" satisfied condition "Succeeded or Failed"
    Jan 17 06:37:22.317: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b container client-container: <nil>
    STEP: delete the pod 01/17/23 06:37:22.33
    Jan 17 06:37:22.360: INFO: Waiting for pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b to disappear
    Jan 17 06:37:22.368: INFO: Pod downwardapi-volume-506fc1f9-6d5e-41e9-976e-0c5358743d6b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 06:37:22.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6353" for this suite. 01/17/23 06:37:22.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:37:22.406
Jan 17 06:37:22.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:37:22.407
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:22.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:22.444
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 17 06:37:22.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 06:37:29.808
Jan 17 06:37:29.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 create -f -'
Jan 17 06:37:31.393: INFO: stderr: ""
Jan 17 06:37:31.393: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 06:37:31.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 delete e2e-test-crd-publish-openapi-3258-crds test-cr'
Jan 17 06:37:31.535: INFO: stderr: ""
Jan 17 06:37:31.535: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 17 06:37:31.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 apply -f -'
Jan 17 06:37:32.915: INFO: stderr: ""
Jan 17 06:37:32.916: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 06:37:32.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 delete e2e-test-crd-publish-openapi-3258-crds test-cr'
Jan 17 06:37:33.074: INFO: stderr: ""
Jan 17 06:37:33.075: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/17/23 06:37:33.075
Jan 17 06:37:33.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 explain e2e-test-crd-publish-openapi-3258-crds'
Jan 17 06:37:34.057: INFO: stderr: ""
Jan 17 06:37:34.057: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3258-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:37:38.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3291" for this suite. 01/17/23 06:37:38.55
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":41,"skipped":712,"failed":0}
------------------------------
• [SLOW TEST] [16.157 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:37:22.406
    Jan 17 06:37:22.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:37:22.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:22.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:22.444
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 17 06:37:22.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 06:37:29.808
    Jan 17 06:37:29.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 create -f -'
    Jan 17 06:37:31.393: INFO: stderr: ""
    Jan 17 06:37:31.393: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 17 06:37:31.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 delete e2e-test-crd-publish-openapi-3258-crds test-cr'
    Jan 17 06:37:31.535: INFO: stderr: ""
    Jan 17 06:37:31.535: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 17 06:37:31.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 apply -f -'
    Jan 17 06:37:32.915: INFO: stderr: ""
    Jan 17 06:37:32.916: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 17 06:37:32.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 --namespace=crd-publish-openapi-3291 delete e2e-test-crd-publish-openapi-3258-crds test-cr'
    Jan 17 06:37:33.074: INFO: stderr: ""
    Jan 17 06:37:33.075: INFO: stdout: "e2e-test-crd-publish-openapi-3258-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/17/23 06:37:33.075
    Jan 17 06:37:33.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-3291 explain e2e-test-crd-publish-openapi-3258-crds'
    Jan 17 06:37:34.057: INFO: stderr: ""
    Jan 17 06:37:34.057: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3258-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:37:38.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3291" for this suite. 01/17/23 06:37:38.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:37:38.564
Jan 17 06:37:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context-test 01/17/23 06:37:38.567
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:38.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:38.599
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 17 06:37:38.623: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f" in namespace "security-context-test-1716" to be "Succeeded or Failed"
Jan 17 06:37:38.634: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.505073ms
Jan 17 06:37:40.640: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01767165s
Jan 17 06:37:42.642: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018862415s
Jan 17 06:37:42.642: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 06:37:42.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1716" for this suite. 01/17/23 06:37:42.649
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":42,"skipped":734,"failed":0}
------------------------------
• [4.096 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:37:38.564
    Jan 17 06:37:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context-test 01/17/23 06:37:38.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:38.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:38.599
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 17 06:37:38.623: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f" in namespace "security-context-test-1716" to be "Succeeded or Failed"
    Jan 17 06:37:38.634: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.505073ms
    Jan 17 06:37:40.640: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01767165s
    Jan 17 06:37:42.642: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018862415s
    Jan 17 06:37:42.642: INFO: Pod "busybox-readonly-false-2c8a4c89-c2f2-4264-b8ee-552da8bca99f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 06:37:42.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1716" for this suite. 01/17/23 06:37:42.649
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:37:42.666
Jan 17 06:37:42.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 06:37:42.668
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:42.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:42.736
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 in namespace container-probe-7538 01/17/23 06:37:42.742
Jan 17 06:37:42.766: INFO: Waiting up to 5m0s for pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6" in namespace "container-probe-7538" to be "not pending"
Jan 17 06:37:42.781: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.600557ms
Jan 17 06:37:44.792: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6": Phase="Running", Reason="", readiness=true. Elapsed: 2.02557198s
Jan 17 06:37:44.795: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6" satisfied condition "not pending"
Jan 17 06:37:44.795: INFO: Started pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 in namespace container-probe-7538
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:37:44.795
Jan 17 06:37:44.818: INFO: Initial restart count of pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 is 0
STEP: deleting the pod 01/17/23 06:41:45.653
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 06:41:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7538" for this suite. 01/17/23 06:41:45.713
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":43,"skipped":737,"failed":0}
------------------------------
• [SLOW TEST] [243.061 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:37:42.666
    Jan 17 06:37:42.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 06:37:42.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:37:42.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:37:42.736
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 in namespace container-probe-7538 01/17/23 06:37:42.742
    Jan 17 06:37:42.766: INFO: Waiting up to 5m0s for pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6" in namespace "container-probe-7538" to be "not pending"
    Jan 17 06:37:42.781: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.600557ms
    Jan 17 06:37:44.792: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6": Phase="Running", Reason="", readiness=true. Elapsed: 2.02557198s
    Jan 17 06:37:44.795: INFO: Pod "busybox-a8e606a8-852a-4d41-b105-73427f091ed6" satisfied condition "not pending"
    Jan 17 06:37:44.795: INFO: Started pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 in namespace container-probe-7538
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:37:44.795
    Jan 17 06:37:44.818: INFO: Initial restart count of pod busybox-a8e606a8-852a-4d41-b105-73427f091ed6 is 0
    STEP: deleting the pod 01/17/23 06:41:45.653
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 06:41:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7538" for this suite. 01/17/23 06:41:45.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:41:45.733
Jan 17 06:41:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 06:41:45.734
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:45.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:45.78
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 17 06:41:45.823: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/17/23 06:41:45.84
Jan 17 06:41:45.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:45.858: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/17/23 06:41:45.858
Jan 17 06:41:45.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:45.900: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
Jan 17 06:41:46.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:46.906: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
Jan 17 06:41:47.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 06:41:47.906: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/17/23 06:41:47.911
Jan 17 06:41:47.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 06:41:47.938: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 17 06:41:48.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:48.943: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/17/23 06:41:48.944
Jan 17 06:41:48.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:48.975: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
Jan 17 06:41:49.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:49.981: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
Jan 17 06:41:50.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:50.982: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
Jan 17 06:41:51.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 06:41:51.980: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 06:41:51.989
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7417, will wait for the garbage collector to delete the pods 01/17/23 06:41:51.99
Jan 17 06:41:52.055: INFO: Deleting DaemonSet.extensions daemon-set took: 11.713532ms
Jan 17 06:41:52.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.285997ms
Jan 17 06:41:54.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:41:54.561: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 06:41:54.565: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7552"},"items":null}

Jan 17 06:41:54.576: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7552"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:41:54.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7417" for this suite. 01/17/23 06:41:54.621
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":44,"skipped":764,"failed":0}
------------------------------
• [SLOW TEST] [8.901 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:41:45.733
    Jan 17 06:41:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 06:41:45.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:45.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:45.78
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 17 06:41:45.823: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/17/23 06:41:45.84
    Jan 17 06:41:45.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:45.858: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/17/23 06:41:45.858
    Jan 17 06:41:45.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:45.900: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
    Jan 17 06:41:46.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:46.906: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
    Jan 17 06:41:47.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 06:41:47.906: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/17/23 06:41:47.911
    Jan 17 06:41:47.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 06:41:47.938: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 17 06:41:48.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:48.943: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/17/23 06:41:48.944
    Jan 17 06:41:48.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:48.975: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
    Jan 17 06:41:49.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:49.981: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
    Jan 17 06:41:50.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:50.982: INFO: Node cluster125-w73dz53kvqes-node-2 is running 0 daemon pod, expected 1
    Jan 17 06:41:51.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 06:41:51.980: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 06:41:51.989
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7417, will wait for the garbage collector to delete the pods 01/17/23 06:41:51.99
    Jan 17 06:41:52.055: INFO: Deleting DaemonSet.extensions daemon-set took: 11.713532ms
    Jan 17 06:41:52.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.285997ms
    Jan 17 06:41:54.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:41:54.561: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 06:41:54.565: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7552"},"items":null}

    Jan 17 06:41:54.576: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7552"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:41:54.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7417" for this suite. 01/17/23 06:41:54.621
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:41:54.635
Jan 17 06:41:54.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 06:41:54.638
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:54.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:54.677
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-a29c6016-19fc-46d7-b435-7cdaf96ab140 01/17/23 06:41:54.684
STEP: Creating a pod to test consume secrets 01/17/23 06:41:54.695
Jan 17 06:41:54.715: INFO: Waiting up to 5m0s for pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219" in namespace "secrets-7642" to be "Succeeded or Failed"
Jan 17 06:41:54.724: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Pending", Reason="", readiness=false. Elapsed: 8.770976ms
Jan 17 06:41:56.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014227288s
Jan 17 06:41:58.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014106097s
STEP: Saw pod success 01/17/23 06:41:58.729
Jan 17 06:41:58.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219" satisfied condition "Succeeded or Failed"
Jan 17 06:41:58.733: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 06:41:58.796
Jan 17 06:41:58.816: INFO: Waiting for pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 to disappear
Jan 17 06:41:58.821: INFO: Pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 06:41:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7642" for this suite. 01/17/23 06:41:58.826
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":767,"failed":0}
------------------------------
• [4.202 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:41:54.635
    Jan 17 06:41:54.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 06:41:54.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:54.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:54.677
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-a29c6016-19fc-46d7-b435-7cdaf96ab140 01/17/23 06:41:54.684
    STEP: Creating a pod to test consume secrets 01/17/23 06:41:54.695
    Jan 17 06:41:54.715: INFO: Waiting up to 5m0s for pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219" in namespace "secrets-7642" to be "Succeeded or Failed"
    Jan 17 06:41:54.724: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Pending", Reason="", readiness=false. Elapsed: 8.770976ms
    Jan 17 06:41:56.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014227288s
    Jan 17 06:41:58.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014106097s
    STEP: Saw pod success 01/17/23 06:41:58.729
    Jan 17 06:41:58.729: INFO: Pod "pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219" satisfied condition "Succeeded or Failed"
    Jan 17 06:41:58.733: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 06:41:58.796
    Jan 17 06:41:58.816: INFO: Waiting for pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 to disappear
    Jan 17 06:41:58.821: INFO: Pod pod-secrets-4f0fc8d1-87f4-4d56-9428-69db97916219 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 06:41:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7642" for this suite. 01/17/23 06:41:58.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:41:58.838
Jan 17 06:41:58.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:41:58.84
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:58.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:58.876
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2974 01/17/23 06:41:58.886
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[] 01/17/23 06:41:58.911
Jan 17 06:41:58.944: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2974 01/17/23 06:41:58.944
Jan 17 06:41:58.959: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2974" to be "running and ready"
Jan 17 06:41:58.974: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.996757ms
Jan 17 06:41:58.974: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:42:00.980: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020214666s
Jan 17 06:42:00.980: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 06:42:00.980: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod1:[80]] 01/17/23 06:42:00.983
Jan 17 06:42:00.997: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/17/23 06:42:00.997
Jan 17 06:42:00.997: INFO: Creating new exec pod
Jan 17 06:42:01.009: INFO: Waiting up to 5m0s for pod "execpodqkxnv" in namespace "services-2974" to be "running"
Jan 17 06:42:01.020: INFO: Pod "execpodqkxnv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768022ms
Jan 17 06:42:03.026: INFO: Pod "execpodqkxnv": Phase="Running", Reason="", readiness=true. Elapsed: 2.017024047s
Jan 17 06:42:03.026: INFO: Pod "execpodqkxnv" satisfied condition "running"
Jan 17 06:42:04.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 06:42:04.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:04.416: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:42:04.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
Jan 17 06:42:04.729: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:04.730: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2974 01/17/23 06:42:04.73
Jan 17 06:42:04.741: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2974" to be "running and ready"
Jan 17 06:42:04.747: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617986ms
Jan 17 06:42:04.747: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:42:06.751: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010208055s
Jan 17 06:42:06.752: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 06:42:06.752: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod1:[80] pod2:[80]] 01/17/23 06:42:06.755
Jan 17 06:42:06.771: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/17/23 06:42:06.771
Jan 17 06:42:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 06:42:08.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:08.063: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:42:08.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
Jan 17 06:42:08.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:08.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2974 01/17/23 06:42:08.345
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod2:[80]] 01/17/23 06:42:08.372
Jan 17 06:42:08.406: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/17/23 06:42:08.406
Jan 17 06:42:09.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 06:42:09.822: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:09.822: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:42:09.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
Jan 17 06:42:10.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
Jan 17 06:42:10.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2974 01/17/23 06:42:10.139
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[] 01/17/23 06:42:10.19
Jan 17 06:42:10.208: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:42:10.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2974" for this suite. 01/17/23 06:42:10.267
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":46,"skipped":778,"failed":0}
------------------------------
• [SLOW TEST] [11.446 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:41:58.838
    Jan 17 06:41:58.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:41:58.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:41:58.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:41:58.876
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2974 01/17/23 06:41:58.886
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[] 01/17/23 06:41:58.911
    Jan 17 06:41:58.944: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2974 01/17/23 06:41:58.944
    Jan 17 06:41:58.959: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2974" to be "running and ready"
    Jan 17 06:41:58.974: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.996757ms
    Jan 17 06:41:58.974: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:42:00.980: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020214666s
    Jan 17 06:42:00.980: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 06:42:00.980: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod1:[80]] 01/17/23 06:42:00.983
    Jan 17 06:42:00.997: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/17/23 06:42:00.997
    Jan 17 06:42:00.997: INFO: Creating new exec pod
    Jan 17 06:42:01.009: INFO: Waiting up to 5m0s for pod "execpodqkxnv" in namespace "services-2974" to be "running"
    Jan 17 06:42:01.020: INFO: Pod "execpodqkxnv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768022ms
    Jan 17 06:42:03.026: INFO: Pod "execpodqkxnv": Phase="Running", Reason="", readiness=true. Elapsed: 2.017024047s
    Jan 17 06:42:03.026: INFO: Pod "execpodqkxnv" satisfied condition "running"
    Jan 17 06:42:04.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 06:42:04.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:04.416: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:42:04.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
    Jan 17 06:42:04.729: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:04.730: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2974 01/17/23 06:42:04.73
    Jan 17 06:42:04.741: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2974" to be "running and ready"
    Jan 17 06:42:04.747: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617986ms
    Jan 17 06:42:04.747: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:42:06.751: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010208055s
    Jan 17 06:42:06.752: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 06:42:06.752: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod1:[80] pod2:[80]] 01/17/23 06:42:06.755
    Jan 17 06:42:06.771: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/17/23 06:42:06.771
    Jan 17 06:42:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 06:42:08.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:08.063: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:42:08.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
    Jan 17 06:42:08.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:08.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2974 01/17/23 06:42:08.345
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[pod2:[80]] 01/17/23 06:42:08.372
    Jan 17 06:42:08.406: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/17/23 06:42:08.406
    Jan 17 06:42:09.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 06:42:09.822: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:09.822: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:42:09.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-2974 exec execpodqkxnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.187.73 80'
    Jan 17 06:42:10.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.187.73 80\nConnection to 10.254.187.73 80 port [tcp/http] succeeded!\n"
    Jan 17 06:42:10.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2974 01/17/23 06:42:10.139
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2974 to expose endpoints map[] 01/17/23 06:42:10.19
    Jan 17 06:42:10.208: INFO: successfully validated that service endpoint-test2 in namespace services-2974 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:42:10.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2974" for this suite. 01/17/23 06:42:10.267
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:10.285
Jan 17 06:42:10.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 06:42:10.287
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:10.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:10.321
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/17/23 06:42:10.327
Jan 17 06:42:10.344: INFO: Waiting up to 5m0s for pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5" in namespace "var-expansion-4618" to be "Succeeded or Failed"
Jan 17 06:42:10.357: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.893965ms
Jan 17 06:42:12.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020927937s
Jan 17 06:42:14.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020830745s
STEP: Saw pod success 01/17/23 06:42:14.365
Jan 17 06:42:14.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5" satisfied condition "Succeeded or Failed"
Jan 17 06:42:14.369: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 container dapi-container: <nil>
STEP: delete the pod 01/17/23 06:42:14.427
Jan 17 06:42:14.456: INFO: Waiting for pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 to disappear
Jan 17 06:42:14.461: INFO: Pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 06:42:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4618" for this suite. 01/17/23 06:42:14.468
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":47,"skipped":796,"failed":0}
------------------------------
• [4.201 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:10.285
    Jan 17 06:42:10.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 06:42:10.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:10.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:10.321
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/17/23 06:42:10.327
    Jan 17 06:42:10.344: INFO: Waiting up to 5m0s for pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5" in namespace "var-expansion-4618" to be "Succeeded or Failed"
    Jan 17 06:42:10.357: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.893965ms
    Jan 17 06:42:12.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020927937s
    Jan 17 06:42:14.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020830745s
    STEP: Saw pod success 01/17/23 06:42:14.365
    Jan 17 06:42:14.365: INFO: Pod "var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:14.369: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 06:42:14.427
    Jan 17 06:42:14.456: INFO: Waiting for pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 to disappear
    Jan 17 06:42:14.461: INFO: Pod var-expansion-0bdedeb1-d09e-4937-9ed7-60aa7667cea5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 06:42:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4618" for this suite. 01/17/23 06:42:14.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:14.486
Jan 17 06:42:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:42:14.487
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:14.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:14.52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:42:14.526
Jan 17 06:42:14.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377" in namespace "downward-api-7507" to be "Succeeded or Failed"
Jan 17 06:42:14.554: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Pending", Reason="", readiness=false. Elapsed: 12.691388ms
Jan 17 06:42:16.560: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018584615s
Jan 17 06:42:18.560: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018912643s
STEP: Saw pod success 01/17/23 06:42:18.56
Jan 17 06:42:18.561: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377" satisfied condition "Succeeded or Failed"
Jan 17 06:42:18.565: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 container client-container: <nil>
STEP: delete the pod 01/17/23 06:42:18.575
Jan 17 06:42:18.602: INFO: Waiting for pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 to disappear
Jan 17 06:42:18.608: INFO: Pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 06:42:18.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7507" for this suite. 01/17/23 06:42:18.615
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":48,"skipped":802,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:14.486
    Jan 17 06:42:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:42:14.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:14.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:14.52
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:42:14.526
    Jan 17 06:42:14.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377" in namespace "downward-api-7507" to be "Succeeded or Failed"
    Jan 17 06:42:14.554: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Pending", Reason="", readiness=false. Elapsed: 12.691388ms
    Jan 17 06:42:16.560: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018584615s
    Jan 17 06:42:18.560: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018912643s
    STEP: Saw pod success 01/17/23 06:42:18.56
    Jan 17 06:42:18.561: INFO: Pod "downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:18.565: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 container client-container: <nil>
    STEP: delete the pod 01/17/23 06:42:18.575
    Jan 17 06:42:18.602: INFO: Waiting for pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 to disappear
    Jan 17 06:42:18.608: INFO: Pod downwardapi-volume-60834119-14bc-4d55-b222-fe6a11a9b377 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 06:42:18.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7507" for this suite. 01/17/23 06:42:18.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:18.632
Jan 17 06:42:18.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-pred 01/17/23 06:42:18.634
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:18.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:18.677
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 06:42:18.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 06:42:18.694: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 06:42:18.699: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
Jan 17 06:42:18.709: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.711: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 06:42:18.711: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.711: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 06:42:18.711: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 06:42:18.711: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.711: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 06:42:18.712: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 06:42:18.712: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 06:42:18.712: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 06:42:18.712: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 06:42:18.712: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 06:42:18.712: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 06:42:18.712: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 06:42:18.712: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 06:42:18.712: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
Jan 17 06:42:18.721: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 06:42:18.721: INFO: csi-cinder-nodeplugin-n6clg from kube-system started at 2023-01-17 06:29:31 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 06:42:18.721: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 06:42:18.721: INFO: magnum-prometheus-node-exporter-t9fp4 from kube-system started at 2023-01-17 06:29:31 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 06:42:18.721: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 06:42:18.721: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 06:42:18.721: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container e2e ready: true, restart count 0
Jan 17 06:42:18.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 06:42:18.721: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 06:42:18.721: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 06:42:18.721: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
Jan 17 06:42:18.731: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.731: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 06:42:18.731: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.731: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 06:42:18.731: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 06:42:18.731: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
Jan 17 06:42:18.731: INFO: 	Container grafana ready: true, restart count 0
Jan 17 06:42:18.731: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 06:42:18.731: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 06:42:18.732: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.732: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 06:42:18.732: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.732: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 06:42:18.732: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 06:42:18.732: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 06:42:18.732: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 06:42:18.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 06:42:18.732: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/17/23 06:42:18.733
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b058127f17bb2], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 01/17/23 06:42:18.794
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:42:19.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7955" for this suite. 01/17/23 06:42:19.787
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":49,"skipped":814,"failed":0}
------------------------------
• [1.171 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:18.632
    Jan 17 06:42:18.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-pred 01/17/23 06:42:18.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:18.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:18.677
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 06:42:18.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 06:42:18.694: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 06:42:18.699: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
    Jan 17 06:42:18.709: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.711: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 06:42:18.711: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.711: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 06:42:18.711: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 06:42:18.711: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.711: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 06:42:18.712: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
    Jan 17 06:42:18.721: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: csi-cinder-nodeplugin-n6clg from kube-system started at 2023-01-17 06:29:31 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: magnum-prometheus-node-exporter-t9fp4 from kube-system started at 2023-01-17 06:29:31 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 06:42:18.721: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
    Jan 17 06:42:18.731: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.731: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 06:42:18.731: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.731: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 06:42:18.731: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 06:42:18.731: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
    Jan 17 06:42:18.731: INFO: 	Container grafana ready: true, restart count 0
    Jan 17 06:42:18.731: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Jan 17 06:42:18.731: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Jan 17 06:42:18.732: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.732: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 06:42:18.732: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.732: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 06:42:18.732: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 06:42:18.732: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 06:42:18.732: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 06:42:18.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 06:42:18.732: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/17/23 06:42:18.733
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b058127f17bb2], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 01/17/23 06:42:18.794
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:42:19.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7955" for this suite. 01/17/23 06:42:19.787
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:19.807
Jan 17 06:42:19.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename events 01/17/23 06:42:19.808
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:19.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:19.844
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/17/23 06:42:19.883
Jan 17 06:42:19.892: INFO: created test-event-1
Jan 17 06:42:19.901: INFO: created test-event-2
Jan 17 06:42:19.907: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/17/23 06:42:19.907
STEP: delete collection of events 01/17/23 06:42:19.912
Jan 17 06:42:19.912: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/17/23 06:42:19.936
Jan 17 06:42:19.937: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 17 06:42:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4254" for this suite. 01/17/23 06:42:19.948
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":50,"skipped":860,"failed":0}
------------------------------
• [0.156 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:19.807
    Jan 17 06:42:19.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename events 01/17/23 06:42:19.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:19.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:19.844
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/17/23 06:42:19.883
    Jan 17 06:42:19.892: INFO: created test-event-1
    Jan 17 06:42:19.901: INFO: created test-event-2
    Jan 17 06:42:19.907: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/17/23 06:42:19.907
    STEP: delete collection of events 01/17/23 06:42:19.912
    Jan 17 06:42:19.912: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/17/23 06:42:19.936
    Jan 17 06:42:19.937: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 17 06:42:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4254" for this suite. 01/17/23 06:42:19.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:19.964
Jan 17 06:42:19.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 06:42:19.967
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:19.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:20.002
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-93dd28a6-4998-44db-ac55-783961aaba9f 01/17/23 06:42:20.073
STEP: Creating a pod to test consume secrets 01/17/23 06:42:20.08
Jan 17 06:42:20.094: INFO: Waiting up to 5m0s for pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a" in namespace "secrets-6820" to be "Succeeded or Failed"
Jan 17 06:42:20.102: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.309988ms
Jan 17 06:42:22.108: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014129073s
Jan 17 06:42:24.110: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015919761s
STEP: Saw pod success 01/17/23 06:42:24.11
Jan 17 06:42:24.111: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a" satisfied condition "Succeeded or Failed"
Jan 17 06:42:24.115: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 06:42:24.124
Jan 17 06:42:24.150: INFO: Waiting for pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a to disappear
Jan 17 06:42:24.157: INFO: Pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 06:42:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6820" for this suite. 01/17/23 06:42:24.168
STEP: Destroying namespace "secret-namespace-6059" for this suite. 01/17/23 06:42:24.195
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":51,"skipped":866,"failed":0}
------------------------------
• [4.244 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:19.964
    Jan 17 06:42:19.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 06:42:19.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:19.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:20.002
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-93dd28a6-4998-44db-ac55-783961aaba9f 01/17/23 06:42:20.073
    STEP: Creating a pod to test consume secrets 01/17/23 06:42:20.08
    Jan 17 06:42:20.094: INFO: Waiting up to 5m0s for pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a" in namespace "secrets-6820" to be "Succeeded or Failed"
    Jan 17 06:42:20.102: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.309988ms
    Jan 17 06:42:22.108: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014129073s
    Jan 17 06:42:24.110: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015919761s
    STEP: Saw pod success 01/17/23 06:42:24.11
    Jan 17 06:42:24.111: INFO: Pod "pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:24.115: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 06:42:24.124
    Jan 17 06:42:24.150: INFO: Waiting for pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a to disappear
    Jan 17 06:42:24.157: INFO: Pod pod-secrets-c81fe20f-545f-4958-8083-c6dbb23d956a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 06:42:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6820" for this suite. 01/17/23 06:42:24.168
    STEP: Destroying namespace "secret-namespace-6059" for this suite. 01/17/23 06:42:24.195
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:24.209
Jan 17 06:42:24.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 06:42:24.211
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:24.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:24.247
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-a3526a50-fd83-42b6-a548-ecc710b1af14 01/17/23 06:42:24.253
STEP: Creating a pod to test consume configMaps 01/17/23 06:42:24.267
Jan 17 06:42:24.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0" in namespace "configmap-7394" to be "Succeeded or Failed"
Jan 17 06:42:24.286: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907461ms
Jan 17 06:42:26.292: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010644587s
Jan 17 06:42:28.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011854499s
Jan 17 06:42:30.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012032315s
STEP: Saw pod success 01/17/23 06:42:30.293
Jan 17 06:42:30.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0" satisfied condition "Succeeded or Failed"
Jan 17 06:42:30.298: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 06:42:30.308
Jan 17 06:42:30.338: INFO: Waiting for pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 to disappear
Jan 17 06:42:30.343: INFO: Pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 06:42:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7394" for this suite. 01/17/23 06:42:30.35
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":870,"failed":0}
------------------------------
• [SLOW TEST] [6.160 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:24.209
    Jan 17 06:42:24.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 06:42:24.211
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:24.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:24.247
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-a3526a50-fd83-42b6-a548-ecc710b1af14 01/17/23 06:42:24.253
    STEP: Creating a pod to test consume configMaps 01/17/23 06:42:24.267
    Jan 17 06:42:24.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0" in namespace "configmap-7394" to be "Succeeded or Failed"
    Jan 17 06:42:24.286: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907461ms
    Jan 17 06:42:26.292: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010644587s
    Jan 17 06:42:28.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011854499s
    Jan 17 06:42:30.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012032315s
    STEP: Saw pod success 01/17/23 06:42:30.293
    Jan 17 06:42:30.293: INFO: Pod "pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:30.298: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 06:42:30.308
    Jan 17 06:42:30.338: INFO: Waiting for pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 to disappear
    Jan 17 06:42:30.343: INFO: Pod pod-configmaps-dd745a3f-c381-4175-a9da-1073854917d0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 06:42:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7394" for this suite. 01/17/23 06:42:30.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:30.372
Jan 17 06:42:30.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:42:30.374
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:30.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:30.416
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-d6a1bd4d-b2f9-4777-93f6-8458acdd1040 01/17/23 06:42:30.422
STEP: Creating a pod to test consume configMaps 01/17/23 06:42:30.429
Jan 17 06:42:30.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8" in namespace "projected-4714" to be "Succeeded or Failed"
Jan 17 06:42:30.456: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046613ms
Jan 17 06:42:32.462: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01408111s
Jan 17 06:42:34.462: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014287218s
Jan 17 06:42:36.463: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015250912s
STEP: Saw pod success 01/17/23 06:42:36.463
Jan 17 06:42:36.464: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8" satisfied condition "Succeeded or Failed"
Jan 17 06:42:36.468: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 06:42:36.477
Jan 17 06:42:36.496: INFO: Waiting for pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 to disappear
Jan 17 06:42:36.507: INFO: Pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 06:42:36.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4714" for this suite. 01/17/23 06:42:36.516
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":53,"skipped":881,"failed":0}
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:30.372
    Jan 17 06:42:30.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:42:30.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:30.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:30.416
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-d6a1bd4d-b2f9-4777-93f6-8458acdd1040 01/17/23 06:42:30.422
    STEP: Creating a pod to test consume configMaps 01/17/23 06:42:30.429
    Jan 17 06:42:30.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8" in namespace "projected-4714" to be "Succeeded or Failed"
    Jan 17 06:42:30.456: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046613ms
    Jan 17 06:42:32.462: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01408111s
    Jan 17 06:42:34.462: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014287218s
    Jan 17 06:42:36.463: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015250912s
    STEP: Saw pod success 01/17/23 06:42:36.463
    Jan 17 06:42:36.464: INFO: Pod "pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:36.468: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 06:42:36.477
    Jan 17 06:42:36.496: INFO: Waiting for pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 to disappear
    Jan 17 06:42:36.507: INFO: Pod pod-projected-configmaps-b566c27a-06b3-40f0-8d41-62ab394be8d8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 06:42:36.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4714" for this suite. 01/17/23 06:42:36.516
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:36.538
Jan 17 06:42:36.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:42:36.555
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:36.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:36.591
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/17/23 06:42:36.598
Jan 17 06:42:36.618: INFO: Waiting up to 5m0s for pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee" in namespace "downward-api-5353" to be "Succeeded or Failed"
Jan 17 06:42:36.625: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.069384ms
Jan 17 06:42:38.633: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014756388s
Jan 17 06:42:40.631: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012771131s
STEP: Saw pod success 01/17/23 06:42:40.631
Jan 17 06:42:40.631: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee" satisfied condition "Succeeded or Failed"
Jan 17 06:42:40.635: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee container dapi-container: <nil>
STEP: delete the pod 01/17/23 06:42:40.645
Jan 17 06:42:40.675: INFO: Waiting for pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee to disappear
Jan 17 06:42:40.680: INFO: Pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 06:42:40.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5353" for this suite. 01/17/23 06:42:40.686
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":54,"skipped":881,"failed":0}
------------------------------
• [4.161 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:36.538
    Jan 17 06:42:36.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:42:36.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:36.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:36.591
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/17/23 06:42:36.598
    Jan 17 06:42:36.618: INFO: Waiting up to 5m0s for pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee" in namespace "downward-api-5353" to be "Succeeded or Failed"
    Jan 17 06:42:36.625: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.069384ms
    Jan 17 06:42:38.633: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014756388s
    Jan 17 06:42:40.631: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012771131s
    STEP: Saw pod success 01/17/23 06:42:40.631
    Jan 17 06:42:40.631: INFO: Pod "downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee" satisfied condition "Succeeded or Failed"
    Jan 17 06:42:40.635: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee container dapi-container: <nil>
    STEP: delete the pod 01/17/23 06:42:40.645
    Jan 17 06:42:40.675: INFO: Waiting for pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee to disappear
    Jan 17 06:42:40.680: INFO: Pod downward-api-b0ca940a-82a8-4420-a2a8-22e130edf9ee no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 06:42:40.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5353" for this suite. 01/17/23 06:42:40.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:40.699
Jan 17 06:42:40.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 06:42:40.701
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:40.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:40.739
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 17 06:42:40.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 create -f -'
Jan 17 06:42:41.924: INFO: stderr: ""
Jan 17 06:42:41.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 17 06:42:41.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 create -f -'
Jan 17 06:42:43.420: INFO: stderr: ""
Jan 17 06:42:43.420: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 06:42:43.42
Jan 17 06:42:44.426: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 06:42:44.427: INFO: Found 1 / 1
Jan 17 06:42:44.427: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 06:42:44.430: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 06:42:44.430: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 06:42:44.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe pod agnhost-primary-zwnzn'
Jan 17 06:42:44.558: INFO: stderr: ""
Jan 17 06:42:44.558: INFO: stdout: "Name:             agnhost-primary-zwnzn\nNamespace:        kubectl-5870\nPriority:         0\nService Account:  default\nNode:             cluster125-w73dz53kvqes-node-1/10.0.0.22\nStart Time:       Tue, 17 Jan 2023 06:42:41 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 9c631ab508bbd58407ccba540cb40e901e3505bb3e22150325e51fef2deb5943\n                  cni.projectcalico.org/podIP: 10.100.135.39/32\n                  cni.projectcalico.org/podIPs: 10.100.135.39/32\nStatus:           Running\nIP:               10.100.135.39\nIPs:\n  IP:           10.100.135.39\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://67ee7e0a8f79d1e806199a73962933fd3cd16099c3fdf012418dae4e8ea54216\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Jan 2023 06:42:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wk9kg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wk9kg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5870/agnhost-primary-zwnzn to cluster125-w73dz53kvqes-node-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 17 06:42:44.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe rc agnhost-primary'
Jan 17 06:42:44.728: INFO: stderr: ""
Jan 17 06:42:44.728: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5870\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-zwnzn\n"
Jan 17 06:42:44.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe service agnhost-primary'
Jan 17 06:42:44.883: INFO: stderr: ""
Jan 17 06:42:44.883: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5870\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.109.107\nIPs:               10.254.109.107\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.135.39:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 17 06:42:44.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe node cluster125-w73dz53kvqes-master-0'
Jan 17 06:42:45.063: INFO: stderr: ""
Jan 17 06:42:45.064: INFO: stdout: "Name:               cluster125-w73dz53kvqes-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-ppd-1\n                    failure-domain.beta.kubernetes.io/zone=nz-ppd-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cluster125-w73dz53kvqes-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-ppd-1\n                    topology.kubernetes.io/zone=nz-ppd-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.9/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Jan 2023 06:15:24 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cluster125-w73dz53kvqes-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 17 Jan 2023 06:42:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 17 Jan 2023 06:16:21 +0000   Tue, 17 Jan 2023 06:16:21 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:16:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.9\n  ExternalIP:  10.10.8.50\n  Hostname:    cluster125-w73dz53kvqes-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20435948Ki\n  hugepages-2Mi:      0\n  memory:             4011816Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  18833769646\n  hugepages-2Mi:      0\n  memory:             3909416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b1d77d95803b46acb466850032b5fa3b\n  System UUID:                b1d77d95-803b-46ac-b466-850032b5fa3b\n  Boot ID:                    45c87763-7115-4311-92a4-7946f2f05575\n  Kernel Version:             6.0.7-301.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20221106.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.13\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.100.2.0/24\nPodCIDRs:                     10.100.2.0/24\nProviderID:                   openstack:///b1d77d95-803b-46ac-b466-850032b5fa3b\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-2jl5z                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 csi-cinder-controllerplugin-0                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 k8s-keystone-auth-slfqk                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 magnum-prometheus-node-exporter-jdr6p                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 openstack-cloud-controller-manager-lr4jt                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         27m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7t88n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (40%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 27m                kube-proxy             \n  Normal  NodeHasNoDiskPressure    27m (x7 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     27m (x7 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  27m (x8 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           27m                node-controller        Node cluster125-w73dz53kvqes-master-0 event: Registered Node cluster125-w73dz53kvqes-master-0 in Controller\n  Normal  Synced                   26m                cloud-node-controller  Node synced successfully\n"
Jan 17 06:42:45.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe namespace kubectl-5870'
Jan 17 06:42:45.189: INFO: stderr: ""
Jan 17 06:42:45.190: INFO: stdout: "Name:         kubectl-5870\nLabels:       e2e-framework=kubectl\n              e2e-run=dc5ce341-7d8d-4db7-8b68-67ab063698df\n              kubernetes.io/metadata.name=kubectl-5870\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 06:42:45.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5870" for this suite. 01/17/23 06:42:45.195
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":55,"skipped":886,"failed":0}
------------------------------
• [4.506 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:40.699
    Jan 17 06:42:40.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 06:42:40.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:40.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:40.739
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 17 06:42:40.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 create -f -'
    Jan 17 06:42:41.924: INFO: stderr: ""
    Jan 17 06:42:41.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 17 06:42:41.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 create -f -'
    Jan 17 06:42:43.420: INFO: stderr: ""
    Jan 17 06:42:43.420: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 06:42:43.42
    Jan 17 06:42:44.426: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 06:42:44.427: INFO: Found 1 / 1
    Jan 17 06:42:44.427: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 17 06:42:44.430: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 06:42:44.430: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 06:42:44.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe pod agnhost-primary-zwnzn'
    Jan 17 06:42:44.558: INFO: stderr: ""
    Jan 17 06:42:44.558: INFO: stdout: "Name:             agnhost-primary-zwnzn\nNamespace:        kubectl-5870\nPriority:         0\nService Account:  default\nNode:             cluster125-w73dz53kvqes-node-1/10.0.0.22\nStart Time:       Tue, 17 Jan 2023 06:42:41 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 9c631ab508bbd58407ccba540cb40e901e3505bb3e22150325e51fef2deb5943\n                  cni.projectcalico.org/podIP: 10.100.135.39/32\n                  cni.projectcalico.org/podIPs: 10.100.135.39/32\nStatus:           Running\nIP:               10.100.135.39\nIPs:\n  IP:           10.100.135.39\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://67ee7e0a8f79d1e806199a73962933fd3cd16099c3fdf012418dae4e8ea54216\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Jan 2023 06:42:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wk9kg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wk9kg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5870/agnhost-primary-zwnzn to cluster125-w73dz53kvqes-node-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 17 06:42:44.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe rc agnhost-primary'
    Jan 17 06:42:44.728: INFO: stderr: ""
    Jan 17 06:42:44.728: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5870\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-zwnzn\n"
    Jan 17 06:42:44.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe service agnhost-primary'
    Jan 17 06:42:44.883: INFO: stderr: ""
    Jan 17 06:42:44.883: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5870\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.109.107\nIPs:               10.254.109.107\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.135.39:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 17 06:42:44.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe node cluster125-w73dz53kvqes-master-0'
    Jan 17 06:42:45.063: INFO: stderr: ""
    Jan 17 06:42:45.064: INFO: stdout: "Name:               cluster125-w73dz53kvqes-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-ppd-1\n                    failure-domain.beta.kubernetes.io/zone=nz-ppd-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cluster125-w73dz53kvqes-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-ppd-1\n                    topology.kubernetes.io/zone=nz-ppd-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.9/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Jan 2023 06:15:24 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cluster125-w73dz53kvqes-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 17 Jan 2023 06:42:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 17 Jan 2023 06:16:21 +0000   Tue, 17 Jan 2023 06:16:21 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:15:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 17 Jan 2023 06:42:02 +0000   Tue, 17 Jan 2023 06:16:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.9\n  ExternalIP:  10.10.8.50\n  Hostname:    cluster125-w73dz53kvqes-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20435948Ki\n  hugepages-2Mi:      0\n  memory:             4011816Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  18833769646\n  hugepages-2Mi:      0\n  memory:             3909416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b1d77d95803b46acb466850032b5fa3b\n  System UUID:                b1d77d95-803b-46ac-b466-850032b5fa3b\n  Boot ID:                    45c87763-7115-4311-92a4-7946f2f05575\n  Kernel Version:             6.0.7-301.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20221106.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.13\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.100.2.0/24\nPodCIDRs:                     10.100.2.0/24\nProviderID:                   openstack:///b1d77d95-803b-46ac-b466-850032b5fa3b\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-2jl5z                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 csi-cinder-controllerplugin-0                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 k8s-keystone-auth-slfqk                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 magnum-prometheus-node-exporter-jdr6p                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 openstack-cloud-controller-manager-lr4jt                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         27m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7t88n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (40%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 27m                kube-proxy             \n  Normal  NodeHasNoDiskPressure    27m (x7 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     27m (x7 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  27m (x8 over 28m)  kubelet                Node cluster125-w73dz53kvqes-master-0 status is now: NodeHasSufficientMemory\n  Normal  RegisteredNode           27m                node-controller        Node cluster125-w73dz53kvqes-master-0 event: Registered Node cluster125-w73dz53kvqes-master-0 in Controller\n  Normal  Synced                   26m                cloud-node-controller  Node synced successfully\n"
    Jan 17 06:42:45.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5870 describe namespace kubectl-5870'
    Jan 17 06:42:45.189: INFO: stderr: ""
    Jan 17 06:42:45.190: INFO: stdout: "Name:         kubectl-5870\nLabels:       e2e-framework=kubectl\n              e2e-run=dc5ce341-7d8d-4db7-8b68-67ab063698df\n              kubernetes.io/metadata.name=kubectl-5870\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 06:42:45.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5870" for this suite. 01/17/23 06:42:45.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:45.207
Jan 17 06:42:45.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 06:42:45.209
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:45.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:45.243
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 17 06:42:45.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 17 06:42:50.274: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 06:42:50.274
Jan 17 06:42:50.274: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/17/23 06:42:50.293
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 06:42:52.330: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4943  b6df9a12-2863-4a40-b319-27cebeef1c34 8169 1 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ead878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 06:42:50 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-17 06:42:51 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 06:42:52.335: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4943  048a965b-6efb-4cca-8d68-814bc8d17ced 8155 1 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b6df9a12-2863-4a40-b319-27cebeef1c34 0xc004eadc37 0xc004eadc38}] [] [{kube-controller-manager Update apps/v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6df9a12-2863-4a40-b319-27cebeef1c34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eadce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 06:42:52.340: INFO: Pod "test-cleanup-deployment-69cb9c5497-7j5m5" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-7j5m5 test-cleanup-deployment-69cb9c5497- deployment-4943  c36a93d6-a2e8-4207-a3cd-a3565df8d067 8154 0 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:0de718aad1479c4e213f4052b1fa93edcbc68b0fed489ef1f99701a083a1e616 cni.projectcalico.org/podIP:10.100.168.83/32 cni.projectcalico.org/podIPs:10.100.168.83/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 048a965b-6efb-4cca-8d68-814bc8d17ced 0xc003ecbb47 0xc003ecbb48}] [] [{Go-http-client Update v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"048a965b-6efb-4cca-8d68-814bc8d17ced\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tsjqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tsjqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.83,StartTime:2023-01-17 06:42:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 06:42:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ca48dbd151d1db10deefa920f09b68da74948c5ee7a59032e4bd476201845dac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 06:42:52.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4943" for this suite. 01/17/23 06:42:52.346
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":56,"skipped":910,"failed":0}
------------------------------
• [SLOW TEST] [7.149 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:45.207
    Jan 17 06:42:45.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 06:42:45.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:45.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:45.243
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 17 06:42:45.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 17 06:42:50.274: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 06:42:50.274
    Jan 17 06:42:50.274: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/17/23 06:42:50.293
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 06:42:52.330: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4943  b6df9a12-2863-4a40-b319-27cebeef1c34 8169 1 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ead878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 06:42:50 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-17 06:42:51 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 06:42:52.335: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4943  048a965b-6efb-4cca-8d68-814bc8d17ced 8155 1 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b6df9a12-2863-4a40-b319-27cebeef1c34 0xc004eadc37 0xc004eadc38}] [] [{kube-controller-manager Update apps/v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6df9a12-2863-4a40-b319-27cebeef1c34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eadce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 06:42:52.340: INFO: Pod "test-cleanup-deployment-69cb9c5497-7j5m5" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-7j5m5 test-cleanup-deployment-69cb9c5497- deployment-4943  c36a93d6-a2e8-4207-a3cd-a3565df8d067 8154 0 2023-01-17 06:42:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:0de718aad1479c4e213f4052b1fa93edcbc68b0fed489ef1f99701a083a1e616 cni.projectcalico.org/podIP:10.100.168.83/32 cni.projectcalico.org/podIPs:10.100.168.83/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 048a965b-6efb-4cca-8d68-814bc8d17ced 0xc003ecbb47 0xc003ecbb48}] [] [{Go-http-client Update v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 06:42:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"048a965b-6efb-4cca-8d68-814bc8d17ced\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 06:42:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tsjqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tsjqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 06:42:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.83,StartTime:2023-01-17 06:42:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 06:42:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ca48dbd151d1db10deefa920f09b68da74948c5ee7a59032e4bd476201845dac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 06:42:52.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4943" for this suite. 01/17/23 06:42:52.346
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:42:52.357
Jan 17 06:42:52.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 06:42:52.359
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:52.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:52.396
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 06:42:52.44
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 06:42:52.458
Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:52.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:42:52.482: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:53.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:42:53.496: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:54.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:54.496: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:55.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:55.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:56.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:56.498: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:57.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:57.494: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:58.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:58.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:42:59.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:42:59.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:00.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:00.499: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:01.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:01.501: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:02.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:02.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:02.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:02.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:02.497: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:03.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:03.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:04.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:04.493: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:05.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:05.500: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 06:43:06.494: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/17/23 06:43:06.499
Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:06.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:06.530: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:07.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:07.541: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:08.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:08.540: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:09.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:43:09.541: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:43:10.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 06:43:10.541: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 06:43:10.545
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-189, will wait for the garbage collector to delete the pods 01/17/23 06:43:10.545
Jan 17 06:43:10.611: INFO: Deleting DaemonSet.extensions daemon-set took: 11.409806ms
Jan 17 06:43:10.711: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.244471ms
Jan 17 06:43:12.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:43:12.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 06:43:12.924: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8355"},"items":null}

Jan 17 06:43:12.927: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8355"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:43:12.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-189" for this suite. 01/17/23 06:43:12.953
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":57,"skipped":910,"failed":0}
------------------------------
• [SLOW TEST] [20.608 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:42:52.357
    Jan 17 06:42:52.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 06:42:52.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:42:52.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:42:52.396
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 06:42:52.44
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 06:42:52.458
    Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:52.470: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:52.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:42:52.482: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:53.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:53.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:42:53.496: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:54.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:54.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:54.496: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:55.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:55.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:55.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:56.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:56.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:56.498: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:57.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:57.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:57.494: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:58.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:58.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:58.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:59.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:42:59.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:42:59.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:00.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:00.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:00.499: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:01.496: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:01.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:01.501: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:02.491: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:02.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:02.492: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:02.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:02.497: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:03.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:03.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:03.495: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:04.489: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:04.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:04.493: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:05.494: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:05.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:05.500: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.490: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 06:43:06.494: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/17/23 06:43:06.499
    Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.526: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:06.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:06.530: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:07.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:07.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:07.541: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:08.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:08.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:08.540: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:09.536: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:09.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:43:09.541: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:10.537: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:43:10.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 06:43:10.541: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 06:43:10.545
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-189, will wait for the garbage collector to delete the pods 01/17/23 06:43:10.545
    Jan 17 06:43:10.611: INFO: Deleting DaemonSet.extensions daemon-set took: 11.409806ms
    Jan 17 06:43:10.711: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.244471ms
    Jan 17 06:43:12.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:43:12.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 06:43:12.924: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8355"},"items":null}

    Jan 17 06:43:12.927: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8355"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:43:12.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-189" for this suite. 01/17/23 06:43:12.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:43:12.969
Jan 17 06:43:12.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 06:43:12.971
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:43:13.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:43:13.004
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/17/23 06:43:13.011
Jan 17 06:43:13.030: INFO: Waiting up to 2m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835" to be "running"
Jan 17 06:43:13.046: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 15.620185ms
Jan 17 06:43:15.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026181674s
Jan 17 06:43:17.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021353769s
Jan 17 06:43:19.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023586841s
Jan 17 06:43:21.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021678641s
Jan 17 06:43:23.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022139053s
Jan 17 06:43:25.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021052536s
Jan 17 06:43:27.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021347053s
Jan 17 06:43:29.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025596636s
Jan 17 06:43:31.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021965411s
Jan 17 06:43:33.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 20.021982653s
Jan 17 06:43:35.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021123904s
Jan 17 06:43:37.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022169925s
Jan 17 06:43:39.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023203926s
Jan 17 06:43:41.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022027047s
Jan 17 06:43:43.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022891458s
Jan 17 06:43:45.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021275283s
Jan 17 06:43:47.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021071395s
Jan 17 06:43:49.065: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 36.034774673s
Jan 17 06:43:51.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021528272s
Jan 17 06:43:53.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023158758s
Jan 17 06:43:55.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023045865s
Jan 17 06:43:57.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 44.021294132s
Jan 17 06:43:59.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023761349s
Jan 17 06:44:01.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023027356s
Jan 17 06:44:03.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02212606s
Jan 17 06:44:05.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021588924s
Jan 17 06:44:07.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02187655s
Jan 17 06:44:09.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021912952s
Jan 17 06:44:11.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021596215s
Jan 17 06:44:13.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022468148s
Jan 17 06:44:15.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023968639s
Jan 17 06:44:17.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02086893s
Jan 17 06:44:19.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021875579s
Jan 17 06:44:21.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.021694397s
Jan 17 06:44:23.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022351246s
Jan 17 06:44:25.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022669994s
Jan 17 06:44:27.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021154576s
Jan 17 06:44:29.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026363198s
Jan 17 06:44:31.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021717617s
Jan 17 06:44:33.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.02225359s
Jan 17 06:44:35.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022781375s
Jan 17 06:44:37.057: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027148095s
Jan 17 06:44:39.050: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020068714s
Jan 17 06:44:41.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021014793s
Jan 17 06:44:43.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022663697s
Jan 17 06:44:45.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021647103s
Jan 17 06:44:47.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021643392s
Jan 17 06:44:49.055: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024893897s
Jan 17 06:44:51.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.02141438s
Jan 17 06:44:53.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.022102933s
Jan 17 06:44:55.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022500757s
Jan 17 06:44:57.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.021554767s
Jan 17 06:44:59.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021817996s
Jan 17 06:45:01.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023156152s
Jan 17 06:45:03.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02129282s
Jan 17 06:45:05.050: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020314347s
Jan 17 06:45:07.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021482678s
Jan 17 06:45:09.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020803264s
Jan 17 06:45:11.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026302913s
Jan 17 06:45:13.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022127124s
Jan 17 06:45:13.057: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027042017s
STEP: updating the pod 01/17/23 06:45:13.057
Jan 17 06:45:13.583: INFO: Successfully updated pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00"
STEP: waiting for pod running 01/17/23 06:45:13.583
Jan 17 06:45:13.583: INFO: Waiting up to 2m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835" to be "running"
Jan 17 06:45:13.595: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.449274ms
Jan 17 06:45:15.601: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.017799492s
Jan 17 06:45:15.608: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 06:45:15.608
Jan 17 06:45:15.608: INFO: Deleting pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835"
Jan 17 06:45:15.627: INFO: Wait up to 5m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 06:45:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7835" for this suite. 01/17/23 06:45:47.651
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":58,"skipped":947,"failed":0}
------------------------------
• [SLOW TEST] [154.697 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:43:12.969
    Jan 17 06:43:12.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 06:43:12.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:43:13.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:43:13.004
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/17/23 06:43:13.011
    Jan 17 06:43:13.030: INFO: Waiting up to 2m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835" to be "running"
    Jan 17 06:43:13.046: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 15.620185ms
    Jan 17 06:43:15.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026181674s
    Jan 17 06:43:17.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021353769s
    Jan 17 06:43:19.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023586841s
    Jan 17 06:43:21.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021678641s
    Jan 17 06:43:23.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022139053s
    Jan 17 06:43:25.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021052536s
    Jan 17 06:43:27.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021347053s
    Jan 17 06:43:29.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025596636s
    Jan 17 06:43:31.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021965411s
    Jan 17 06:43:33.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 20.021982653s
    Jan 17 06:43:35.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 22.021123904s
    Jan 17 06:43:37.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022169925s
    Jan 17 06:43:39.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023203926s
    Jan 17 06:43:41.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022027047s
    Jan 17 06:43:43.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022891458s
    Jan 17 06:43:45.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021275283s
    Jan 17 06:43:47.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021071395s
    Jan 17 06:43:49.065: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 36.034774673s
    Jan 17 06:43:51.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021528272s
    Jan 17 06:43:53.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 40.023158758s
    Jan 17 06:43:55.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023045865s
    Jan 17 06:43:57.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 44.021294132s
    Jan 17 06:43:59.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023761349s
    Jan 17 06:44:01.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023027356s
    Jan 17 06:44:03.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02212606s
    Jan 17 06:44:05.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021588924s
    Jan 17 06:44:07.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02187655s
    Jan 17 06:44:09.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021912952s
    Jan 17 06:44:11.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021596215s
    Jan 17 06:44:13.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022468148s
    Jan 17 06:44:15.054: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023968639s
    Jan 17 06:44:17.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02086893s
    Jan 17 06:44:19.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021875579s
    Jan 17 06:44:21.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.021694397s
    Jan 17 06:44:23.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022351246s
    Jan 17 06:44:25.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.022669994s
    Jan 17 06:44:27.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021154576s
    Jan 17 06:44:29.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026363198s
    Jan 17 06:44:31.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.021717617s
    Jan 17 06:44:33.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.02225359s
    Jan 17 06:44:35.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.022781375s
    Jan 17 06:44:37.057: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027148095s
    Jan 17 06:44:39.050: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020068714s
    Jan 17 06:44:41.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021014793s
    Jan 17 06:44:43.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022663697s
    Jan 17 06:44:45.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021647103s
    Jan 17 06:44:47.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021643392s
    Jan 17 06:44:49.055: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024893897s
    Jan 17 06:44:51.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.02141438s
    Jan 17 06:44:53.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.022102933s
    Jan 17 06:44:55.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022500757s
    Jan 17 06:44:57.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.021554767s
    Jan 17 06:44:59.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.021817996s
    Jan 17 06:45:01.053: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023156152s
    Jan 17 06:45:03.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02129282s
    Jan 17 06:45:05.050: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020314347s
    Jan 17 06:45:07.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.021482678s
    Jan 17 06:45:09.051: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020803264s
    Jan 17 06:45:11.056: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026302913s
    Jan 17 06:45:13.052: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022127124s
    Jan 17 06:45:13.057: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027042017s
    STEP: updating the pod 01/17/23 06:45:13.057
    Jan 17 06:45:13.583: INFO: Successfully updated pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00"
    STEP: waiting for pod running 01/17/23 06:45:13.583
    Jan 17 06:45:13.583: INFO: Waiting up to 2m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835" to be "running"
    Jan 17 06:45:13.595: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.449274ms
    Jan 17 06:45:15.601: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.017799492s
    Jan 17 06:45:15.608: INFO: Pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 06:45:15.608
    Jan 17 06:45:15.608: INFO: Deleting pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" in namespace "var-expansion-7835"
    Jan 17 06:45:15.627: INFO: Wait up to 5m0s for pod "var-expansion-69f033c2-6b08-478a-b908-c43335835c00" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 06:45:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7835" for this suite. 01/17/23 06:45:47.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:45:47.667
Jan 17 06:45:47.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename proxy 01/17/23 06:45:47.668
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:47.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:47.704
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 17 06:45:47.710: INFO: Creating pod...
Jan 17 06:45:47.730: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6364" to be "running"
Jan 17 06:45:47.736: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410344ms
Jan 17 06:45:49.745: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014913491s
Jan 17 06:45:49.745: INFO: Pod "agnhost" satisfied condition "running"
Jan 17 06:45:49.745: INFO: Creating service...
Jan 17 06:45:49.772: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=DELETE
Jan 17 06:45:49.798: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 06:45:49.798: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=OPTIONS
Jan 17 06:45:49.808: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 06:45:49.808: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=PATCH
Jan 17 06:45:49.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 06:45:49.815: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=POST
Jan 17 06:45:49.822: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 06:45:49.822: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=PUT
Jan 17 06:45:49.831: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 06:45:49.831: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 17 06:45:49.840: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 06:45:49.840: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 17 06:45:49.853: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 06:45:49.853: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 17 06:45:49.861: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 06:45:49.861: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=POST
Jan 17 06:45:49.869: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 06:45:49.869: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=PUT
Jan 17 06:45:49.876: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 06:45:49.876: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=GET
Jan 17 06:45:49.888: INFO: http.Client request:GET StatusCode:301
Jan 17 06:45:49.888: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=GET
Jan 17 06:45:49.902: INFO: http.Client request:GET StatusCode:301
Jan 17 06:45:49.902: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=HEAD
Jan 17 06:45:49.906: INFO: http.Client request:HEAD StatusCode:301
Jan 17 06:45:49.906: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 17 06:45:49.911: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 06:45:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6364" for this suite. 01/17/23 06:45:49.917
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":59,"skipped":973,"failed":0}
------------------------------
• [2.267 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:45:47.667
    Jan 17 06:45:47.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename proxy 01/17/23 06:45:47.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:47.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:47.704
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 17 06:45:47.710: INFO: Creating pod...
    Jan 17 06:45:47.730: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6364" to be "running"
    Jan 17 06:45:47.736: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410344ms
    Jan 17 06:45:49.745: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014913491s
    Jan 17 06:45:49.745: INFO: Pod "agnhost" satisfied condition "running"
    Jan 17 06:45:49.745: INFO: Creating service...
    Jan 17 06:45:49.772: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=DELETE
    Jan 17 06:45:49.798: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 06:45:49.798: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=OPTIONS
    Jan 17 06:45:49.808: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 06:45:49.808: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=PATCH
    Jan 17 06:45:49.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 06:45:49.815: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=POST
    Jan 17 06:45:49.822: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 06:45:49.822: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=PUT
    Jan 17 06:45:49.831: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 06:45:49.831: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 17 06:45:49.840: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 06:45:49.840: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 17 06:45:49.853: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 06:45:49.853: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 17 06:45:49.861: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 06:45:49.861: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=POST
    Jan 17 06:45:49.869: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 06:45:49.869: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 17 06:45:49.876: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 06:45:49.876: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=GET
    Jan 17 06:45:49.888: INFO: http.Client request:GET StatusCode:301
    Jan 17 06:45:49.888: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=GET
    Jan 17 06:45:49.902: INFO: http.Client request:GET StatusCode:301
    Jan 17 06:45:49.902: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/pods/agnhost/proxy?method=HEAD
    Jan 17 06:45:49.906: INFO: http.Client request:HEAD StatusCode:301
    Jan 17 06:45:49.906: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-6364/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 17 06:45:49.911: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 06:45:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6364" for this suite. 01/17/23 06:45:49.917
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:45:49.934
Jan 17 06:45:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename runtimeclass 01/17/23 06:45:49.936
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:49.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:49.974
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 17 06:45:50.014: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4749 to be scheduled
Jan 17 06:45:50.019: INFO: 1 pods are not scheduled: [runtimeclass-4749/test-runtimeclass-runtimeclass-4749-preconfigured-handler-xbp7s(9cdcf23b-f654-4933-86a4-da7e85c2ebf9)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 06:45:52.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4749" for this suite. 01/17/23 06:45:52.049
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":60,"skipped":975,"failed":0}
------------------------------
• [2.140 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:45:49.934
    Jan 17 06:45:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 06:45:49.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:49.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:49.974
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 17 06:45:50.014: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4749 to be scheduled
    Jan 17 06:45:50.019: INFO: 1 pods are not scheduled: [runtimeclass-4749/test-runtimeclass-runtimeclass-4749-preconfigured-handler-xbp7s(9cdcf23b-f654-4933-86a4-da7e85c2ebf9)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 06:45:52.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4749" for this suite. 01/17/23 06:45:52.049
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:45:52.075
Jan 17 06:45:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 06:45:52.077
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:52.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:52.12
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/17/23 06:45:52.126
Jan 17 06:45:52.147: INFO: created test-pod-1
Jan 17 06:45:52.164: INFO: created test-pod-2
Jan 17 06:45:52.180: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/17/23 06:45:52.18
Jan 17 06:45:52.181: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5872' to be running and ready
Jan 17 06:45:52.242: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 06:45:52.242: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 06:45:52.242: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 06:45:52.242: INFO: 0 / 3 pods in namespace 'pods-5872' are running and ready (0 seconds elapsed)
Jan 17 06:45:52.242: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
Jan 17 06:45:52.242: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Jan 17 06:45:52.242: INFO: test-pod-1  cluster125-w73dz53kvqes-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
Jan 17 06:45:52.243: INFO: test-pod-2  cluster125-w73dz53kvqes-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
Jan 17 06:45:52.243: INFO: test-pod-3  cluster125-w73dz53kvqes-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
Jan 17 06:45:52.243: INFO: 
Jan 17 06:45:54.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 06:45:54.257: INFO: 2 / 3 pods in namespace 'pods-5872' are running and ready (2 seconds elapsed)
Jan 17 06:45:54.257: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
Jan 17 06:45:54.257: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Jan 17 06:45:54.257: INFO: test-pod-2  cluster125-w73dz53kvqes-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
Jan 17 06:45:54.257: INFO: 
Jan 17 06:45:56.255: INFO: 3 / 3 pods in namespace 'pods-5872' are running and ready (4 seconds elapsed)
Jan 17 06:45:56.256: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/17/23 06:45:56.306
Jan 17 06:45:56.315: INFO: Pod quantity 3 is different from expected quantity 0
Jan 17 06:45:57.321: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 06:45:58.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5872" for this suite. 01/17/23 06:45:58.329
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":61,"skipped":978,"failed":0}
------------------------------
• [SLOW TEST] [6.269 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:45:52.075
    Jan 17 06:45:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 06:45:52.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:52.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:52.12
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/17/23 06:45:52.126
    Jan 17 06:45:52.147: INFO: created test-pod-1
    Jan 17 06:45:52.164: INFO: created test-pod-2
    Jan 17 06:45:52.180: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/17/23 06:45:52.18
    Jan 17 06:45:52.181: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5872' to be running and ready
    Jan 17 06:45:52.242: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 06:45:52.242: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 06:45:52.242: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 06:45:52.242: INFO: 0 / 3 pods in namespace 'pods-5872' are running and ready (0 seconds elapsed)
    Jan 17 06:45:52.242: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
    Jan 17 06:45:52.242: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Jan 17 06:45:52.242: INFO: test-pod-1  cluster125-w73dz53kvqes-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
    Jan 17 06:45:52.243: INFO: test-pod-2  cluster125-w73dz53kvqes-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
    Jan 17 06:45:52.243: INFO: test-pod-3  cluster125-w73dz53kvqes-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
    Jan 17 06:45:52.243: INFO: 
    Jan 17 06:45:54.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 06:45:54.257: INFO: 2 / 3 pods in namespace 'pods-5872' are running and ready (2 seconds elapsed)
    Jan 17 06:45:54.257: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
    Jan 17 06:45:54.257: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Jan 17 06:45:54.257: INFO: test-pod-2  cluster125-w73dz53kvqes-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 06:45:52 +0000 UTC  }]
    Jan 17 06:45:54.257: INFO: 
    Jan 17 06:45:56.255: INFO: 3 / 3 pods in namespace 'pods-5872' are running and ready (4 seconds elapsed)
    Jan 17 06:45:56.256: INFO: expected 0 pod replicas in namespace 'pods-5872', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/17/23 06:45:56.306
    Jan 17 06:45:56.315: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 17 06:45:57.321: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 06:45:58.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5872" for this suite. 01/17/23 06:45:58.329
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:45:58.345
Jan 17 06:45:58.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context-test 01/17/23 06:45:58.346
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:58.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:58.38
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 17 06:45:58.409: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6" in namespace "security-context-test-7518" to be "Succeeded or Failed"
Jan 17 06:45:58.415: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.519817ms
Jan 17 06:46:00.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011217303s
Jan 17 06:46:02.420: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010539642s
Jan 17 06:46:04.423: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013637048s
Jan 17 06:46:06.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011528828s
Jan 17 06:46:08.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011198619s
Jan 17 06:46:10.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.012074552s
Jan 17 06:46:10.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 06:46:10.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7518" for this suite. 01/17/23 06:46:10.492
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":62,"skipped":981,"failed":0}
------------------------------
• [SLOW TEST] [12.159 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:45:58.345
    Jan 17 06:45:58.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context-test 01/17/23 06:45:58.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:45:58.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:45:58.38
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 17 06:45:58.409: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6" in namespace "security-context-test-7518" to be "Succeeded or Failed"
    Jan 17 06:45:58.415: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.519817ms
    Jan 17 06:46:00.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011217303s
    Jan 17 06:46:02.420: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010539642s
    Jan 17 06:46:04.423: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013637048s
    Jan 17 06:46:06.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011528828s
    Jan 17 06:46:08.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011198619s
    Jan 17 06:46:10.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.012074552s
    Jan 17 06:46:10.421: INFO: Pod "alpine-nnp-false-2a8e1393-d666-47d6-bb91-a0362a1610b6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 06:46:10.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7518" for this suite. 01/17/23 06:46:10.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:46:10.508
Jan 17 06:46:10.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename namespaces 01/17/23 06:46:10.509
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:10.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:10.544
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/17/23 06:46:10.55
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:10.579
STEP: Creating a pod in the namespace 01/17/23 06:46:10.584
STEP: Waiting for the pod to have running status 01/17/23 06:46:10.597
Jan 17 06:46:10.597: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7756" to be "running"
Jan 17 06:46:10.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188719ms
Jan 17 06:46:12.611: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0141603s
Jan 17 06:46:12.612: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/17/23 06:46:12.612
STEP: Waiting for the namespace to be removed. 01/17/23 06:46:12.627
STEP: Recreating the namespace 01/17/23 06:46:23.636
STEP: Verifying there are no pods in the namespace 01/17/23 06:46:23.67
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:46:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8729" for this suite. 01/17/23 06:46:23.685
STEP: Destroying namespace "nsdeletetest-7756" for this suite. 01/17/23 06:46:23.699
Jan 17 06:46:23.704: INFO: Namespace nsdeletetest-7756 was already deleted
STEP: Destroying namespace "nsdeletetest-9695" for this suite. 01/17/23 06:46:23.704
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":63,"skipped":998,"failed":0}
------------------------------
• [SLOW TEST] [13.209 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:46:10.508
    Jan 17 06:46:10.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename namespaces 01/17/23 06:46:10.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:10.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:10.544
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/17/23 06:46:10.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:10.579
    STEP: Creating a pod in the namespace 01/17/23 06:46:10.584
    STEP: Waiting for the pod to have running status 01/17/23 06:46:10.597
    Jan 17 06:46:10.597: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7756" to be "running"
    Jan 17 06:46:10.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188719ms
    Jan 17 06:46:12.611: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0141603s
    Jan 17 06:46:12.612: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/17/23 06:46:12.612
    STEP: Waiting for the namespace to be removed. 01/17/23 06:46:12.627
    STEP: Recreating the namespace 01/17/23 06:46:23.636
    STEP: Verifying there are no pods in the namespace 01/17/23 06:46:23.67
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:46:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8729" for this suite. 01/17/23 06:46:23.685
    STEP: Destroying namespace "nsdeletetest-7756" for this suite. 01/17/23 06:46:23.699
    Jan 17 06:46:23.704: INFO: Namespace nsdeletetest-7756 was already deleted
    STEP: Destroying namespace "nsdeletetest-9695" for this suite. 01/17/23 06:46:23.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:46:23.722
Jan 17 06:46:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 06:46:23.723
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:23.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:23.767
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 17 06:46:23.794: INFO: Waiting up to 2m0s for pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" in namespace "var-expansion-1955" to be "container 0 failed with reason CreateContainerConfigError"
Jan 17 06:46:23.800: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710816ms
Jan 17 06:46:25.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01178143s
Jan 17 06:46:27.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01201342s
Jan 17 06:46:27.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 17 06:46:27.806: INFO: Deleting pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" in namespace "var-expansion-1955"
Jan 17 06:46:27.823: INFO: Wait up to 5m0s for pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 06:46:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1955" for this suite. 01/17/23 06:46:29.838
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":64,"skipped":1058,"failed":0}
------------------------------
• [SLOW TEST] [6.128 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:46:23.722
    Jan 17 06:46:23.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 06:46:23.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:23.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:23.767
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 17 06:46:23.794: INFO: Waiting up to 2m0s for pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" in namespace "var-expansion-1955" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 17 06:46:23.800: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710816ms
    Jan 17 06:46:25.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01178143s
    Jan 17 06:46:27.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01201342s
    Jan 17 06:46:27.806: INFO: Pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 17 06:46:27.806: INFO: Deleting pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" in namespace "var-expansion-1955"
    Jan 17 06:46:27.823: INFO: Wait up to 5m0s for pod "var-expansion-05ca1703-d544-429f-baf4-5018d7b31721" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 06:46:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1955" for this suite. 01/17/23 06:46:29.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:46:29.851
Jan 17 06:46:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:46:29.853
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:29.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:29.898
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-502 01/17/23 06:46:29.906
Jan 17 06:46:29.926: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-502" to be "running and ready"
Jan 17 06:46:29.932: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.766076ms
Jan 17 06:46:29.932: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:46:31.941: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846493s
Jan 17 06:46:31.941: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 06:46:31.941: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 17 06:46:31.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 06:46:32.242: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 17 06:46:32.242: INFO: stdout: "iptables"
Jan 17 06:46:32.242: INFO: proxyMode: iptables
Jan 17 06:46:32.265: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 06:46:32.278: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-502 01/17/23 06:46:32.278
STEP: creating replication controller affinity-clusterip-timeout in namespace services-502 01/17/23 06:46:32.297
I0117 06:46:32.322544      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-502, replica count: 3
I0117 06:46:35.373279      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 06:46:35.384: INFO: Creating new exec pod
Jan 17 06:46:35.395: INFO: Waiting up to 5m0s for pod "execpod-affinityrmr49" in namespace "services-502" to be "running"
Jan 17 06:46:35.400: INFO: Pod "execpod-affinityrmr49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.301292ms
Jan 17 06:46:37.407: INFO: Pod "execpod-affinityrmr49": Phase="Running", Reason="", readiness=true. Elapsed: 2.011641322s
Jan 17 06:46:37.407: INFO: Pod "execpod-affinityrmr49" satisfied condition "running"
Jan 17 06:46:38.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 17 06:46:38.816: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 06:46:38.816: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:46:38.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.15.24 80'
Jan 17 06:46:39.161: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.254.15.24 80\nConnection to 10.254.15.24 80 port [tcp/http] succeeded!\n"
Jan 17 06:46:39.162: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:46:39.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.15.24:80/ ; done'
Jan 17 06:46:39.609: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
Jan 17 06:46:39.609: INFO: stdout: "\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt"
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
Jan 17 06:46:39.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
Jan 17 06:46:40.003: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
Jan 17 06:46:40.003: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
Jan 17 06:47:00.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
Jan 17 06:47:00.314: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
Jan 17 06:47:00.314: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
Jan 17 06:47:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
Jan 17 06:47:20.664: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
Jan 17 06:47:20.664: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
Jan 17 06:47:40.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
Jan 17 06:47:41.055: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
Jan 17 06:47:41.055: INFO: stdout: "affinity-clusterip-timeout-nkvfn"
Jan 17 06:47:41.055: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-502, will wait for the garbage collector to delete the pods 01/17/23 06:47:41.084
Jan 17 06:47:41.176: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 30.498132ms
Jan 17 06:47:41.277: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.765233ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:47:43.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-502" for this suite. 01/17/23 06:47:43.925
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":65,"skipped":1072,"failed":0}
------------------------------
• [SLOW TEST] [74.085 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:46:29.851
    Jan 17 06:46:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:46:29.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:46:29.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:46:29.898
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-502 01/17/23 06:46:29.906
    Jan 17 06:46:29.926: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-502" to be "running and ready"
    Jan 17 06:46:29.932: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.766076ms
    Jan 17 06:46:29.932: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:46:31.941: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014846493s
    Jan 17 06:46:31.941: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 17 06:46:31.941: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 17 06:46:31.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 17 06:46:32.242: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 17 06:46:32.242: INFO: stdout: "iptables"
    Jan 17 06:46:32.242: INFO: proxyMode: iptables
    Jan 17 06:46:32.265: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 17 06:46:32.278: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-502 01/17/23 06:46:32.278
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-502 01/17/23 06:46:32.297
    I0117 06:46:32.322544      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-502, replica count: 3
    I0117 06:46:35.373279      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 06:46:35.384: INFO: Creating new exec pod
    Jan 17 06:46:35.395: INFO: Waiting up to 5m0s for pod "execpod-affinityrmr49" in namespace "services-502" to be "running"
    Jan 17 06:46:35.400: INFO: Pod "execpod-affinityrmr49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.301292ms
    Jan 17 06:46:37.407: INFO: Pod "execpod-affinityrmr49": Phase="Running", Reason="", readiness=true. Elapsed: 2.011641322s
    Jan 17 06:46:37.407: INFO: Pod "execpod-affinityrmr49" satisfied condition "running"
    Jan 17 06:46:38.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 17 06:46:38.816: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 17 06:46:38.816: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:46:38.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.15.24 80'
    Jan 17 06:46:39.161: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.254.15.24 80\nConnection to 10.254.15.24 80 port [tcp/http] succeeded!\n"
    Jan 17 06:46:39.162: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:46:39.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.15.24:80/ ; done'
    Jan 17 06:46:39.609: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
    Jan 17 06:46:39.609: INFO: stdout: "\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt\naffinity-clusterip-timeout-wq5bt"
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Received response from host: affinity-clusterip-timeout-wq5bt
    Jan 17 06:46:39.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
    Jan 17 06:46:40.003: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
    Jan 17 06:46:40.003: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
    Jan 17 06:47:00.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
    Jan 17 06:47:00.314: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
    Jan 17 06:47:00.314: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
    Jan 17 06:47:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
    Jan 17 06:47:20.664: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
    Jan 17 06:47:20.664: INFO: stdout: "affinity-clusterip-timeout-wq5bt"
    Jan 17 06:47:40.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-502 exec execpod-affinityrmr49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.15.24:80/'
    Jan 17 06:47:41.055: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.15.24:80/\n"
    Jan 17 06:47:41.055: INFO: stdout: "affinity-clusterip-timeout-nkvfn"
    Jan 17 06:47:41.055: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-502, will wait for the garbage collector to delete the pods 01/17/23 06:47:41.084
    Jan 17 06:47:41.176: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 30.498132ms
    Jan 17 06:47:41.277: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.765233ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:47:43.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-502" for this suite. 01/17/23 06:47:43.925
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:47:43.939
Jan 17 06:47:43.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-runtime 01/17/23 06:47:43.941
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:43.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:43.978
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/17/23 06:47:43.985
STEP: wait for the container to reach Succeeded 01/17/23 06:47:44.006
STEP: get the container status 01/17/23 06:47:48.038
STEP: the container should be terminated 01/17/23 06:47:48.043
STEP: the termination message should be set 01/17/23 06:47:48.043
Jan 17 06:47:48.044: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/17/23 06:47:48.044
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 06:47:48.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2830" for this suite. 01/17/23 06:47:48.083
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":66,"skipped":1095,"failed":0}
------------------------------
• [4.158 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:47:43.939
    Jan 17 06:47:43.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-runtime 01/17/23 06:47:43.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:43.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:43.978
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/17/23 06:47:43.985
    STEP: wait for the container to reach Succeeded 01/17/23 06:47:44.006
    STEP: get the container status 01/17/23 06:47:48.038
    STEP: the container should be terminated 01/17/23 06:47:48.043
    STEP: the termination message should be set 01/17/23 06:47:48.043
    Jan 17 06:47:48.044: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/17/23 06:47:48.044
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 06:47:48.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2830" for this suite. 01/17/23 06:47:48.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:47:48.109
Jan 17 06:47:48.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 06:47:48.11
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:48.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:48.151
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 06:47:48.182
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:47:48.772
STEP: Deploying the webhook pod 01/17/23 06:47:48.798
STEP: Wait for the deployment to be ready 01/17/23 06:47:48.848
Jan 17 06:47:48.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 06:47:50.903
STEP: Verifying the service has paired with the endpoint 01/17/23 06:47:50.927
Jan 17 06:47:51.928: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/17/23 06:47:51.938
STEP: create a pod that should be updated by the webhook 01/17/23 06:47:51.965
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:47:52.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6390" for this suite. 01/17/23 06:47:52.014
STEP: Destroying namespace "webhook-6390-markers" for this suite. 01/17/23 06:47:52.047
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":67,"skipped":1137,"failed":0}
------------------------------
• [4.055 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:47:48.109
    Jan 17 06:47:48.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 06:47:48.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:48.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:48.151
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 06:47:48.182
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:47:48.772
    STEP: Deploying the webhook pod 01/17/23 06:47:48.798
    STEP: Wait for the deployment to be ready 01/17/23 06:47:48.848
    Jan 17 06:47:48.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 06:47:50.903
    STEP: Verifying the service has paired with the endpoint 01/17/23 06:47:50.927
    Jan 17 06:47:51.928: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/17/23 06:47:51.938
    STEP: create a pod that should be updated by the webhook 01/17/23 06:47:51.965
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:47:52.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6390" for this suite. 01/17/23 06:47:52.014
    STEP: Destroying namespace "webhook-6390-markers" for this suite. 01/17/23 06:47:52.047
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:47:52.171
Jan 17 06:47:52.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context-test 01/17/23 06:47:52.2
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:52.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:52.272
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 17 06:47:52.309: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d" in namespace "security-context-test-4223" to be "Succeeded or Failed"
Jan 17 06:47:52.318: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.04491ms
Jan 17 06:47:54.324: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015057484s
Jan 17 06:47:56.323: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014796023s
Jan 17 06:47:56.324: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 06:47:56.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4223" for this suite. 01/17/23 06:47:56.332
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":68,"skipped":1180,"failed":0}
------------------------------
• [4.174 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:47:52.171
    Jan 17 06:47:52.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context-test 01/17/23 06:47:52.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:52.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:52.272
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 17 06:47:52.309: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d" in namespace "security-context-test-4223" to be "Succeeded or Failed"
    Jan 17 06:47:52.318: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.04491ms
    Jan 17 06:47:54.324: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015057484s
    Jan 17 06:47:56.323: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014796023s
    Jan 17 06:47:56.324: INFO: Pod "busybox-user-65534-d7638568-4d7f-4c71-8385-f7683e5d138d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 06:47:56.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4223" for this suite. 01/17/23 06:47:56.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:47:56.346
Jan 17 06:47:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubelet-test 01/17/23 06:47:56.347
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:56.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:56.383
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 17 06:47:56.403: INFO: Waiting up to 5m0s for pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727" in namespace "kubelet-test-5428" to be "running and ready"
Jan 17 06:47:56.416: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727": Phase="Pending", Reason="", readiness=false. Elapsed: 12.747923ms
Jan 17 06:47:56.416: INFO: The phase of Pod busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:47:58.420: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727": Phase="Running", Reason="", readiness=true. Elapsed: 2.016848288s
Jan 17 06:47:58.420: INFO: The phase of Pod busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727 is Running (Ready = true)
Jan 17 06:47:58.420: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 06:47:58.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5428" for this suite. 01/17/23 06:47:58.504
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":69,"skipped":1196,"failed":0}
------------------------------
• [2.170 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:47:56.346
    Jan 17 06:47:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 06:47:56.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:56.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:56.383
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 17 06:47:56.403: INFO: Waiting up to 5m0s for pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727" in namespace "kubelet-test-5428" to be "running and ready"
    Jan 17 06:47:56.416: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727": Phase="Pending", Reason="", readiness=false. Elapsed: 12.747923ms
    Jan 17 06:47:56.416: INFO: The phase of Pod busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:47:58.420: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727": Phase="Running", Reason="", readiness=true. Elapsed: 2.016848288s
    Jan 17 06:47:58.420: INFO: The phase of Pod busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727 is Running (Ready = true)
    Jan 17 06:47:58.420: INFO: Pod "busybox-scheduling-60639128-2bc9-4f1c-84b5-141b9c0f7727" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 06:47:58.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5428" for this suite. 01/17/23 06:47:58.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:47:58.517
Jan 17 06:47:58.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:47:58.518
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:58.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:58.55
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6034 01/17/23 06:47:58.555
STEP: changing the ExternalName service to type=ClusterIP 01/17/23 06:47:58.565
STEP: creating replication controller externalname-service in namespace services-6034 01/17/23 06:47:58.606
I0117 06:47:58.621508      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6034, replica count: 2
I0117 06:48:01.673305      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 06:48:01.673: INFO: Creating new exec pod
Jan 17 06:48:01.689: INFO: Waiting up to 5m0s for pod "execpod58rdw" in namespace "services-6034" to be "running"
Jan 17 06:48:01.695: INFO: Pod "execpod58rdw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524529ms
Jan 17 06:48:03.702: INFO: Pod "execpod58rdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.012434482s
Jan 17 06:48:03.702: INFO: Pod "execpod58rdw" satisfied condition "running"
Jan 17 06:48:04.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6034 exec execpod58rdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 06:48:04.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 06:48:04.979: INFO: stdout: "externalname-service-gnmff"
Jan 17 06:48:04.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6034 exec execpod58rdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.83.130 80'
Jan 17 06:48:05.253: INFO: stderr: "+ nc -v -t -w 2 10.254.83.130 80\n+ echo hostName\nConnection to 10.254.83.130 80 port [tcp/http] succeeded!\n"
Jan 17 06:48:05.253: INFO: stdout: "externalname-service-gnmff"
Jan 17 06:48:05.253: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:48:05.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6034" for this suite. 01/17/23 06:48:05.305
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":70,"skipped":1206,"failed":0}
------------------------------
• [SLOW TEST] [6.804 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:47:58.517
    Jan 17 06:47:58.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:47:58.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:47:58.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:47:58.55
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6034 01/17/23 06:47:58.555
    STEP: changing the ExternalName service to type=ClusterIP 01/17/23 06:47:58.565
    STEP: creating replication controller externalname-service in namespace services-6034 01/17/23 06:47:58.606
    I0117 06:47:58.621508      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6034, replica count: 2
    I0117 06:48:01.673305      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 06:48:01.673: INFO: Creating new exec pod
    Jan 17 06:48:01.689: INFO: Waiting up to 5m0s for pod "execpod58rdw" in namespace "services-6034" to be "running"
    Jan 17 06:48:01.695: INFO: Pod "execpod58rdw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524529ms
    Jan 17 06:48:03.702: INFO: Pod "execpod58rdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.012434482s
    Jan 17 06:48:03.702: INFO: Pod "execpod58rdw" satisfied condition "running"
    Jan 17 06:48:04.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6034 exec execpod58rdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 17 06:48:04.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 17 06:48:04.979: INFO: stdout: "externalname-service-gnmff"
    Jan 17 06:48:04.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6034 exec execpod58rdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.83.130 80'
    Jan 17 06:48:05.253: INFO: stderr: "+ nc -v -t -w 2 10.254.83.130 80\n+ echo hostName\nConnection to 10.254.83.130 80 port [tcp/http] succeeded!\n"
    Jan 17 06:48:05.253: INFO: stdout: "externalname-service-gnmff"
    Jan 17 06:48:05.253: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:48:05.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6034" for this suite. 01/17/23 06:48:05.305
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:05.326
Jan 17 06:48:05.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 06:48:05.327
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:05.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:05.366
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 06:48:05.412
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:48:06.707
STEP: Deploying the webhook pod 01/17/23 06:48:06.714
STEP: Wait for the deployment to be ready 01/17/23 06:48:06.744
Jan 17 06:48:06.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 06:48:08.796
STEP: Verifying the service has paired with the endpoint 01/17/23 06:48:08.866
Jan 17 06:48:09.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/17/23 06:48:09.874
STEP: Creating a custom resource definition that should be denied by the webhook 01/17/23 06:48:09.903
Jan 17 06:48:09.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:48:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6100" for this suite. 01/17/23 06:48:09.94
STEP: Destroying namespace "webhook-6100-markers" for this suite. 01/17/23 06:48:09.95
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":71,"skipped":1218,"failed":0}
------------------------------
• [4.718 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:05.326
    Jan 17 06:48:05.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 06:48:05.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:05.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:05.366
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 06:48:05.412
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:48:06.707
    STEP: Deploying the webhook pod 01/17/23 06:48:06.714
    STEP: Wait for the deployment to be ready 01/17/23 06:48:06.744
    Jan 17 06:48:06.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 06:48:08.796
    STEP: Verifying the service has paired with the endpoint 01/17/23 06:48:08.866
    Jan 17 06:48:09.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/17/23 06:48:09.874
    STEP: Creating a custom resource definition that should be denied by the webhook 01/17/23 06:48:09.903
    Jan 17 06:48:09.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:48:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6100" for this suite. 01/17/23 06:48:09.94
    STEP: Destroying namespace "webhook-6100-markers" for this suite. 01/17/23 06:48:09.95
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:10.044
Jan 17 06:48:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 06:48:10.046
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:10.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:10.116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 06:48:10.153
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:48:11.284
STEP: Deploying the webhook pod 01/17/23 06:48:11.295
STEP: Wait for the deployment to be ready 01/17/23 06:48:11.328
Jan 17 06:48:11.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 06:48:13.368
STEP: Verifying the service has paired with the endpoint 01/17/23 06:48:13.391
Jan 17 06:48:14.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 17 06:48:14.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4951-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 06:48:14.915
STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 06:48:14.94
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:48:17.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2480" for this suite. 01/17/23 06:48:17.546
STEP: Destroying namespace "webhook-2480-markers" for this suite. 01/17/23 06:48:17.559
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":72,"skipped":1218,"failed":0}
------------------------------
• [SLOW TEST] [7.602 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:10.044
    Jan 17 06:48:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 06:48:10.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:10.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:10.116
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 06:48:10.153
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:48:11.284
    STEP: Deploying the webhook pod 01/17/23 06:48:11.295
    STEP: Wait for the deployment to be ready 01/17/23 06:48:11.328
    Jan 17 06:48:11.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 06:48:13.368
    STEP: Verifying the service has paired with the endpoint 01/17/23 06:48:13.391
    Jan 17 06:48:14.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 17 06:48:14.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4951-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 06:48:14.915
    STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 06:48:14.94
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:48:17.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2480" for this suite. 01/17/23 06:48:17.546
    STEP: Destroying namespace "webhook-2480-markers" for this suite. 01/17/23 06:48:17.559
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:17.648
Jan 17 06:48:17.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption 01/17/23 06:48:17.65
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:17.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:17.741
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/17/23 06:48:17.754
STEP: Waiting for the pdb to be processed 01/17/23 06:48:17.769
STEP: updating the pdb 01/17/23 06:48:19.782
STEP: Waiting for the pdb to be processed 01/17/23 06:48:19.802
STEP: patching the pdb 01/17/23 06:48:19.807
STEP: Waiting for the pdb to be processed 01/17/23 06:48:19.828
STEP: Waiting for the pdb to be deleted 01/17/23 06:48:21.851
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 06:48:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6385" for this suite. 01/17/23 06:48:21.862
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":73,"skipped":1225,"failed":0}
------------------------------
• [4.224 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:17.648
    Jan 17 06:48:17.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption 01/17/23 06:48:17.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:17.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:17.741
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/17/23 06:48:17.754
    STEP: Waiting for the pdb to be processed 01/17/23 06:48:17.769
    STEP: updating the pdb 01/17/23 06:48:19.782
    STEP: Waiting for the pdb to be processed 01/17/23 06:48:19.802
    STEP: patching the pdb 01/17/23 06:48:19.807
    STEP: Waiting for the pdb to be processed 01/17/23 06:48:19.828
    STEP: Waiting for the pdb to be deleted 01/17/23 06:48:21.851
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 06:48:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6385" for this suite. 01/17/23 06:48:21.862
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:21.874
Jan 17 06:48:21.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:48:21.876
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:21.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:21.912
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/17/23 06:48:21.918
Jan 17 06:48:21.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0" in namespace "downward-api-2025" to be "Succeeded or Failed"
Jan 17 06:48:21.969: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.922548ms
Jan 17 06:48:23.976: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025221028s
Jan 17 06:48:25.976: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025412168s
Jan 17 06:48:27.975: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0244827s
STEP: Saw pod success 01/17/23 06:48:27.975
Jan 17 06:48:27.985: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0" satisfied condition "Succeeded or Failed"
Jan 17 06:48:27.989: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 container client-container: <nil>
STEP: delete the pod 01/17/23 06:48:27.998
Jan 17 06:48:28.025: INFO: Waiting for pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 to disappear
Jan 17 06:48:28.032: INFO: Pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 06:48:28.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2025" for this suite. 01/17/23 06:48:28.038
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":74,"skipped":1229,"failed":0}
------------------------------
• [SLOW TEST] [6.173 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:21.874
    Jan 17 06:48:21.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:48:21.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:21.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:21.912
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/17/23 06:48:21.918
    Jan 17 06:48:21.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0" in namespace "downward-api-2025" to be "Succeeded or Failed"
    Jan 17 06:48:21.969: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.922548ms
    Jan 17 06:48:23.976: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025221028s
    Jan 17 06:48:25.976: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025412168s
    Jan 17 06:48:27.975: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0244827s
    STEP: Saw pod success 01/17/23 06:48:27.975
    Jan 17 06:48:27.985: INFO: Pod "downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0" satisfied condition "Succeeded or Failed"
    Jan 17 06:48:27.989: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 container client-container: <nil>
    STEP: delete the pod 01/17/23 06:48:27.998
    Jan 17 06:48:28.025: INFO: Waiting for pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 to disappear
    Jan 17 06:48:28.032: INFO: Pod downwardapi-volume-656a86c5-1d86-4fcf-92f8-bc4fbf7a06c0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 06:48:28.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2025" for this suite. 01/17/23 06:48:28.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:28.048
Jan 17 06:48:28.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 06:48:28.049
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:28.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:28.089
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 in namespace container-probe-9860 01/17/23 06:48:28.096
Jan 17 06:48:28.109: INFO: Waiting up to 5m0s for pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11" in namespace "container-probe-9860" to be "not pending"
Jan 17 06:48:28.114: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62021ms
Jan 17 06:48:30.125: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11": Phase="Running", Reason="", readiness=true. Elapsed: 2.016703868s
Jan 17 06:48:30.125: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11" satisfied condition "not pending"
Jan 17 06:48:30.125: INFO: Started pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 in namespace container-probe-9860
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:48:30.125
Jan 17 06:48:30.130: INFO: Initial restart count of pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 is 0
Jan 17 06:48:50.202: INFO: Restart count of pod container-probe-9860/liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 is now 1 (20.07194038s elapsed)
STEP: deleting the pod 01/17/23 06:48:50.202
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 06:48:50.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9860" for this suite. 01/17/23 06:48:50.242
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":75,"skipped":1245,"failed":0}
------------------------------
• [SLOW TEST] [22.220 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:28.048
    Jan 17 06:48:28.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 06:48:28.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:28.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:28.089
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 in namespace container-probe-9860 01/17/23 06:48:28.096
    Jan 17 06:48:28.109: INFO: Waiting up to 5m0s for pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11" in namespace "container-probe-9860" to be "not pending"
    Jan 17 06:48:28.114: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62021ms
    Jan 17 06:48:30.125: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11": Phase="Running", Reason="", readiness=true. Elapsed: 2.016703868s
    Jan 17 06:48:30.125: INFO: Pod "liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11" satisfied condition "not pending"
    Jan 17 06:48:30.125: INFO: Started pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 in namespace container-probe-9860
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:48:30.125
    Jan 17 06:48:30.130: INFO: Initial restart count of pod liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 is 0
    Jan 17 06:48:50.202: INFO: Restart count of pod container-probe-9860/liveness-802f8589-afb5-42b8-b5bb-3f9cb9332a11 is now 1 (20.07194038s elapsed)
    STEP: deleting the pod 01/17/23 06:48:50.202
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 06:48:50.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9860" for this suite. 01/17/23 06:48:50.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:48:50.269
Jan 17 06:48:50.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename cronjob 01/17/23 06:48:50.27
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:50.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:50.308
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/17/23 06:48:50.315
STEP: Ensuring no jobs are scheduled 01/17/23 06:48:50.331
STEP: Ensuring no job exists by listing jobs explicitly 01/17/23 06:53:50.341
STEP: Removing cronjob 01/17/23 06:53:50.346
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 06:53:50.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5629" for this suite. 01/17/23 06:53:50.366
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":76,"skipped":1257,"failed":0}
------------------------------
• [SLOW TEST] [300.107 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:48:50.269
    Jan 17 06:48:50.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename cronjob 01/17/23 06:48:50.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:48:50.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:48:50.308
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/17/23 06:48:50.315
    STEP: Ensuring no jobs are scheduled 01/17/23 06:48:50.331
    STEP: Ensuring no job exists by listing jobs explicitly 01/17/23 06:53:50.341
    STEP: Removing cronjob 01/17/23 06:53:50.346
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 06:53:50.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5629" for this suite. 01/17/23 06:53:50.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:53:50.382
Jan 17 06:53:50.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-runtime 01/17/23 06:53:50.383
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:50.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:50.414
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/17/23 06:53:50.421
STEP: wait for the container to reach Succeeded 01/17/23 06:53:50.44
STEP: get the container status 01/17/23 06:53:54.48
STEP: the container should be terminated 01/17/23 06:53:54.483
STEP: the termination message should be set 01/17/23 06:53:54.483
Jan 17 06:53:54.483: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/17/23 06:53:54.483
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 06:53:54.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1811" for this suite. 01/17/23 06:53:54.512
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":77,"skipped":1276,"failed":0}
------------------------------
• [4.144 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:53:50.382
    Jan 17 06:53:50.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-runtime 01/17/23 06:53:50.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:50.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:50.414
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/17/23 06:53:50.421
    STEP: wait for the container to reach Succeeded 01/17/23 06:53:50.44
    STEP: get the container status 01/17/23 06:53:54.48
    STEP: the container should be terminated 01/17/23 06:53:54.483
    STEP: the termination message should be set 01/17/23 06:53:54.483
    Jan 17 06:53:54.483: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/17/23 06:53:54.483
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 06:53:54.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1811" for this suite. 01/17/23 06:53:54.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:53:54.527
Jan 17 06:53:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 06:53:54.528
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:54.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:54.57
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 17 06:53:54.657: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9c61ab0e-fce9-4de8-a3ed-226970bb442e", Controller:(*bool)(0xc002f00a7e), BlockOwnerDeletion:(*bool)(0xc002f00a7f)}}
Jan 17 06:53:54.692: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"00e52bde-dfab-40ba-9953-4fedb00d2086", Controller:(*bool)(0xc003eca8ce), BlockOwnerDeletion:(*bool)(0xc003eca8cf)}}
Jan 17 06:53:54.703: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"856467bf-9e8f-49f4-be2a-4fb42b2c50dc", Controller:(*bool)(0xc003ecab2a), BlockOwnerDeletion:(*bool)(0xc003ecab2b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 06:53:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3221" for this suite. 01/17/23 06:53:59.737
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":78,"skipped":1289,"failed":0}
------------------------------
• [SLOW TEST] [5.219 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:53:54.527
    Jan 17 06:53:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 06:53:54.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:54.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:54.57
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 17 06:53:54.657: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9c61ab0e-fce9-4de8-a3ed-226970bb442e", Controller:(*bool)(0xc002f00a7e), BlockOwnerDeletion:(*bool)(0xc002f00a7f)}}
    Jan 17 06:53:54.692: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"00e52bde-dfab-40ba-9953-4fedb00d2086", Controller:(*bool)(0xc003eca8ce), BlockOwnerDeletion:(*bool)(0xc003eca8cf)}}
    Jan 17 06:53:54.703: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"856467bf-9e8f-49f4-be2a-4fb42b2c50dc", Controller:(*bool)(0xc003ecab2a), BlockOwnerDeletion:(*bool)(0xc003ecab2b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 06:53:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3221" for this suite. 01/17/23 06:53:59.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:53:59.748
Jan 17 06:53:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 06:53:59.75
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:59.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:59.79
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/17/23 06:53:59.796
Jan 17 06:53:59.812: INFO: Waiting up to 5m0s for pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95" in namespace "downward-api-7181" to be "running and ready"
Jan 17 06:53:59.818: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663724ms
Jan 17 06:53:59.818: INFO: The phase of Pod labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:54:01.824: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95": Phase="Running", Reason="", readiness=true. Elapsed: 2.01194059s
Jan 17 06:54:01.824: INFO: The phase of Pod labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95 is Running (Ready = true)
Jan 17 06:54:01.824: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95" satisfied condition "running and ready"
Jan 17 06:54:02.419: INFO: Successfully updated pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 06:54:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7181" for this suite. 01/17/23 06:54:06.463
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":79,"skipped":1308,"failed":0}
------------------------------
• [SLOW TEST] [6.726 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:53:59.748
    Jan 17 06:53:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 06:53:59.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:53:59.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:53:59.79
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/17/23 06:53:59.796
    Jan 17 06:53:59.812: INFO: Waiting up to 5m0s for pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95" in namespace "downward-api-7181" to be "running and ready"
    Jan 17 06:53:59.818: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663724ms
    Jan 17 06:53:59.818: INFO: The phase of Pod labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:54:01.824: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95": Phase="Running", Reason="", readiness=true. Elapsed: 2.01194059s
    Jan 17 06:54:01.824: INFO: The phase of Pod labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95 is Running (Ready = true)
    Jan 17 06:54:01.824: INFO: Pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95" satisfied condition "running and ready"
    Jan 17 06:54:02.419: INFO: Successfully updated pod "labelsupdate85b06806-746e-4506-8b7d-cb8c54e0ce95"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 06:54:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7181" for this suite. 01/17/23 06:54:06.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:54:06.48
Jan 17 06:54:06.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 06:54:06.481
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:06.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:06.52
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0e8d8eca-1d86-4f8e-aa94-9e3eefc4c33f 01/17/23 06:54:06.531
STEP: Creating the pod 01/17/23 06:54:06.54
Jan 17 06:54:06.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397" in namespace "projected-8338" to be "running and ready"
Jan 17 06:54:06.560: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380693ms
Jan 17 06:54:06.560: INFO: The phase of Pod pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 06:54:08.567: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397": Phase="Running", Reason="", readiness=true. Elapsed: 2.013621886s
Jan 17 06:54:08.567: INFO: The phase of Pod pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397 is Running (Ready = true)
Jan 17 06:54:08.567: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-0e8d8eca-1d86-4f8e-aa94-9e3eefc4c33f 01/17/23 06:54:08.626
STEP: waiting to observe update in volume 01/17/23 06:54:08.639
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 06:54:10.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8338" for this suite. 01/17/23 06:54:10.66
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":80,"skipped":1316,"failed":0}
------------------------------
• [4.194 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:54:06.48
    Jan 17 06:54:06.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 06:54:06.481
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:06.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:06.52
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-0e8d8eca-1d86-4f8e-aa94-9e3eefc4c33f 01/17/23 06:54:06.531
    STEP: Creating the pod 01/17/23 06:54:06.54
    Jan 17 06:54:06.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397" in namespace "projected-8338" to be "running and ready"
    Jan 17 06:54:06.560: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380693ms
    Jan 17 06:54:06.560: INFO: The phase of Pod pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 06:54:08.567: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397": Phase="Running", Reason="", readiness=true. Elapsed: 2.013621886s
    Jan 17 06:54:08.567: INFO: The phase of Pod pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397 is Running (Ready = true)
    Jan 17 06:54:08.567: INFO: Pod "pod-projected-configmaps-1298b068-9f93-49ea-903a-74011cfdb397" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-0e8d8eca-1d86-4f8e-aa94-9e3eefc4c33f 01/17/23 06:54:08.626
    STEP: waiting to observe update in volume 01/17/23 06:54:08.639
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 06:54:10.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8338" for this suite. 01/17/23 06:54:10.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:54:10.679
Jan 17 06:54:10.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 06:54:10.681
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:10.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:10.714
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3241 01/17/23 06:54:10.72
STEP: creating service affinity-clusterip in namespace services-3241 01/17/23 06:54:10.72
STEP: creating replication controller affinity-clusterip in namespace services-3241 01/17/23 06:54:10.747
I0117 06:54:10.760615      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3241, replica count: 3
I0117 06:54:13.812645      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 06:54:13.823: INFO: Creating new exec pod
Jan 17 06:54:13.836: INFO: Waiting up to 5m0s for pod "execpod-affinityf7f46" in namespace "services-3241" to be "running"
Jan 17 06:54:13.842: INFO: Pod "execpod-affinityf7f46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902598ms
Jan 17 06:54:15.854: INFO: Pod "execpod-affinityf7f46": Phase="Running", Reason="", readiness=true. Elapsed: 2.017478067s
Jan 17 06:54:15.854: INFO: Pod "execpod-affinityf7f46" satisfied condition "running"
Jan 17 06:54:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 17 06:54:17.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 17 06:54:17.242: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:54:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.157.196 80'
Jan 17 06:54:17.593: INFO: stderr: "+ + echonc -v -t -w hostName 2\n 10.254.157.196 80\nConnection to 10.254.157.196 80 port [tcp/http] succeeded!\n"
Jan 17 06:54:17.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 06:54:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.157.196:80/ ; done'
Jan 17 06:54:18.019: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n"
Jan 17 06:54:18.019: INFO: stdout: "\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5"
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
Jan 17 06:54:18.019: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3241, will wait for the garbage collector to delete the pods 01/17/23 06:54:18.042
Jan 17 06:54:18.112: INFO: Deleting ReplicationController affinity-clusterip took: 11.238704ms
Jan 17 06:54:18.213: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.808451ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 06:54:20.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3241" for this suite. 01/17/23 06:54:20.459
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":81,"skipped":1341,"failed":0}
------------------------------
• [SLOW TEST] [9.796 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:54:10.679
    Jan 17 06:54:10.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 06:54:10.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:10.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:10.714
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3241 01/17/23 06:54:10.72
    STEP: creating service affinity-clusterip in namespace services-3241 01/17/23 06:54:10.72
    STEP: creating replication controller affinity-clusterip in namespace services-3241 01/17/23 06:54:10.747
    I0117 06:54:10.760615      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3241, replica count: 3
    I0117 06:54:13.812645      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 06:54:13.823: INFO: Creating new exec pod
    Jan 17 06:54:13.836: INFO: Waiting up to 5m0s for pod "execpod-affinityf7f46" in namespace "services-3241" to be "running"
    Jan 17 06:54:13.842: INFO: Pod "execpod-affinityf7f46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902598ms
    Jan 17 06:54:15.854: INFO: Pod "execpod-affinityf7f46": Phase="Running", Reason="", readiness=true. Elapsed: 2.017478067s
    Jan 17 06:54:15.854: INFO: Pod "execpod-affinityf7f46" satisfied condition "running"
    Jan 17 06:54:16.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 17 06:54:17.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 17 06:54:17.242: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:54:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.157.196 80'
    Jan 17 06:54:17.593: INFO: stderr: "+ + echonc -v -t -w hostName 2\n 10.254.157.196 80\nConnection to 10.254.157.196 80 port [tcp/http] succeeded!\n"
    Jan 17 06:54:17.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 06:54:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3241 exec execpod-affinityf7f46 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.157.196:80/ ; done'
    Jan 17 06:54:18.019: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.157.196:80/\n"
    Jan 17 06:54:18.019: INFO: stdout: "\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5\naffinity-clusterip-5jhm5"
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Received response from host: affinity-clusterip-5jhm5
    Jan 17 06:54:18.019: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3241, will wait for the garbage collector to delete the pods 01/17/23 06:54:18.042
    Jan 17 06:54:18.112: INFO: Deleting ReplicationController affinity-clusterip took: 11.238704ms
    Jan 17 06:54:18.213: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.808451ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 06:54:20.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3241" for this suite. 01/17/23 06:54:20.459
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:54:20.476
Jan 17 06:54:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:54:20.478
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:20.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:20.513
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/17/23 06:54:20.522
Jan 17 06:54:20.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/17/23 06:54:44.96
Jan 17 06:54:44.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 06:54:51.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:55:15.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5707" for this suite. 01/17/23 06:55:15.115
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":82,"skipped":1371,"failed":0}
------------------------------
• [SLOW TEST] [54.649 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:54:20.476
    Jan 17 06:54:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 06:54:20.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:54:20.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:54:20.513
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/17/23 06:54:20.522
    Jan 17 06:54:20.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/17/23 06:54:44.96
    Jan 17 06:54:44.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 06:54:51.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:55:15.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5707" for this suite. 01/17/23 06:55:15.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:55:15.133
Jan 17 06:55:15.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename init-container 01/17/23 06:55:15.134
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:15.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:15.172
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/17/23 06:55:15.182
Jan 17 06:55:15.182: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 06:55:20.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4301" for this suite. 01/17/23 06:55:20.612
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":83,"skipped":1403,"failed":0}
------------------------------
• [SLOW TEST] [5.493 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:55:15.133
    Jan 17 06:55:15.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename init-container 01/17/23 06:55:15.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:15.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:15.172
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/17/23 06:55:15.182
    Jan 17 06:55:15.182: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 06:55:20.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4301" for this suite. 01/17/23 06:55:20.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:55:20.638
Jan 17 06:55:20.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 06:55:20.64
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:20.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:20.69
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/17/23 06:55:20.699
STEP: setting up watch 01/17/23 06:55:20.7
STEP: submitting the pod to kubernetes 01/17/23 06:55:20.81
STEP: verifying the pod is in kubernetes 01/17/23 06:55:20.836
STEP: verifying pod creation was observed 01/17/23 06:55:20.849
Jan 17 06:55:20.850: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2" in namespace "pods-6755" to be "running"
Jan 17 06:55:20.859: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.125787ms
Jan 17 06:55:22.864: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014129104s
Jan 17 06:55:22.864: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 06:55:22.867
STEP: verifying pod deletion was observed 01/17/23 06:55:22.88
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 06:55:25.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6755" for this suite. 01/17/23 06:55:25.677
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":84,"skipped":1415,"failed":0}
------------------------------
• [SLOW TEST] [5.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:55:20.638
    Jan 17 06:55:20.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 06:55:20.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:20.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:20.69
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/17/23 06:55:20.699
    STEP: setting up watch 01/17/23 06:55:20.7
    STEP: submitting the pod to kubernetes 01/17/23 06:55:20.81
    STEP: verifying the pod is in kubernetes 01/17/23 06:55:20.836
    STEP: verifying pod creation was observed 01/17/23 06:55:20.849
    Jan 17 06:55:20.850: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2" in namespace "pods-6755" to be "running"
    Jan 17 06:55:20.859: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.125787ms
    Jan 17 06:55:22.864: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014129104s
    Jan 17 06:55:22.864: INFO: Pod "pod-submit-remove-6d23d784-3388-4a3f-8fe8-81f65740a4e2" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 06:55:22.867
    STEP: verifying pod deletion was observed 01/17/23 06:55:22.88
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 06:55:25.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6755" for this suite. 01/17/23 06:55:25.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:55:25.689
Jan 17 06:55:25.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 06:55:25.691
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:25.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:25.724
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/17/23 06:55:25.735
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local;sleep 1; done
 01/17/23 06:55:25.757
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local;sleep 1; done
 01/17/23 06:55:25.757
STEP: creating a pod to probe DNS 01/17/23 06:55:25.757
STEP: submitting the pod to kubernetes 01/17/23 06:55:25.757
Jan 17 06:55:25.780: INFO: Waiting up to 15m0s for pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7" in namespace "dns-2497" to be "running"
Jan 17 06:55:25.788: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.42619ms
Jan 17 06:55:27.793: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012748278s
Jan 17 06:55:27.793: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7" satisfied condition "running"
STEP: retrieving the pod 01/17/23 06:55:27.793
STEP: looking for the results for each expected name from probers 01/17/23 06:55:27.799
Jan 17 06:55:27.806: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.811: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.815: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.819: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.823: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.827: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.834: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.838: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
Jan 17 06:55:27.838: INFO: Lookups using dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local]

Jan 17 06:55:32.870: INFO: DNS probes using dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7 succeeded

STEP: deleting the pod 01/17/23 06:55:32.87
STEP: deleting the test headless service 01/17/23 06:55:32.907
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 06:55:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2497" for this suite. 01/17/23 06:55:32.96
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":85,"skipped":1426,"failed":0}
------------------------------
• [SLOW TEST] [7.284 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:55:25.689
    Jan 17 06:55:25.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 06:55:25.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:25.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:25.724
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/17/23 06:55:25.735
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local;sleep 1; done
     01/17/23 06:55:25.757
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2497.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local;sleep 1; done
     01/17/23 06:55:25.757
    STEP: creating a pod to probe DNS 01/17/23 06:55:25.757
    STEP: submitting the pod to kubernetes 01/17/23 06:55:25.757
    Jan 17 06:55:25.780: INFO: Waiting up to 15m0s for pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7" in namespace "dns-2497" to be "running"
    Jan 17 06:55:25.788: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.42619ms
    Jan 17 06:55:27.793: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012748278s
    Jan 17 06:55:27.793: INFO: Pod "dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 06:55:27.793
    STEP: looking for the results for each expected name from probers 01/17/23 06:55:27.799
    Jan 17 06:55:27.806: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.811: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.815: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.819: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.823: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.827: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.834: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.838: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local from pod dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7: the server could not find the requested resource (get pods dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7)
    Jan 17 06:55:27.838: INFO: Lookups using dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2497.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2497.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2497.svc.cluster.local jessie_udp@dns-test-service-2.dns-2497.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2497.svc.cluster.local]

    Jan 17 06:55:32.870: INFO: DNS probes using dns-2497/dns-test-831fc821-6d1b-4487-97eb-ac2dc9c56ef7 succeeded

    STEP: deleting the pod 01/17/23 06:55:32.87
    STEP: deleting the test headless service 01/17/23 06:55:32.907
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 06:55:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2497" for this suite. 01/17/23 06:55:32.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:55:32.977
Jan 17 06:55:32.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 06:55:32.978
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:33.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:33.022
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 in namespace container-probe-5347 01/17/23 06:55:33.028
Jan 17 06:55:33.041: INFO: Waiting up to 5m0s for pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6" in namespace "container-probe-5347" to be "not pending"
Jan 17 06:55:33.047: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350707ms
Jan 17 06:55:35.052: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010212235s
Jan 17 06:55:35.052: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6" satisfied condition "not pending"
Jan 17 06:55:35.052: INFO: Started pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 in namespace container-probe-5347
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:55:35.052
Jan 17 06:55:35.060: INFO: Initial restart count of pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 is 0
STEP: deleting the pod 01/17/23 06:59:35.838
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 06:59:35.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5347" for this suite. 01/17/23 06:59:35.883
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":86,"skipped":1470,"failed":0}
------------------------------
• [SLOW TEST] [242.920 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:55:32.977
    Jan 17 06:55:32.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 06:55:32.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:55:33.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:55:33.022
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 in namespace container-probe-5347 01/17/23 06:55:33.028
    Jan 17 06:55:33.041: INFO: Waiting up to 5m0s for pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6" in namespace "container-probe-5347" to be "not pending"
    Jan 17 06:55:33.047: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350707ms
    Jan 17 06:55:35.052: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010212235s
    Jan 17 06:55:35.052: INFO: Pod "test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6" satisfied condition "not pending"
    Jan 17 06:55:35.052: INFO: Started pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 in namespace container-probe-5347
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 06:55:35.052
    Jan 17 06:55:35.060: INFO: Initial restart count of pod test-webserver-87bfe270-6893-4780-9ed7-c5fbf02707e6 is 0
    STEP: deleting the pod 01/17/23 06:59:35.838
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 06:59:35.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5347" for this suite. 01/17/23 06:59:35.883
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:59:35.9
Jan 17 06:59:35.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 06:59:35.901
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:35.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:35.943
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 06:59:35.973
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:59:36.777
STEP: Deploying the webhook pod 01/17/23 06:59:36.8
STEP: Wait for the deployment to be ready 01/17/23 06:59:36.849
Jan 17 06:59:36.891: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 06:59:38.902
STEP: Verifying the service has paired with the endpoint 01/17/23 06:59:38.92
Jan 17 06:59:39.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 17 06:59:39.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7183-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 06:59:40.448
STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 06:59:40.473
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 06:59:43.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1172" for this suite. 01/17/23 06:59:43.087
STEP: Destroying namespace "webhook-1172-markers" for this suite. 01/17/23 06:59:43.1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":87,"skipped":1474,"failed":0}
------------------------------
• [SLOW TEST] [7.315 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:59:35.9
    Jan 17 06:59:35.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 06:59:35.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:35.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:35.943
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 06:59:35.973
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 06:59:36.777
    STEP: Deploying the webhook pod 01/17/23 06:59:36.8
    STEP: Wait for the deployment to be ready 01/17/23 06:59:36.849
    Jan 17 06:59:36.891: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 06:59:38.902
    STEP: Verifying the service has paired with the endpoint 01/17/23 06:59:38.92
    Jan 17 06:59:39.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 17 06:59:39.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7183-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 06:59:40.448
    STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 06:59:40.473
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 06:59:43.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1172" for this suite. 01/17/23 06:59:43.087
    STEP: Destroying namespace "webhook-1172-markers" for this suite. 01/17/23 06:59:43.1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:59:43.217
Jan 17 06:59:43.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 06:59:43.219
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:43.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:43.276
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-c62d655a-4d6d-47f7-bbdb-434475c7603b 01/17/23 06:59:43.29
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 06:59:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7503" for this suite. 01/17/23 06:59:43.314
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":88,"skipped":1506,"failed":0}
------------------------------
• [0.116 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:59:43.217
    Jan 17 06:59:43.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 06:59:43.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:43.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:43.276
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-c62d655a-4d6d-47f7-bbdb-434475c7603b 01/17/23 06:59:43.29
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 06:59:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7503" for this suite. 01/17/23 06:59:43.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:59:43.334
Jan 17 06:59:43.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 06:59:43.336
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:43.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:43.371
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 06:59:43.423
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 06:59:43.437
Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:43.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:59:43.469: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 06:59:44.486: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:45.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 06:59:45.487: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 06:59:46.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 06:59:46.480: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/17/23 06:59:46.485
STEP: DeleteCollection of the DaemonSets 01/17/23 06:59:46.497
STEP: Verify that ReplicaSets have been deleted 01/17/23 06:59:46.51
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 17 06:59:46.550: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12393"},"items":null}

Jan 17 06:59:46.554: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12393"},"items":[{"metadata":{"name":"daemon-set-6kzhn","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"657265d7-d565-4979-9805-1d342d9ecaba","resourceVersion":"12392","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6a9f141805cfbb07adbbab393ddcc647900d2991fcaecb9f1821b0b856b14963","cni.projectcalico.org/podIP":"10.100.135.9/32","cni.projectcalico.org/podIPs":"10.100.135.9/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-27ptm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-27ptm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:45Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:45Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.22","podIP":"10.100.135.9","podIPs":[{"ip":"10.100.135.9"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://368554db0818fc117845a837d5b879a699626bc3485324fe93e4cd70032f9124","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-7v2vz","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"9c491eeb-6340-4641-b06f-b39d1956bd4f","resourceVersion":"12393","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d477288ae3c080ee44576279e4621764d288c023c61e0c35bd6297107b739796","cni.projectcalico.org/podIP":"10.100.168.91/32","cni.projectcalico.org/podIPs":"10.100.168.91/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jrd89","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jrd89","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.21","podIP":"10.100.168.91","podIPs":[{"ip":"10.100.168.91"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ece2f9533e760f41c6f6f7695393a5cb561286725579c6b884486b0a9d07c308","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vhpvp","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"885c360b-641a-4913-b27c-44f6440e69a7","resourceVersion":"12391","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"70e07c26fcb8d50d734bef481bc5c11d92b23e0b01ea4d288f89c5717022ee50","cni.projectcalico.org/podIP":"10.100.206.11/32","cni.projectcalico.org/podIPs":"10.100.206.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g24rm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g24rm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.16","podIP":"10.100.206.11","podIPs":[{"ip":"10.100.206.11"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://69b405446b19f56fd472e83f4088b35d34b25cd09d8c7e39fb82a23c7475097b","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 06:59:46.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1684" for this suite. 01/17/23 06:59:46.577
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":89,"skipped":1512,"failed":0}
------------------------------
• [3.255 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:59:43.334
    Jan 17 06:59:43.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 06:59:43.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:43.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:43.371
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 06:59:43.423
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 06:59:43.437
    Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:43.448: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:43.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:59:43.469: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:44.482: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 06:59:44.486: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:45.479: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:45.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 06:59:45.487: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:46.476: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 06:59:46.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 06:59:46.480: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/17/23 06:59:46.485
    STEP: DeleteCollection of the DaemonSets 01/17/23 06:59:46.497
    STEP: Verify that ReplicaSets have been deleted 01/17/23 06:59:46.51
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 17 06:59:46.550: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12393"},"items":null}

    Jan 17 06:59:46.554: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12393"},"items":[{"metadata":{"name":"daemon-set-6kzhn","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"657265d7-d565-4979-9805-1d342d9ecaba","resourceVersion":"12392","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6a9f141805cfbb07adbbab393ddcc647900d2991fcaecb9f1821b0b856b14963","cni.projectcalico.org/podIP":"10.100.135.9/32","cni.projectcalico.org/podIPs":"10.100.135.9/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-27ptm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-27ptm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:45Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:45Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.22","podIP":"10.100.135.9","podIPs":[{"ip":"10.100.135.9"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://368554db0818fc117845a837d5b879a699626bc3485324fe93e4cd70032f9124","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-7v2vz","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"9c491eeb-6340-4641-b06f-b39d1956bd4f","resourceVersion":"12393","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d477288ae3c080ee44576279e4621764d288c023c61e0c35bd6297107b739796","cni.projectcalico.org/podIP":"10.100.168.91/32","cni.projectcalico.org/podIPs":"10.100.168.91/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jrd89","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jrd89","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.21","podIP":"10.100.168.91","podIPs":[{"ip":"10.100.168.91"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ece2f9533e760f41c6f6f7695393a5cb561286725579c6b884486b0a9d07c308","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vhpvp","generateName":"daemon-set-","namespace":"daemonsets-1684","uid":"885c360b-641a-4913-b27c-44f6440e69a7","resourceVersion":"12391","creationTimestamp":"2023-01-17T06:59:43Z","deletionTimestamp":"2023-01-17T07:00:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"70e07c26fcb8d50d734bef481bc5c11d92b23e0b01ea4d288f89c5717022ee50","cni.projectcalico.org/podIP":"10.100.206.11/32","cni.projectcalico.org/podIPs":"10.100.206.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"133ddf7d-2976-48e5-8acf-7a753b6d5abc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133ddf7d-2976-48e5-8acf-7a753b6d5abc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T06:59:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g24rm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g24rm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster125-w73dz53kvqes-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster125-w73dz53kvqes-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T06:59:43Z"}],"hostIP":"10.0.0.16","podIP":"10.100.206.11","podIPs":[{"ip":"10.100.206.11"}],"startTime":"2023-01-17T06:59:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T06:59:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://69b405446b19f56fd472e83f4088b35d34b25cd09d8c7e39fb82a23c7475097b","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 06:59:46.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1684" for this suite. 01/17/23 06:59:46.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:59:46.595
Jan 17 06:59:46.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 06:59:46.596
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:46.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:46.632
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-68733dbf-055d-4c8d-b6b6-b824b04c2ff5 01/17/23 06:59:46.64
STEP: Creating a pod to test consume configMaps 01/17/23 06:59:46.648
Jan 17 06:59:46.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d" in namespace "configmap-1108" to be "Succeeded or Failed"
Jan 17 06:59:46.708: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.762278ms
Jan 17 06:59:48.713: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0251891s
Jan 17 06:59:50.716: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028029516s
STEP: Saw pod success 01/17/23 06:59:50.716
Jan 17 06:59:50.716: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d" satisfied condition "Succeeded or Failed"
Jan 17 06:59:50.721: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d container agnhost-container: <nil>
STEP: delete the pod 01/17/23 06:59:50.788
Jan 17 06:59:50.820: INFO: Waiting for pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d to disappear
Jan 17 06:59:50.827: INFO: Pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 06:59:50.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1108" for this suite. 01/17/23 06:59:50.832
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":90,"skipped":1547,"failed":0}
------------------------------
• [4.252 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:59:46.595
    Jan 17 06:59:46.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 06:59:46.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:46.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:46.632
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-68733dbf-055d-4c8d-b6b6-b824b04c2ff5 01/17/23 06:59:46.64
    STEP: Creating a pod to test consume configMaps 01/17/23 06:59:46.648
    Jan 17 06:59:46.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d" in namespace "configmap-1108" to be "Succeeded or Failed"
    Jan 17 06:59:46.708: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.762278ms
    Jan 17 06:59:48.713: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0251891s
    Jan 17 06:59:50.716: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028029516s
    STEP: Saw pod success 01/17/23 06:59:50.716
    Jan 17 06:59:50.716: INFO: Pod "pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d" satisfied condition "Succeeded or Failed"
    Jan 17 06:59:50.721: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 06:59:50.788
    Jan 17 06:59:50.820: INFO: Waiting for pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d to disappear
    Jan 17 06:59:50.827: INFO: Pod pod-configmaps-f6f9e2b0-6957-4dab-a96a-4b3006fdce6d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 06:59:50.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1108" for this suite. 01/17/23 06:59:50.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 06:59:50.851
Jan 17 06:59:50.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 06:59:50.852
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:50.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:50.886
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/17/23 06:59:50.893
STEP: Ensuring job reaches completions 01/17/23 06:59:50.901
STEP: Ensuring pods with index for job exist 01/17/23 07:00:00.907
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 07:00:00.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8820" for this suite. 01/17/23 07:00:00.918
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":91,"skipped":1566,"failed":0}
------------------------------
• [SLOW TEST] [10.082 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 06:59:50.851
    Jan 17 06:59:50.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 06:59:50.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 06:59:50.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 06:59:50.886
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/17/23 06:59:50.893
    STEP: Ensuring job reaches completions 01/17/23 06:59:50.901
    STEP: Ensuring pods with index for job exist 01/17/23 07:00:00.907
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 07:00:00.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8820" for this suite. 01/17/23 07:00:00.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:00.934
Jan 17 07:00:00.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:00:00.935
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:00.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:00.971
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/17/23 07:00:00.979
Jan 17 07:00:00.994: INFO: Waiting up to 5m0s for pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371" in namespace "emptydir-1461" to be "Succeeded or Failed"
Jan 17 07:00:01.006: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Pending", Reason="", readiness=false. Elapsed: 11.546267ms
Jan 17 07:00:03.016: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021700367s
Jan 17 07:00:05.013: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018622787s
STEP: Saw pod success 01/17/23 07:00:05.013
Jan 17 07:00:05.013: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371" satisfied condition "Succeeded or Failed"
Jan 17 07:00:05.017: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 container test-container: <nil>
STEP: delete the pod 01/17/23 07:00:05.025
Jan 17 07:00:05.054: INFO: Waiting for pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 to disappear
Jan 17 07:00:05.059: INFO: Pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:00:05.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1461" for this suite. 01/17/23 07:00:05.065
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":92,"skipped":1576,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:00.934
    Jan 17 07:00:00.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:00:00.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:00.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:00.971
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/17/23 07:00:00.979
    Jan 17 07:00:00.994: INFO: Waiting up to 5m0s for pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371" in namespace "emptydir-1461" to be "Succeeded or Failed"
    Jan 17 07:00:01.006: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Pending", Reason="", readiness=false. Elapsed: 11.546267ms
    Jan 17 07:00:03.016: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021700367s
    Jan 17 07:00:05.013: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018622787s
    STEP: Saw pod success 01/17/23 07:00:05.013
    Jan 17 07:00:05.013: INFO: Pod "pod-0c44e8ce-e62a-4432-84c4-ae7b68592371" satisfied condition "Succeeded or Failed"
    Jan 17 07:00:05.017: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:00:05.025
    Jan 17 07:00:05.054: INFO: Waiting for pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 to disappear
    Jan 17 07:00:05.059: INFO: Pod pod-0c44e8ce-e62a-4432-84c4-ae7b68592371 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:00:05.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1461" for this suite. 01/17/23 07:00:05.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:05.081
Jan 17 07:00:05.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:00:05.083
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:05.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:05.121
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 17 07:00:05.139: INFO: Waiting up to 5m0s for pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e" in namespace "pods-3353" to be "running and ready"
Jan 17 07:00:05.147: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437211ms
Jan 17 07:00:05.148: INFO: The phase of Pod server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:00:07.152: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012869904s
Jan 17 07:00:07.152: INFO: The phase of Pod server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e is Running (Ready = true)
Jan 17 07:00:07.152: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e" satisfied condition "running and ready"
Jan 17 07:00:07.197: INFO: Waiting up to 5m0s for pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4" in namespace "pods-3353" to be "Succeeded or Failed"
Jan 17 07:00:07.207: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.446531ms
Jan 17 07:00:09.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01427298s
Jan 17 07:00:11.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014607325s
STEP: Saw pod success 01/17/23 07:00:11.212
Jan 17 07:00:11.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4" satisfied condition "Succeeded or Failed"
Jan 17 07:00:11.216: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 container env3cont: <nil>
STEP: delete the pod 01/17/23 07:00:11.223
Jan 17 07:00:11.248: INFO: Waiting for pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 to disappear
Jan 17 07:00:11.254: INFO: Pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:00:11.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3353" for this suite. 01/17/23 07:00:11.261
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":93,"skipped":1607,"failed":0}
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:05.081
    Jan 17 07:00:05.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:00:05.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:05.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:05.121
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 17 07:00:05.139: INFO: Waiting up to 5m0s for pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e" in namespace "pods-3353" to be "running and ready"
    Jan 17 07:00:05.147: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437211ms
    Jan 17 07:00:05.148: INFO: The phase of Pod server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:00:07.152: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012869904s
    Jan 17 07:00:07.152: INFO: The phase of Pod server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e is Running (Ready = true)
    Jan 17 07:00:07.152: INFO: Pod "server-envvars-5f887478-54f6-4bcd-85a5-9f9468dcd65e" satisfied condition "running and ready"
    Jan 17 07:00:07.197: INFO: Waiting up to 5m0s for pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4" in namespace "pods-3353" to be "Succeeded or Failed"
    Jan 17 07:00:07.207: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.446531ms
    Jan 17 07:00:09.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01427298s
    Jan 17 07:00:11.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014607325s
    STEP: Saw pod success 01/17/23 07:00:11.212
    Jan 17 07:00:11.212: INFO: Pod "client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4" satisfied condition "Succeeded or Failed"
    Jan 17 07:00:11.216: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 container env3cont: <nil>
    STEP: delete the pod 01/17/23 07:00:11.223
    Jan 17 07:00:11.248: INFO: Waiting for pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 to disappear
    Jan 17 07:00:11.254: INFO: Pod client-envvars-2df8e244-6a44-47dd-ac6f-dd6f8ae224a4 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:00:11.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3353" for this suite. 01/17/23 07:00:11.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:11.274
Jan 17 07:00:11.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename namespaces 01/17/23 07:00:11.276
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.309
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/17/23 07:00:11.317
STEP: patching the Namespace 01/17/23 07:00:11.343
STEP: get the Namespace and ensuring it has the label 01/17/23 07:00:11.354
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:00:11.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3766" for this suite. 01/17/23 07:00:11.368
STEP: Destroying namespace "nspatchtest-fb543b20-caaa-4241-926c-6e91a3b34429-9700" for this suite. 01/17/23 07:00:11.379
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":94,"skipped":1637,"failed":0}
------------------------------
• [0.114 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:11.274
    Jan 17 07:00:11.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename namespaces 01/17/23 07:00:11.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.309
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/17/23 07:00:11.317
    STEP: patching the Namespace 01/17/23 07:00:11.343
    STEP: get the Namespace and ensuring it has the label 01/17/23 07:00:11.354
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:00:11.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3766" for this suite. 01/17/23 07:00:11.368
    STEP: Destroying namespace "nspatchtest-fb543b20-caaa-4241-926c-6e91a3b34429-9700" for this suite. 01/17/23 07:00:11.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:11.401
Jan 17 07:00:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:00:11.403
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.451
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/17/23 07:00:11.458
STEP: fetching the ConfigMap 01/17/23 07:00:11.467
STEP: patching the ConfigMap 01/17/23 07:00:11.472
STEP: listing all ConfigMaps in all namespaces with a label selector 01/17/23 07:00:11.482
STEP: deleting the ConfigMap by collection with a label selector 01/17/23 07:00:11.5
STEP: listing all ConfigMaps in test namespace 01/17/23 07:00:11.514
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:00:11.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4035" for this suite. 01/17/23 07:00:11.525
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":95,"skipped":1684,"failed":0}
------------------------------
• [0.133 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:11.401
    Jan 17 07:00:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:00:11.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.451
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/17/23 07:00:11.458
    STEP: fetching the ConfigMap 01/17/23 07:00:11.467
    STEP: patching the ConfigMap 01/17/23 07:00:11.472
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/17/23 07:00:11.482
    STEP: deleting the ConfigMap by collection with a label selector 01/17/23 07:00:11.5
    STEP: listing all ConfigMaps in test namespace 01/17/23 07:00:11.514
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:00:11.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4035" for this suite. 01/17/23 07:00:11.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:11.536
Jan 17 07:00:11.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:00:11.537
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.568
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:00:11.576
Jan 17 07:00:11.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9" in namespace "downward-api-1423" to be "Succeeded or Failed"
Jan 17 07:00:11.599: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.673991ms
Jan 17 07:00:13.604: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011771844s
Jan 17 07:00:15.604: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012165079s
STEP: Saw pod success 01/17/23 07:00:15.604
Jan 17 07:00:15.605: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9" satisfied condition "Succeeded or Failed"
Jan 17 07:00:15.609: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 container client-container: <nil>
STEP: delete the pod 01/17/23 07:00:15.677
Jan 17 07:00:15.704: INFO: Waiting for pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 to disappear
Jan 17 07:00:15.709: INFO: Pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:00:15.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1423" for this suite. 01/17/23 07:00:15.715
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":96,"skipped":1704,"failed":0}
------------------------------
• [4.188 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:11.536
    Jan 17 07:00:11.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:00:11.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:11.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:11.568
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:00:11.576
    Jan 17 07:00:11.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9" in namespace "downward-api-1423" to be "Succeeded or Failed"
    Jan 17 07:00:11.599: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.673991ms
    Jan 17 07:00:13.604: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011771844s
    Jan 17 07:00:15.604: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012165079s
    STEP: Saw pod success 01/17/23 07:00:15.604
    Jan 17 07:00:15.605: INFO: Pod "downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9" satisfied condition "Succeeded or Failed"
    Jan 17 07:00:15.609: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 container client-container: <nil>
    STEP: delete the pod 01/17/23 07:00:15.677
    Jan 17 07:00:15.704: INFO: Waiting for pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 to disappear
    Jan 17 07:00:15.709: INFO: Pod downwardapi-volume-79e3e923-9666-411c-9c0b-01ebcb1a46d9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:00:15.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1423" for this suite. 01/17/23 07:00:15.715
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:15.724
Jan 17 07:00:15.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:00:15.727
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:15.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:15.759
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/17/23 07:00:15.764
Jan 17 07:00:15.765: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 17 07:00:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:17.992: INFO: stderr: ""
Jan 17 07:00:17.992: INFO: stdout: "service/agnhost-replica created\n"
Jan 17 07:00:17.992: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 17 07:00:17.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:19.608: INFO: stderr: ""
Jan 17 07:00:19.609: INFO: stdout: "service/agnhost-primary created\n"
Jan 17 07:00:19.609: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 17 07:00:19.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:21.234: INFO: stderr: ""
Jan 17 07:00:21.234: INFO: stdout: "service/frontend created\n"
Jan 17 07:00:21.235: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 17 07:00:21.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:21.733: INFO: stderr: ""
Jan 17 07:00:21.733: INFO: stdout: "deployment.apps/frontend created\n"
Jan 17 07:00:21.734: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 07:00:21.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:22.278: INFO: stderr: ""
Jan 17 07:00:22.278: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 17 07:00:22.279: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 07:00:22.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
Jan 17 07:00:22.788: INFO: stderr: ""
Jan 17 07:00:22.788: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/17/23 07:00:22.788
Jan 17 07:00:22.788: INFO: Waiting for all frontend pods to be Running.
Jan 17 07:00:27.840: INFO: Waiting for frontend to serve content.
Jan 17 07:00:27.852: INFO: Trying to add a new entry to the guestbook.
Jan 17 07:00:27.866: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/17/23 07:00:27.88
Jan 17 07:00:27.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.044: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.044: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 07:00:28.044
Jan 17 07:00:28.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.230: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.230: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 07:00:28.23
Jan 17 07:00:28.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.389: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.389: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 07:00:28.39
Jan 17 07:00:28.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.527: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.527: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 07:00:28.527
Jan 17 07:00:28.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.714: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.714: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 07:00:28.714
Jan 17 07:00:28.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
Jan 17 07:00:28.885: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:00:28.885: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:00:28.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7837" for this suite. 01/17/23 07:00:28.902
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":97,"skipped":1706,"failed":0}
------------------------------
• [SLOW TEST] [13.221 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:15.724
    Jan 17 07:00:15.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:00:15.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:15.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:15.759
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/17/23 07:00:15.764
    Jan 17 07:00:15.765: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 17 07:00:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:17.992: INFO: stderr: ""
    Jan 17 07:00:17.992: INFO: stdout: "service/agnhost-replica created\n"
    Jan 17 07:00:17.992: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 17 07:00:17.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:19.608: INFO: stderr: ""
    Jan 17 07:00:19.609: INFO: stdout: "service/agnhost-primary created\n"
    Jan 17 07:00:19.609: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 17 07:00:19.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:21.234: INFO: stderr: ""
    Jan 17 07:00:21.234: INFO: stdout: "service/frontend created\n"
    Jan 17 07:00:21.235: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 17 07:00:21.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:21.733: INFO: stderr: ""
    Jan 17 07:00:21.733: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 17 07:00:21.734: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 17 07:00:21.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:22.278: INFO: stderr: ""
    Jan 17 07:00:22.278: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 17 07:00:22.279: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 17 07:00:22.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 create -f -'
    Jan 17 07:00:22.788: INFO: stderr: ""
    Jan 17 07:00:22.788: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/17/23 07:00:22.788
    Jan 17 07:00:22.788: INFO: Waiting for all frontend pods to be Running.
    Jan 17 07:00:27.840: INFO: Waiting for frontend to serve content.
    Jan 17 07:00:27.852: INFO: Trying to add a new entry to the guestbook.
    Jan 17 07:00:27.866: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/17/23 07:00:27.88
    Jan 17 07:00:27.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.044: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.044: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 07:00:28.044
    Jan 17 07:00:28.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.230: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.230: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 07:00:28.23
    Jan 17 07:00:28.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.389: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.389: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 07:00:28.39
    Jan 17 07:00:28.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.527: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.527: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 07:00:28.527
    Jan 17 07:00:28.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.714: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.714: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 07:00:28.714
    Jan 17 07:00:28.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7837 delete --grace-period=0 --force -f -'
    Jan 17 07:00:28.885: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:00:28.885: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:00:28.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7837" for this suite. 01/17/23 07:00:28.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:28.967
Jan 17 07:00:28.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:00:28.97
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:29.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:29.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:00:29.123
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:00:30.176
STEP: Deploying the webhook pod 01/17/23 07:00:30.192
STEP: Wait for the deployment to be ready 01/17/23 07:00:30.221
Jan 17 07:00:30.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 17 07:00:32.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/17/23 07:00:34.281
STEP: Verifying the service has paired with the endpoint 01/17/23 07:00:34.302
Jan 17 07:00:35.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/17/23 07:00:35.308
STEP: create a namespace for the webhook 01/17/23 07:00:35.337
STEP: create a configmap should be unconditionally rejected by the webhook 01/17/23 07:00:35.36
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:00:35.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9935" for this suite. 01/17/23 07:00:35.435
STEP: Destroying namespace "webhook-9935-markers" for this suite. 01/17/23 07:00:35.45
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":98,"skipped":1722,"failed":0}
------------------------------
• [SLOW TEST] [6.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:28.967
    Jan 17 07:00:28.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:00:28.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:29.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:29.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:00:29.123
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:00:30.176
    STEP: Deploying the webhook pod 01/17/23 07:00:30.192
    STEP: Wait for the deployment to be ready 01/17/23 07:00:30.221
    Jan 17 07:00:30.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 17 07:00:32.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 0, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/17/23 07:00:34.281
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:00:34.302
    Jan 17 07:00:35.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/17/23 07:00:35.308
    STEP: create a namespace for the webhook 01/17/23 07:00:35.337
    STEP: create a configmap should be unconditionally rejected by the webhook 01/17/23 07:00:35.36
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:00:35.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9935" for this suite. 01/17/23 07:00:35.435
    STEP: Destroying namespace "webhook-9935-markers" for this suite. 01/17/23 07:00:35.45
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:00:35.554
Jan 17 07:00:35.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-watch 01/17/23 07:00:35.556
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:35.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:35.673
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 17 07:00:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Creating first CR  01/17/23 07:00:38.291
Jan 17 07:00:38.298: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:38Z]] name:name1 resourceVersion:13204 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/17/23 07:00:48.299
Jan 17 07:00:48.313: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:48Z]] name:name2 resourceVersion:13245 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/17/23 07:00:58.315
Jan 17 07:00:58.330: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:58Z]] name:name1 resourceVersion:13270 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/17/23 07:01:08.332
Jan 17 07:01:08.345: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:01:08Z]] name:name2 resourceVersion:13295 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/17/23 07:01:18.347
Jan 17 07:01:18.361: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:58Z]] name:name1 resourceVersion:13320 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/17/23 07:01:28.363
Jan 17 07:01:28.375: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:01:08Z]] name:name2 resourceVersion:13345 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:01:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5550" for this suite. 01/17/23 07:01:38.907
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":99,"skipped":1724,"failed":0}
------------------------------
• [SLOW TEST] [63.364 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:00:35.554
    Jan 17 07:00:35.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-watch 01/17/23 07:00:35.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:00:35.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:00:35.673
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 17 07:00:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Creating first CR  01/17/23 07:00:38.291
    Jan 17 07:00:38.298: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:38Z]] name:name1 resourceVersion:13204 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/17/23 07:00:48.299
    Jan 17 07:00:48.313: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:48Z]] name:name2 resourceVersion:13245 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/17/23 07:00:58.315
    Jan 17 07:00:58.330: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:58Z]] name:name1 resourceVersion:13270 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/17/23 07:01:08.332
    Jan 17 07:01:08.345: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:01:08Z]] name:name2 resourceVersion:13295 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/17/23 07:01:18.347
    Jan 17 07:01:18.361: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:00:58Z]] name:name1 resourceVersion:13320 uid:085f064b-eeee-4403-8976-40da0ce51bc1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/17/23 07:01:28.363
    Jan 17 07:01:28.375: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T07:00:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T07:01:08Z]] name:name2 resourceVersion:13345 uid:867b2bcd-5c32-438e-9402-0bd336c2b731] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:01:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-5550" for this suite. 01/17/23 07:01:38.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:38.92
Jan 17 07:01:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:01:38.921
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:38.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:38.955
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/17/23 07:01:38.965
STEP: waiting for available Endpoint 01/17/23 07:01:38.978
STEP: listing all Endpoints 01/17/23 07:01:38.982
STEP: updating the Endpoint 01/17/23 07:01:38.991
STEP: fetching the Endpoint 01/17/23 07:01:39.011
STEP: patching the Endpoint 01/17/23 07:01:39.018
STEP: fetching the Endpoint 01/17/23 07:01:39.045
STEP: deleting the Endpoint by Collection 01/17/23 07:01:39.049
STEP: waiting for Endpoint deletion 01/17/23 07:01:39.068
STEP: fetching the Endpoint 01/17/23 07:01:39.074
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:01:39.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2745" for this suite. 01/17/23 07:01:39.084
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":100,"skipped":1735,"failed":0}
------------------------------
• [0.176 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:38.92
    Jan 17 07:01:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:01:38.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:38.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:38.955
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/17/23 07:01:38.965
    STEP: waiting for available Endpoint 01/17/23 07:01:38.978
    STEP: listing all Endpoints 01/17/23 07:01:38.982
    STEP: updating the Endpoint 01/17/23 07:01:38.991
    STEP: fetching the Endpoint 01/17/23 07:01:39.011
    STEP: patching the Endpoint 01/17/23 07:01:39.018
    STEP: fetching the Endpoint 01/17/23 07:01:39.045
    STEP: deleting the Endpoint by Collection 01/17/23 07:01:39.049
    STEP: waiting for Endpoint deletion 01/17/23 07:01:39.068
    STEP: fetching the Endpoint 01/17/23 07:01:39.074
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:01:39.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2745" for this suite. 01/17/23 07:01:39.084
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:39.096
Jan 17 07:01:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 07:01:39.097
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:39.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:39.129
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 17 07:01:39.173: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 07:01:44.179: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 07:01:44.179
STEP: Scaling up "test-rs" replicaset  01/17/23 07:01:44.179
Jan 17 07:01:44.207: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/17/23 07:01:44.207
W0117 07:01:44.240322      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 07:01:44.250: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 07:01:44.260: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 07:01:44.287: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 07:01:44.315: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 07:01:45.293: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 2, AvailableReplicas 2
Jan 17 07:01:52.058: INFO: observed Replicaset test-rs in namespace replicaset-7215 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 07:01:52.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7215" for this suite. 01/17/23 07:01:52.065
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":101,"skipped":1735,"failed":0}
------------------------------
• [SLOW TEST] [12.984 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:39.096
    Jan 17 07:01:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 07:01:39.097
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:39.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:39.129
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 17 07:01:39.173: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 07:01:44.179: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 07:01:44.179
    STEP: Scaling up "test-rs" replicaset  01/17/23 07:01:44.179
    Jan 17 07:01:44.207: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/17/23 07:01:44.207
    W0117 07:01:44.240322      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 07:01:44.250: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 07:01:44.260: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 07:01:44.287: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 07:01:44.315: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 07:01:45.293: INFO: observed ReplicaSet test-rs in namespace replicaset-7215 with ReadyReplicas 2, AvailableReplicas 2
    Jan 17 07:01:52.058: INFO: observed Replicaset test-rs in namespace replicaset-7215 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 07:01:52.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7215" for this suite. 01/17/23 07:01:52.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:52.082
Jan 17 07:01:52.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:01:52.083
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:52.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:52.115
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/17/23 07:01:52.125
Jan 17 07:01:52.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4435 cluster-info'
Jan 17 07:01:52.253: INFO: stderr: ""
Jan 17 07:01:52.253: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:01:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4435" for this suite. 01/17/23 07:01:52.258
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":102,"skipped":1755,"failed":0}
------------------------------
• [0.188 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:52.082
    Jan 17 07:01:52.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:01:52.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:52.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:52.115
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/17/23 07:01:52.125
    Jan 17 07:01:52.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4435 cluster-info'
    Jan 17 07:01:52.253: INFO: stderr: ""
    Jan 17 07:01:52.253: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:01:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4435" for this suite. 01/17/23 07:01:52.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:52.274
Jan 17 07:01:52.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:01:52.275
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:52.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:52.315
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 07:01:52.321
Jan 17 07:01:52.342: INFO: Waiting up to 5m0s for pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5" in namespace "emptydir-1789" to be "Succeeded or Failed"
Jan 17 07:01:52.349: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885178ms
Jan 17 07:01:54.356: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Running", Reason="", readiness=false. Elapsed: 2.013776376s
Jan 17 07:01:56.357: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Running", Reason="", readiness=false. Elapsed: 4.014575008s
Jan 17 07:01:58.363: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021459409s
STEP: Saw pod success 01/17/23 07:01:58.363
Jan 17 07:01:58.364: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5" satisfied condition "Succeeded or Failed"
Jan 17 07:01:58.367: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 container test-container: <nil>
STEP: delete the pod 01/17/23 07:01:58.436
Jan 17 07:01:58.456: INFO: Waiting for pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 to disappear
Jan 17 07:01:58.465: INFO: Pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:01:58.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1789" for this suite. 01/17/23 07:01:58.474
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":103,"skipped":1784,"failed":0}
------------------------------
• [SLOW TEST] [6.212 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:52.274
    Jan 17 07:01:52.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:01:52.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:52.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:52.315
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 07:01:52.321
    Jan 17 07:01:52.342: INFO: Waiting up to 5m0s for pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5" in namespace "emptydir-1789" to be "Succeeded or Failed"
    Jan 17 07:01:52.349: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885178ms
    Jan 17 07:01:54.356: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Running", Reason="", readiness=false. Elapsed: 2.013776376s
    Jan 17 07:01:56.357: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Running", Reason="", readiness=false. Elapsed: 4.014575008s
    Jan 17 07:01:58.363: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021459409s
    STEP: Saw pod success 01/17/23 07:01:58.363
    Jan 17 07:01:58.364: INFO: Pod "pod-aa337398-f74f-48c3-9769-97afc7a944d5" satisfied condition "Succeeded or Failed"
    Jan 17 07:01:58.367: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:01:58.436
    Jan 17 07:01:58.456: INFO: Waiting for pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 to disappear
    Jan 17 07:01:58.465: INFO: Pod pod-aa337398-f74f-48c3-9769-97afc7a944d5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:01:58.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1789" for this suite. 01/17/23 07:01:58.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:58.488
Jan 17 07:01:58.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:01:58.489
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:58.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:58.517
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/17/23 07:01:58.524
STEP: Getting a ResourceQuota 01/17/23 07:01:58.544
STEP: Listing all ResourceQuotas with LabelSelector 01/17/23 07:01:58.549
STEP: Patching the ResourceQuota 01/17/23 07:01:58.554
STEP: Deleting a Collection of ResourceQuotas 01/17/23 07:01:58.567
STEP: Verifying the deleted ResourceQuota 01/17/23 07:01:58.585
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:01:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3524" for this suite. 01/17/23 07:01:58.597
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":104,"skipped":1818,"failed":0}
------------------------------
• [0.118 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:58.488
    Jan 17 07:01:58.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:01:58.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:58.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:58.517
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/17/23 07:01:58.524
    STEP: Getting a ResourceQuota 01/17/23 07:01:58.544
    STEP: Listing all ResourceQuotas with LabelSelector 01/17/23 07:01:58.549
    STEP: Patching the ResourceQuota 01/17/23 07:01:58.554
    STEP: Deleting a Collection of ResourceQuotas 01/17/23 07:01:58.567
    STEP: Verifying the deleted ResourceQuota 01/17/23 07:01:58.585
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:01:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3524" for this suite. 01/17/23 07:01:58.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:01:58.609
Jan 17 07:01:58.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 07:01:58.61
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:58.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:58.641
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 07:01:58.684
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:01:58.693
Jan 17 07:01:58.699: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:58.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:58.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:58.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:01:58.706: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:01:59.712: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:59.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:59.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:01:59.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:01:59.722: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:00.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:02:00.719: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/17/23 07:02:00.725
Jan 17 07:02:00.732: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/17/23 07:02:00.732
Jan 17 07:02:00.751: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/17/23 07:02:00.751
Jan 17 07:02:00.758: INFO: Observed &DaemonSet event: ADDED
Jan 17 07:02:00.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.760: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.760: INFO: Found daemon set daemon-set in namespace daemonsets-3705 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 07:02:00.760: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/17/23 07:02:00.76
STEP: watching for the daemon set status to be patched 01/17/23 07:02:00.771
Jan 17 07:02:00.773: INFO: Observed &DaemonSet event: ADDED
Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.775: INFO: Observed daemon set daemon-set in namespace daemonsets-3705 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 07:02:00.775: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 07:02:00.775: INFO: Found daemon set daemon-set in namespace daemonsets-3705 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 17 07:02:00.775: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:02:00.791
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3705, will wait for the garbage collector to delete the pods 01/17/23 07:02:00.791
Jan 17 07:02:00.854: INFO: Deleting DaemonSet.extensions daemon-set took: 9.739734ms
Jan 17 07:02:00.955: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.13488ms
Jan 17 07:02:03.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:02:03.461: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 07:02:03.464: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13663"},"items":null}

Jan 17 07:02:03.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:02:03.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3705" for this suite. 01/17/23 07:02:03.515
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":105,"skipped":1843,"failed":0}
------------------------------
• [4.918 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:01:58.609
    Jan 17 07:01:58.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 07:01:58.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:01:58.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:01:58.641
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 07:01:58.684
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:01:58.693
    Jan 17 07:01:58.699: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:58.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:58.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:58.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:01:58.706: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:01:59.712: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:59.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:59.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:01:59.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:01:59.722: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:00.713: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:00.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:02:00.719: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/17/23 07:02:00.725
    Jan 17 07:02:00.732: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/17/23 07:02:00.732
    Jan 17 07:02:00.751: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/17/23 07:02:00.751
    Jan 17 07:02:00.758: INFO: Observed &DaemonSet event: ADDED
    Jan 17 07:02:00.758: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.759: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.760: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.760: INFO: Found daemon set daemon-set in namespace daemonsets-3705 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 07:02:00.760: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/17/23 07:02:00.76
    STEP: watching for the daemon set status to be patched 01/17/23 07:02:00.771
    Jan 17 07:02:00.773: INFO: Observed &DaemonSet event: ADDED
    Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.774: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.775: INFO: Observed daemon set daemon-set in namespace daemonsets-3705 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 07:02:00.775: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 07:02:00.775: INFO: Found daemon set daemon-set in namespace daemonsets-3705 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 17 07:02:00.775: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:02:00.791
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3705, will wait for the garbage collector to delete the pods 01/17/23 07:02:00.791
    Jan 17 07:02:00.854: INFO: Deleting DaemonSet.extensions daemon-set took: 9.739734ms
    Jan 17 07:02:00.955: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.13488ms
    Jan 17 07:02:03.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:02:03.461: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 07:02:03.464: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13663"},"items":null}

    Jan 17 07:02:03.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13663"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:02:03.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3705" for this suite. 01/17/23 07:02:03.515
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:03.527
Jan 17 07:02:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:02:03.529
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:03.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:03.574
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-c7c2a9d3-5ce5-431e-872a-33b03ff2e573 01/17/23 07:02:03.586
STEP: Creating a pod to test consume configMaps 01/17/23 07:02:03.605
Jan 17 07:02:03.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d" in namespace "configmap-1735" to be "Succeeded or Failed"
Jan 17 07:02:03.639: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.594659ms
Jan 17 07:02:05.645: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015769017s
Jan 17 07:02:07.646: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01687235s
STEP: Saw pod success 01/17/23 07:02:07.646
Jan 17 07:02:07.646: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d" satisfied condition "Succeeded or Failed"
Jan 17 07:02:07.649: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:02:07.657
Jan 17 07:02:07.678: INFO: Waiting for pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d to disappear
Jan 17 07:02:07.684: INFO: Pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:02:07.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1735" for this suite. 01/17/23 07:02:07.692
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":106,"skipped":1845,"failed":0}
------------------------------
• [4.175 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:03.527
    Jan 17 07:02:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:02:03.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:03.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:03.574
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-c7c2a9d3-5ce5-431e-872a-33b03ff2e573 01/17/23 07:02:03.586
    STEP: Creating a pod to test consume configMaps 01/17/23 07:02:03.605
    Jan 17 07:02:03.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d" in namespace "configmap-1735" to be "Succeeded or Failed"
    Jan 17 07:02:03.639: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.594659ms
    Jan 17 07:02:05.645: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015769017s
    Jan 17 07:02:07.646: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01687235s
    STEP: Saw pod success 01/17/23 07:02:07.646
    Jan 17 07:02:07.646: INFO: Pod "pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d" satisfied condition "Succeeded or Failed"
    Jan 17 07:02:07.649: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:02:07.657
    Jan 17 07:02:07.678: INFO: Waiting for pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d to disappear
    Jan 17 07:02:07.684: INFO: Pod pod-configmaps-c962963d-a533-4a0e-8d98-37ac5a3a9a7d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:02:07.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1735" for this suite. 01/17/23 07:02:07.692
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:07.704
Jan 17 07:02:07.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:02:07.706
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:07.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:07.732
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 07:02:07.738
Jan 17 07:02:07.752: INFO: Waiting up to 5m0s for pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5" in namespace "emptydir-2280" to be "Succeeded or Failed"
Jan 17 07:02:07.759: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846888ms
Jan 17 07:02:09.764: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012227099s
Jan 17 07:02:11.764: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012172082s
STEP: Saw pod success 01/17/23 07:02:11.764
Jan 17 07:02:11.765: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5" satisfied condition "Succeeded or Failed"
Jan 17 07:02:11.770: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 container test-container: <nil>
STEP: delete the pod 01/17/23 07:02:11.779
Jan 17 07:02:11.802: INFO: Waiting for pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 to disappear
Jan 17 07:02:11.810: INFO: Pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:02:11.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2280" for this suite. 01/17/23 07:02:11.818
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":107,"skipped":1845,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:07.704
    Jan 17 07:02:07.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:02:07.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:07.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:07.732
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 07:02:07.738
    Jan 17 07:02:07.752: INFO: Waiting up to 5m0s for pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5" in namespace "emptydir-2280" to be "Succeeded or Failed"
    Jan 17 07:02:07.759: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846888ms
    Jan 17 07:02:09.764: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012227099s
    Jan 17 07:02:11.764: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012172082s
    STEP: Saw pod success 01/17/23 07:02:11.764
    Jan 17 07:02:11.765: INFO: Pod "pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5" satisfied condition "Succeeded or Failed"
    Jan 17 07:02:11.770: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:02:11.779
    Jan 17 07:02:11.802: INFO: Waiting for pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 to disappear
    Jan 17 07:02:11.810: INFO: Pod pod-4e70926d-52d8-4204-9c05-1fa4060d3ce5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:02:11.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2280" for this suite. 01/17/23 07:02:11.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:11.835
Jan 17 07:02:11.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:02:11.836
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:11.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:11.872
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/17/23 07:02:11.879
Jan 17 07:02:11.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 create -f -'
Jan 17 07:02:13.703: INFO: stderr: ""
Jan 17 07:02:13.703: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:02:13.703
Jan 17 07:02:13.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:02:13.869: INFO: stderr: ""
Jan 17 07:02:13.869: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
Jan 17 07:02:13.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:02:13.984: INFO: stderr: ""
Jan 17 07:02:13.984: INFO: stdout: ""
Jan 17 07:02:13.984: INFO: update-demo-nautilus-87gzl is created but not running
Jan 17 07:02:18.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:02:19.129: INFO: stderr: ""
Jan 17 07:02:19.129: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
Jan 17 07:02:19.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:02:19.248: INFO: stderr: ""
Jan 17 07:02:19.248: INFO: stdout: ""
Jan 17 07:02:19.248: INFO: update-demo-nautilus-87gzl is created but not running
Jan 17 07:02:24.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:02:24.408: INFO: stderr: ""
Jan 17 07:02:24.408: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
Jan 17 07:02:24.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:02:24.561: INFO: stderr: ""
Jan 17 07:02:24.561: INFO: stdout: ""
Jan 17 07:02:24.561: INFO: update-demo-nautilus-87gzl is created but not running
Jan 17 07:02:29.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:02:29.746: INFO: stderr: ""
Jan 17 07:02:29.746: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
Jan 17 07:02:29.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:02:29.941: INFO: stderr: ""
Jan 17 07:02:29.941: INFO: stdout: "true"
Jan 17 07:02:29.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:02:30.115: INFO: stderr: ""
Jan 17 07:02:30.115: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:02:30.115: INFO: validating pod update-demo-nautilus-87gzl
Jan 17 07:02:30.123: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:02:30.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:02:30.123: INFO: update-demo-nautilus-87gzl is verified up and running
Jan 17 07:02:30.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-qfg2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:02:30.307: INFO: stderr: ""
Jan 17 07:02:30.307: INFO: stdout: "true"
Jan 17 07:02:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-qfg2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:02:30.425: INFO: stderr: ""
Jan 17 07:02:30.425: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:02:30.425: INFO: validating pod update-demo-nautilus-qfg2s
Jan 17 07:02:30.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:02:30.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:02:30.432: INFO: update-demo-nautilus-qfg2s is verified up and running
STEP: using delete to clean up resources 01/17/23 07:02:30.432
Jan 17 07:02:30.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 delete --grace-period=0 --force -f -'
Jan 17 07:02:30.622: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:02:30.622: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 07:02:30.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get rc,svc -l name=update-demo --no-headers'
Jan 17 07:02:30.776: INFO: stderr: "No resources found in kubectl-9372 namespace.\n"
Jan 17 07:02:30.776: INFO: stdout: ""
Jan 17 07:02:30.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 07:02:31.000: INFO: stderr: ""
Jan 17 07:02:31.000: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:02:31.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9372" for this suite. 01/17/23 07:02:31.012
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":108,"skipped":1855,"failed":0}
------------------------------
• [SLOW TEST] [19.194 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:11.835
    Jan 17 07:02:11.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:02:11.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:11.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:11.872
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/17/23 07:02:11.879
    Jan 17 07:02:11.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 create -f -'
    Jan 17 07:02:13.703: INFO: stderr: ""
    Jan 17 07:02:13.703: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:02:13.703
    Jan 17 07:02:13.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:02:13.869: INFO: stderr: ""
    Jan 17 07:02:13.869: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
    Jan 17 07:02:13.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:02:13.984: INFO: stderr: ""
    Jan 17 07:02:13.984: INFO: stdout: ""
    Jan 17 07:02:13.984: INFO: update-demo-nautilus-87gzl is created but not running
    Jan 17 07:02:18.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:02:19.129: INFO: stderr: ""
    Jan 17 07:02:19.129: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
    Jan 17 07:02:19.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:02:19.248: INFO: stderr: ""
    Jan 17 07:02:19.248: INFO: stdout: ""
    Jan 17 07:02:19.248: INFO: update-demo-nautilus-87gzl is created but not running
    Jan 17 07:02:24.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:02:24.408: INFO: stderr: ""
    Jan 17 07:02:24.408: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
    Jan 17 07:02:24.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:02:24.561: INFO: stderr: ""
    Jan 17 07:02:24.561: INFO: stdout: ""
    Jan 17 07:02:24.561: INFO: update-demo-nautilus-87gzl is created but not running
    Jan 17 07:02:29.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:02:29.746: INFO: stderr: ""
    Jan 17 07:02:29.746: INFO: stdout: "update-demo-nautilus-87gzl update-demo-nautilus-qfg2s "
    Jan 17 07:02:29.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:02:29.941: INFO: stderr: ""
    Jan 17 07:02:29.941: INFO: stdout: "true"
    Jan 17 07:02:29.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-87gzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:02:30.115: INFO: stderr: ""
    Jan 17 07:02:30.115: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:02:30.115: INFO: validating pod update-demo-nautilus-87gzl
    Jan 17 07:02:30.123: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:02:30.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:02:30.123: INFO: update-demo-nautilus-87gzl is verified up and running
    Jan 17 07:02:30.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-qfg2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:02:30.307: INFO: stderr: ""
    Jan 17 07:02:30.307: INFO: stdout: "true"
    Jan 17 07:02:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods update-demo-nautilus-qfg2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:02:30.425: INFO: stderr: ""
    Jan 17 07:02:30.425: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:02:30.425: INFO: validating pod update-demo-nautilus-qfg2s
    Jan 17 07:02:30.432: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:02:30.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:02:30.432: INFO: update-demo-nautilus-qfg2s is verified up and running
    STEP: using delete to clean up resources 01/17/23 07:02:30.432
    Jan 17 07:02:30.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 delete --grace-period=0 --force -f -'
    Jan 17 07:02:30.622: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:02:30.622: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 17 07:02:30.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get rc,svc -l name=update-demo --no-headers'
    Jan 17 07:02:30.776: INFO: stderr: "No resources found in kubectl-9372 namespace.\n"
    Jan 17 07:02:30.776: INFO: stdout: ""
    Jan 17 07:02:30.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-9372 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 07:02:31.000: INFO: stderr: ""
    Jan 17 07:02:31.000: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:02:31.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9372" for this suite. 01/17/23 07:02:31.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:31.03
Jan 17 07:02:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 07:02:31.034
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:31.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:31.08
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/17/23 07:02:31.1
STEP: Verify that the required pods have come up. 01/17/23 07:02:31.111
Jan 17 07:02:31.124: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 07:02:36.129: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 07:02:36.129
STEP: Getting /status 01/17/23 07:02:36.129
Jan 17 07:02:36.135: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/17/23 07:02:36.135
Jan 17 07:02:36.165: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/17/23 07:02:36.165
Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: ADDED
Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.175: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.175: INFO: Found replicaset test-rs in namespace replicaset-5962 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 07:02:36.175: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/17/23 07:02:36.175
Jan 17 07:02:36.175: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 07:02:36.192: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/17/23 07:02:36.192
Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: ADDED
Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.196: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.196: INFO: Observed replicaset test-rs in namespace replicaset-5962 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 07:02:36.196: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 07:02:36.196: INFO: Found replicaset test-rs in namespace replicaset-5962 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 07:02:36.196: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 07:02:36.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5962" for this suite. 01/17/23 07:02:36.201
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":109,"skipped":1862,"failed":0}
------------------------------
• [SLOW TEST] [5.187 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:31.03
    Jan 17 07:02:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 07:02:31.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:31.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:31.08
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/17/23 07:02:31.1
    STEP: Verify that the required pods have come up. 01/17/23 07:02:31.111
    Jan 17 07:02:31.124: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 07:02:36.129: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 07:02:36.129
    STEP: Getting /status 01/17/23 07:02:36.129
    Jan 17 07:02:36.135: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/17/23 07:02:36.135
    Jan 17 07:02:36.165: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/17/23 07:02:36.165
    Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: ADDED
    Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.174: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.175: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.175: INFO: Found replicaset test-rs in namespace replicaset-5962 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 07:02:36.175: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/17/23 07:02:36.175
    Jan 17 07:02:36.175: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 07:02:36.192: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/17/23 07:02:36.192
    Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: ADDED
    Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.195: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.196: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.196: INFO: Observed replicaset test-rs in namespace replicaset-5962 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 07:02:36.196: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 07:02:36.196: INFO: Found replicaset test-rs in namespace replicaset-5962 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 17 07:02:36.196: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 07:02:36.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5962" for this suite. 01/17/23 07:02:36.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:36.219
Jan 17 07:02:36.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:02:36.221
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:36.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:36.254
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:02:36.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-316" for this suite. 01/17/23 07:02:36.278
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":110,"skipped":1898,"failed":0}
------------------------------
• [0.073 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:36.219
    Jan 17 07:02:36.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:02:36.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:36.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:36.254
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:02:36.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-316" for this suite. 01/17/23 07:02:36.278
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:36.301
Jan 17 07:02:36.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 07:02:36.303
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:36.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:36.344
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 17 07:02:36.393: INFO: Create a RollingUpdate DaemonSet
Jan 17 07:02:36.405: INFO: Check that daemon pods launch on every node of the cluster
Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:36.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:02:36.429: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:37.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:02:37.445: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:38.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 07:02:38.443: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:39.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:02:39.441: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 17 07:02:39.441: INFO: Update the DaemonSet to trigger a rollout
Jan 17 07:02:39.460: INFO: Updating DaemonSet daemon-set
Jan 17 07:02:41.521: INFO: Roll back the DaemonSet before rollout is complete
Jan 17 07:02:41.556: INFO: Updating DaemonSet daemon-set
Jan 17 07:02:41.556: INFO: Make sure DaemonSet rollback is complete
Jan 17 07:02:41.571: INFO: Wrong image for pod: daemon-set-cxs42. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 17 07:02:41.571: INFO: Pod daemon-set-cxs42 is not available
Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:46.601: INFO: Pod daemon-set-t4fhd is not available
Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:02:46.617
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7174, will wait for the garbage collector to delete the pods 01/17/23 07:02:46.617
Jan 17 07:02:46.688: INFO: Deleting DaemonSet.extensions daemon-set took: 14.804064ms
Jan 17 07:02:46.788: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.867676ms
Jan 17 07:02:48.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:02:48.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 07:02:48.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14126"},"items":null}

Jan 17 07:02:48.501: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14126"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:02:48.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7174" for this suite. 01/17/23 07:02:48.52
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":111,"skipped":1925,"failed":0}
------------------------------
• [SLOW TEST] [12.231 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:36.301
    Jan 17 07:02:36.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 07:02:36.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:36.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:36.344
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 17 07:02:36.393: INFO: Create a RollingUpdate DaemonSet
    Jan 17 07:02:36.405: INFO: Check that daemon pods launch on every node of the cluster
    Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:36.411: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:36.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:02:36.429: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:37.440: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:37.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:02:37.445: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:38.438: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:38.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 07:02:38.443: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:39.436: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:39.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:02:39.441: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan 17 07:02:39.441: INFO: Update the DaemonSet to trigger a rollout
    Jan 17 07:02:39.460: INFO: Updating DaemonSet daemon-set
    Jan 17 07:02:41.521: INFO: Roll back the DaemonSet before rollout is complete
    Jan 17 07:02:41.556: INFO: Updating DaemonSet daemon-set
    Jan 17 07:02:41.556: INFO: Make sure DaemonSet rollback is complete
    Jan 17 07:02:41.571: INFO: Wrong image for pod: daemon-set-cxs42. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 17 07:02:41.571: INFO: Pod daemon-set-cxs42 is not available
    Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:41.595: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:42.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:43.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:44.606: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:45.605: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:46.601: INFO: Pod daemon-set-t4fhd is not available
    Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:02:46.607: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:02:46.617
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7174, will wait for the garbage collector to delete the pods 01/17/23 07:02:46.617
    Jan 17 07:02:46.688: INFO: Deleting DaemonSet.extensions daemon-set took: 14.804064ms
    Jan 17 07:02:46.788: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.867676ms
    Jan 17 07:02:48.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:02:48.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 07:02:48.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14126"},"items":null}

    Jan 17 07:02:48.501: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14126"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:02:48.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7174" for this suite. 01/17/23 07:02:48.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:48.535
Jan 17 07:02:48.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:02:48.536
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:48.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:48.577
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-c28db3f2-81d9-4264-8d00-28678bf4df94 01/17/23 07:02:48.584
STEP: Creating a pod to test consume secrets 01/17/23 07:02:48.6
Jan 17 07:02:48.633: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4" in namespace "projected-5426" to be "Succeeded or Failed"
Jan 17 07:02:48.642: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.203876ms
Jan 17 07:02:50.656: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023343724s
Jan 17 07:02:52.648: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Running", Reason="", readiness=false. Elapsed: 4.015020046s
Jan 17 07:02:54.646: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013723235s
STEP: Saw pod success 01/17/23 07:02:54.646
Jan 17 07:02:54.647: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4" satisfied condition "Succeeded or Failed"
Jan 17 07:02:54.651: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:02:54.661
Jan 17 07:02:54.692: INFO: Waiting for pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 to disappear
Jan 17 07:02:54.698: INFO: Pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 07:02:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5426" for this suite. 01/17/23 07:02:54.709
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":112,"skipped":1948,"failed":0}
------------------------------
• [SLOW TEST] [6.185 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:48.535
    Jan 17 07:02:48.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:02:48.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:48.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:48.577
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-c28db3f2-81d9-4264-8d00-28678bf4df94 01/17/23 07:02:48.584
    STEP: Creating a pod to test consume secrets 01/17/23 07:02:48.6
    Jan 17 07:02:48.633: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4" in namespace "projected-5426" to be "Succeeded or Failed"
    Jan 17 07:02:48.642: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.203876ms
    Jan 17 07:02:50.656: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.023343724s
    Jan 17 07:02:52.648: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Running", Reason="", readiness=false. Elapsed: 4.015020046s
    Jan 17 07:02:54.646: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013723235s
    STEP: Saw pod success 01/17/23 07:02:54.646
    Jan 17 07:02:54.647: INFO: Pod "pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4" satisfied condition "Succeeded or Failed"
    Jan 17 07:02:54.651: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:02:54.661
    Jan 17 07:02:54.692: INFO: Waiting for pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 to disappear
    Jan 17 07:02:54.698: INFO: Pod pod-projected-secrets-9fcde58a-84d2-4a31-95f3-6b8cdba7e2e4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 07:02:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5426" for this suite. 01/17/23 07:02:54.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:02:54.722
Jan 17 07:02:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename subpath 01/17/23 07:02:54.724
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:54.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:54.769
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 07:02:54.775
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-sn9x 01/17/23 07:02:54.799
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:02:54.799
Jan 17 07:02:54.821: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sn9x" in namespace "subpath-9235" to be "Succeeded or Failed"
Jan 17 07:02:54.831: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055054ms
Jan 17 07:02:56.838: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 2.017186564s
Jan 17 07:02:58.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 4.01586519s
Jan 17 07:03:00.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 6.015329775s
Jan 17 07:03:02.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 8.015862643s
Jan 17 07:03:04.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 10.01517225s
Jan 17 07:03:06.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 12.015247642s
Jan 17 07:03:08.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 14.016041499s
Jan 17 07:03:10.842: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 16.020927901s
Jan 17 07:03:12.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 18.016215142s
Jan 17 07:03:14.839: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 20.017924401s
Jan 17 07:03:16.851: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=false. Elapsed: 22.029790503s
Jan 17 07:03:18.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015478123s
STEP: Saw pod success 01/17/23 07:03:18.836
Jan 17 07:03:18.836: INFO: Pod "pod-subpath-test-configmap-sn9x" satisfied condition "Succeeded or Failed"
Jan 17 07:03:18.840: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-configmap-sn9x container test-container-subpath-configmap-sn9x: <nil>
STEP: delete the pod 01/17/23 07:03:18.851
Jan 17 07:03:18.871: INFO: Waiting for pod pod-subpath-test-configmap-sn9x to disappear
Jan 17 07:03:18.891: INFO: Pod pod-subpath-test-configmap-sn9x no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sn9x 01/17/23 07:03:18.891
Jan 17 07:03:18.891: INFO: Deleting pod "pod-subpath-test-configmap-sn9x" in namespace "subpath-9235"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 07:03:18.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9235" for this suite. 01/17/23 07:03:18.9
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":113,"skipped":1976,"failed":0}
------------------------------
• [SLOW TEST] [24.192 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:02:54.722
    Jan 17 07:02:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename subpath 01/17/23 07:02:54.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:02:54.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:02:54.769
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 07:02:54.775
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-sn9x 01/17/23 07:02:54.799
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:02:54.799
    Jan 17 07:02:54.821: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sn9x" in namespace "subpath-9235" to be "Succeeded or Failed"
    Jan 17 07:02:54.831: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055054ms
    Jan 17 07:02:56.838: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 2.017186564s
    Jan 17 07:02:58.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 4.01586519s
    Jan 17 07:03:00.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 6.015329775s
    Jan 17 07:03:02.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 8.015862643s
    Jan 17 07:03:04.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 10.01517225s
    Jan 17 07:03:06.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 12.015247642s
    Jan 17 07:03:08.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 14.016041499s
    Jan 17 07:03:10.842: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 16.020927901s
    Jan 17 07:03:12.837: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 18.016215142s
    Jan 17 07:03:14.839: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=true. Elapsed: 20.017924401s
    Jan 17 07:03:16.851: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Running", Reason="", readiness=false. Elapsed: 22.029790503s
    Jan 17 07:03:18.836: INFO: Pod "pod-subpath-test-configmap-sn9x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015478123s
    STEP: Saw pod success 01/17/23 07:03:18.836
    Jan 17 07:03:18.836: INFO: Pod "pod-subpath-test-configmap-sn9x" satisfied condition "Succeeded or Failed"
    Jan 17 07:03:18.840: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-configmap-sn9x container test-container-subpath-configmap-sn9x: <nil>
    STEP: delete the pod 01/17/23 07:03:18.851
    Jan 17 07:03:18.871: INFO: Waiting for pod pod-subpath-test-configmap-sn9x to disappear
    Jan 17 07:03:18.891: INFO: Pod pod-subpath-test-configmap-sn9x no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-sn9x 01/17/23 07:03:18.891
    Jan 17 07:03:18.891: INFO: Deleting pod "pod-subpath-test-configmap-sn9x" in namespace "subpath-9235"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 07:03:18.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9235" for this suite. 01/17/23 07:03:18.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:03:18.916
Jan 17 07:03:18.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-runtime 01/17/23 07:03:18.918
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:18.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:18.955
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/17/23 07:03:18.962
STEP: wait for the container to reach Succeeded 01/17/23 07:03:18.981
STEP: get the container status 01/17/23 07:03:23.018
STEP: the container should be terminated 01/17/23 07:03:23.023
STEP: the termination message should be set 01/17/23 07:03:23.023
Jan 17 07:03:23.023: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/17/23 07:03:23.023
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 07:03:23.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6339" for this suite. 01/17/23 07:03:23.064
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":114,"skipped":1984,"failed":0}
------------------------------
• [4.157 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:03:18.916
    Jan 17 07:03:18.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-runtime 01/17/23 07:03:18.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:18.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:18.955
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/17/23 07:03:18.962
    STEP: wait for the container to reach Succeeded 01/17/23 07:03:18.981
    STEP: get the container status 01/17/23 07:03:23.018
    STEP: the container should be terminated 01/17/23 07:03:23.023
    STEP: the termination message should be set 01/17/23 07:03:23.023
    Jan 17 07:03:23.023: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/17/23 07:03:23.023
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 07:03:23.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6339" for this suite. 01/17/23 07:03:23.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:03:23.074
Jan 17 07:03:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:03:23.077
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:23.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:23.106
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/17/23 07:03:23.112
STEP: Creating a ResourceQuota 01/17/23 07:03:28.116
STEP: Ensuring resource quota status is calculated 01/17/23 07:03:28.132
STEP: Creating a Service 01/17/23 07:03:30.14
STEP: Creating a NodePort Service 01/17/23 07:03:30.172
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/17/23 07:03:30.217
STEP: Ensuring resource quota status captures service creation 01/17/23 07:03:30.257
STEP: Deleting Services 01/17/23 07:03:32.263
STEP: Ensuring resource quota status released usage 01/17/23 07:03:32.36
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:03:34.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8710" for this suite. 01/17/23 07:03:34.372
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":115,"skipped":1990,"failed":0}
------------------------------
• [SLOW TEST] [11.311 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:03:23.074
    Jan 17 07:03:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:03:23.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:23.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:23.106
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/17/23 07:03:23.112
    STEP: Creating a ResourceQuota 01/17/23 07:03:28.116
    STEP: Ensuring resource quota status is calculated 01/17/23 07:03:28.132
    STEP: Creating a Service 01/17/23 07:03:30.14
    STEP: Creating a NodePort Service 01/17/23 07:03:30.172
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/17/23 07:03:30.217
    STEP: Ensuring resource quota status captures service creation 01/17/23 07:03:30.257
    STEP: Deleting Services 01/17/23 07:03:32.263
    STEP: Ensuring resource quota status released usage 01/17/23 07:03:32.36
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:03:34.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8710" for this suite. 01/17/23 07:03:34.372
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:03:34.387
Jan 17 07:03:34.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:03:34.39
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:34.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:34.423
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/17/23 07:03:51.436
STEP: Creating a ResourceQuota 01/17/23 07:03:56.442
STEP: Ensuring resource quota status is calculated 01/17/23 07:03:56.453
STEP: Creating a ConfigMap 01/17/23 07:03:58.46
STEP: Ensuring resource quota status captures configMap creation 01/17/23 07:03:58.484
STEP: Deleting a ConfigMap 01/17/23 07:04:00.49
STEP: Ensuring resource quota status released usage 01/17/23 07:04:00.502
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:04:02.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3403" for this suite. 01/17/23 07:04:02.523
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":116,"skipped":1993,"failed":0}
------------------------------
• [SLOW TEST] [28.149 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:03:34.387
    Jan 17 07:03:34.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:03:34.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:03:34.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:03:34.423
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/17/23 07:03:51.436
    STEP: Creating a ResourceQuota 01/17/23 07:03:56.442
    STEP: Ensuring resource quota status is calculated 01/17/23 07:03:56.453
    STEP: Creating a ConfigMap 01/17/23 07:03:58.46
    STEP: Ensuring resource quota status captures configMap creation 01/17/23 07:03:58.484
    STEP: Deleting a ConfigMap 01/17/23 07:04:00.49
    STEP: Ensuring resource quota status released usage 01/17/23 07:04:00.502
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:04:02.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3403" for this suite. 01/17/23 07:04:02.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:02.537
Jan 17 07:04:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:04:02.538
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:02.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:02.571
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/17/23 07:04:02.578
Jan 17 07:04:02.578: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-625 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/17/23 07:04:02.674
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:04:02.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-625" for this suite. 01/17/23 07:04:02.71
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":117,"skipped":2016,"failed":0}
------------------------------
• [0.189 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:02.537
    Jan 17 07:04:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:04:02.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:02.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:02.571
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/17/23 07:04:02.578
    Jan 17 07:04:02.578: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-625 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/17/23 07:04:02.674
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:04:02.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-625" for this suite. 01/17/23 07:04:02.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:02.726
Jan 17 07:04:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubelet-test 01/17/23 07:04:02.733
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:02.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:02.784
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 17 07:04:02.806: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3" in namespace "kubelet-test-6877" to be "running and ready"
Jan 17 07:04:02.816: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.481453ms
Jan 17 07:04:02.816: INFO: The phase of Pod busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:04:04.824: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018757946s
Jan 17 07:04:04.824: INFO: The phase of Pod busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3 is Running (Ready = true)
Jan 17 07:04:04.824: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 07:04:04.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6877" for this suite. 01/17/23 07:04:04.842
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":118,"skipped":2025,"failed":0}
------------------------------
• [2.125 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:02.726
    Jan 17 07:04:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 07:04:02.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:02.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:02.784
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 17 07:04:02.806: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3" in namespace "kubelet-test-6877" to be "running and ready"
    Jan 17 07:04:02.816: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.481453ms
    Jan 17 07:04:02.816: INFO: The phase of Pod busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:04:04.824: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018757946s
    Jan 17 07:04:04.824: INFO: The phase of Pod busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3 is Running (Ready = true)
    Jan 17 07:04:04.824: INFO: Pod "busybox-readonly-fs7eccc9a0-58ff-4b03-83ca-7a72142ae0e3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 07:04:04.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6877" for this suite. 01/17/23 07:04:04.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:04.855
Jan 17 07:04:04.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:04:04.856
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:04.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:04.894
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:04:04.901
Jan 17 07:04:04.914: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41" in namespace "downward-api-7460" to be "Succeeded or Failed"
Jan 17 07:04:04.922: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Pending", Reason="", readiness=false. Elapsed: 7.621084ms
Jan 17 07:04:06.926: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Running", Reason="", readiness=true. Elapsed: 2.011848372s
Jan 17 07:04:08.926: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Running", Reason="", readiness=false. Elapsed: 4.01223935s
Jan 17 07:04:10.928: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01421424s
STEP: Saw pod success 01/17/23 07:04:10.928
Jan 17 07:04:10.928: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41" satisfied condition "Succeeded or Failed"
Jan 17 07:04:10.934: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 container client-container: <nil>
STEP: delete the pod 01/17/23 07:04:10.944
Jan 17 07:04:10.966: INFO: Waiting for pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 to disappear
Jan 17 07:04:10.972: INFO: Pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:04:10.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7460" for this suite. 01/17/23 07:04:10.977
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":119,"skipped":2051,"failed":0}
------------------------------
• [SLOW TEST] [6.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:04.855
    Jan 17 07:04:04.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:04:04.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:04.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:04.894
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:04:04.901
    Jan 17 07:04:04.914: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41" in namespace "downward-api-7460" to be "Succeeded or Failed"
    Jan 17 07:04:04.922: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Pending", Reason="", readiness=false. Elapsed: 7.621084ms
    Jan 17 07:04:06.926: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Running", Reason="", readiness=true. Elapsed: 2.011848372s
    Jan 17 07:04:08.926: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Running", Reason="", readiness=false. Elapsed: 4.01223935s
    Jan 17 07:04:10.928: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01421424s
    STEP: Saw pod success 01/17/23 07:04:10.928
    Jan 17 07:04:10.928: INFO: Pod "downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41" satisfied condition "Succeeded or Failed"
    Jan 17 07:04:10.934: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 container client-container: <nil>
    STEP: delete the pod 01/17/23 07:04:10.944
    Jan 17 07:04:10.966: INFO: Waiting for pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 to disappear
    Jan 17 07:04:10.972: INFO: Pod downwardapi-volume-67977f75-d9cf-424d-9fe6-e5799c54bc41 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:04:10.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7460" for this suite. 01/17/23 07:04:10.977
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:10.993
Jan 17 07:04:10.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:04:10.994
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:11.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:11.034
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-ae0afc99-d689-4680-baf7-f51ffcaab366 01/17/23 07:04:11.042
STEP: Creating a pod to test consume configMaps 01/17/23 07:04:11.059
Jan 17 07:04:11.076: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e" in namespace "configmap-8610" to be "Succeeded or Failed"
Jan 17 07:04:11.087: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.265876ms
Jan 17 07:04:13.092: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016792606s
Jan 17 07:04:15.092: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016474343s
STEP: Saw pod success 01/17/23 07:04:15.092
Jan 17 07:04:15.093: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e" satisfied condition "Succeeded or Failed"
Jan 17 07:04:15.097: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:04:15.149
Jan 17 07:04:15.172: INFO: Waiting for pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e to disappear
Jan 17 07:04:15.178: INFO: Pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:04:15.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8610" for this suite. 01/17/23 07:04:15.184
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":120,"skipped":2051,"failed":0}
------------------------------
• [4.215 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:10.993
    Jan 17 07:04:10.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:04:10.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:11.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:11.034
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-ae0afc99-d689-4680-baf7-f51ffcaab366 01/17/23 07:04:11.042
    STEP: Creating a pod to test consume configMaps 01/17/23 07:04:11.059
    Jan 17 07:04:11.076: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e" in namespace "configmap-8610" to be "Succeeded or Failed"
    Jan 17 07:04:11.087: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.265876ms
    Jan 17 07:04:13.092: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016792606s
    Jan 17 07:04:15.092: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016474343s
    STEP: Saw pod success 01/17/23 07:04:15.092
    Jan 17 07:04:15.093: INFO: Pod "pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e" satisfied condition "Succeeded or Failed"
    Jan 17 07:04:15.097: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:04:15.149
    Jan 17 07:04:15.172: INFO: Waiting for pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e to disappear
    Jan 17 07:04:15.178: INFO: Pod pod-configmaps-c1a2f342-e7bf-4659-b060-6d8dd8f7199e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:04:15.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8610" for this suite. 01/17/23 07:04:15.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:15.213
Jan 17 07:04:15.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename watch 01/17/23 07:04:15.215
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:15.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:15.251
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/17/23 07:04:15.258
STEP: starting a background goroutine to produce watch events 01/17/23 07:04:15.268
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/17/23 07:04:15.268
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 07:04:18.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9752" for this suite. 01/17/23 07:04:18.07
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":121,"skipped":2071,"failed":0}
------------------------------
• [2.912 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:15.213
    Jan 17 07:04:15.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename watch 01/17/23 07:04:15.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:15.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:15.251
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/17/23 07:04:15.258
    STEP: starting a background goroutine to produce watch events 01/17/23 07:04:15.268
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/17/23 07:04:15.268
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 07:04:18.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9752" for this suite. 01/17/23 07:04:18.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:18.128
Jan 17 07:04:18.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:04:18.129
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:18.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:18.167
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/17/23 07:04:18.173
STEP: Creating a ResourceQuota 01/17/23 07:04:23.176
STEP: Ensuring resource quota status is calculated 01/17/23 07:04:23.189
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:04:25.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2244" for this suite. 01/17/23 07:04:25.201
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":122,"skipped":2078,"failed":0}
------------------------------
• [SLOW TEST] [7.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:18.128
    Jan 17 07:04:18.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:04:18.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:18.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:18.167
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/17/23 07:04:18.173
    STEP: Creating a ResourceQuota 01/17/23 07:04:23.176
    STEP: Ensuring resource quota status is calculated 01/17/23 07:04:23.189
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:04:25.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2244" for this suite. 01/17/23 07:04:25.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:25.213
Jan 17 07:04:25.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename proxy 01/17/23 07:04:25.214
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:25.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:25.249
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 17 07:04:25.254: INFO: Creating pod...
Jan 17 07:04:25.268: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5054" to be "running"
Jan 17 07:04:25.274: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.427481ms
Jan 17 07:04:27.279: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759844s
Jan 17 07:04:27.279: INFO: Pod "agnhost" satisfied condition "running"
Jan 17 07:04:27.279: INFO: Creating service...
Jan 17 07:04:27.300: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/DELETE
Jan 17 07:04:27.315: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 07:04:27.315: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/GET
Jan 17 07:04:27.329: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 07:04:27.329: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/HEAD
Jan 17 07:04:27.340: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 07:04:27.340: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 17 07:04:27.348: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 07:04:27.348: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/PATCH
Jan 17 07:04:27.354: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 07:04:27.354: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/POST
Jan 17 07:04:27.368: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 07:04:27.383: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/PUT
Jan 17 07:04:27.402: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 07:04:27.402: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/DELETE
Jan 17 07:04:27.412: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 07:04:27.412: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/GET
Jan 17 07:04:27.419: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 07:04:27.419: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/HEAD
Jan 17 07:04:27.424: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 07:04:27.424: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/OPTIONS
Jan 17 07:04:27.433: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 07:04:27.433: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/PATCH
Jan 17 07:04:27.444: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 07:04:27.444: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/POST
Jan 17 07:04:27.465: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 07:04:27.465: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/PUT
Jan 17 07:04:27.474: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 07:04:27.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5054" for this suite. 01/17/23 07:04:27.486
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":123,"skipped":2122,"failed":0}
------------------------------
• [2.283 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:25.213
    Jan 17 07:04:25.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename proxy 01/17/23 07:04:25.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:25.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:25.249
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 17 07:04:25.254: INFO: Creating pod...
    Jan 17 07:04:25.268: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5054" to be "running"
    Jan 17 07:04:25.274: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.427481ms
    Jan 17 07:04:27.279: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759844s
    Jan 17 07:04:27.279: INFO: Pod "agnhost" satisfied condition "running"
    Jan 17 07:04:27.279: INFO: Creating service...
    Jan 17 07:04:27.300: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/DELETE
    Jan 17 07:04:27.315: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 07:04:27.315: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/GET
    Jan 17 07:04:27.329: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 17 07:04:27.329: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/HEAD
    Jan 17 07:04:27.340: INFO: http.Client request:HEAD | StatusCode:200
    Jan 17 07:04:27.340: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 17 07:04:27.348: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 07:04:27.348: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/PATCH
    Jan 17 07:04:27.354: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 07:04:27.354: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/POST
    Jan 17 07:04:27.368: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 07:04:27.383: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/pods/agnhost/proxy/some/path/with/PUT
    Jan 17 07:04:27.402: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 07:04:27.402: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/DELETE
    Jan 17 07:04:27.412: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 07:04:27.412: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/GET
    Jan 17 07:04:27.419: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 17 07:04:27.419: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/HEAD
    Jan 17 07:04:27.424: INFO: http.Client request:HEAD | StatusCode:200
    Jan 17 07:04:27.424: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/OPTIONS
    Jan 17 07:04:27.433: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 07:04:27.433: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/PATCH
    Jan 17 07:04:27.444: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 07:04:27.444: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/POST
    Jan 17 07:04:27.465: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 07:04:27.465: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-5054/services/test-service/proxy/some/path/with/PUT
    Jan 17 07:04:27.474: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 07:04:27.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5054" for this suite. 01/17/23 07:04:27.486
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:27.497
Jan 17 07:04:27.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 07:04:27.498
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:27.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:27.536
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 17 07:04:27.556: INFO: Waiting up to 5m0s for pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7" in namespace "container-probe-485" to be "running and ready"
Jan 17 07:04:27.561: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.762593ms
Jan 17 07:04:27.562: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:04:29.569: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 2.012897678s
Jan 17 07:04:29.569: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:31.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 4.011736349s
Jan 17 07:04:31.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:33.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 6.011246931s
Jan 17 07:04:33.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:35.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 8.011402446s
Jan 17 07:04:35.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:37.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 10.01119243s
Jan 17 07:04:37.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:39.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 12.012312792s
Jan 17 07:04:39.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:41.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 14.01220997s
Jan 17 07:04:41.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:43.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 16.01191403s
Jan 17 07:04:43.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:45.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 18.011875781s
Jan 17 07:04:45.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:47.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 20.011168266s
Jan 17 07:04:47.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
Jan 17 07:04:49.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=true. Elapsed: 22.011971902s
Jan 17 07:04:49.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = true)
Jan 17 07:04:49.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7" satisfied condition "running and ready"
Jan 17 07:04:49.572: INFO: Container started at 2023-01-17 07:04:28 +0000 UTC, pod became ready at 2023-01-17 07:04:47 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 07:04:49.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-485" for this suite. 01/17/23 07:04:49.58
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":124,"skipped":2126,"failed":0}
------------------------------
• [SLOW TEST] [22.097 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:27.497
    Jan 17 07:04:27.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 07:04:27.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:27.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:27.536
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 17 07:04:27.556: INFO: Waiting up to 5m0s for pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7" in namespace "container-probe-485" to be "running and ready"
    Jan 17 07:04:27.561: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.762593ms
    Jan 17 07:04:27.562: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:04:29.569: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 2.012897678s
    Jan 17 07:04:29.569: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:31.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 4.011736349s
    Jan 17 07:04:31.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:33.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 6.011246931s
    Jan 17 07:04:33.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:35.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 8.011402446s
    Jan 17 07:04:35.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:37.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 10.01119243s
    Jan 17 07:04:37.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:39.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 12.012312792s
    Jan 17 07:04:39.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:41.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 14.01220997s
    Jan 17 07:04:41.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:43.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 16.01191403s
    Jan 17 07:04:43.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:45.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 18.011875781s
    Jan 17 07:04:45.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:47.567: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=false. Elapsed: 20.011168266s
    Jan 17 07:04:47.567: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = false)
    Jan 17 07:04:49.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7": Phase="Running", Reason="", readiness=true. Elapsed: 22.011971902s
    Jan 17 07:04:49.568: INFO: The phase of Pod test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7 is Running (Ready = true)
    Jan 17 07:04:49.568: INFO: Pod "test-webserver-61d960f9-4f93-4168-b9f6-db5d0a65fda7" satisfied condition "running and ready"
    Jan 17 07:04:49.572: INFO: Container started at 2023-01-17 07:04:28 +0000 UTC, pod became ready at 2023-01-17 07:04:47 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 07:04:49.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-485" for this suite. 01/17/23 07:04:49.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:04:49.594
Jan 17 07:04:49.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pod-network-test 01/17/23 07:04:49.596
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:49.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:49.646
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1475 01/17/23 07:04:49.659
STEP: creating a selector 01/17/23 07:04:49.659
STEP: Creating the service pods in kubernetes 01/17/23 07:04:49.662
Jan 17 07:04:49.662: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 07:04:49.724: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1475" to be "running and ready"
Jan 17 07:04:49.736: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.19577ms
Jan 17 07:04:49.737: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:04:51.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018954608s
Jan 17 07:04:51.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:04:53.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018067656s
Jan 17 07:04:53.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:04:55.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025506041s
Jan 17 07:04:55.750: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:04:57.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023904015s
Jan 17 07:04:57.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:04:59.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017288596s
Jan 17 07:04:59.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:05:01.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018581554s
Jan 17 07:05:01.743: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 07:05:01.743: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 07:05:01.746: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1475" to be "running and ready"
Jan 17 07:05:01.755: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.516229ms
Jan 17 07:05:01.755: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 07:05:01.755: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 07:05:01.760: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1475" to be "running and ready"
Jan 17 07:05:01.766: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.910811ms
Jan 17 07:05:01.766: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 07:05:01.766: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 07:05:01.771
Jan 17 07:05:01.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1475" to be "running"
Jan 17 07:05:01.790: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379485ms
Jan 17 07:05:03.802: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017037878s
Jan 17 07:05:03.802: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 07:05:03.805: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 07:05:03.805: INFO: Breadth first check of 10.100.206.16 on host 10.0.0.16...
Jan 17 07:05:03.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.206.16&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:05:03.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:05:03.809: INFO: ExecWithOptions: Clientset creation
Jan 17 07:05:03.809: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.206.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:05:04.008: INFO: Waiting for responses: map[]
Jan 17 07:05:04.008: INFO: reached 10.100.206.16 after 0/1 tries
Jan 17 07:05:04.008: INFO: Breadth first check of 10.100.135.34 on host 10.0.0.22...
Jan 17 07:05:04.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.135.34&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:05:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:05:04.013: INFO: ExecWithOptions: Clientset creation
Jan 17 07:05:04.013: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.135.34%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:05:04.205: INFO: Waiting for responses: map[]
Jan 17 07:05:04.205: INFO: reached 10.100.135.34 after 0/1 tries
Jan 17 07:05:04.205: INFO: Breadth first check of 10.100.168.105 on host 10.0.0.21...
Jan 17 07:05:04.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.168.105&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:05:04.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:05:04.210: INFO: ExecWithOptions: Clientset creation
Jan 17 07:05:04.210: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.168.105%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:05:04.415: INFO: Waiting for responses: map[]
Jan 17 07:05:04.415: INFO: reached 10.100.168.105 after 0/1 tries
Jan 17 07:05:04.415: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 07:05:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1475" for this suite. 01/17/23 07:05:04.421
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":125,"skipped":2131,"failed":0}
------------------------------
• [SLOW TEST] [14.841 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:04:49.594
    Jan 17 07:04:49.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 07:04:49.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:04:49.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:04:49.646
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1475 01/17/23 07:04:49.659
    STEP: creating a selector 01/17/23 07:04:49.659
    STEP: Creating the service pods in kubernetes 01/17/23 07:04:49.662
    Jan 17 07:04:49.662: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 07:04:49.724: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1475" to be "running and ready"
    Jan 17 07:04:49.736: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.19577ms
    Jan 17 07:04:49.737: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:04:51.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018954608s
    Jan 17 07:04:51.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:04:53.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018067656s
    Jan 17 07:04:53.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:04:55.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025506041s
    Jan 17 07:04:55.750: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:04:57.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023904015s
    Jan 17 07:04:57.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:04:59.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017288596s
    Jan 17 07:04:59.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:05:01.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018581554s
    Jan 17 07:05:01.743: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 07:05:01.743: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 07:05:01.746: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1475" to be "running and ready"
    Jan 17 07:05:01.755: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.516229ms
    Jan 17 07:05:01.755: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 07:05:01.755: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 07:05:01.760: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1475" to be "running and ready"
    Jan 17 07:05:01.766: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.910811ms
    Jan 17 07:05:01.766: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 07:05:01.766: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 07:05:01.771
    Jan 17 07:05:01.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1475" to be "running"
    Jan 17 07:05:01.790: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379485ms
    Jan 17 07:05:03.802: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017037878s
    Jan 17 07:05:03.802: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 07:05:03.805: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 07:05:03.805: INFO: Breadth first check of 10.100.206.16 on host 10.0.0.16...
    Jan 17 07:05:03.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.206.16&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:05:03.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:05:03.809: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:05:03.809: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.206.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:05:04.008: INFO: Waiting for responses: map[]
    Jan 17 07:05:04.008: INFO: reached 10.100.206.16 after 0/1 tries
    Jan 17 07:05:04.008: INFO: Breadth first check of 10.100.135.34 on host 10.0.0.22...
    Jan 17 07:05:04.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.135.34&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:05:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:05:04.013: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:05:04.013: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.135.34%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:05:04.205: INFO: Waiting for responses: map[]
    Jan 17 07:05:04.205: INFO: reached 10.100.135.34 after 0/1 tries
    Jan 17 07:05:04.205: INFO: Breadth first check of 10.100.168.105 on host 10.0.0.21...
    Jan 17 07:05:04.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.35:9080/dial?request=hostname&protocol=http&host=10.100.168.105&port=8083&tries=1'] Namespace:pod-network-test-1475 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:05:04.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:05:04.210: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:05:04.210: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1475/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.35%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.168.105%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:05:04.415: INFO: Waiting for responses: map[]
    Jan 17 07:05:04.415: INFO: reached 10.100.168.105 after 0/1 tries
    Jan 17 07:05:04.415: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 07:05:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1475" for this suite. 01/17/23 07:05:04.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:05:04.437
Jan 17 07:05:04.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:05:04.44
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:04.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:04.516
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 17 07:05:04.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/17/23 07:05:11.509
Jan 17 07:05:11.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
Jan 17 07:05:12.755: INFO: stderr: ""
Jan 17 07:05:12.755: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 07:05:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-9189-crds test-foo'
Jan 17 07:05:12.912: INFO: stderr: ""
Jan 17 07:05:12.912: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 17 07:05:12.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
Jan 17 07:05:13.976: INFO: stderr: ""
Jan 17 07:05:13.976: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 07:05:13.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-9189-crds test-foo'
Jan 17 07:05:14.156: INFO: stderr: ""
Jan 17 07:05:14.156: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/17/23 07:05:14.156
Jan 17 07:05:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
Jan 17 07:05:15.188: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/17/23 07:05:15.188
Jan 17 07:05:15.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
Jan 17 07:05:15.646: INFO: rc: 1
Jan 17 07:05:15.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
Jan 17 07:05:16.083: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/17/23 07:05:16.083
Jan 17 07:05:16.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
Jan 17 07:05:16.521: INFO: rc: 1
Jan 17 07:05:16.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
Jan 17 07:05:16.981: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/17/23 07:05:16.981
Jan 17 07:05:16.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds'
Jan 17 07:05:17.442: INFO: stderr: ""
Jan 17 07:05:17.443: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/17/23 07:05:17.443
Jan 17 07:05:17.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.metadata'
Jan 17 07:05:17.881: INFO: stderr: ""
Jan 17 07:05:17.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 17 07:05:17.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec'
Jan 17 07:05:18.294: INFO: stderr: ""
Jan 17 07:05:18.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 17 07:05:18.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec.bars'
Jan 17 07:05:18.768: INFO: stderr: ""
Jan 17 07:05:18.768: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/17/23 07:05:18.768
Jan 17 07:05:18.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec.bars2'
Jan 17 07:05:19.187: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:05:25.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7691" for this suite. 01/17/23 07:05:25.862
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":126,"skipped":2156,"failed":0}
------------------------------
• [SLOW TEST] [21.437 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:05:04.437
    Jan 17 07:05:04.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:05:04.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:04.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:04.516
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 17 07:05:04.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/17/23 07:05:11.509
    Jan 17 07:05:11.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
    Jan 17 07:05:12.755: INFO: stderr: ""
    Jan 17 07:05:12.755: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 17 07:05:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-9189-crds test-foo'
    Jan 17 07:05:12.912: INFO: stderr: ""
    Jan 17 07:05:12.912: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 17 07:05:12.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
    Jan 17 07:05:13.976: INFO: stderr: ""
    Jan 17 07:05:13.976: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 17 07:05:13.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-9189-crds test-foo'
    Jan 17 07:05:14.156: INFO: stderr: ""
    Jan 17 07:05:14.156: INFO: stdout: "e2e-test-crd-publish-openapi-9189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/17/23 07:05:14.156
    Jan 17 07:05:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
    Jan 17 07:05:15.188: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/17/23 07:05:15.188
    Jan 17 07:05:15.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
    Jan 17 07:05:15.646: INFO: rc: 1
    Jan 17 07:05:15.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
    Jan 17 07:05:16.083: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/17/23 07:05:16.083
    Jan 17 07:05:16.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 create -f -'
    Jan 17 07:05:16.521: INFO: rc: 1
    Jan 17 07:05:16.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 --namespace=crd-publish-openapi-7691 apply -f -'
    Jan 17 07:05:16.981: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/17/23 07:05:16.981
    Jan 17 07:05:16.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds'
    Jan 17 07:05:17.442: INFO: stderr: ""
    Jan 17 07:05:17.443: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/17/23 07:05:17.443
    Jan 17 07:05:17.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.metadata'
    Jan 17 07:05:17.881: INFO: stderr: ""
    Jan 17 07:05:17.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 17 07:05:17.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec'
    Jan 17 07:05:18.294: INFO: stderr: ""
    Jan 17 07:05:18.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 17 07:05:18.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec.bars'
    Jan 17 07:05:18.768: INFO: stderr: ""
    Jan 17 07:05:18.768: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/17/23 07:05:18.768
    Jan 17 07:05:18.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-7691 explain e2e-test-crd-publish-openapi-9189-crds.spec.bars2'
    Jan 17 07:05:19.187: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:05:25.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7691" for this suite. 01/17/23 07:05:25.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:05:25.876
Jan 17 07:05:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename ingress 01/17/23 07:05:25.878
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:25.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:25.912
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/17/23 07:05:25.92
STEP: getting /apis/networking.k8s.io 01/17/23 07:05:25.926
STEP: getting /apis/networking.k8s.iov1 01/17/23 07:05:25.929
STEP: creating 01/17/23 07:05:25.932
STEP: getting 01/17/23 07:05:25.962
STEP: listing 01/17/23 07:05:25.967
STEP: watching 01/17/23 07:05:25.972
Jan 17 07:05:25.972: INFO: starting watch
STEP: cluster-wide listing 01/17/23 07:05:25.975
STEP: cluster-wide watching 01/17/23 07:05:25.98
Jan 17 07:05:25.980: INFO: starting watch
STEP: patching 01/17/23 07:05:25.983
STEP: updating 01/17/23 07:05:25.994
Jan 17 07:05:26.010: INFO: waiting for watch events with expected annotations
Jan 17 07:05:26.010: INFO: saw patched and updated annotations
STEP: patching /status 01/17/23 07:05:26.01
STEP: updating /status 01/17/23 07:05:26.022
STEP: get /status 01/17/23 07:05:26.038
STEP: deleting 01/17/23 07:05:26.044
STEP: deleting a collection 01/17/23 07:05:26.064
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 17 07:05:26.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9686" for this suite. 01/17/23 07:05:26.099
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":127,"skipped":2169,"failed":0}
------------------------------
• [0.245 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:05:25.876
    Jan 17 07:05:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename ingress 01/17/23 07:05:25.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:25.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:25.912
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/17/23 07:05:25.92
    STEP: getting /apis/networking.k8s.io 01/17/23 07:05:25.926
    STEP: getting /apis/networking.k8s.iov1 01/17/23 07:05:25.929
    STEP: creating 01/17/23 07:05:25.932
    STEP: getting 01/17/23 07:05:25.962
    STEP: listing 01/17/23 07:05:25.967
    STEP: watching 01/17/23 07:05:25.972
    Jan 17 07:05:25.972: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 07:05:25.975
    STEP: cluster-wide watching 01/17/23 07:05:25.98
    Jan 17 07:05:25.980: INFO: starting watch
    STEP: patching 01/17/23 07:05:25.983
    STEP: updating 01/17/23 07:05:25.994
    Jan 17 07:05:26.010: INFO: waiting for watch events with expected annotations
    Jan 17 07:05:26.010: INFO: saw patched and updated annotations
    STEP: patching /status 01/17/23 07:05:26.01
    STEP: updating /status 01/17/23 07:05:26.022
    STEP: get /status 01/17/23 07:05:26.038
    STEP: deleting 01/17/23 07:05:26.044
    STEP: deleting a collection 01/17/23 07:05:26.064
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 17 07:05:26.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9686" for this suite. 01/17/23 07:05:26.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:05:26.123
Jan 17 07:05:26.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename watch 01/17/23 07:05:26.124
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:26.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:26.163
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/17/23 07:05:26.175
STEP: creating a watch on configmaps with label B 01/17/23 07:05:26.179
STEP: creating a watch on configmaps with label A or B 01/17/23 07:05:26.181
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.184
Jan 17 07:05:26.194: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15133 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:26.195: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15133 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.195
Jan 17 07:05:26.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15134 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:26.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15134 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/17/23 07:05:26.217
Jan 17 07:05:26.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15135 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:26.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15135 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.24
Jan 17 07:05:26.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15136 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:26.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15136 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/17/23 07:05:26.258
Jan 17 07:05:26.269: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15137 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:26.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15137 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/17/23 07:05:36.271
Jan 17 07:05:36.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15170 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:05:36.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15170 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 07:05:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9554" for this suite. 01/17/23 07:05:46.301
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":128,"skipped":2186,"failed":0}
------------------------------
• [SLOW TEST] [20.211 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:05:26.123
    Jan 17 07:05:26.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename watch 01/17/23 07:05:26.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:26.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:26.163
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/17/23 07:05:26.175
    STEP: creating a watch on configmaps with label B 01/17/23 07:05:26.179
    STEP: creating a watch on configmaps with label A or B 01/17/23 07:05:26.181
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.184
    Jan 17 07:05:26.194: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15133 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:26.195: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15133 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.195
    Jan 17 07:05:26.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15134 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:26.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15134 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/17/23 07:05:26.217
    Jan 17 07:05:26.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15135 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:26.239: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15135 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/17/23 07:05:26.24
    Jan 17 07:05:26.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15136 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:26.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9554  a8f2a354-d452-423a-80b9-3728ed74f86b 15136 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/17/23 07:05:26.258
    Jan 17 07:05:26.269: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15137 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:26.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15137 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/17/23 07:05:36.271
    Jan 17 07:05:36.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15170 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:05:36.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9554  99a8aefd-6592-4248-8365-3f29ec48c7cb 15170 0 2023-01-17 07:05:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 07:05:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 07:05:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9554" for this suite. 01/17/23 07:05:46.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:05:46.335
Jan 17 07:05:46.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replication-controller 01/17/23 07:05:46.336
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:46.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:46.378
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/17/23 07:05:46.388
STEP: When the matched label of one of its pods change 01/17/23 07:05:46.399
Jan 17 07:05:46.438: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/17/23 07:05:47.505
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 07:05:48.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5367" for this suite. 01/17/23 07:05:48.541
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":129,"skipped":2195,"failed":0}
------------------------------
• [2.231 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:05:46.335
    Jan 17 07:05:46.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replication-controller 01/17/23 07:05:46.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:46.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:46.378
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/17/23 07:05:46.388
    STEP: When the matched label of one of its pods change 01/17/23 07:05:46.399
    Jan 17 07:05:46.438: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/17/23 07:05:47.505
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 07:05:48.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5367" for this suite. 01/17/23 07:05:48.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:05:48.569
Jan 17 07:05:48.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename taint-multiple-pods 01/17/23 07:05:48.571
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:48.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:48.62
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 17 07:05:48.626: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 07:06:48.698: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 17 07:06:48.704: INFO: Starting informer...
STEP: Starting pods... 01/17/23 07:06:48.705
Jan 17 07:06:48.942: INFO: Pod1 is running on cluster125-w73dz53kvqes-node-1. Tainting Node
Jan 17 07:06:49.167: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1695" to be "running"
Jan 17 07:06:49.175: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080532ms
Jan 17 07:06:51.183: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015427899s
Jan 17 07:06:51.183: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 17 07:06:51.183: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1695" to be "running"
Jan 17 07:06:51.188: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.274766ms
Jan 17 07:06:51.188: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 17 07:06:51.188: INFO: Pod2 is running on cluster125-w73dz53kvqes-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/17/23 07:06:51.188
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 07:06:51.22
STEP: Waiting for Pod1 and Pod2 to be deleted 01/17/23 07:06:51.233
Jan 17 07:06:57.127: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 17 07:07:16.929: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 07:07:16.964
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:07:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1695" for this suite. 01/17/23 07:07:17.002
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":130,"skipped":2202,"failed":0}
------------------------------
• [SLOW TEST] [88.486 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:05:48.569
    Jan 17 07:05:48.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename taint-multiple-pods 01/17/23 07:05:48.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:05:48.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:05:48.62
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 17 07:05:48.626: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 07:06:48.698: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 17 07:06:48.704: INFO: Starting informer...
    STEP: Starting pods... 01/17/23 07:06:48.705
    Jan 17 07:06:48.942: INFO: Pod1 is running on cluster125-w73dz53kvqes-node-1. Tainting Node
    Jan 17 07:06:49.167: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1695" to be "running"
    Jan 17 07:06:49.175: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080532ms
    Jan 17 07:06:51.183: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015427899s
    Jan 17 07:06:51.183: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 17 07:06:51.183: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1695" to be "running"
    Jan 17 07:06:51.188: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.274766ms
    Jan 17 07:06:51.188: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 17 07:06:51.188: INFO: Pod2 is running on cluster125-w73dz53kvqes-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/17/23 07:06:51.188
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 07:06:51.22
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/17/23 07:06:51.233
    Jan 17 07:06:57.127: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 17 07:07:16.929: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 07:07:16.964
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:07:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-1695" for this suite. 01/17/23 07:07:17.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:17.056
Jan 17 07:07:17.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:07:17.057
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:17.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:17.165
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/17/23 07:07:17.174
Jan 17 07:07:17.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1647 create -f -'
Jan 17 07:07:18.399: INFO: stderr: ""
Jan 17 07:07:18.399: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 07:07:18.399
Jan 17 07:07:19.408: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 07:07:19.408: INFO: Found 0 / 1
Jan 17 07:07:20.404: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 07:07:20.404: INFO: Found 1 / 1
Jan 17 07:07:20.404: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/17/23 07:07:20.404
Jan 17 07:07:20.410: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 07:07:20.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 07:07:20.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1647 patch pod agnhost-primary-8htn8 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 17 07:07:20.546: INFO: stderr: ""
Jan 17 07:07:20.546: INFO: stdout: "pod/agnhost-primary-8htn8 patched\n"
STEP: checking annotations 01/17/23 07:07:20.546
Jan 17 07:07:20.552: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 07:07:20.552: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:07:20.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1647" for this suite. 01/17/23 07:07:20.558
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":131,"skipped":2211,"failed":0}
------------------------------
• [3.532 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:17.056
    Jan 17 07:07:17.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:07:17.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:17.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:17.165
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/17/23 07:07:17.174
    Jan 17 07:07:17.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1647 create -f -'
    Jan 17 07:07:18.399: INFO: stderr: ""
    Jan 17 07:07:18.399: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 07:07:18.399
    Jan 17 07:07:19.408: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 07:07:19.408: INFO: Found 0 / 1
    Jan 17 07:07:20.404: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 07:07:20.404: INFO: Found 1 / 1
    Jan 17 07:07:20.404: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/17/23 07:07:20.404
    Jan 17 07:07:20.410: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 07:07:20.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 07:07:20.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1647 patch pod agnhost-primary-8htn8 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 17 07:07:20.546: INFO: stderr: ""
    Jan 17 07:07:20.546: INFO: stdout: "pod/agnhost-primary-8htn8 patched\n"
    STEP: checking annotations 01/17/23 07:07:20.546
    Jan 17 07:07:20.552: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 07:07:20.552: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:07:20.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1647" for this suite. 01/17/23 07:07:20.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:20.589
Jan 17 07:07:20.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 07:07:20.59
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:20.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:20.651
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/17/23 07:07:20.691
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:07:20.711
Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:20.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:07:20.728: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:21.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 07:07:21.754: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:22.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 07:07:22.748: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:07:23.740: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/17/23 07:07:23.745
Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:23.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 07:07:23.793: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:07:24.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:07:24.842: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/17/23 07:07:24.842
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:07:24.855
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7237, will wait for the garbage collector to delete the pods 01/17/23 07:07:24.855
Jan 17 07:07:24.922: INFO: Deleting DaemonSet.extensions daemon-set took: 11.784051ms
Jan 17 07:07:25.023: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.639967ms
Jan 17 07:07:27.830: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:07:27.830: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 07:07:27.835: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15770"},"items":null}

Jan 17 07:07:27.840: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15770"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:07:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7237" for this suite. 01/17/23 07:07:27.886
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":132,"skipped":2225,"failed":0}
------------------------------
• [SLOW TEST] [7.311 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:20.589
    Jan 17 07:07:20.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 07:07:20.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:20.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:20.651
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/17/23 07:07:20.691
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:07:20.711
    Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:20.723: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:20.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:07:20.728: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:21.748: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:21.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 07:07:21.754: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:22.741: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:22.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 07:07:22.748: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.735: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:07:23.740: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/17/23 07:07:23.745
    Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.780: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:23.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 07:07:23.793: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:24.836: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:07:24.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:07:24.842: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/17/23 07:07:24.842
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:07:24.855
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7237, will wait for the garbage collector to delete the pods 01/17/23 07:07:24.855
    Jan 17 07:07:24.922: INFO: Deleting DaemonSet.extensions daemon-set took: 11.784051ms
    Jan 17 07:07:25.023: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.639967ms
    Jan 17 07:07:27.830: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:07:27.830: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 07:07:27.835: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15770"},"items":null}

    Jan 17 07:07:27.840: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15770"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:07:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7237" for this suite. 01/17/23 07:07:27.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:27.908
Jan 17 07:07:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:07:27.91
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:27.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:27.953
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/17/23 07:07:27.972
Jan 17 07:07:27.973: INFO: Creating simple deployment test-deployment-vn44t
Jan 17 07:07:28.051: INFO: deployment "test-deployment-vn44t" doesn't have the required revision set
STEP: Getting /status 01/17/23 07:07:30.085
Jan 17 07:07:30.093: INFO: Deployment test-deployment-vn44t has Conditions: [{Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/17/23 07:07:30.093
Jan 17 07:07:30.122: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 7, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vn44t-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/17/23 07:07:30.122
Jan 17 07:07:30.126: INFO: Observed &Deployment event: ADDED
Jan 17 07:07:30.126: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vn44t-777898ffcc" is progressing.}
Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.128: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 07:07:30.128: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
Jan 17 07:07:30.128: INFO: Found Deployment test-deployment-vn44t in namespace deployment-7490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 07:07:30.128: INFO: Deployment test-deployment-vn44t has an updated status
STEP: patching the Statefulset Status 01/17/23 07:07:30.128
Jan 17 07:07:30.128: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 07:07:30.146: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/17/23 07:07:30.146
Jan 17 07:07:30.149: INFO: Observed &Deployment event: ADDED
Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
Jan 17 07:07:30.149: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 07:07:30.149: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vn44t-777898ffcc" is progressing.}
Jan 17 07:07:30.168: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
Jan 17 07:07:30.168: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 07:07:30.169: INFO: Observed &Deployment event: MODIFIED
Jan 17 07:07:30.169: INFO: Found deployment test-deployment-vn44t in namespace deployment-7490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 07:07:30.169: INFO: Deployment test-deployment-vn44t has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:07:30.180: INFO: Deployment "test-deployment-vn44t":
&Deployment{ObjectMeta:{test-deployment-vn44t  deployment-7490  e074de21-9899-42aa-9fda-e0d33d459cf4 15807 1 2023-01-17 07:07:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0018e9088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:07:30 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.,LastUpdateTime:2023-01-17 07:07:30 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 07:07:30.188: INFO: New ReplicaSet "test-deployment-vn44t-777898ffcc" of Deployment "test-deployment-vn44t":
&ReplicaSet{ObjectMeta:{test-deployment-vn44t-777898ffcc  deployment-7490  8f44a941-f521-4b8e-b751-f84034bfd359 15802 1 2023-01-17 07:07:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vn44t e074de21-9899-42aa-9fda-e0d33d459cf4 0xc0018e9ed0 0xc0018e9ed1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e074de21-9899-42aa-9fda-e0d33d459cf4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0018e9fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:07:30.205: INFO: Pod "test-deployment-vn44t-777898ffcc-pqbq4" is available:
&Pod{ObjectMeta:{test-deployment-vn44t-777898ffcc-pqbq4 test-deployment-vn44t-777898ffcc- deployment-7490  a9b68434-c37a-4cb0-927f-240672fa2229 15800 0 2023-01-17 07:07:28 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:1f62cdce8baaa02da10d6837decf3c628e8cc7f5390b7e166482bf1c042089ef cni.projectcalico.org/podIP:10.100.135.42/32 cni.projectcalico.org/podIPs:10.100.135.42/32] [{apps/v1 ReplicaSet test-deployment-vn44t-777898ffcc 8f44a941-f521-4b8e-b751-f84034bfd359 0xc004d32e50 0xc004d32e51}] [] [{kube-controller-manager Update v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f44a941-f521-4b8e-b751-f84034bfd359\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hw57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hw57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.42,StartTime:2023-01-17 07:07:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:07:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a6dbc36768e8aaa9590dec717455b10e72ef7e22fee23e87e68c575e5fce3c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:07:30.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7490" for this suite. 01/17/23 07:07:30.216
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":133,"skipped":2260,"failed":0}
------------------------------
• [2.324 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:27.908
    Jan 17 07:07:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:07:27.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:27.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:27.953
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/17/23 07:07:27.972
    Jan 17 07:07:27.973: INFO: Creating simple deployment test-deployment-vn44t
    Jan 17 07:07:28.051: INFO: deployment "test-deployment-vn44t" doesn't have the required revision set
    STEP: Getting /status 01/17/23 07:07:30.085
    Jan 17 07:07:30.093: INFO: Deployment test-deployment-vn44t has Conditions: [{Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/17/23 07:07:30.093
    Jan 17 07:07:30.122: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 7, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 7, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vn44t-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/17/23 07:07:30.122
    Jan 17 07:07:30.126: INFO: Observed &Deployment event: ADDED
    Jan 17 07:07:30.126: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
    Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vn44t-777898ffcc" is progressing.}
    Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 07:07:30.127: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
    Jan 17 07:07:30.127: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.128: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 07:07:30.128: INFO: Observed Deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
    Jan 17 07:07:30.128: INFO: Found Deployment test-deployment-vn44t in namespace deployment-7490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 07:07:30.128: INFO: Deployment test-deployment-vn44t has an updated status
    STEP: patching the Statefulset Status 01/17/23 07:07:30.128
    Jan 17 07:07:30.128: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 07:07:30.146: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/17/23 07:07:30.146
    Jan 17 07:07:30.149: INFO: Observed &Deployment event: ADDED
    Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
    Jan 17 07:07:30.149: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:27 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vn44t-777898ffcc"}
    Jan 17 07:07:30.149: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 07:07:30.149: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:28 +0000 UTC 2023-01-17 07:07:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vn44t-777898ffcc" is progressing.}
    Jan 17 07:07:30.168: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
    Jan 17 07:07:30.168: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 07:07:30 +0000 UTC 2023-01-17 07:07:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.}
    Jan 17 07:07:30.168: INFO: Observed deployment test-deployment-vn44t in namespace deployment-7490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 07:07:30.169: INFO: Observed &Deployment event: MODIFIED
    Jan 17 07:07:30.169: INFO: Found deployment test-deployment-vn44t in namespace deployment-7490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 17 07:07:30.169: INFO: Deployment test-deployment-vn44t has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:07:30.180: INFO: Deployment "test-deployment-vn44t":
    &Deployment{ObjectMeta:{test-deployment-vn44t  deployment-7490  e074de21-9899-42aa-9fda-e0d33d459cf4 15807 1 2023-01-17 07:07:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0018e9088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:07:30 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-vn44t-777898ffcc" has successfully progressed.,LastUpdateTime:2023-01-17 07:07:30 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 07:07:30.188: INFO: New ReplicaSet "test-deployment-vn44t-777898ffcc" of Deployment "test-deployment-vn44t":
    &ReplicaSet{ObjectMeta:{test-deployment-vn44t-777898ffcc  deployment-7490  8f44a941-f521-4b8e-b751-f84034bfd359 15802 1 2023-01-17 07:07:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vn44t e074de21-9899-42aa-9fda-e0d33d459cf4 0xc0018e9ed0 0xc0018e9ed1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e074de21-9899-42aa-9fda-e0d33d459cf4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0018e9fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:07:30.205: INFO: Pod "test-deployment-vn44t-777898ffcc-pqbq4" is available:
    &Pod{ObjectMeta:{test-deployment-vn44t-777898ffcc-pqbq4 test-deployment-vn44t-777898ffcc- deployment-7490  a9b68434-c37a-4cb0-927f-240672fa2229 15800 0 2023-01-17 07:07:28 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:1f62cdce8baaa02da10d6837decf3c628e8cc7f5390b7e166482bf1c042089ef cni.projectcalico.org/podIP:10.100.135.42/32 cni.projectcalico.org/podIPs:10.100.135.42/32] [{apps/v1 ReplicaSet test-deployment-vn44t-777898ffcc 8f44a941-f521-4b8e-b751-f84034bfd359 0xc004d32e50 0xc004d32e51}] [] [{kube-controller-manager Update v1 2023-01-17 07:07:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f44a941-f521-4b8e-b751-f84034bfd359\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:07:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hw57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hw57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.42,StartTime:2023-01-17 07:07:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:07:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1a6dbc36768e8aaa9590dec717455b10e72ef7e22fee23e87e68c575e5fce3c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:07:30.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7490" for this suite. 01/17/23 07:07:30.216
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:30.24
Jan 17 07:07:30.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:07:30.242
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:30.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:30.285
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 17 07:07:30.294: INFO: Creating deployment "test-recreate-deployment"
Jan 17 07:07:30.309: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 17 07:07:30.325: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 17 07:07:32.337: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 17 07:07:32.344: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 17 07:07:32.383: INFO: Updating deployment test-recreate-deployment
Jan 17 07:07:32.383: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:07:32.556: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9418  4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 15861 2 2023-01-17 07:07:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c865a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:07:32 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-17 07:07:32 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 17 07:07:32.562: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9418  7cfd4ff6-dedd-4925-8938-eed57ace725c 15860 1 2023-01-17 07:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 0xc003c86a70 0xc003c86a71}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c86b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:07:32.562: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 17 07:07:32.562: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9418  cb9f6e29-fdcd-4fda-8929-507f03a77528 15849 2 2023-01-17 07:07:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 0xc003c86957 0xc003c86958}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c86a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:07:32.570: INFO: Pod "test-recreate-deployment-9d58999df-jhs4x" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jhs4x test-recreate-deployment-9d58999df- deployment-9418  0b5e4263-e706-45f6-8b1d-43a2ce974ce1 15858 0 2023-01-17 07:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 7cfd4ff6-dedd-4925-8938-eed57ace725c 0xc004de4110 0xc004de4111}] [] [{kube-controller-manager Update v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cfd4ff6-dedd-4925-8938-eed57ace725c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcsqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcsqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:07:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9418" for this suite. 01/17/23 07:07:32.582
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":134,"skipped":2262,"failed":0}
------------------------------
• [2.353 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:30.24
    Jan 17 07:07:30.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:07:30.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:30.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:30.285
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 17 07:07:30.294: INFO: Creating deployment "test-recreate-deployment"
    Jan 17 07:07:30.309: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 17 07:07:30.325: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 17 07:07:32.337: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 17 07:07:32.344: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 17 07:07:32.383: INFO: Updating deployment test-recreate-deployment
    Jan 17 07:07:32.383: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:07:32.556: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9418  4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 15861 2 2023-01-17 07:07:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c865a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:07:32 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-17 07:07:32 +0000 UTC,LastTransitionTime:2023-01-17 07:07:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 17 07:07:32.562: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9418  7cfd4ff6-dedd-4925-8938-eed57ace725c 15860 1 2023-01-17 07:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 0xc003c86a70 0xc003c86a71}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c86b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:07:32.562: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 17 07:07:32.562: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9418  cb9f6e29-fdcd-4fda-8929-507f03a77528 15849 2 2023-01-17 07:07:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7 0xc003c86957 0xc003c86958}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0e27c5-ea76-42c5-9b1a-0ada9ecde8d7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c86a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:07:32.570: INFO: Pod "test-recreate-deployment-9d58999df-jhs4x" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jhs4x test-recreate-deployment-9d58999df- deployment-9418  0b5e4263-e706-45f6-8b1d-43a2ce974ce1 15858 0 2023-01-17 07:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 7cfd4ff6-dedd-4925-8938-eed57ace725c 0xc004de4110 0xc004de4111}] [] [{kube-controller-manager Update v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cfd4ff6-dedd-4925-8938-eed57ace725c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcsqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcsqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:07:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:07:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9418" for this suite. 01/17/23 07:07:32.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:32.594
Jan 17 07:07:32.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:07:32.595
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:32.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:32.636
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-049a6667-a77c-4564-a243-ec7fb26bffc7 01/17/23 07:07:32.648
STEP: Creating a pod to test consume configMaps 01/17/23 07:07:32.66
Jan 17 07:07:32.679: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9" in namespace "projected-1636" to be "Succeeded or Failed"
Jan 17 07:07:32.694: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.443855ms
Jan 17 07:07:34.702: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022299278s
Jan 17 07:07:36.704: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024207407s
STEP: Saw pod success 01/17/23 07:07:36.704
Jan 17 07:07:36.704: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9" satisfied condition "Succeeded or Failed"
Jan 17 07:07:36.716: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:07:36.817
Jan 17 07:07:36.906: INFO: Waiting for pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 to disappear
Jan 17 07:07:36.914: INFO: Pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 07:07:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1636" for this suite. 01/17/23 07:07:36.92
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":135,"skipped":2270,"failed":0}
------------------------------
• [4.339 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:32.594
    Jan 17 07:07:32.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:07:32.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:32.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:32.636
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-049a6667-a77c-4564-a243-ec7fb26bffc7 01/17/23 07:07:32.648
    STEP: Creating a pod to test consume configMaps 01/17/23 07:07:32.66
    Jan 17 07:07:32.679: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9" in namespace "projected-1636" to be "Succeeded or Failed"
    Jan 17 07:07:32.694: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.443855ms
    Jan 17 07:07:34.702: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022299278s
    Jan 17 07:07:36.704: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024207407s
    STEP: Saw pod success 01/17/23 07:07:36.704
    Jan 17 07:07:36.704: INFO: Pod "pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9" satisfied condition "Succeeded or Failed"
    Jan 17 07:07:36.716: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:07:36.817
    Jan 17 07:07:36.906: INFO: Waiting for pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 to disappear
    Jan 17 07:07:36.914: INFO: Pod pod-projected-configmaps-df5f3123-bc97-4eb6-a9fd-5628713763e9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 07:07:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1636" for this suite. 01/17/23 07:07:36.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:36.937
Jan 17 07:07:36.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:07:36.94
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:36.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:36.998
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-38ca7b27-5cfb-4ee8-bfc9-5a62893e124e 01/17/23 07:07:37.006
STEP: Creating a pod to test consume configMaps 01/17/23 07:07:37.024
Jan 17 07:07:37.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90" in namespace "projected-7376" to be "Succeeded or Failed"
Jan 17 07:07:37.070: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Pending", Reason="", readiness=false. Elapsed: 12.597361ms
Jan 17 07:07:39.084: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026414487s
Jan 17 07:07:41.076: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01863739s
STEP: Saw pod success 01/17/23 07:07:41.076
Jan 17 07:07:41.076: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90" satisfied condition "Succeeded or Failed"
Jan 17 07:07:41.080: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:07:41.091
Jan 17 07:07:41.121: INFO: Waiting for pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 to disappear
Jan 17 07:07:41.126: INFO: Pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 07:07:41.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7376" for this suite. 01/17/23 07:07:41.135
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":136,"skipped":2284,"failed":0}
------------------------------
• [4.221 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:36.937
    Jan 17 07:07:36.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:07:36.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:36.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:36.998
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-38ca7b27-5cfb-4ee8-bfc9-5a62893e124e 01/17/23 07:07:37.006
    STEP: Creating a pod to test consume configMaps 01/17/23 07:07:37.024
    Jan 17 07:07:37.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90" in namespace "projected-7376" to be "Succeeded or Failed"
    Jan 17 07:07:37.070: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Pending", Reason="", readiness=false. Elapsed: 12.597361ms
    Jan 17 07:07:39.084: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026414487s
    Jan 17 07:07:41.076: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01863739s
    STEP: Saw pod success 01/17/23 07:07:41.076
    Jan 17 07:07:41.076: INFO: Pod "pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90" satisfied condition "Succeeded or Failed"
    Jan 17 07:07:41.080: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:07:41.091
    Jan 17 07:07:41.121: INFO: Waiting for pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 to disappear
    Jan 17 07:07:41.126: INFO: Pod pod-projected-configmaps-d7a350bf-9bb6-4c08-81d4-58b01e98fd90 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 07:07:41.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7376" for this suite. 01/17/23 07:07:41.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:41.159
Jan 17 07:07:41.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:07:41.16
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:41.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:41.196
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 17 07:07:41.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:07:47.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1900" for this suite. 01/17/23 07:07:47.572
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":137,"skipped":2293,"failed":0}
------------------------------
• [SLOW TEST] [6.426 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:41.159
    Jan 17 07:07:41.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:07:41.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:41.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:41.196
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 17 07:07:41.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:07:47.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1900" for this suite. 01/17/23 07:07:47.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:47.588
Jan 17 07:07:47.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:07:47.589
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:47.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:47.628
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-224 01/17/23 07:07:47.636
STEP: creating service affinity-nodeport-transition in namespace services-224 01/17/23 07:07:47.637
STEP: creating replication controller affinity-nodeport-transition in namespace services-224 01/17/23 07:07:47.675
I0117 07:07:47.691861      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-224, replica count: 3
I0117 07:07:50.743513      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:07:50.769: INFO: Creating new exec pod
Jan 17 07:07:50.791: INFO: Waiting up to 5m0s for pod "execpod-affinitybsxc4" in namespace "services-224" to be "running"
Jan 17 07:07:50.814: INFO: Pod "execpod-affinitybsxc4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.995134ms
Jan 17 07:07:52.835: INFO: Pod "execpod-affinitybsxc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.043786119s
Jan 17 07:07:52.835: INFO: Pod "execpod-affinitybsxc4" satisfied condition "running"
Jan 17 07:07:53.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 17 07:07:54.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 17 07:07:54.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:07:54.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.130.3 80'
Jan 17 07:07:54.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.130.3 80\nConnection to 10.254.130.3 80 port [tcp/http] succeeded!\n"
Jan 17 07:07:54.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:07:54.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30151'
Jan 17 07:07:54.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30151\nConnection to 10.0.0.16 30151 port [tcp/*] succeeded!\n"
Jan 17 07:07:54.722: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:07:54.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30151'
Jan 17 07:07:55.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30151\nConnection to 10.0.0.22 30151 port [tcp/*] succeeded!\n"
Jan 17 07:07:55.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:07:55.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30151/ ; done'
Jan 17 07:07:55.479: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n"
Jan 17 07:07:55.479: INFO: stdout: "\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-jpshc"
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-g5jgf
Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-jpshc
Jan 17 07:07:55.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30151/ ; done'
Jan 17 07:07:55.938: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n"
Jan 17 07:07:55.939: INFO: stdout: "\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw"
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
Jan 17 07:07:55.939: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-224, will wait for the garbage collector to delete the pods 01/17/23 07:07:55.985
Jan 17 07:07:56.082: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.396038ms
Jan 17 07:07:56.183: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.753232ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:07:58.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-224" for this suite. 01/17/23 07:07:58.342
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":138,"skipped":2317,"failed":0}
------------------------------
• [SLOW TEST] [10.768 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:47.588
    Jan 17 07:07:47.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:07:47.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:47.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:47.628
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-224 01/17/23 07:07:47.636
    STEP: creating service affinity-nodeport-transition in namespace services-224 01/17/23 07:07:47.637
    STEP: creating replication controller affinity-nodeport-transition in namespace services-224 01/17/23 07:07:47.675
    I0117 07:07:47.691861      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-224, replica count: 3
    I0117 07:07:50.743513      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:07:50.769: INFO: Creating new exec pod
    Jan 17 07:07:50.791: INFO: Waiting up to 5m0s for pod "execpod-affinitybsxc4" in namespace "services-224" to be "running"
    Jan 17 07:07:50.814: INFO: Pod "execpod-affinitybsxc4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.995134ms
    Jan 17 07:07:52.835: INFO: Pod "execpod-affinitybsxc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.043786119s
    Jan 17 07:07:52.835: INFO: Pod "execpod-affinitybsxc4" satisfied condition "running"
    Jan 17 07:07:53.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 17 07:07:54.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 17 07:07:54.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:07:54.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.130.3 80'
    Jan 17 07:07:54.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.130.3 80\nConnection to 10.254.130.3 80 port [tcp/http] succeeded!\n"
    Jan 17 07:07:54.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:07:54.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30151'
    Jan 17 07:07:54.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30151\nConnection to 10.0.0.16 30151 port [tcp/*] succeeded!\n"
    Jan 17 07:07:54.722: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:07:54.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30151'
    Jan 17 07:07:55.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30151\nConnection to 10.0.0.22 30151 port [tcp/*] succeeded!\n"
    Jan 17 07:07:55.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:07:55.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30151/ ; done'
    Jan 17 07:07:55.479: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n"
    Jan 17 07:07:55.479: INFO: stdout: "\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-jpshc\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-g5jgf\naffinity-nodeport-transition-jpshc"
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-jpshc
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.479: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-g5jgf
    Jan 17 07:07:55.480: INFO: Received response from host: affinity-nodeport-transition-jpshc
    Jan 17 07:07:55.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-224 exec execpod-affinitybsxc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30151/ ; done'
    Jan 17 07:07:55.938: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30151/\n"
    Jan 17 07:07:55.939: INFO: stdout: "\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw\naffinity-nodeport-transition-9fgqw"
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Received response from host: affinity-nodeport-transition-9fgqw
    Jan 17 07:07:55.939: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-224, will wait for the garbage collector to delete the pods 01/17/23 07:07:55.985
    Jan 17 07:07:56.082: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.396038ms
    Jan 17 07:07:56.183: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.753232ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:07:58.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-224" for this suite. 01/17/23 07:07:58.342
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:07:58.361
Jan 17 07:07:58.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 07:07:58.362
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:58.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:58.4
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5199 01/17/23 07:07:58.407
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/17/23 07:07:58.432
Jan 17 07:07:58.464: INFO: Found 0 stateful pods, waiting for 3
Jan 17 07:08:08.473: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:08:08.473: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:08:08.473: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:08:08.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:08:08.787: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:08:08.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:08:08.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 07:08:18.851
Jan 17 07:08:18.882: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/17/23 07:08:18.882
STEP: Updating Pods in reverse ordinal order 01/17/23 07:08:28.926
Jan 17 07:08:28.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:08:29.232: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:08:29.232: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:08:29.232: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:08:39.274: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
Jan 17 07:08:39.275: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 07:08:39.275: INFO: Waiting for Pod statefulset-5199/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 07:08:49.285: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
Jan 17 07:08:49.285: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 07:08:59.289: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
Jan 17 07:08:59.289: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 07:09:09.300: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
STEP: Rolling back to a previous revision 01/17/23 07:09:19.287
Jan 17 07:09:19.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:09:19.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:09:19.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:09:19.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:09:29.624: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/17/23 07:09:39.658
Jan 17 07:09:39.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:09:39.949: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:09:39.949: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:09:39.949: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:09:49.980: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 07:09:59.999: INFO: Deleting all statefulset in ns statefulset-5199
Jan 17 07:10:00.006: INFO: Scaling statefulset ss2 to 0
Jan 17 07:10:10.058: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:10:10.068: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 07:10:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5199" for this suite. 01/17/23 07:10:10.112
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":139,"skipped":2350,"failed":0}
------------------------------
• [SLOW TEST] [131.769 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:07:58.361
    Jan 17 07:07:58.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 07:07:58.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:07:58.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:07:58.4
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5199 01/17/23 07:07:58.407
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/17/23 07:07:58.432
    Jan 17 07:07:58.464: INFO: Found 0 stateful pods, waiting for 3
    Jan 17 07:08:08.473: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:08:08.473: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:08:08.473: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:08:08.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:08:08.787: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:08:08.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:08:08.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 07:08:18.851
    Jan 17 07:08:18.882: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/17/23 07:08:18.882
    STEP: Updating Pods in reverse ordinal order 01/17/23 07:08:28.926
    Jan 17 07:08:28.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:08:29.232: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:08:29.232: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:08:29.232: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:08:39.274: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
    Jan 17 07:08:39.275: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 07:08:39.275: INFO: Waiting for Pod statefulset-5199/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 07:08:49.285: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
    Jan 17 07:08:49.285: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 07:08:59.289: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
    Jan 17 07:08:59.289: INFO: Waiting for Pod statefulset-5199/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 07:09:09.300: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
    STEP: Rolling back to a previous revision 01/17/23 07:09:19.287
    Jan 17 07:09:19.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:09:19.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:09:19.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:09:19.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:09:29.624: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/17/23 07:09:39.658
    Jan 17 07:09:39.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5199 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:09:39.949: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:09:39.949: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:09:39.949: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:09:49.980: INFO: Waiting for StatefulSet statefulset-5199/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 07:09:59.999: INFO: Deleting all statefulset in ns statefulset-5199
    Jan 17 07:10:00.006: INFO: Scaling statefulset ss2 to 0
    Jan 17 07:10:10.058: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:10:10.068: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 07:10:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5199" for this suite. 01/17/23 07:10:10.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:10:10.136
Jan 17 07:10:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:10:10.138
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:10.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:10.177
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 07:10:10.183
Jan 17 07:10:10.203: INFO: Waiting up to 5m0s for pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77" in namespace "emptydir-7656" to be "Succeeded or Failed"
Jan 17 07:10:10.211: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28092ms
Jan 17 07:10:12.217: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013737163s
Jan 17 07:10:14.219: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015605597s
STEP: Saw pod success 01/17/23 07:10:14.219
Jan 17 07:10:14.219: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77" satisfied condition "Succeeded or Failed"
Jan 17 07:10:14.225: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 container test-container: <nil>
STEP: delete the pod 01/17/23 07:10:14.308
Jan 17 07:10:14.344: INFO: Waiting for pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 to disappear
Jan 17 07:10:14.351: INFO: Pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:10:14.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7656" for this suite. 01/17/23 07:10:14.362
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":140,"skipped":2398,"failed":0}
------------------------------
• [4.242 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:10:10.136
    Jan 17 07:10:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:10:10.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:10.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:10.177
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 07:10:10.183
    Jan 17 07:10:10.203: INFO: Waiting up to 5m0s for pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77" in namespace "emptydir-7656" to be "Succeeded or Failed"
    Jan 17 07:10:10.211: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28092ms
    Jan 17 07:10:12.217: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013737163s
    Jan 17 07:10:14.219: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015605597s
    STEP: Saw pod success 01/17/23 07:10:14.219
    Jan 17 07:10:14.219: INFO: Pod "pod-787cca0a-0c2c-4eed-82a0-905e32184a77" satisfied condition "Succeeded or Failed"
    Jan 17 07:10:14.225: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:10:14.308
    Jan 17 07:10:14.344: INFO: Waiting for pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 to disappear
    Jan 17 07:10:14.351: INFO: Pod pod-787cca0a-0c2c-4eed-82a0-905e32184a77 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:10:14.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7656" for this suite. 01/17/23 07:10:14.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:10:14.38
Jan 17 07:10:14.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 07:10:14.381
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:14.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:14.419
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/17/23 07:10:14.428
STEP: Ensuring active pods == parallelism 01/17/23 07:10:14.446
STEP: Orphaning one of the Job's Pods 01/17/23 07:10:16.458
Jan 17 07:10:16.995: INFO: Successfully updated pod "adopt-release-bwmmw"
STEP: Checking that the Job readopts the Pod 01/17/23 07:10:16.995
Jan 17 07:10:16.996: INFO: Waiting up to 15m0s for pod "adopt-release-bwmmw" in namespace "job-6891" to be "adopted"
Jan 17 07:10:17.009: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 13.023328ms
Jan 17 07:10:19.021: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.025221589s
Jan 17 07:10:19.021: INFO: Pod "adopt-release-bwmmw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/17/23 07:10:19.021
Jan 17 07:10:19.540: INFO: Successfully updated pod "adopt-release-bwmmw"
STEP: Checking that the Job releases the Pod 01/17/23 07:10:19.541
Jan 17 07:10:19.541: INFO: Waiting up to 15m0s for pod "adopt-release-bwmmw" in namespace "job-6891" to be "released"
Jan 17 07:10:19.545: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 4.282499ms
Jan 17 07:10:21.569: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.028260018s
Jan 17 07:10:21.569: INFO: Pod "adopt-release-bwmmw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 07:10:21.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6891" for this suite. 01/17/23 07:10:21.575
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":141,"skipped":2412,"failed":0}
------------------------------
• [SLOW TEST] [7.213 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:10:14.38
    Jan 17 07:10:14.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 07:10:14.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:14.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:14.419
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/17/23 07:10:14.428
    STEP: Ensuring active pods == parallelism 01/17/23 07:10:14.446
    STEP: Orphaning one of the Job's Pods 01/17/23 07:10:16.458
    Jan 17 07:10:16.995: INFO: Successfully updated pod "adopt-release-bwmmw"
    STEP: Checking that the Job readopts the Pod 01/17/23 07:10:16.995
    Jan 17 07:10:16.996: INFO: Waiting up to 15m0s for pod "adopt-release-bwmmw" in namespace "job-6891" to be "adopted"
    Jan 17 07:10:17.009: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 13.023328ms
    Jan 17 07:10:19.021: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.025221589s
    Jan 17 07:10:19.021: INFO: Pod "adopt-release-bwmmw" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/17/23 07:10:19.021
    Jan 17 07:10:19.540: INFO: Successfully updated pod "adopt-release-bwmmw"
    STEP: Checking that the Job releases the Pod 01/17/23 07:10:19.541
    Jan 17 07:10:19.541: INFO: Waiting up to 15m0s for pod "adopt-release-bwmmw" in namespace "job-6891" to be "released"
    Jan 17 07:10:19.545: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 4.282499ms
    Jan 17 07:10:21.569: INFO: Pod "adopt-release-bwmmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.028260018s
    Jan 17 07:10:21.569: INFO: Pod "adopt-release-bwmmw" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 07:10:21.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6891" for this suite. 01/17/23 07:10:21.575
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:10:21.593
Jan 17 07:10:21.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename aggregator 01/17/23 07:10:21.595
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:21.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:21.634
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 17 07:10:21.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/17/23 07:10:21.642
Jan 17 07:10:23.602: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 17 07:10:25.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:27.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:29.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:31.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:33.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:35.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:37.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:39.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:41.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:43.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:45.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:47.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:49.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:51.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:10:53.916: INFO: Waited 191.862292ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/17/23 07:10:54.094
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/17/23 07:10:54.101
STEP: List APIServices 01/17/23 07:10:54.118
Jan 17 07:10:54.128: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 17 07:10:54.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1120" for this suite. 01/17/23 07:10:54.448
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":142,"skipped":2416,"failed":0}
------------------------------
• [SLOW TEST] [32.868 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:10:21.593
    Jan 17 07:10:21.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename aggregator 01/17/23 07:10:21.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:21.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:21.634
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 17 07:10:21.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/17/23 07:10:21.642
    Jan 17 07:10:23.602: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 17 07:10:25.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:27.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:29.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:31.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:33.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:35.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:37.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:39.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:41.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:43.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:45.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:47.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:49.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:51.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 10, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:10:53.916: INFO: Waited 191.862292ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/17/23 07:10:54.094
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/17/23 07:10:54.101
    STEP: List APIServices 01/17/23 07:10:54.118
    Jan 17 07:10:54.128: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 17 07:10:54.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-1120" for this suite. 01/17/23 07:10:54.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:10:54.462
Jan 17 07:10:54.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:10:54.463
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:54.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:54.561
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:10:54.583
Jan 17 07:10:54.609: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6794" to be "running and ready"
Jan 17 07:10:54.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.548341ms
Jan 17 07:10:54.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:10:56.630: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.020096445s
Jan 17 07:10:56.630: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 07:10:56.630: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/17/23 07:10:56.634
Jan 17 07:10:56.646: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6794" to be "running and ready"
Jan 17 07:10:56.654: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.86277ms
Jan 17 07:10:56.654: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:10:58.670: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024281628s
Jan 17 07:10:58.670: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 17 07:10:58.670: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/17/23 07:10:58.676
STEP: delete the pod with lifecycle hook 01/17/23 07:10:58.744
Jan 17 07:10:58.761: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 07:10:58.770: INFO: Pod pod-with-poststart-http-hook still exists
Jan 17 07:11:00.770: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 07:11:00.784: INFO: Pod pod-with-poststart-http-hook still exists
Jan 17 07:11:02.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 07:11:02.778: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 07:11:02.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6794" for this suite. 01/17/23 07:11:02.795
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":143,"skipped":2421,"failed":0}
------------------------------
• [SLOW TEST] [8.350 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:10:54.462
    Jan 17 07:10:54.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:10:54.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:10:54.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:10:54.561
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:10:54.583
    Jan 17 07:10:54.609: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6794" to be "running and ready"
    Jan 17 07:10:54.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.548341ms
    Jan 17 07:10:54.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:10:56.630: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.020096445s
    Jan 17 07:10:56.630: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 07:10:56.630: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/17/23 07:10:56.634
    Jan 17 07:10:56.646: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6794" to be "running and ready"
    Jan 17 07:10:56.654: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.86277ms
    Jan 17 07:10:56.654: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:10:58.670: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024281628s
    Jan 17 07:10:58.670: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 17 07:10:58.670: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/17/23 07:10:58.676
    STEP: delete the pod with lifecycle hook 01/17/23 07:10:58.744
    Jan 17 07:10:58.761: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 17 07:10:58.770: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 17 07:11:00.770: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 17 07:11:00.784: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 17 07:11:02.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 17 07:11:02.778: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 07:11:02.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6794" for this suite. 01/17/23 07:11:02.795
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:02.819
Jan 17 07:11:02.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:11:02.82
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:02.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:02.86
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 17 07:11:02.914: INFO: created pod pod-service-account-defaultsa
Jan 17 07:11:02.914: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 17 07:11:02.932: INFO: created pod pod-service-account-mountsa
Jan 17 07:11:02.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 17 07:11:02.955: INFO: created pod pod-service-account-nomountsa
Jan 17 07:11:02.955: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 17 07:11:03.001: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 17 07:11:03.002: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 17 07:11:03.024: INFO: created pod pod-service-account-mountsa-mountspec
Jan 17 07:11:03.024: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 17 07:11:03.047: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 17 07:11:03.047: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 17 07:11:03.074: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 17 07:11:03.074: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 17 07:11:03.104: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 17 07:11:03.104: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 17 07:11:03.121: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 17 07:11:03.121: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 07:11:03.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3528" for this suite. 01/17/23 07:11:03.147
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":144,"skipped":2423,"failed":0}
------------------------------
• [0.352 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:02.819
    Jan 17 07:11:02.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:11:02.82
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:02.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:02.86
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 17 07:11:02.914: INFO: created pod pod-service-account-defaultsa
    Jan 17 07:11:02.914: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 17 07:11:02.932: INFO: created pod pod-service-account-mountsa
    Jan 17 07:11:02.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 17 07:11:02.955: INFO: created pod pod-service-account-nomountsa
    Jan 17 07:11:02.955: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 17 07:11:03.001: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 17 07:11:03.002: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 17 07:11:03.024: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 17 07:11:03.024: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 17 07:11:03.047: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 17 07:11:03.047: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 17 07:11:03.074: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 17 07:11:03.074: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 17 07:11:03.104: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 17 07:11:03.104: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 17 07:11:03.121: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 17 07:11:03.121: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 07:11:03.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3528" for this suite. 01/17/23 07:11:03.147
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:03.173
Jan 17 07:11:03.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:11:03.176
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:03.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:03.255
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 17 07:11:03.393: INFO: Waiting up to 5m0s for pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d" in namespace "svcaccounts-4108" to be "running"
Jan 17 07:11:03.418: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.634619ms
Jan 17 07:11:05.424: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031425407s
Jan 17 07:11:07.426: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Running", Reason="", readiness=true. Elapsed: 4.032826813s
Jan 17 07:11:07.426: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d" satisfied condition "running"
STEP: reading a file in the container 01/17/23 07:11:07.426
Jan 17 07:11:07.426: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/17/23 07:11:07.727
Jan 17 07:11:07.727: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/17/23 07:11:08.044
Jan 17 07:11:08.044: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 17 07:11:08.476: INFO: Got root ca configmap in namespace "svcaccounts-4108"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 07:11:08.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4108" for this suite. 01/17/23 07:11:08.488
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":145,"skipped":2425,"failed":0}
------------------------------
• [SLOW TEST] [5.342 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:03.173
    Jan 17 07:11:03.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:11:03.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:03.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:03.255
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 17 07:11:03.393: INFO: Waiting up to 5m0s for pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d" in namespace "svcaccounts-4108" to be "running"
    Jan 17 07:11:03.418: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.634619ms
    Jan 17 07:11:05.424: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031425407s
    Jan 17 07:11:07.426: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d": Phase="Running", Reason="", readiness=true. Elapsed: 4.032826813s
    Jan 17 07:11:07.426: INFO: Pod "pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d" satisfied condition "running"
    STEP: reading a file in the container 01/17/23 07:11:07.426
    Jan 17 07:11:07.426: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/17/23 07:11:07.727
    Jan 17 07:11:07.727: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/17/23 07:11:08.044
    Jan 17 07:11:08.044: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4108 pod-service-account-702fb14a-84e1-4a27-a378-01b978dca73d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 17 07:11:08.476: INFO: Got root ca configmap in namespace "svcaccounts-4108"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 07:11:08.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4108" for this suite. 01/17/23 07:11:08.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:08.518
Jan 17 07:11:08.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption 01/17/23 07:11:08.519
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:08.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:08.562
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/17/23 07:11:08.588
STEP: Updating PodDisruptionBudget status 01/17/23 07:11:08.596
STEP: Waiting for all pods to be running 01/17/23 07:11:08.622
Jan 17 07:11:08.642: INFO: running pods: 0 < 1
STEP: locating a running pod 01/17/23 07:11:10.65
STEP: Waiting for the pdb to be processed 01/17/23 07:11:10.671
STEP: Patching PodDisruptionBudget status 01/17/23 07:11:10.689
STEP: Waiting for the pdb to be processed 01/17/23 07:11:10.709
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 07:11:10.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4047" for this suite. 01/17/23 07:11:10.728
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":146,"skipped":2465,"failed":0}
------------------------------
• [2.222 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:08.518
    Jan 17 07:11:08.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption 01/17/23 07:11:08.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:08.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:08.562
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/17/23 07:11:08.588
    STEP: Updating PodDisruptionBudget status 01/17/23 07:11:08.596
    STEP: Waiting for all pods to be running 01/17/23 07:11:08.622
    Jan 17 07:11:08.642: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/17/23 07:11:10.65
    STEP: Waiting for the pdb to be processed 01/17/23 07:11:10.671
    STEP: Patching PodDisruptionBudget status 01/17/23 07:11:10.689
    STEP: Waiting for the pdb to be processed 01/17/23 07:11:10.709
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 07:11:10.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4047" for this suite. 01/17/23 07:11:10.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:10.741
Jan 17 07:11:10.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:11:10.743
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:10.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:10.775
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5229-delete-me 01/17/23 07:11:10.801
STEP: Waiting for the RuntimeClass to disappear 01/17/23 07:11:10.812
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 07:11:10.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5229" for this suite. 01/17/23 07:11:10.839
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":147,"skipped":2481,"failed":0}
------------------------------
• [0.125 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:10.741
    Jan 17 07:11:10.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:11:10.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:10.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:10.775
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5229-delete-me 01/17/23 07:11:10.801
    STEP: Waiting for the RuntimeClass to disappear 01/17/23 07:11:10.812
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 07:11:10.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5229" for this suite. 01/17/23 07:11:10.839
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:10.869
Jan 17 07:11:10.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pod-network-test 01/17/23 07:11:10.87
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:10.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:10.914
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7812 01/17/23 07:11:10.923
STEP: creating a selector 01/17/23 07:11:10.923
STEP: Creating the service pods in kubernetes 01/17/23 07:11:10.923
Jan 17 07:11:10.923: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 07:11:11.012: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7812" to be "running and ready"
Jan 17 07:11:11.019: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.387997ms
Jan 17 07:11:11.019: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:11:13.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013690043s
Jan 17 07:11:13.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:11:15.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014697924s
Jan 17 07:11:15.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:11:17.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021246585s
Jan 17 07:11:17.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:11:19.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014199796s
Jan 17 07:11:19.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:11:21.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018666162s
Jan 17 07:11:21.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 07:11:23.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01572966s
Jan 17 07:11:23.028: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 07:11:23.028: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 07:11:23.032: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7812" to be "running and ready"
Jan 17 07:11:23.038: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.268491ms
Jan 17 07:11:23.038: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 07:11:23.038: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 07:11:23.044: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7812" to be "running and ready"
Jan 17 07:11:23.048: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.5011ms
Jan 17 07:11:23.048: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Jan 17 07:11:25.055: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011137942s
Jan 17 07:11:25.055: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Jan 17 07:11:27.053: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.009393196s
Jan 17 07:11:27.053: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Jan 17 07:11:29.054: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.010489916s
Jan 17 07:11:29.054: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Jan 17 07:11:31.056: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01208109s
Jan 17 07:11:31.056: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Jan 17 07:11:33.055: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011158346s
Jan 17 07:11:33.055: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 07:11:33.055: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 07:11:33.06
Jan 17 07:11:33.074: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7812" to be "running"
Jan 17 07:11:33.085: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.59103ms
Jan 17 07:11:35.091: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016561824s
Jan 17 07:11:35.091: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 07:11:35.094: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 07:11:35.094: INFO: Breadth first check of 10.100.206.27 on host 10.0.0.16...
Jan 17 07:11:35.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.206.27&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:35.099: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:35.099: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.206.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:11:35.300: INFO: Waiting for responses: map[]
Jan 17 07:11:35.301: INFO: reached 10.100.206.27 after 0/1 tries
Jan 17 07:11:35.301: INFO: Breadth first check of 10.100.135.57 on host 10.0.0.22...
Jan 17 07:11:35.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.135.57&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:35.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:35.308: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:35.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.135.57%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:11:35.475: INFO: Waiting for responses: map[]
Jan 17 07:11:35.476: INFO: reached 10.100.135.57 after 0/1 tries
Jan 17 07:11:35.476: INFO: Breadth first check of 10.100.168.117 on host 10.0.0.21...
Jan 17 07:11:35.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.168.117&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:35.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:35.484: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:35.484: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.168.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 07:11:35.668: INFO: Waiting for responses: map[]
Jan 17 07:11:35.668: INFO: reached 10.100.168.117 after 0/1 tries
Jan 17 07:11:35.668: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 07:11:35.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7812" for this suite. 01/17/23 07:11:35.676
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":148,"skipped":2485,"failed":0}
------------------------------
• [SLOW TEST] [24.820 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:10.869
    Jan 17 07:11:10.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 07:11:10.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:10.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:10.914
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7812 01/17/23 07:11:10.923
    STEP: creating a selector 01/17/23 07:11:10.923
    STEP: Creating the service pods in kubernetes 01/17/23 07:11:10.923
    Jan 17 07:11:10.923: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 07:11:11.012: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7812" to be "running and ready"
    Jan 17 07:11:11.019: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.387997ms
    Jan 17 07:11:11.019: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:11:13.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013690043s
    Jan 17 07:11:13.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:11:15.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014697924s
    Jan 17 07:11:15.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:11:17.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021246585s
    Jan 17 07:11:17.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:11:19.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014199796s
    Jan 17 07:11:19.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:11:21.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018666162s
    Jan 17 07:11:21.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 07:11:23.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01572966s
    Jan 17 07:11:23.028: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 07:11:23.028: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 07:11:23.032: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7812" to be "running and ready"
    Jan 17 07:11:23.038: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.268491ms
    Jan 17 07:11:23.038: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 07:11:23.038: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 07:11:23.044: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7812" to be "running and ready"
    Jan 17 07:11:23.048: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.5011ms
    Jan 17 07:11:23.048: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Jan 17 07:11:25.055: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011137942s
    Jan 17 07:11:25.055: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Jan 17 07:11:27.053: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.009393196s
    Jan 17 07:11:27.053: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Jan 17 07:11:29.054: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.010489916s
    Jan 17 07:11:29.054: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Jan 17 07:11:31.056: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01208109s
    Jan 17 07:11:31.056: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Jan 17 07:11:33.055: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.011158346s
    Jan 17 07:11:33.055: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 07:11:33.055: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 07:11:33.06
    Jan 17 07:11:33.074: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7812" to be "running"
    Jan 17 07:11:33.085: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.59103ms
    Jan 17 07:11:35.091: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016561824s
    Jan 17 07:11:35.091: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 07:11:35.094: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 07:11:35.094: INFO: Breadth first check of 10.100.206.27 on host 10.0.0.16...
    Jan 17 07:11:35.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.206.27&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:35.099: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:35.099: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.206.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:11:35.300: INFO: Waiting for responses: map[]
    Jan 17 07:11:35.301: INFO: reached 10.100.206.27 after 0/1 tries
    Jan 17 07:11:35.301: INFO: Breadth first check of 10.100.135.57 on host 10.0.0.22...
    Jan 17 07:11:35.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.135.57&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:35.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:35.308: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:35.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.135.57%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:11:35.475: INFO: Waiting for responses: map[]
    Jan 17 07:11:35.476: INFO: reached 10.100.135.57 after 0/1 tries
    Jan 17 07:11:35.476: INFO: Breadth first check of 10.100.168.117 on host 10.0.0.21...
    Jan 17 07:11:35.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.135.55:9080/dial?request=hostname&protocol=udp&host=10.100.168.117&port=8081&tries=1'] Namespace:pod-network-test-7812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:35.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:35.484: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:35.484: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7812/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.135.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.168.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 07:11:35.668: INFO: Waiting for responses: map[]
    Jan 17 07:11:35.668: INFO: reached 10.100.168.117 after 0/1 tries
    Jan 17 07:11:35.668: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 07:11:35.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7812" for this suite. 01/17/23 07:11:35.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:35.69
Jan 17 07:11:35.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/17/23 07:11:35.692
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:35.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:35.735
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/17/23 07:11:35.746
STEP: Creating hostNetwork=false pod 01/17/23 07:11:35.746
Jan 17 07:11:35.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3651" to be "running and ready"
Jan 17 07:11:35.784: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095476ms
Jan 17 07:11:35.784: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:11:37.792: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015306277s
Jan 17 07:11:37.792: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 17 07:11:37.792: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/17/23 07:11:37.797
Jan 17 07:11:37.811: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3651" to be "running and ready"
Jan 17 07:11:37.818: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.881939ms
Jan 17 07:11:37.819: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:11:39.825: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014742554s
Jan 17 07:11:39.826: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 17 07:11:39.826: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/17/23 07:11:39.83
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/17/23 07:11:39.83
Jan 17 07:11:39.830: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:39.832: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:39.832: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 07:11:39.969: INFO: Exec stderr: ""
Jan 17 07:11:39.969: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:39.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:39.970: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:39.971: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 07:11:40.096: INFO: Exec stderr: ""
Jan 17 07:11:40.096: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.097: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.097: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 07:11:40.216: INFO: Exec stderr: ""
Jan 17 07:11:40.216: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.217: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.217: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 07:11:40.396: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/17/23 07:11:40.397
Jan 17 07:11:40.397: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.398: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.398: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 07:11:40.531: INFO: Exec stderr: ""
Jan 17 07:11:40.531: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.532: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.532: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 07:11:40.654: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/17/23 07:11:40.654
Jan 17 07:11:40.654: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.654: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.655: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 07:11:40.784: INFO: Exec stderr: ""
Jan 17 07:11:40.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.787: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.788: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 07:11:40.926: INFO: Exec stderr: ""
Jan 17 07:11:40.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:40.928: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:40.928: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 07:11:41.136: INFO: Exec stderr: ""
Jan 17 07:11:41.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:11:41.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:11:41.137: INFO: ExecWithOptions: Clientset creation
Jan 17 07:11:41.137: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 07:11:41.291: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 17 07:11:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3651" for this suite. 01/17/23 07:11:41.325
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2527,"failed":0}
------------------------------
• [SLOW TEST] [5.656 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:35.69
    Jan 17 07:11:35.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/17/23 07:11:35.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:35.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:35.735
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/17/23 07:11:35.746
    STEP: Creating hostNetwork=false pod 01/17/23 07:11:35.746
    Jan 17 07:11:35.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3651" to be "running and ready"
    Jan 17 07:11:35.784: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095476ms
    Jan 17 07:11:35.784: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:11:37.792: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015306277s
    Jan 17 07:11:37.792: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 17 07:11:37.792: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/17/23 07:11:37.797
    Jan 17 07:11:37.811: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3651" to be "running and ready"
    Jan 17 07:11:37.818: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.881939ms
    Jan 17 07:11:37.819: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:11:39.825: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014742554s
    Jan 17 07:11:39.826: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 17 07:11:39.826: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/17/23 07:11:39.83
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/17/23 07:11:39.83
    Jan 17 07:11:39.830: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:39.832: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:39.832: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 07:11:39.969: INFO: Exec stderr: ""
    Jan 17 07:11:39.969: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:39.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:39.970: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:39.971: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 07:11:40.096: INFO: Exec stderr: ""
    Jan 17 07:11:40.096: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.097: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.097: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 07:11:40.216: INFO: Exec stderr: ""
    Jan 17 07:11:40.216: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.217: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.217: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 07:11:40.396: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/17/23 07:11:40.397
    Jan 17 07:11:40.397: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.398: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.398: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 17 07:11:40.531: INFO: Exec stderr: ""
    Jan 17 07:11:40.531: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.532: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.532: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 17 07:11:40.654: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/17/23 07:11:40.654
    Jan 17 07:11:40.654: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.654: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.655: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 07:11:40.784: INFO: Exec stderr: ""
    Jan 17 07:11:40.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.787: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.788: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 07:11:40.926: INFO: Exec stderr: ""
    Jan 17 07:11:40.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:40.928: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:40.928: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 07:11:41.136: INFO: Exec stderr: ""
    Jan 17 07:11:41.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3651 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:11:41.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:11:41.137: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:11:41.137: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3651/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 07:11:41.291: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 17 07:11:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3651" for this suite. 01/17/23 07:11:41.325
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:41.346
Jan 17 07:11:41.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:11:41.348
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:41.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:41.401
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 07:11:41.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2755" for this suite. 01/17/23 07:11:41.474
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":150,"skipped":2527,"failed":0}
------------------------------
• [0.181 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:41.346
    Jan 17 07:11:41.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:11:41.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:41.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:41.401
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 07:11:41.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2755" for this suite. 01/17/23 07:11:41.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:11:41.542
Jan 17 07:11:41.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename cronjob 01/17/23 07:11:41.544
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:41.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:41.685
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/17/23 07:11:41.707
STEP: Ensuring more than one job is running at a time 01/17/23 07:11:41.722
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/17/23 07:13:01.741
STEP: Removing cronjob 01/17/23 07:13:01.753
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 07:13:01.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9511" for this suite. 01/17/23 07:13:01.787
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":151,"skipped":2546,"failed":0}
------------------------------
• [SLOW TEST] [80.284 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:11:41.542
    Jan 17 07:11:41.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename cronjob 01/17/23 07:11:41.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:11:41.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:11:41.685
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/17/23 07:11:41.707
    STEP: Ensuring more than one job is running at a time 01/17/23 07:11:41.722
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/17/23 07:13:01.741
    STEP: Removing cronjob 01/17/23 07:13:01.753
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 07:13:01.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9511" for this suite. 01/17/23 07:13:01.787
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:01.827
Jan 17 07:13:01.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:13:01.829
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:01.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:01.889
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 17 07:13:01.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: creating the pod 01/17/23 07:13:01.908
STEP: submitting the pod to kubernetes 01/17/23 07:13:01.908
Jan 17 07:13:01.933: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560" in namespace "pods-4041" to be "running and ready"
Jan 17 07:13:01.946: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 13.193763ms
Jan 17 07:13:01.946: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:13:03.952: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019489168s
Jan 17 07:13:03.952: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:13:05.952: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019371533s
Jan 17 07:13:05.952: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:13:07.957: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024069508s
Jan 17 07:13:07.957: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:13:09.953: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Running", Reason="", readiness=true. Elapsed: 8.020247277s
Jan 17 07:13:09.953: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Running (Ready = true)
Jan 17 07:13:09.953: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:13:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4041" for this suite. 01/17/23 07:13:10.103
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":152,"skipped":2548,"failed":0}
------------------------------
• [SLOW TEST] [8.290 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:01.827
    Jan 17 07:13:01.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:13:01.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:01.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:01.889
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 17 07:13:01.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: creating the pod 01/17/23 07:13:01.908
    STEP: submitting the pod to kubernetes 01/17/23 07:13:01.908
    Jan 17 07:13:01.933: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560" in namespace "pods-4041" to be "running and ready"
    Jan 17 07:13:01.946: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 13.193763ms
    Jan 17 07:13:01.946: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:13:03.952: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019489168s
    Jan 17 07:13:03.952: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:13:05.952: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019371533s
    Jan 17 07:13:05.952: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:13:07.957: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024069508s
    Jan 17 07:13:07.957: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:13:09.953: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560": Phase="Running", Reason="", readiness=true. Elapsed: 8.020247277s
    Jan 17 07:13:09.953: INFO: The phase of Pod pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560 is Running (Ready = true)
    Jan 17 07:13:09.953: INFO: Pod "pod-exec-websocket-76e16cd5-2f74-4162-83cb-65a125e87560" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:13:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4041" for this suite. 01/17/23 07:13:10.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:10.118
Jan 17 07:13:10.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename subpath 01/17/23 07:13:10.119
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:10.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:10.162
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 07:13:10.171
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-t5vn 01/17/23 07:13:10.196
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:13:10.196
Jan 17 07:13:10.218: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t5vn" in namespace "subpath-1860" to be "Succeeded or Failed"
Jan 17 07:13:10.236: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174338ms
Jan 17 07:13:12.241: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 2.023417068s
Jan 17 07:13:14.247: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 4.029279205s
Jan 17 07:13:16.242: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 6.024209368s
Jan 17 07:13:18.242: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 8.024343585s
Jan 17 07:13:20.243: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 10.025016012s
Jan 17 07:13:22.253: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 12.035660883s
Jan 17 07:13:24.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 14.02615436s
Jan 17 07:13:26.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 16.026443241s
Jan 17 07:13:28.243: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 18.025188825s
Jan 17 07:13:30.249: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 20.031046439s
Jan 17 07:13:32.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=false. Elapsed: 22.025824668s
Jan 17 07:13:34.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.025920063s
STEP: Saw pod success 01/17/23 07:13:34.244
Jan 17 07:13:34.244: INFO: Pod "pod-subpath-test-projected-t5vn" satisfied condition "Succeeded or Failed"
Jan 17 07:13:34.248: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-subpath-test-projected-t5vn container test-container-subpath-projected-t5vn: <nil>
STEP: delete the pod 01/17/23 07:13:34.311
Jan 17 07:13:34.335: INFO: Waiting for pod pod-subpath-test-projected-t5vn to disappear
Jan 17 07:13:34.343: INFO: Pod pod-subpath-test-projected-t5vn no longer exists
STEP: Deleting pod pod-subpath-test-projected-t5vn 01/17/23 07:13:34.343
Jan 17 07:13:34.343: INFO: Deleting pod "pod-subpath-test-projected-t5vn" in namespace "subpath-1860"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 07:13:34.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1860" for this suite. 01/17/23 07:13:34.356
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":153,"skipped":2550,"failed":0}
------------------------------
• [SLOW TEST] [24.252 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:10.118
    Jan 17 07:13:10.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename subpath 01/17/23 07:13:10.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:10.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:10.162
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 07:13:10.171
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-t5vn 01/17/23 07:13:10.196
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:13:10.196
    Jan 17 07:13:10.218: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t5vn" in namespace "subpath-1860" to be "Succeeded or Failed"
    Jan 17 07:13:10.236: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174338ms
    Jan 17 07:13:12.241: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 2.023417068s
    Jan 17 07:13:14.247: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 4.029279205s
    Jan 17 07:13:16.242: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 6.024209368s
    Jan 17 07:13:18.242: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 8.024343585s
    Jan 17 07:13:20.243: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 10.025016012s
    Jan 17 07:13:22.253: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 12.035660883s
    Jan 17 07:13:24.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 14.02615436s
    Jan 17 07:13:26.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 16.026443241s
    Jan 17 07:13:28.243: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 18.025188825s
    Jan 17 07:13:30.249: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=true. Elapsed: 20.031046439s
    Jan 17 07:13:32.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Running", Reason="", readiness=false. Elapsed: 22.025824668s
    Jan 17 07:13:34.244: INFO: Pod "pod-subpath-test-projected-t5vn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.025920063s
    STEP: Saw pod success 01/17/23 07:13:34.244
    Jan 17 07:13:34.244: INFO: Pod "pod-subpath-test-projected-t5vn" satisfied condition "Succeeded or Failed"
    Jan 17 07:13:34.248: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-subpath-test-projected-t5vn container test-container-subpath-projected-t5vn: <nil>
    STEP: delete the pod 01/17/23 07:13:34.311
    Jan 17 07:13:34.335: INFO: Waiting for pod pod-subpath-test-projected-t5vn to disappear
    Jan 17 07:13:34.343: INFO: Pod pod-subpath-test-projected-t5vn no longer exists
    STEP: Deleting pod pod-subpath-test-projected-t5vn 01/17/23 07:13:34.343
    Jan 17 07:13:34.343: INFO: Deleting pod "pod-subpath-test-projected-t5vn" in namespace "subpath-1860"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 07:13:34.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1860" for this suite. 01/17/23 07:13:34.356
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:34.37
Jan 17 07:13:34.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:13:34.371
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:34.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:34.408
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-785 01/17/23 07:13:34.416
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 07:13:34.452
STEP: creating service externalsvc in namespace services-785 01/17/23 07:13:34.453
STEP: creating replication controller externalsvc in namespace services-785 01/17/23 07:13:34.487
I0117 07:13:34.509586      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-785, replica count: 2
I0117 07:13:37.566533      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/17/23 07:13:37.573
Jan 17 07:13:37.645: INFO: Creating new exec pod
Jan 17 07:13:37.668: INFO: Waiting up to 5m0s for pod "execpodds5tz" in namespace "services-785" to be "running"
Jan 17 07:13:37.684: INFO: Pod "execpodds5tz": Phase="Pending", Reason="", readiness=false. Elapsed: 16.053329ms
Jan 17 07:13:39.698: INFO: Pod "execpodds5tz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030139742s
Jan 17 07:13:41.694: INFO: Pod "execpodds5tz": Phase="Running", Reason="", readiness=true. Elapsed: 4.026234262s
Jan 17 07:13:41.694: INFO: Pod "execpodds5tz" satisfied condition "running"
Jan 17 07:13:41.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-785 exec execpodds5tz -- /bin/sh -x -c nslookup clusterip-service.services-785.svc.cluster.local'
Jan 17 07:13:42.047: INFO: stderr: "+ nslookup clusterip-service.services-785.svc.cluster.local\n"
Jan 17 07:13:42.047: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-785.svc.cluster.local\tcanonical name = externalsvc.services-785.svc.cluster.local.\nName:\texternalsvc.services-785.svc.cluster.local\nAddress: 10.254.34.171\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-785, will wait for the garbage collector to delete the pods 01/17/23 07:13:42.047
Jan 17 07:13:42.141: INFO: Deleting ReplicationController externalsvc took: 30.706619ms
Jan 17 07:13:42.242: INFO: Terminating ReplicationController externalsvc pods took: 100.860318ms
Jan 17 07:13:44.288: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:13:44.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-785" for this suite. 01/17/23 07:13:44.347
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":154,"skipped":2552,"failed":0}
------------------------------
• [SLOW TEST] [9.992 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:34.37
    Jan 17 07:13:34.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:13:34.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:34.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:34.408
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-785 01/17/23 07:13:34.416
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 07:13:34.452
    STEP: creating service externalsvc in namespace services-785 01/17/23 07:13:34.453
    STEP: creating replication controller externalsvc in namespace services-785 01/17/23 07:13:34.487
    I0117 07:13:34.509586      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-785, replica count: 2
    I0117 07:13:37.566533      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/17/23 07:13:37.573
    Jan 17 07:13:37.645: INFO: Creating new exec pod
    Jan 17 07:13:37.668: INFO: Waiting up to 5m0s for pod "execpodds5tz" in namespace "services-785" to be "running"
    Jan 17 07:13:37.684: INFO: Pod "execpodds5tz": Phase="Pending", Reason="", readiness=false. Elapsed: 16.053329ms
    Jan 17 07:13:39.698: INFO: Pod "execpodds5tz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030139742s
    Jan 17 07:13:41.694: INFO: Pod "execpodds5tz": Phase="Running", Reason="", readiness=true. Elapsed: 4.026234262s
    Jan 17 07:13:41.694: INFO: Pod "execpodds5tz" satisfied condition "running"
    Jan 17 07:13:41.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-785 exec execpodds5tz -- /bin/sh -x -c nslookup clusterip-service.services-785.svc.cluster.local'
    Jan 17 07:13:42.047: INFO: stderr: "+ nslookup clusterip-service.services-785.svc.cluster.local\n"
    Jan 17 07:13:42.047: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-785.svc.cluster.local\tcanonical name = externalsvc.services-785.svc.cluster.local.\nName:\texternalsvc.services-785.svc.cluster.local\nAddress: 10.254.34.171\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-785, will wait for the garbage collector to delete the pods 01/17/23 07:13:42.047
    Jan 17 07:13:42.141: INFO: Deleting ReplicationController externalsvc took: 30.706619ms
    Jan 17 07:13:42.242: INFO: Terminating ReplicationController externalsvc pods took: 100.860318ms
    Jan 17 07:13:44.288: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:13:44.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-785" for this suite. 01/17/23 07:13:44.347
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:44.364
Jan 17 07:13:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:13:44.366
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:44.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:44.399
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/17/23 07:13:44.413
Jan 17 07:13:44.438: INFO: Waiting up to 5m0s for pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08" in namespace "downward-api-227" to be "running and ready"
Jan 17 07:13:44.452: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.438893ms
Jan 17 07:13:44.452: INFO: The phase of Pod annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:13:46.458: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08": Phase="Running", Reason="", readiness=true. Elapsed: 2.020796689s
Jan 17 07:13:46.459: INFO: The phase of Pod annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08 is Running (Ready = true)
Jan 17 07:13:46.459: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08" satisfied condition "running and ready"
Jan 17 07:13:46.998: INFO: Successfully updated pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:13:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-227" for this suite. 01/17/23 07:13:49.032
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":155,"skipped":2564,"failed":0}
------------------------------
• [4.688 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:44.364
    Jan 17 07:13:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:13:44.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:44.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:44.399
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/17/23 07:13:44.413
    Jan 17 07:13:44.438: INFO: Waiting up to 5m0s for pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08" in namespace "downward-api-227" to be "running and ready"
    Jan 17 07:13:44.452: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.438893ms
    Jan 17 07:13:44.452: INFO: The phase of Pod annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:13:46.458: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08": Phase="Running", Reason="", readiness=true. Elapsed: 2.020796689s
    Jan 17 07:13:46.459: INFO: The phase of Pod annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08 is Running (Ready = true)
    Jan 17 07:13:46.459: INFO: Pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08" satisfied condition "running and ready"
    Jan 17 07:13:46.998: INFO: Successfully updated pod "annotationupdate1ae29ba5-de92-457b-991e-996d0ed00e08"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:13:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-227" for this suite. 01/17/23 07:13:49.032
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:49.054
Jan 17 07:13:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:13:49.056
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:49.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:49.093
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 07:13:49.109
Jan 17 07:13:49.131: INFO: Waiting up to 5m0s for pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877" in namespace "emptydir-2273" to be "Succeeded or Failed"
Jan 17 07:13:49.148: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Pending", Reason="", readiness=false. Elapsed: 17.346727ms
Jan 17 07:13:51.158: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027129651s
Jan 17 07:13:53.156: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024783523s
STEP: Saw pod success 01/17/23 07:13:53.156
Jan 17 07:13:53.156: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877" satisfied condition "Succeeded or Failed"
Jan 17 07:13:53.162: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 container test-container: <nil>
STEP: delete the pod 01/17/23 07:13:53.256
Jan 17 07:13:53.281: INFO: Waiting for pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 to disappear
Jan 17 07:13:53.298: INFO: Pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:13:53.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2273" for this suite. 01/17/23 07:13:53.306
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":156,"skipped":2567,"failed":0}
------------------------------
• [4.266 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:49.054
    Jan 17 07:13:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:13:49.056
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:49.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:49.093
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 07:13:49.109
    Jan 17 07:13:49.131: INFO: Waiting up to 5m0s for pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877" in namespace "emptydir-2273" to be "Succeeded or Failed"
    Jan 17 07:13:49.148: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Pending", Reason="", readiness=false. Elapsed: 17.346727ms
    Jan 17 07:13:51.158: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027129651s
    Jan 17 07:13:53.156: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024783523s
    STEP: Saw pod success 01/17/23 07:13:53.156
    Jan 17 07:13:53.156: INFO: Pod "pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877" satisfied condition "Succeeded or Failed"
    Jan 17 07:13:53.162: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:13:53.256
    Jan 17 07:13:53.281: INFO: Waiting for pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 to disappear
    Jan 17 07:13:53.298: INFO: Pod pod-4ba2b555-1eec-4cf2-838a-be1a04b6e877 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:13:53.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2273" for this suite. 01/17/23 07:13:53.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:13:53.322
Jan 17 07:13:53.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 07:13:53.323
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:53.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:53.418
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/17/23 07:13:53.44
STEP: delete the rc 01/17/23 07:13:58.509
STEP: wait for the rc to be deleted 01/17/23 07:13:58.536
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/17/23 07:14:03.581
STEP: Gathering metrics 01/17/23 07:14:33.606
W0117 07:14:33.628904      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 07:14:33.628: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 07:14:33.629: INFO: Deleting pod "simpletest.rc-224x4" in namespace "gc-5929"
Jan 17 07:14:33.682: INFO: Deleting pod "simpletest.rc-24zr7" in namespace "gc-5929"
Jan 17 07:14:33.724: INFO: Deleting pod "simpletest.rc-2s7zm" in namespace "gc-5929"
Jan 17 07:14:33.789: INFO: Deleting pod "simpletest.rc-458sh" in namespace "gc-5929"
Jan 17 07:14:33.831: INFO: Deleting pod "simpletest.rc-4gn49" in namespace "gc-5929"
Jan 17 07:14:33.868: INFO: Deleting pod "simpletest.rc-4m9lm" in namespace "gc-5929"
Jan 17 07:14:33.914: INFO: Deleting pod "simpletest.rc-4thl6" in namespace "gc-5929"
Jan 17 07:14:33.956: INFO: Deleting pod "simpletest.rc-4zvxg" in namespace "gc-5929"
Jan 17 07:14:34.002: INFO: Deleting pod "simpletest.rc-55w55" in namespace "gc-5929"
Jan 17 07:14:34.032: INFO: Deleting pod "simpletest.rc-5grf7" in namespace "gc-5929"
Jan 17 07:14:34.108: INFO: Deleting pod "simpletest.rc-5sbgp" in namespace "gc-5929"
Jan 17 07:14:34.185: INFO: Deleting pod "simpletest.rc-5t4cp" in namespace "gc-5929"
Jan 17 07:14:34.259: INFO: Deleting pod "simpletest.rc-67vpf" in namespace "gc-5929"
Jan 17 07:14:34.324: INFO: Deleting pod "simpletest.rc-69qd5" in namespace "gc-5929"
Jan 17 07:14:34.417: INFO: Deleting pod "simpletest.rc-6jjr4" in namespace "gc-5929"
Jan 17 07:14:34.476: INFO: Deleting pod "simpletest.rc-6t2n2" in namespace "gc-5929"
Jan 17 07:14:34.535: INFO: Deleting pod "simpletest.rc-792rr" in namespace "gc-5929"
Jan 17 07:14:34.583: INFO: Deleting pod "simpletest.rc-7v7hp" in namespace "gc-5929"
Jan 17 07:14:34.630: INFO: Deleting pod "simpletest.rc-887cf" in namespace "gc-5929"
Jan 17 07:14:34.675: INFO: Deleting pod "simpletest.rc-888tt" in namespace "gc-5929"
Jan 17 07:14:34.715: INFO: Deleting pod "simpletest.rc-8l28d" in namespace "gc-5929"
Jan 17 07:14:34.788: INFO: Deleting pod "simpletest.rc-8mqqn" in namespace "gc-5929"
Jan 17 07:14:34.878: INFO: Deleting pod "simpletest.rc-8v9jd" in namespace "gc-5929"
Jan 17 07:14:34.939: INFO: Deleting pod "simpletest.rc-8xvcw" in namespace "gc-5929"
Jan 17 07:14:35.075: INFO: Deleting pod "simpletest.rc-94bbn" in namespace "gc-5929"
Jan 17 07:14:35.157: INFO: Deleting pod "simpletest.rc-974ql" in namespace "gc-5929"
Jan 17 07:14:35.257: INFO: Deleting pod "simpletest.rc-98zgw" in namespace "gc-5929"
Jan 17 07:14:35.323: INFO: Deleting pod "simpletest.rc-9d2dq" in namespace "gc-5929"
Jan 17 07:14:35.383: INFO: Deleting pod "simpletest.rc-9jzsw" in namespace "gc-5929"
Jan 17 07:14:35.477: INFO: Deleting pod "simpletest.rc-9pcbw" in namespace "gc-5929"
Jan 17 07:14:35.529: INFO: Deleting pod "simpletest.rc-bvvgz" in namespace "gc-5929"
Jan 17 07:14:35.611: INFO: Deleting pod "simpletest.rc-c7b5x" in namespace "gc-5929"
Jan 17 07:14:35.647: INFO: Deleting pod "simpletest.rc-c7gkz" in namespace "gc-5929"
Jan 17 07:14:35.705: INFO: Deleting pod "simpletest.rc-cbmvz" in namespace "gc-5929"
Jan 17 07:14:35.747: INFO: Deleting pod "simpletest.rc-ccqn7" in namespace "gc-5929"
Jan 17 07:14:35.841: INFO: Deleting pod "simpletest.rc-cfwl7" in namespace "gc-5929"
Jan 17 07:14:35.918: INFO: Deleting pod "simpletest.rc-cjfrt" in namespace "gc-5929"
Jan 17 07:14:35.981: INFO: Deleting pod "simpletest.rc-dcjcz" in namespace "gc-5929"
Jan 17 07:14:36.044: INFO: Deleting pod "simpletest.rc-dhnbp" in namespace "gc-5929"
Jan 17 07:14:36.242: INFO: Deleting pod "simpletest.rc-f6q5h" in namespace "gc-5929"
Jan 17 07:14:36.288: INFO: Deleting pod "simpletest.rc-fr5vh" in namespace "gc-5929"
Jan 17 07:14:36.405: INFO: Deleting pod "simpletest.rc-fx7ml" in namespace "gc-5929"
Jan 17 07:14:36.472: INFO: Deleting pod "simpletest.rc-fzlck" in namespace "gc-5929"
Jan 17 07:14:36.638: INFO: Deleting pod "simpletest.rc-g7gh4" in namespace "gc-5929"
Jan 17 07:14:36.730: INFO: Deleting pod "simpletest.rc-gm8lc" in namespace "gc-5929"
Jan 17 07:14:36.795: INFO: Deleting pod "simpletest.rc-h26bw" in namespace "gc-5929"
Jan 17 07:14:36.952: INFO: Deleting pod "simpletest.rc-h86bm" in namespace "gc-5929"
Jan 17 07:14:37.020: INFO: Deleting pod "simpletest.rc-hbpcq" in namespace "gc-5929"
Jan 17 07:14:37.125: INFO: Deleting pod "simpletest.rc-hghrx" in namespace "gc-5929"
Jan 17 07:14:37.174: INFO: Deleting pod "simpletest.rc-hns96" in namespace "gc-5929"
Jan 17 07:14:37.228: INFO: Deleting pod "simpletest.rc-hpccx" in namespace "gc-5929"
Jan 17 07:14:37.306: INFO: Deleting pod "simpletest.rc-j5v62" in namespace "gc-5929"
Jan 17 07:14:37.362: INFO: Deleting pod "simpletest.rc-j9kk7" in namespace "gc-5929"
Jan 17 07:14:37.427: INFO: Deleting pod "simpletest.rc-jcz49" in namespace "gc-5929"
Jan 17 07:14:37.469: INFO: Deleting pod "simpletest.rc-jzksc" in namespace "gc-5929"
Jan 17 07:14:37.538: INFO: Deleting pod "simpletest.rc-kw4gm" in namespace "gc-5929"
Jan 17 07:14:37.637: INFO: Deleting pod "simpletest.rc-l289n" in namespace "gc-5929"
Jan 17 07:14:37.679: INFO: Deleting pod "simpletest.rc-lblvv" in namespace "gc-5929"
Jan 17 07:14:37.710: INFO: Deleting pod "simpletest.rc-lkq92" in namespace "gc-5929"
Jan 17 07:14:37.756: INFO: Deleting pod "simpletest.rc-m2ghg" in namespace "gc-5929"
Jan 17 07:14:37.794: INFO: Deleting pod "simpletest.rc-mdtfl" in namespace "gc-5929"
Jan 17 07:14:37.871: INFO: Deleting pod "simpletest.rc-mvdvn" in namespace "gc-5929"
Jan 17 07:14:37.920: INFO: Deleting pod "simpletest.rc-n9m9p" in namespace "gc-5929"
Jan 17 07:14:37.958: INFO: Deleting pod "simpletest.rc-nblfd" in namespace "gc-5929"
Jan 17 07:14:38.009: INFO: Deleting pod "simpletest.rc-nc7xp" in namespace "gc-5929"
Jan 17 07:14:38.060: INFO: Deleting pod "simpletest.rc-nmncg" in namespace "gc-5929"
Jan 17 07:14:38.114: INFO: Deleting pod "simpletest.rc-nqfd2" in namespace "gc-5929"
Jan 17 07:14:38.149: INFO: Deleting pod "simpletest.rc-p28ss" in namespace "gc-5929"
Jan 17 07:14:38.182: INFO: Deleting pod "simpletest.rc-pnx64" in namespace "gc-5929"
Jan 17 07:14:38.229: INFO: Deleting pod "simpletest.rc-prgzs" in namespace "gc-5929"
Jan 17 07:14:38.280: INFO: Deleting pod "simpletest.rc-q27lr" in namespace "gc-5929"
Jan 17 07:14:38.345: INFO: Deleting pod "simpletest.rc-q4f2m" in namespace "gc-5929"
Jan 17 07:14:38.391: INFO: Deleting pod "simpletest.rc-q7spf" in namespace "gc-5929"
Jan 17 07:14:38.434: INFO: Deleting pod "simpletest.rc-q9prs" in namespace "gc-5929"
Jan 17 07:14:38.484: INFO: Deleting pod "simpletest.rc-qm7pj" in namespace "gc-5929"
Jan 17 07:14:38.568: INFO: Deleting pod "simpletest.rc-qzk8v" in namespace "gc-5929"
Jan 17 07:14:38.632: INFO: Deleting pod "simpletest.rc-rd72g" in namespace "gc-5929"
Jan 17 07:14:38.671: INFO: Deleting pod "simpletest.rc-rd777" in namespace "gc-5929"
Jan 17 07:14:38.714: INFO: Deleting pod "simpletest.rc-rm599" in namespace "gc-5929"
Jan 17 07:14:38.784: INFO: Deleting pod "simpletest.rc-rmw4f" in namespace "gc-5929"
Jan 17 07:14:38.813: INFO: Deleting pod "simpletest.rc-sg8jj" in namespace "gc-5929"
Jan 17 07:14:38.862: INFO: Deleting pod "simpletest.rc-shh54" in namespace "gc-5929"
Jan 17 07:14:38.964: INFO: Deleting pod "simpletest.rc-snrsx" in namespace "gc-5929"
Jan 17 07:14:39.040: INFO: Deleting pod "simpletest.rc-t5p6h" in namespace "gc-5929"
Jan 17 07:14:39.130: INFO: Deleting pod "simpletest.rc-t65jr" in namespace "gc-5929"
Jan 17 07:14:39.168: INFO: Deleting pod "simpletest.rc-t9t9q" in namespace "gc-5929"
Jan 17 07:14:39.223: INFO: Deleting pod "simpletest.rc-trwgh" in namespace "gc-5929"
Jan 17 07:14:39.263: INFO: Deleting pod "simpletest.rc-vc9t4" in namespace "gc-5929"
Jan 17 07:14:39.298: INFO: Deleting pod "simpletest.rc-vd6wm" in namespace "gc-5929"
Jan 17 07:14:39.362: INFO: Deleting pod "simpletest.rc-vg6kq" in namespace "gc-5929"
Jan 17 07:14:39.414: INFO: Deleting pod "simpletest.rc-vjfjb" in namespace "gc-5929"
Jan 17 07:14:39.465: INFO: Deleting pod "simpletest.rc-vk84g" in namespace "gc-5929"
Jan 17 07:14:39.515: INFO: Deleting pod "simpletest.rc-vpz6m" in namespace "gc-5929"
Jan 17 07:14:39.631: INFO: Deleting pod "simpletest.rc-w9js8" in namespace "gc-5929"
Jan 17 07:14:39.736: INFO: Deleting pod "simpletest.rc-wfjts" in namespace "gc-5929"
Jan 17 07:14:39.780: INFO: Deleting pod "simpletest.rc-xjmwm" in namespace "gc-5929"
Jan 17 07:14:39.925: INFO: Deleting pod "simpletest.rc-xln78" in namespace "gc-5929"
Jan 17 07:14:40.010: INFO: Deleting pod "simpletest.rc-xp6qr" in namespace "gc-5929"
Jan 17 07:14:40.079: INFO: Deleting pod "simpletest.rc-z6rsf" in namespace "gc-5929"
Jan 17 07:14:40.114: INFO: Deleting pod "simpletest.rc-zv4m6" in namespace "gc-5929"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 07:14:40.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5929" for this suite. 01/17/23 07:14:40.177
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":157,"skipped":2590,"failed":0}
------------------------------
• [SLOW TEST] [46.886 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:13:53.322
    Jan 17 07:13:53.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 07:13:53.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:13:53.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:13:53.418
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/17/23 07:13:53.44
    STEP: delete the rc 01/17/23 07:13:58.509
    STEP: wait for the rc to be deleted 01/17/23 07:13:58.536
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/17/23 07:14:03.581
    STEP: Gathering metrics 01/17/23 07:14:33.606
    W0117 07:14:33.628904      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 07:14:33.628: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 17 07:14:33.629: INFO: Deleting pod "simpletest.rc-224x4" in namespace "gc-5929"
    Jan 17 07:14:33.682: INFO: Deleting pod "simpletest.rc-24zr7" in namespace "gc-5929"
    Jan 17 07:14:33.724: INFO: Deleting pod "simpletest.rc-2s7zm" in namespace "gc-5929"
    Jan 17 07:14:33.789: INFO: Deleting pod "simpletest.rc-458sh" in namespace "gc-5929"
    Jan 17 07:14:33.831: INFO: Deleting pod "simpletest.rc-4gn49" in namespace "gc-5929"
    Jan 17 07:14:33.868: INFO: Deleting pod "simpletest.rc-4m9lm" in namespace "gc-5929"
    Jan 17 07:14:33.914: INFO: Deleting pod "simpletest.rc-4thl6" in namespace "gc-5929"
    Jan 17 07:14:33.956: INFO: Deleting pod "simpletest.rc-4zvxg" in namespace "gc-5929"
    Jan 17 07:14:34.002: INFO: Deleting pod "simpletest.rc-55w55" in namespace "gc-5929"
    Jan 17 07:14:34.032: INFO: Deleting pod "simpletest.rc-5grf7" in namespace "gc-5929"
    Jan 17 07:14:34.108: INFO: Deleting pod "simpletest.rc-5sbgp" in namespace "gc-5929"
    Jan 17 07:14:34.185: INFO: Deleting pod "simpletest.rc-5t4cp" in namespace "gc-5929"
    Jan 17 07:14:34.259: INFO: Deleting pod "simpletest.rc-67vpf" in namespace "gc-5929"
    Jan 17 07:14:34.324: INFO: Deleting pod "simpletest.rc-69qd5" in namespace "gc-5929"
    Jan 17 07:14:34.417: INFO: Deleting pod "simpletest.rc-6jjr4" in namespace "gc-5929"
    Jan 17 07:14:34.476: INFO: Deleting pod "simpletest.rc-6t2n2" in namespace "gc-5929"
    Jan 17 07:14:34.535: INFO: Deleting pod "simpletest.rc-792rr" in namespace "gc-5929"
    Jan 17 07:14:34.583: INFO: Deleting pod "simpletest.rc-7v7hp" in namespace "gc-5929"
    Jan 17 07:14:34.630: INFO: Deleting pod "simpletest.rc-887cf" in namespace "gc-5929"
    Jan 17 07:14:34.675: INFO: Deleting pod "simpletest.rc-888tt" in namespace "gc-5929"
    Jan 17 07:14:34.715: INFO: Deleting pod "simpletest.rc-8l28d" in namespace "gc-5929"
    Jan 17 07:14:34.788: INFO: Deleting pod "simpletest.rc-8mqqn" in namespace "gc-5929"
    Jan 17 07:14:34.878: INFO: Deleting pod "simpletest.rc-8v9jd" in namespace "gc-5929"
    Jan 17 07:14:34.939: INFO: Deleting pod "simpletest.rc-8xvcw" in namespace "gc-5929"
    Jan 17 07:14:35.075: INFO: Deleting pod "simpletest.rc-94bbn" in namespace "gc-5929"
    Jan 17 07:14:35.157: INFO: Deleting pod "simpletest.rc-974ql" in namespace "gc-5929"
    Jan 17 07:14:35.257: INFO: Deleting pod "simpletest.rc-98zgw" in namespace "gc-5929"
    Jan 17 07:14:35.323: INFO: Deleting pod "simpletest.rc-9d2dq" in namespace "gc-5929"
    Jan 17 07:14:35.383: INFO: Deleting pod "simpletest.rc-9jzsw" in namespace "gc-5929"
    Jan 17 07:14:35.477: INFO: Deleting pod "simpletest.rc-9pcbw" in namespace "gc-5929"
    Jan 17 07:14:35.529: INFO: Deleting pod "simpletest.rc-bvvgz" in namespace "gc-5929"
    Jan 17 07:14:35.611: INFO: Deleting pod "simpletest.rc-c7b5x" in namespace "gc-5929"
    Jan 17 07:14:35.647: INFO: Deleting pod "simpletest.rc-c7gkz" in namespace "gc-5929"
    Jan 17 07:14:35.705: INFO: Deleting pod "simpletest.rc-cbmvz" in namespace "gc-5929"
    Jan 17 07:14:35.747: INFO: Deleting pod "simpletest.rc-ccqn7" in namespace "gc-5929"
    Jan 17 07:14:35.841: INFO: Deleting pod "simpletest.rc-cfwl7" in namespace "gc-5929"
    Jan 17 07:14:35.918: INFO: Deleting pod "simpletest.rc-cjfrt" in namespace "gc-5929"
    Jan 17 07:14:35.981: INFO: Deleting pod "simpletest.rc-dcjcz" in namespace "gc-5929"
    Jan 17 07:14:36.044: INFO: Deleting pod "simpletest.rc-dhnbp" in namespace "gc-5929"
    Jan 17 07:14:36.242: INFO: Deleting pod "simpletest.rc-f6q5h" in namespace "gc-5929"
    Jan 17 07:14:36.288: INFO: Deleting pod "simpletest.rc-fr5vh" in namespace "gc-5929"
    Jan 17 07:14:36.405: INFO: Deleting pod "simpletest.rc-fx7ml" in namespace "gc-5929"
    Jan 17 07:14:36.472: INFO: Deleting pod "simpletest.rc-fzlck" in namespace "gc-5929"
    Jan 17 07:14:36.638: INFO: Deleting pod "simpletest.rc-g7gh4" in namespace "gc-5929"
    Jan 17 07:14:36.730: INFO: Deleting pod "simpletest.rc-gm8lc" in namespace "gc-5929"
    Jan 17 07:14:36.795: INFO: Deleting pod "simpletest.rc-h26bw" in namespace "gc-5929"
    Jan 17 07:14:36.952: INFO: Deleting pod "simpletest.rc-h86bm" in namespace "gc-5929"
    Jan 17 07:14:37.020: INFO: Deleting pod "simpletest.rc-hbpcq" in namespace "gc-5929"
    Jan 17 07:14:37.125: INFO: Deleting pod "simpletest.rc-hghrx" in namespace "gc-5929"
    Jan 17 07:14:37.174: INFO: Deleting pod "simpletest.rc-hns96" in namespace "gc-5929"
    Jan 17 07:14:37.228: INFO: Deleting pod "simpletest.rc-hpccx" in namespace "gc-5929"
    Jan 17 07:14:37.306: INFO: Deleting pod "simpletest.rc-j5v62" in namespace "gc-5929"
    Jan 17 07:14:37.362: INFO: Deleting pod "simpletest.rc-j9kk7" in namespace "gc-5929"
    Jan 17 07:14:37.427: INFO: Deleting pod "simpletest.rc-jcz49" in namespace "gc-5929"
    Jan 17 07:14:37.469: INFO: Deleting pod "simpletest.rc-jzksc" in namespace "gc-5929"
    Jan 17 07:14:37.538: INFO: Deleting pod "simpletest.rc-kw4gm" in namespace "gc-5929"
    Jan 17 07:14:37.637: INFO: Deleting pod "simpletest.rc-l289n" in namespace "gc-5929"
    Jan 17 07:14:37.679: INFO: Deleting pod "simpletest.rc-lblvv" in namespace "gc-5929"
    Jan 17 07:14:37.710: INFO: Deleting pod "simpletest.rc-lkq92" in namespace "gc-5929"
    Jan 17 07:14:37.756: INFO: Deleting pod "simpletest.rc-m2ghg" in namespace "gc-5929"
    Jan 17 07:14:37.794: INFO: Deleting pod "simpletest.rc-mdtfl" in namespace "gc-5929"
    Jan 17 07:14:37.871: INFO: Deleting pod "simpletest.rc-mvdvn" in namespace "gc-5929"
    Jan 17 07:14:37.920: INFO: Deleting pod "simpletest.rc-n9m9p" in namespace "gc-5929"
    Jan 17 07:14:37.958: INFO: Deleting pod "simpletest.rc-nblfd" in namespace "gc-5929"
    Jan 17 07:14:38.009: INFO: Deleting pod "simpletest.rc-nc7xp" in namespace "gc-5929"
    Jan 17 07:14:38.060: INFO: Deleting pod "simpletest.rc-nmncg" in namespace "gc-5929"
    Jan 17 07:14:38.114: INFO: Deleting pod "simpletest.rc-nqfd2" in namespace "gc-5929"
    Jan 17 07:14:38.149: INFO: Deleting pod "simpletest.rc-p28ss" in namespace "gc-5929"
    Jan 17 07:14:38.182: INFO: Deleting pod "simpletest.rc-pnx64" in namespace "gc-5929"
    Jan 17 07:14:38.229: INFO: Deleting pod "simpletest.rc-prgzs" in namespace "gc-5929"
    Jan 17 07:14:38.280: INFO: Deleting pod "simpletest.rc-q27lr" in namespace "gc-5929"
    Jan 17 07:14:38.345: INFO: Deleting pod "simpletest.rc-q4f2m" in namespace "gc-5929"
    Jan 17 07:14:38.391: INFO: Deleting pod "simpletest.rc-q7spf" in namespace "gc-5929"
    Jan 17 07:14:38.434: INFO: Deleting pod "simpletest.rc-q9prs" in namespace "gc-5929"
    Jan 17 07:14:38.484: INFO: Deleting pod "simpletest.rc-qm7pj" in namespace "gc-5929"
    Jan 17 07:14:38.568: INFO: Deleting pod "simpletest.rc-qzk8v" in namespace "gc-5929"
    Jan 17 07:14:38.632: INFO: Deleting pod "simpletest.rc-rd72g" in namespace "gc-5929"
    Jan 17 07:14:38.671: INFO: Deleting pod "simpletest.rc-rd777" in namespace "gc-5929"
    Jan 17 07:14:38.714: INFO: Deleting pod "simpletest.rc-rm599" in namespace "gc-5929"
    Jan 17 07:14:38.784: INFO: Deleting pod "simpletest.rc-rmw4f" in namespace "gc-5929"
    Jan 17 07:14:38.813: INFO: Deleting pod "simpletest.rc-sg8jj" in namespace "gc-5929"
    Jan 17 07:14:38.862: INFO: Deleting pod "simpletest.rc-shh54" in namespace "gc-5929"
    Jan 17 07:14:38.964: INFO: Deleting pod "simpletest.rc-snrsx" in namespace "gc-5929"
    Jan 17 07:14:39.040: INFO: Deleting pod "simpletest.rc-t5p6h" in namespace "gc-5929"
    Jan 17 07:14:39.130: INFO: Deleting pod "simpletest.rc-t65jr" in namespace "gc-5929"
    Jan 17 07:14:39.168: INFO: Deleting pod "simpletest.rc-t9t9q" in namespace "gc-5929"
    Jan 17 07:14:39.223: INFO: Deleting pod "simpletest.rc-trwgh" in namespace "gc-5929"
    Jan 17 07:14:39.263: INFO: Deleting pod "simpletest.rc-vc9t4" in namespace "gc-5929"
    Jan 17 07:14:39.298: INFO: Deleting pod "simpletest.rc-vd6wm" in namespace "gc-5929"
    Jan 17 07:14:39.362: INFO: Deleting pod "simpletest.rc-vg6kq" in namespace "gc-5929"
    Jan 17 07:14:39.414: INFO: Deleting pod "simpletest.rc-vjfjb" in namespace "gc-5929"
    Jan 17 07:14:39.465: INFO: Deleting pod "simpletest.rc-vk84g" in namespace "gc-5929"
    Jan 17 07:14:39.515: INFO: Deleting pod "simpletest.rc-vpz6m" in namespace "gc-5929"
    Jan 17 07:14:39.631: INFO: Deleting pod "simpletest.rc-w9js8" in namespace "gc-5929"
    Jan 17 07:14:39.736: INFO: Deleting pod "simpletest.rc-wfjts" in namespace "gc-5929"
    Jan 17 07:14:39.780: INFO: Deleting pod "simpletest.rc-xjmwm" in namespace "gc-5929"
    Jan 17 07:14:39.925: INFO: Deleting pod "simpletest.rc-xln78" in namespace "gc-5929"
    Jan 17 07:14:40.010: INFO: Deleting pod "simpletest.rc-xp6qr" in namespace "gc-5929"
    Jan 17 07:14:40.079: INFO: Deleting pod "simpletest.rc-z6rsf" in namespace "gc-5929"
    Jan 17 07:14:40.114: INFO: Deleting pod "simpletest.rc-zv4m6" in namespace "gc-5929"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 07:14:40.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5929" for this suite. 01/17/23 07:14:40.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:14:40.21
Jan 17 07:14:40.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:14:40.213
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:14:40.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:14:40.269
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 07:14:40.376: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 07:15:40.471: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:15:40.477
Jan 17 07:15:40.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 07:15:40.479
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:15:40.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:15:40.515
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/17/23 07:15:40.522
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:15:40.525
Jan 17 07:15:40.545: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3240" to be "running"
Jan 17 07:15:40.554: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.099762ms
Jan 17 07:15:42.561: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015850432s
Jan 17 07:15:42.561: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:15:42.566
Jan 17 07:15:42.597: INFO: found a healthy node: cluster125-w73dz53kvqes-node-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 17 07:15:58.880: INFO: pods created so far: [1 1 1]
Jan 17 07:15:58.880: INFO: length of pods created so far: 3
Jan 17 07:16:02.899: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 17 07:16:09.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3240" for this suite. 01/17/23 07:16:09.911
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:16:09.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9204" for this suite. 01/17/23 07:16:10.002
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":158,"skipped":2605,"failed":0}
------------------------------
• [SLOW TEST] [89.896 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:14:40.21
    Jan 17 07:14:40.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:14:40.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:14:40.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:14:40.269
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 07:14:40.376: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 07:15:40.471: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:15:40.477
    Jan 17 07:15:40.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 07:15:40.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:15:40.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:15:40.515
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/17/23 07:15:40.522
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:15:40.525
    Jan 17 07:15:40.545: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3240" to be "running"
    Jan 17 07:15:40.554: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.099762ms
    Jan 17 07:15:42.561: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015850432s
    Jan 17 07:15:42.561: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:15:42.566
    Jan 17 07:15:42.597: INFO: found a healthy node: cluster125-w73dz53kvqes-node-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 17 07:15:58.880: INFO: pods created so far: [1 1 1]
    Jan 17 07:15:58.880: INFO: length of pods created so far: 3
    Jan 17 07:16:02.899: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 17 07:16:09.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3240" for this suite. 01/17/23 07:16:09.911
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:16:09.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9204" for this suite. 01/17/23 07:16:10.002
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:16:10.107
Jan 17 07:16:10.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:16:10.109
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:10.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:10.146
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-d9ea50b9-e8c4-4965-b790-55c1ec9d9192 01/17/23 07:16:10.154
STEP: Creating a pod to test consume secrets 01/17/23 07:16:10.164
Jan 17 07:16:10.181: INFO: Waiting up to 5m0s for pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7" in namespace "secrets-7588" to be "Succeeded or Failed"
Jan 17 07:16:10.190: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.429869ms
Jan 17 07:16:12.198: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016535437s
Jan 17 07:16:14.197: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01577505s
STEP: Saw pod success 01/17/23 07:16:14.197
Jan 17 07:16:14.197: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7" satisfied condition "Succeeded or Failed"
Jan 17 07:16:14.202: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:16:14.267
Jan 17 07:16:14.302: INFO: Waiting for pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 to disappear
Jan 17 07:16:14.313: INFO: Pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:16:14.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7588" for this suite. 01/17/23 07:16:14.329
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":159,"skipped":2615,"failed":0}
------------------------------
• [4.239 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:16:10.107
    Jan 17 07:16:10.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:16:10.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:10.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:10.146
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-d9ea50b9-e8c4-4965-b790-55c1ec9d9192 01/17/23 07:16:10.154
    STEP: Creating a pod to test consume secrets 01/17/23 07:16:10.164
    Jan 17 07:16:10.181: INFO: Waiting up to 5m0s for pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7" in namespace "secrets-7588" to be "Succeeded or Failed"
    Jan 17 07:16:10.190: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.429869ms
    Jan 17 07:16:12.198: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016535437s
    Jan 17 07:16:14.197: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01577505s
    STEP: Saw pod success 01/17/23 07:16:14.197
    Jan 17 07:16:14.197: INFO: Pod "pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7" satisfied condition "Succeeded or Failed"
    Jan 17 07:16:14.202: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:16:14.267
    Jan 17 07:16:14.302: INFO: Waiting for pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 to disappear
    Jan 17 07:16:14.313: INFO: Pod pod-secrets-a6d23b4b-9b3b-490f-80b8-14b998da7ce7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:16:14.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7588" for this suite. 01/17/23 07:16:14.329
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:16:14.346
Jan 17 07:16:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename ingressclass 01/17/23 07:16:14.348
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:14.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:14.408
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/17/23 07:16:14.417
STEP: getting /apis/networking.k8s.io 01/17/23 07:16:14.429
STEP: getting /apis/networking.k8s.iov1 01/17/23 07:16:14.434
STEP: creating 01/17/23 07:16:14.439
STEP: getting 01/17/23 07:16:14.483
STEP: listing 01/17/23 07:16:14.49
STEP: watching 01/17/23 07:16:14.497
Jan 17 07:16:14.497: INFO: starting watch
STEP: patching 01/17/23 07:16:14.503
STEP: updating 01/17/23 07:16:14.52
Jan 17 07:16:14.535: INFO: waiting for watch events with expected annotations
Jan 17 07:16:14.535: INFO: saw patched and updated annotations
STEP: deleting 01/17/23 07:16:14.535
STEP: deleting a collection 01/17/23 07:16:14.554
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 17 07:16:14.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7928" for this suite. 01/17/23 07:16:14.594
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":160,"skipped":2616,"failed":0}
------------------------------
• [0.262 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:16:14.346
    Jan 17 07:16:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename ingressclass 01/17/23 07:16:14.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:14.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:14.408
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/17/23 07:16:14.417
    STEP: getting /apis/networking.k8s.io 01/17/23 07:16:14.429
    STEP: getting /apis/networking.k8s.iov1 01/17/23 07:16:14.434
    STEP: creating 01/17/23 07:16:14.439
    STEP: getting 01/17/23 07:16:14.483
    STEP: listing 01/17/23 07:16:14.49
    STEP: watching 01/17/23 07:16:14.497
    Jan 17 07:16:14.497: INFO: starting watch
    STEP: patching 01/17/23 07:16:14.503
    STEP: updating 01/17/23 07:16:14.52
    Jan 17 07:16:14.535: INFO: waiting for watch events with expected annotations
    Jan 17 07:16:14.535: INFO: saw patched and updated annotations
    STEP: deleting 01/17/23 07:16:14.535
    STEP: deleting a collection 01/17/23 07:16:14.554
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 17 07:16:14.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7928" for this suite. 01/17/23 07:16:14.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:16:14.62
Jan 17 07:16:14.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:16:14.622
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:14.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:14.668
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 17 07:16:14.710: INFO: created pod
Jan 17 07:16:14.710: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1698" to be "Succeeded or Failed"
Jan 17 07:16:14.734: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 23.033515ms
Jan 17 07:16:16.742: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.031071941s
Jan 17 07:16:18.741: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.030519425s
Jan 17 07:16:20.740: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029848207s
STEP: Saw pod success 01/17/23 07:16:20.74
Jan 17 07:16:20.741: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 17 07:16:50.741: INFO: polling logs
Jan 17 07:16:50.761: INFO: Pod logs: 
I0117 07:16:15.676429       1 log.go:195] OK: Got token
I0117 07:16:15.676479       1 log.go:195] validating with in-cluster discovery
I0117 07:16:15.677064       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0117 07:16:15.677103       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1698:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673940374, NotBefore:1673939774, IssuedAt:1673939774, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1698", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aebab2bd-cf47-4664-9062-e3755ae459f5"}}}
I0117 07:16:15.739000       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0117 07:16:15.773440       1 log.go:195] OK: Validated signature on JWT
I0117 07:16:15.773553       1 log.go:195] OK: Got valid claims from token!
I0117 07:16:15.773593       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1698:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673940374, NotBefore:1673939774, IssuedAt:1673939774, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1698", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aebab2bd-cf47-4664-9062-e3755ae459f5"}}}

Jan 17 07:16:50.761: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 07:16:50.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1698" for this suite. 01/17/23 07:16:50.809
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":161,"skipped":2649,"failed":0}
------------------------------
• [SLOW TEST] [36.212 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:16:14.62
    Jan 17 07:16:14.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:16:14.622
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:14.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:14.668
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 17 07:16:14.710: INFO: created pod
    Jan 17 07:16:14.710: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1698" to be "Succeeded or Failed"
    Jan 17 07:16:14.734: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 23.033515ms
    Jan 17 07:16:16.742: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.031071941s
    Jan 17 07:16:18.741: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.030519425s
    Jan 17 07:16:20.740: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029848207s
    STEP: Saw pod success 01/17/23 07:16:20.74
    Jan 17 07:16:20.741: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 17 07:16:50.741: INFO: polling logs
    Jan 17 07:16:50.761: INFO: Pod logs: 
    I0117 07:16:15.676429       1 log.go:195] OK: Got token
    I0117 07:16:15.676479       1 log.go:195] validating with in-cluster discovery
    I0117 07:16:15.677064       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0117 07:16:15.677103       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1698:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673940374, NotBefore:1673939774, IssuedAt:1673939774, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1698", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aebab2bd-cf47-4664-9062-e3755ae459f5"}}}
    I0117 07:16:15.739000       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0117 07:16:15.773440       1 log.go:195] OK: Validated signature on JWT
    I0117 07:16:15.773553       1 log.go:195] OK: Got valid claims from token!
    I0117 07:16:15.773593       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1698:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673940374, NotBefore:1673939774, IssuedAt:1673939774, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1698", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aebab2bd-cf47-4664-9062-e3755ae459f5"}}}

    Jan 17 07:16:50.761: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 07:16:50.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1698" for this suite. 01/17/23 07:16:50.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:16:50.834
Jan 17 07:16:50.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-pred 01/17/23 07:16:50.835
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:50.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:50.889
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 07:16:50.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 07:16:50.916: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 07:16:50.931: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
Jan 17 07:16:50.944: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.944: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:16:50.944: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.944: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:16:50.944: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:16:50.944: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.944: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 07:16:50.944: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 07:16:50.945: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 07:16:50.945: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:16:50.945: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:16:50.945: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 07:16:50.945: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 07:16:50.945: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:16:50.945: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:16:50.945: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
Jan 17 07:16:50.958: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.958: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:16:50.958: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.958: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:16:50.958: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:16:50.958: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.958: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:16:50.959: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.959: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:16:50.959: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 07:16:50.959: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.959: INFO: 	Container e2e ready: true, restart count 0
Jan 17 07:16:50.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:16:50.959: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:16:50.959: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:16:50.960: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
Jan 17 07:16:50.984: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:16:50.984: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:16:50.984: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:16:50.984: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container grafana ready: true, restart count 0
Jan 17 07:16:50.984: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 07:16:50.984: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 07:16:50.984: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 07:16:50.984: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:16:50.984: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:16:50.984: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:16:50.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:16:50.984: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:16:50.984: INFO: oidc-discovery-validator from svcaccounts-1698 started at 2023-01-17 07:16:14 +0000 UTC (1 container statuses recorded)
Jan 17 07:16:50.985: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:16:50.985
Jan 17 07:16:51.008: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8799" to be "running"
Jan 17 07:16:51.028: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.350593ms
Jan 17 07:16:53.047: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.038936525s
Jan 17 07:16:53.047: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:16:53.05
STEP: Trying to apply a random label on the found node. 01/17/23 07:16:53.084
STEP: verifying the node has the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb 95 01/17/23 07:16:53.118
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/17/23 07:16:53.125
Jan 17 07:16:53.141: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8799" to be "not pending"
Jan 17 07:16:53.162: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.664815ms
Jan 17 07:16:55.168: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027008908s
Jan 17 07:16:57.170: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02893671s
Jan 17 07:16:57.170: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.22 on the node which pod4 resides and expect not scheduled 01/17/23 07:16:57.17
Jan 17 07:16:57.200: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8799" to be "not pending"
Jan 17 07:16:57.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 39.744252ms
Jan 17 07:16:59.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044978234s
Jan 17 07:17:01.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049633967s
Jan 17 07:17:03.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046514381s
Jan 17 07:17:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047690113s
Jan 17 07:17:07.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045405777s
Jan 17 07:17:09.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047381329s
Jan 17 07:17:11.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050105178s
Jan 17 07:17:13.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.04511671s
Jan 17 07:17:15.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.045463685s
Jan 17 07:17:17.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.04906528s
Jan 17 07:17:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.046564416s
Jan 17 07:17:21.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.045878015s
Jan 17 07:17:23.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.04713035s
Jan 17 07:17:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.047141598s
Jan 17 07:17:27.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.0500424s
Jan 17 07:17:29.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.047352905s
Jan 17 07:17:31.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.046500666s
Jan 17 07:17:33.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.045834152s
Jan 17 07:17:35.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.045974301s
Jan 17 07:17:37.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.047343373s
Jan 17 07:17:39.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.060950379s
Jan 17 07:17:41.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.054809417s
Jan 17 07:17:43.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.045951273s
Jan 17 07:17:45.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.046046195s
Jan 17 07:17:47.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.047189239s
Jan 17 07:17:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.045628457s
Jan 17 07:17:51.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.045464629s
Jan 17 07:17:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.048076665s
Jan 17 07:17:55.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.046538312s
Jan 17 07:17:57.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.045143171s
Jan 17 07:17:59.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.049383217s
Jan 17 07:18:01.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.04869501s
Jan 17 07:18:03.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.045276253s
Jan 17 07:18:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.047065181s
Jan 17 07:18:07.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.046828523s
Jan 17 07:18:09.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.046702242s
Jan 17 07:18:11.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.047788153s
Jan 17 07:18:13.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.04700485s
Jan 17 07:18:15.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.045394184s
Jan 17 07:18:17.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.045948759s
Jan 17 07:18:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.045914741s
Jan 17 07:18:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.046274821s
Jan 17 07:18:23.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.046525911s
Jan 17 07:18:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.046976446s
Jan 17 07:18:27.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.04865858s
Jan 17 07:18:29.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.059940006s
Jan 17 07:18:31.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.048164792s
Jan 17 07:18:33.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.045630509s
Jan 17 07:18:35.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.045627102s
Jan 17 07:18:37.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.054423635s
Jan 17 07:18:39.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.045305715s
Jan 17 07:18:41.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.045365645s
Jan 17 07:18:43.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.047129873s
Jan 17 07:18:45.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.055851415s
Jan 17 07:18:47.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.047991968s
Jan 17 07:18:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.045106374s
Jan 17 07:18:51.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.045940657s
Jan 17 07:18:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.048227096s
Jan 17 07:18:55.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.04721176s
Jan 17 07:18:57.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.046736681s
Jan 17 07:18:59.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.059158596s
Jan 17 07:19:01.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.04980643s
Jan 17 07:19:03.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.048820404s
Jan 17 07:19:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.047473543s
Jan 17 07:19:07.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.050604131s
Jan 17 07:19:09.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.045359928s
Jan 17 07:19:11.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.045823066s
Jan 17 07:19:13.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.044957138s
Jan 17 07:19:15.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.047335478s
Jan 17 07:19:17.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.046269201s
Jan 17 07:19:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.046299024s
Jan 17 07:19:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.046139281s
Jan 17 07:19:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.045860434s
Jan 17 07:19:25.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.049442894s
Jan 17 07:19:27.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.052142262s
Jan 17 07:19:29.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.047035613s
Jan 17 07:19:31.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.050266797s
Jan 17 07:19:33.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.047895175s
Jan 17 07:19:35.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.05368723s
Jan 17 07:19:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.071891176s
Jan 17 07:19:39.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.049180363s
Jan 17 07:19:41.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.04602399s
Jan 17 07:19:43.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.047776573s
Jan 17 07:19:45.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.048825803s
Jan 17 07:19:47.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.046544048s
Jan 17 07:19:49.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.046045661s
Jan 17 07:19:51.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.04594157s
Jan 17 07:19:53.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.047153155s
Jan 17 07:19:55.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.046264514s
Jan 17 07:19:57.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.050088314s
Jan 17 07:19:59.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.048084706s
Jan 17 07:20:01.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.047502682s
Jan 17 07:20:03.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.044487698s
Jan 17 07:20:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.054126907s
Jan 17 07:20:07.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.046950822s
Jan 17 07:20:09.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.049365633s
Jan 17 07:20:11.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.046359338s
Jan 17 07:20:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.051084738s
Jan 17 07:20:15.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.046381208s
Jan 17 07:20:17.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.057091507s
Jan 17 07:20:19.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.04573905s
Jan 17 07:20:21.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.046997421s
Jan 17 07:20:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.045486226s
Jan 17 07:20:25.293: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.093583208s
Jan 17 07:20:27.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.052436911s
Jan 17 07:20:29.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.04872337s
Jan 17 07:20:31.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.047274376s
Jan 17 07:20:33.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.046160321s
Jan 17 07:20:35.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.046903167s
Jan 17 07:20:37.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.064771159s
Jan 17 07:20:39.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.045639387s
Jan 17 07:20:41.262: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.061940194s
Jan 17 07:20:43.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.050426626s
Jan 17 07:20:45.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.047082992s
Jan 17 07:20:47.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.056572473s
Jan 17 07:20:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.045874859s
Jan 17 07:20:51.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.048547415s
Jan 17 07:20:53.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.047083285s
Jan 17 07:20:55.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.053612549s
Jan 17 07:20:57.267: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.067069806s
Jan 17 07:20:59.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.04607483s
Jan 17 07:21:01.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.05175875s
Jan 17 07:21:03.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.048218487s
Jan 17 07:21:05.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.04555731s
Jan 17 07:21:07.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.049136622s
Jan 17 07:21:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.051257473s
Jan 17 07:21:11.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.045102043s
Jan 17 07:21:13.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.047291541s
Jan 17 07:21:15.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.047834927s
Jan 17 07:21:17.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.047353872s
Jan 17 07:21:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.046119249s
Jan 17 07:21:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.046210244s
Jan 17 07:21:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.045336899s
Jan 17 07:21:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.047404339s
Jan 17 07:21:27.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.057796994s
Jan 17 07:21:29.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.044991696s
Jan 17 07:21:31.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.045400837s
Jan 17 07:21:33.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.046529939s
Jan 17 07:21:35.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.044923988s
Jan 17 07:21:37.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.055302418s
Jan 17 07:21:39.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.056639084s
Jan 17 07:21:41.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.049061702s
Jan 17 07:21:43.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.046090832s
Jan 17 07:21:45.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.045263986s
Jan 17 07:21:47.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.047544163s
Jan 17 07:21:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.045725427s
Jan 17 07:21:51.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.046966416s
Jan 17 07:21:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.04838534s
Jan 17 07:21:55.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.045729979s
Jan 17 07:21:57.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.05345424s
Jan 17 07:21:57.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.057978657s
STEP: removing the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:21:57.258
STEP: verifying the node doesn't have the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb 01/17/23 07:21:57.3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:21:57.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8799" for this suite. 01/17/23 07:21:57.325
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":162,"skipped":2655,"failed":0}
------------------------------
• [SLOW TEST] [306.515 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:16:50.834
    Jan 17 07:16:50.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-pred 01/17/23 07:16:50.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:16:50.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:16:50.889
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 07:16:50.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 07:16:50.916: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 07:16:50.931: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
    Jan 17 07:16:50.944: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.944: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:16:50.944: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.944: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:16:50.944: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:16:50.944: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.944: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 17 07:16:50.944: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:16:50.945: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
    Jan 17 07:16:50.958: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.958: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:16:50.958: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.958: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:16:50.958: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:16:50.958: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.958: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.959: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.959: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:16:50.959: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:16:50.960: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
    Jan 17 07:16:50.984: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container grafana ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:16:50.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:16:50.984: INFO: oidc-discovery-validator from svcaccounts-1698 started at 2023-01-17 07:16:14 +0000 UTC (1 container statuses recorded)
    Jan 17 07:16:50.985: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:16:50.985
    Jan 17 07:16:51.008: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8799" to be "running"
    Jan 17 07:16:51.028: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.350593ms
    Jan 17 07:16:53.047: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.038936525s
    Jan 17 07:16:53.047: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:16:53.05
    STEP: Trying to apply a random label on the found node. 01/17/23 07:16:53.084
    STEP: verifying the node has the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb 95 01/17/23 07:16:53.118
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/17/23 07:16:53.125
    Jan 17 07:16:53.141: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8799" to be "not pending"
    Jan 17 07:16:53.162: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.664815ms
    Jan 17 07:16:55.168: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027008908s
    Jan 17 07:16:57.170: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.02893671s
    Jan 17 07:16:57.170: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.22 on the node which pod4 resides and expect not scheduled 01/17/23 07:16:57.17
    Jan 17 07:16:57.200: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8799" to be "not pending"
    Jan 17 07:16:57.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 39.744252ms
    Jan 17 07:16:59.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044978234s
    Jan 17 07:17:01.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049633967s
    Jan 17 07:17:03.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046514381s
    Jan 17 07:17:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047690113s
    Jan 17 07:17:07.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045405777s
    Jan 17 07:17:09.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047381329s
    Jan 17 07:17:11.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050105178s
    Jan 17 07:17:13.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.04511671s
    Jan 17 07:17:15.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.045463685s
    Jan 17 07:17:17.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.04906528s
    Jan 17 07:17:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.046564416s
    Jan 17 07:17:21.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.045878015s
    Jan 17 07:17:23.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.04713035s
    Jan 17 07:17:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.047141598s
    Jan 17 07:17:27.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.0500424s
    Jan 17 07:17:29.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.047352905s
    Jan 17 07:17:31.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.046500666s
    Jan 17 07:17:33.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.045834152s
    Jan 17 07:17:35.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.045974301s
    Jan 17 07:17:37.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.047343373s
    Jan 17 07:17:39.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.060950379s
    Jan 17 07:17:41.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.054809417s
    Jan 17 07:17:43.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.045951273s
    Jan 17 07:17:45.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.046046195s
    Jan 17 07:17:47.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.047189239s
    Jan 17 07:17:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.045628457s
    Jan 17 07:17:51.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.045464629s
    Jan 17 07:17:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.048076665s
    Jan 17 07:17:55.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.046538312s
    Jan 17 07:17:57.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.045143171s
    Jan 17 07:17:59.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.049383217s
    Jan 17 07:18:01.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.04869501s
    Jan 17 07:18:03.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.045276253s
    Jan 17 07:18:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.047065181s
    Jan 17 07:18:07.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.046828523s
    Jan 17 07:18:09.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.046702242s
    Jan 17 07:18:11.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.047788153s
    Jan 17 07:18:13.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.04700485s
    Jan 17 07:18:15.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.045394184s
    Jan 17 07:18:17.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.045948759s
    Jan 17 07:18:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.045914741s
    Jan 17 07:18:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.046274821s
    Jan 17 07:18:23.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.046525911s
    Jan 17 07:18:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.046976446s
    Jan 17 07:18:27.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.04865858s
    Jan 17 07:18:29.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.059940006s
    Jan 17 07:18:31.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.048164792s
    Jan 17 07:18:33.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.045630509s
    Jan 17 07:18:35.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.045627102s
    Jan 17 07:18:37.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.054423635s
    Jan 17 07:18:39.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.045305715s
    Jan 17 07:18:41.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.045365645s
    Jan 17 07:18:43.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.047129873s
    Jan 17 07:18:45.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.055851415s
    Jan 17 07:18:47.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.047991968s
    Jan 17 07:18:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.045106374s
    Jan 17 07:18:51.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.045940657s
    Jan 17 07:18:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.048227096s
    Jan 17 07:18:55.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.04721176s
    Jan 17 07:18:57.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.046736681s
    Jan 17 07:18:59.259: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.059158596s
    Jan 17 07:19:01.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.04980643s
    Jan 17 07:19:03.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.048820404s
    Jan 17 07:19:05.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.047473543s
    Jan 17 07:19:07.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.050604131s
    Jan 17 07:19:09.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.045359928s
    Jan 17 07:19:11.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.045823066s
    Jan 17 07:19:13.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.044957138s
    Jan 17 07:19:15.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.047335478s
    Jan 17 07:19:17.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.046269201s
    Jan 17 07:19:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.046299024s
    Jan 17 07:19:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.046139281s
    Jan 17 07:19:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.045860434s
    Jan 17 07:19:25.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.049442894s
    Jan 17 07:19:27.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.052142262s
    Jan 17 07:19:29.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.047035613s
    Jan 17 07:19:31.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.050266797s
    Jan 17 07:19:33.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.047895175s
    Jan 17 07:19:35.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.05368723s
    Jan 17 07:19:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.071891176s
    Jan 17 07:19:39.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.049180363s
    Jan 17 07:19:41.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.04602399s
    Jan 17 07:19:43.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.047776573s
    Jan 17 07:19:45.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.048825803s
    Jan 17 07:19:47.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.046544048s
    Jan 17 07:19:49.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.046045661s
    Jan 17 07:19:51.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.04594157s
    Jan 17 07:19:53.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.047153155s
    Jan 17 07:19:55.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.046264514s
    Jan 17 07:19:57.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.050088314s
    Jan 17 07:19:59.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.048084706s
    Jan 17 07:20:01.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.047502682s
    Jan 17 07:20:03.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.044487698s
    Jan 17 07:20:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.054126907s
    Jan 17 07:20:07.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.046950822s
    Jan 17 07:20:09.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.049365633s
    Jan 17 07:20:11.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.046359338s
    Jan 17 07:20:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.051084738s
    Jan 17 07:20:15.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.046381208s
    Jan 17 07:20:17.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.057091507s
    Jan 17 07:20:19.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.04573905s
    Jan 17 07:20:21.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.046997421s
    Jan 17 07:20:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.045486226s
    Jan 17 07:20:25.293: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.093583208s
    Jan 17 07:20:27.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.052436911s
    Jan 17 07:20:29.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.04872337s
    Jan 17 07:20:31.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.047274376s
    Jan 17 07:20:33.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.046160321s
    Jan 17 07:20:35.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.046903167s
    Jan 17 07:20:37.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.064771159s
    Jan 17 07:20:39.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.045639387s
    Jan 17 07:20:41.262: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.061940194s
    Jan 17 07:20:43.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.050426626s
    Jan 17 07:20:45.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.047082992s
    Jan 17 07:20:47.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.056572473s
    Jan 17 07:20:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.045874859s
    Jan 17 07:20:51.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.048547415s
    Jan 17 07:20:53.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.047083285s
    Jan 17 07:20:55.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.053612549s
    Jan 17 07:20:57.267: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.067069806s
    Jan 17 07:20:59.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.04607483s
    Jan 17 07:21:01.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.05175875s
    Jan 17 07:21:03.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.048218487s
    Jan 17 07:21:05.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.04555731s
    Jan 17 07:21:07.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.049136622s
    Jan 17 07:21:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.051257473s
    Jan 17 07:21:11.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.045102043s
    Jan 17 07:21:13.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.047291541s
    Jan 17 07:21:15.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.047834927s
    Jan 17 07:21:17.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.047353872s
    Jan 17 07:21:19.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.046119249s
    Jan 17 07:21:21.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.046210244s
    Jan 17 07:21:23.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.045336899s
    Jan 17 07:21:25.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.047404339s
    Jan 17 07:21:27.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.057796994s
    Jan 17 07:21:29.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.044991696s
    Jan 17 07:21:31.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.045400837s
    Jan 17 07:21:33.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.046529939s
    Jan 17 07:21:35.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.044923988s
    Jan 17 07:21:37.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.055302418s
    Jan 17 07:21:39.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.056639084s
    Jan 17 07:21:41.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.049061702s
    Jan 17 07:21:43.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.046090832s
    Jan 17 07:21:45.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.045263986s
    Jan 17 07:21:47.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.047544163s
    Jan 17 07:21:49.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.045725427s
    Jan 17 07:21:51.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.046966416s
    Jan 17 07:21:53.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.04838534s
    Jan 17 07:21:55.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.045729979s
    Jan 17 07:21:57.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.05345424s
    Jan 17 07:21:57.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.057978657s
    STEP: removing the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:21:57.258
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-00926e8d-237b-4599-9765-5c49e92d0adb 01/17/23 07:21:57.3
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:21:57.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8799" for this suite. 01/17/23 07:21:57.325
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:21:57.354
Jan 17 07:21:57.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:21:57.357
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:21:57.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:21:57.416
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-e4d70921-b206-483c-8bcc-fdd37fd2927e 01/17/23 07:21:57.433
STEP: Creating a pod to test consume secrets 01/17/23 07:21:57.456
Jan 17 07:21:57.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb" in namespace "projected-4908" to be "Succeeded or Failed"
Jan 17 07:21:57.495: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.153493ms
Jan 17 07:21:59.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018900951s
Jan 17 07:22:01.502: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Running", Reason="", readiness=false. Elapsed: 4.019221244s
Jan 17 07:22:03.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018138445s
STEP: Saw pod success 01/17/23 07:22:03.501
Jan 17 07:22:03.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb" satisfied condition "Succeeded or Failed"
Jan 17 07:22:03.506: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:22:03.574
Jan 17 07:22:03.601: INFO: Waiting for pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb to disappear
Jan 17 07:22:03.608: INFO: Pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 07:22:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4908" for this suite. 01/17/23 07:22:03.614
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":163,"skipped":2682,"failed":0}
------------------------------
• [SLOW TEST] [6.273 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:21:57.354
    Jan 17 07:21:57.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:21:57.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:21:57.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:21:57.416
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-e4d70921-b206-483c-8bcc-fdd37fd2927e 01/17/23 07:21:57.433
    STEP: Creating a pod to test consume secrets 01/17/23 07:21:57.456
    Jan 17 07:21:57.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb" in namespace "projected-4908" to be "Succeeded or Failed"
    Jan 17 07:21:57.495: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.153493ms
    Jan 17 07:21:59.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018900951s
    Jan 17 07:22:01.502: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Running", Reason="", readiness=false. Elapsed: 4.019221244s
    Jan 17 07:22:03.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018138445s
    STEP: Saw pod success 01/17/23 07:22:03.501
    Jan 17 07:22:03.501: INFO: Pod "pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb" satisfied condition "Succeeded or Failed"
    Jan 17 07:22:03.506: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:22:03.574
    Jan 17 07:22:03.601: INFO: Waiting for pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb to disappear
    Jan 17 07:22:03.608: INFO: Pod pod-projected-secrets-8e438491-3b7a-4fb5-9e82-ba70d3a32fcb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 07:22:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4908" for this suite. 01/17/23 07:22:03.614
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:03.628
Jan 17 07:22:03.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 07:22:03.63
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:03.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:03.662
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 17 07:22:03.709: INFO: Creating ReplicaSet my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a
Jan 17 07:22:03.744: INFO: Pod name my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Found 0 pods out of 1
Jan 17 07:22:08.752: INFO: Pod name my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Found 1 pods out of 1
Jan 17 07:22:08.752: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a" is running
Jan 17 07:22:08.752: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" in namespace "replicaset-4419" to be "running"
Jan 17 07:22:08.759: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5": Phase="Running", Reason="", readiness=true. Elapsed: 7.119628ms
Jan 17 07:22:08.764: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" satisfied condition "running"
Jan 17 07:22:08.764: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:03 +0000 UTC Reason: Message:}])
Jan 17 07:22:08.764: INFO: Trying to dial the pod
Jan 17 07:22:13.784: INFO: Controller my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Got expected result from replica 1 [my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5]: "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 07:22:13.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4419" for this suite. 01/17/23 07:22:13.791
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":164,"skipped":2683,"failed":0}
------------------------------
• [SLOW TEST] [10.181 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:03.628
    Jan 17 07:22:03.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 07:22:03.63
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:03.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:03.662
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 17 07:22:03.709: INFO: Creating ReplicaSet my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a
    Jan 17 07:22:03.744: INFO: Pod name my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Found 0 pods out of 1
    Jan 17 07:22:08.752: INFO: Pod name my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Found 1 pods out of 1
    Jan 17 07:22:08.752: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a" is running
    Jan 17 07:22:08.752: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" in namespace "replicaset-4419" to be "running"
    Jan 17 07:22:08.759: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5": Phase="Running", Reason="", readiness=true. Elapsed: 7.119628ms
    Jan 17 07:22:08.764: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" satisfied condition "running"
    Jan 17 07:22:08.764: INFO: Pod "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:22:03 +0000 UTC Reason: Message:}])
    Jan 17 07:22:08.764: INFO: Trying to dial the pod
    Jan 17 07:22:13.784: INFO: Controller my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a: Got expected result from replica 1 [my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5]: "my-hostname-basic-e8d2db5f-eab7-4b9e-bfd7-7a71c945f91a-wdvf5", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 07:22:13.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4419" for this suite. 01/17/23 07:22:13.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:13.81
Jan 17 07:22:13.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:22:13.812
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:13.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:13.86
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:22:13.906
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:22:14.572
STEP: Deploying the webhook pod 01/17/23 07:22:14.589
STEP: Wait for the deployment to be ready 01/17/23 07:22:14.619
Jan 17 07:22:14.647: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:22:16.662
STEP: Verifying the service has paired with the endpoint 01/17/23 07:22:16.694
Jan 17 07:22:17.694: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 07:22:17.703
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 07:22:17.734
STEP: Creating a dummy validating-webhook-configuration object 01/17/23 07:22:17.765
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/17/23 07:22:17.784
STEP: Creating a dummy mutating-webhook-configuration object 01/17/23 07:22:17.8
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/17/23 07:22:17.818
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:22:17.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1359" for this suite. 01/17/23 07:22:17.871
STEP: Destroying namespace "webhook-1359-markers" for this suite. 01/17/23 07:22:17.888
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":165,"skipped":2700,"failed":0}
------------------------------
• [4.276 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:13.81
    Jan 17 07:22:13.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:22:13.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:13.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:13.86
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:22:13.906
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:22:14.572
    STEP: Deploying the webhook pod 01/17/23 07:22:14.589
    STEP: Wait for the deployment to be ready 01/17/23 07:22:14.619
    Jan 17 07:22:14.647: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:22:16.662
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:22:16.694
    Jan 17 07:22:17.694: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 07:22:17.703
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 07:22:17.734
    STEP: Creating a dummy validating-webhook-configuration object 01/17/23 07:22:17.765
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/17/23 07:22:17.784
    STEP: Creating a dummy mutating-webhook-configuration object 01/17/23 07:22:17.8
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/17/23 07:22:17.818
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:22:17.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1359" for this suite. 01/17/23 07:22:17.871
    STEP: Destroying namespace "webhook-1359-markers" for this suite. 01/17/23 07:22:17.888
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:18.087
Jan 17 07:22:18.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:22:18.089
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:18.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:18.147
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/17/23 07:22:18.154
STEP: submitting the pod to kubernetes 01/17/23 07:22:18.155
Jan 17 07:22:18.182: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" in namespace "pods-4565" to be "running and ready"
Jan 17 07:22:18.196: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Pending", Reason="", readiness=false. Elapsed: 14.574529ms
Jan 17 07:22:18.196: INFO: The phase of Pod pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:22:20.204: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 2.022186351s
Jan 17 07:22:20.204: INFO: The phase of Pod pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89 is Running (Ready = true)
Jan 17 07:22:20.204: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/17/23 07:22:20.218
STEP: updating the pod 01/17/23 07:22:20.231
Jan 17 07:22:20.754: INFO: Successfully updated pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89"
Jan 17 07:22:20.754: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" in namespace "pods-4565" to be "terminated with reason DeadlineExceeded"
Jan 17 07:22:20.768: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 13.642518ms
Jan 17 07:22:22.789: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 2.034898796s
Jan 17 07:22:24.773: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=false. Elapsed: 4.019172542s
Jan 17 07:22:26.778: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.023774468s
Jan 17 07:22:26.778: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:22:26.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4565" for this suite. 01/17/23 07:22:26.791
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":166,"skipped":2703,"failed":0}
------------------------------
• [SLOW TEST] [8.727 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:18.087
    Jan 17 07:22:18.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:22:18.089
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:18.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:18.147
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/17/23 07:22:18.154
    STEP: submitting the pod to kubernetes 01/17/23 07:22:18.155
    Jan 17 07:22:18.182: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" in namespace "pods-4565" to be "running and ready"
    Jan 17 07:22:18.196: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Pending", Reason="", readiness=false. Elapsed: 14.574529ms
    Jan 17 07:22:18.196: INFO: The phase of Pod pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:22:20.204: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 2.022186351s
    Jan 17 07:22:20.204: INFO: The phase of Pod pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89 is Running (Ready = true)
    Jan 17 07:22:20.204: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/17/23 07:22:20.218
    STEP: updating the pod 01/17/23 07:22:20.231
    Jan 17 07:22:20.754: INFO: Successfully updated pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89"
    Jan 17 07:22:20.754: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" in namespace "pods-4565" to be "terminated with reason DeadlineExceeded"
    Jan 17 07:22:20.768: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 13.642518ms
    Jan 17 07:22:22.789: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=true. Elapsed: 2.034898796s
    Jan 17 07:22:24.773: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Running", Reason="", readiness=false. Elapsed: 4.019172542s
    Jan 17 07:22:26.778: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.023774468s
    Jan 17 07:22:26.778: INFO: Pod "pod-update-activedeadlineseconds-69134db8-2cda-4ab5-89fb-24206274be89" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:22:26.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4565" for this suite. 01/17/23 07:22:26.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:26.816
Jan 17 07:22:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 07:22:26.817
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:26.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:26.877
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 17 07:22:26.925: INFO: Waiting up to 2m0s for pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" in namespace "var-expansion-2934" to be "container 0 failed with reason CreateContainerConfigError"
Jan 17 07:22:26.942: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.850016ms
Jan 17 07:22:28.955: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029872599s
Jan 17 07:22:28.955: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 17 07:22:28.955: INFO: Deleting pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" in namespace "var-expansion-2934"
Jan 17 07:22:28.972: INFO: Wait up to 5m0s for pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 07:22:32.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2934" for this suite. 01/17/23 07:22:33.007
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":167,"skipped":2734,"failed":0}
------------------------------
• [SLOW TEST] [6.214 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:26.816
    Jan 17 07:22:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 07:22:26.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:26.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:26.877
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 17 07:22:26.925: INFO: Waiting up to 2m0s for pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" in namespace "var-expansion-2934" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 17 07:22:26.942: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.850016ms
    Jan 17 07:22:28.955: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029872599s
    Jan 17 07:22:28.955: INFO: Pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 17 07:22:28.955: INFO: Deleting pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" in namespace "var-expansion-2934"
    Jan 17 07:22:28.972: INFO: Wait up to 5m0s for pod "var-expansion-69efd5e9-5441-4b8b-ba17-1ce094b5361d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 07:22:32.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2934" for this suite. 01/17/23 07:22:33.007
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:33.03
Jan 17 07:22:33.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename namespaces 01/17/23 07:22:33.032
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:33.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:33.087
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/17/23 07:22:33.096
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:33.136
STEP: Creating a service in the namespace 01/17/23 07:22:33.142
STEP: Deleting the namespace 01/17/23 07:22:33.169
STEP: Waiting for the namespace to be removed. 01/17/23 07:22:33.194
STEP: Recreating the namespace 01/17/23 07:22:39.201
STEP: Verifying there is no service in the namespace 01/17/23 07:22:39.24
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:22:39.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9672" for this suite. 01/17/23 07:22:39.253
STEP: Destroying namespace "nsdeletetest-1272" for this suite. 01/17/23 07:22:39.268
Jan 17 07:22:39.274: INFO: Namespace nsdeletetest-1272 was already deleted
STEP: Destroying namespace "nsdeletetest-7023" for this suite. 01/17/23 07:22:39.274
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":168,"skipped":2738,"failed":0}
------------------------------
• [SLOW TEST] [6.275 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:33.03
    Jan 17 07:22:33.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename namespaces 01/17/23 07:22:33.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:33.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:33.087
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/17/23 07:22:33.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:33.136
    STEP: Creating a service in the namespace 01/17/23 07:22:33.142
    STEP: Deleting the namespace 01/17/23 07:22:33.169
    STEP: Waiting for the namespace to be removed. 01/17/23 07:22:33.194
    STEP: Recreating the namespace 01/17/23 07:22:39.201
    STEP: Verifying there is no service in the namespace 01/17/23 07:22:39.24
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:22:39.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9672" for this suite. 01/17/23 07:22:39.253
    STEP: Destroying namespace "nsdeletetest-1272" for this suite. 01/17/23 07:22:39.268
    Jan 17 07:22:39.274: INFO: Namespace nsdeletetest-1272 was already deleted
    STEP: Destroying namespace "nsdeletetest-7023" for this suite. 01/17/23 07:22:39.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:39.308
Jan 17 07:22:39.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename hostport 01/17/23 07:22:39.309
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:39.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:39.346
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/17/23 07:22:39.366
Jan 17 07:22:39.387: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4871" to be "running and ready"
Jan 17 07:22:39.394: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.126768ms
Jan 17 07:22:39.394: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:22:41.403: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.016719033s
Jan 17 07:22:41.403: INFO: The phase of Pod pod1 is Running (Ready = false)
Jan 17 07:22:43.404: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016916124s
Jan 17 07:22:43.404: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 07:22:43.404: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.22 on the node which pod1 resides and expect scheduled 01/17/23 07:22:43.404
Jan 17 07:22:43.418: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4871" to be "running and ready"
Jan 17 07:22:43.426: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.938237ms
Jan 17 07:22:43.427: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:22:45.433: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014954576s
Jan 17 07:22:45.434: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 07:22:45.434: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.22 but use UDP protocol on the node which pod2 resides 01/17/23 07:22:45.434
Jan 17 07:22:45.449: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4871" to be "running and ready"
Jan 17 07:22:45.459: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.835713ms
Jan 17 07:22:45.459: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:22:47.470: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.021115646s
Jan 17 07:22:47.470: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 17 07:22:47.470: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 17 07:22:47.494: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4871" to be "running and ready"
Jan 17 07:22:47.517: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.398358ms
Jan 17 07:22:47.517: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:22:49.532: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037726194s
Jan 17 07:22:49.532: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 17 07:22:49.532: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/17/23 07:22:49.54
Jan 17 07:22:49.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.22 http://127.0.0.1:54323/hostname] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:22:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:22:49.541: INFO: ExecWithOptions: Clientset creation
Jan 17 07:22:49.541: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.22+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.22, port: 54323 01/17/23 07:22:49.715
Jan 17 07:22:49.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.22:54323/hostname] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:22:49.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:22:49.717: INFO: ExecWithOptions: Clientset creation
Jan 17 07:22:49.718: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.22%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.22, port: 54323 UDP 01/17/23 07:22:49.927
Jan 17 07:22:49.927: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.22 54323] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:22:49.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:22:49.928: INFO: ExecWithOptions: Clientset creation
Jan 17 07:22:49.928: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.22+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 17 07:22:55.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4871" for this suite. 01/17/23 07:22:55.096
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":169,"skipped":2788,"failed":0}
------------------------------
• [SLOW TEST] [15.802 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:39.308
    Jan 17 07:22:39.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename hostport 01/17/23 07:22:39.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:39.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:39.346
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/17/23 07:22:39.366
    Jan 17 07:22:39.387: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4871" to be "running and ready"
    Jan 17 07:22:39.394: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.126768ms
    Jan 17 07:22:39.394: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:22:41.403: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.016719033s
    Jan 17 07:22:41.403: INFO: The phase of Pod pod1 is Running (Ready = false)
    Jan 17 07:22:43.404: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016916124s
    Jan 17 07:22:43.404: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 07:22:43.404: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.22 on the node which pod1 resides and expect scheduled 01/17/23 07:22:43.404
    Jan 17 07:22:43.418: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4871" to be "running and ready"
    Jan 17 07:22:43.426: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.938237ms
    Jan 17 07:22:43.427: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:22:45.433: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014954576s
    Jan 17 07:22:45.434: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 07:22:45.434: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.22 but use UDP protocol on the node which pod2 resides 01/17/23 07:22:45.434
    Jan 17 07:22:45.449: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4871" to be "running and ready"
    Jan 17 07:22:45.459: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.835713ms
    Jan 17 07:22:45.459: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:22:47.470: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.021115646s
    Jan 17 07:22:47.470: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 17 07:22:47.470: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 17 07:22:47.494: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4871" to be "running and ready"
    Jan 17 07:22:47.517: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.398358ms
    Jan 17 07:22:47.517: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:22:49.532: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037726194s
    Jan 17 07:22:49.532: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 17 07:22:49.532: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/17/23 07:22:49.54
    Jan 17 07:22:49.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.22 http://127.0.0.1:54323/hostname] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:22:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:22:49.541: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:22:49.541: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.22+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.22, port: 54323 01/17/23 07:22:49.715
    Jan 17 07:22:49.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.22:54323/hostname] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:22:49.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:22:49.717: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:22:49.718: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.22%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.22, port: 54323 UDP 01/17/23 07:22:49.927
    Jan 17 07:22:49.927: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.22 54323] Namespace:hostport-4871 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:22:49.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:22:49.928: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:22:49.928: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4871/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.22+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 17 07:22:55.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-4871" for this suite. 01/17/23 07:22:55.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:55.114
Jan 17 07:22:55.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:22:55.116
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:55.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:55.154
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 17 07:22:55.176: INFO: Got root ca configmap in namespace "svcaccounts-2149"
Jan 17 07:22:55.191: INFO: Deleted root ca configmap in namespace "svcaccounts-2149"
STEP: waiting for a new root ca configmap created 01/17/23 07:22:55.698
Jan 17 07:22:55.704: INFO: Recreated root ca configmap in namespace "svcaccounts-2149"
Jan 17 07:22:55.715: INFO: Updated root ca configmap in namespace "svcaccounts-2149"
STEP: waiting for the root ca configmap reconciled 01/17/23 07:22:56.215
Jan 17 07:22:56.221: INFO: Reconciled root ca configmap in namespace "svcaccounts-2149"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 07:22:56.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2149" for this suite. 01/17/23 07:22:56.229
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":170,"skipped":2805,"failed":0}
------------------------------
• [1.132 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:55.114
    Jan 17 07:22:55.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:22:55.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:55.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:55.154
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 17 07:22:55.176: INFO: Got root ca configmap in namespace "svcaccounts-2149"
    Jan 17 07:22:55.191: INFO: Deleted root ca configmap in namespace "svcaccounts-2149"
    STEP: waiting for a new root ca configmap created 01/17/23 07:22:55.698
    Jan 17 07:22:55.704: INFO: Recreated root ca configmap in namespace "svcaccounts-2149"
    Jan 17 07:22:55.715: INFO: Updated root ca configmap in namespace "svcaccounts-2149"
    STEP: waiting for the root ca configmap reconciled 01/17/23 07:22:56.215
    Jan 17 07:22:56.221: INFO: Reconciled root ca configmap in namespace "svcaccounts-2149"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 07:22:56.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2149" for this suite. 01/17/23 07:22:56.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:56.249
Jan 17 07:22:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename server-version 01/17/23 07:22:56.251
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.313
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/17/23 07:22:56.325
STEP: Confirm major version 01/17/23 07:22:56.328
Jan 17 07:22:56.328: INFO: Major version: 1
STEP: Confirm minor version 01/17/23 07:22:56.328
Jan 17 07:22:56.328: INFO: cleanMinorVersion: 25
Jan 17 07:22:56.328: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 17 07:22:56.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6429" for this suite. 01/17/23 07:22:56.335
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":171,"skipped":2828,"failed":0}
------------------------------
• [0.105 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:56.249
    Jan 17 07:22:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename server-version 01/17/23 07:22:56.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.313
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/17/23 07:22:56.325
    STEP: Confirm major version 01/17/23 07:22:56.328
    Jan 17 07:22:56.328: INFO: Major version: 1
    STEP: Confirm minor version 01/17/23 07:22:56.328
    Jan 17 07:22:56.328: INFO: cleanMinorVersion: 25
    Jan 17 07:22:56.328: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 17 07:22:56.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-6429" for this suite. 01/17/23 07:22:56.335
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:56.356
Jan 17 07:22:56.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename cronjob 01/17/23 07:22:56.358
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.399
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/17/23 07:22:56.41
STEP: creating 01/17/23 07:22:56.411
STEP: getting 01/17/23 07:22:56.428
STEP: listing 01/17/23 07:22:56.436
STEP: watching 01/17/23 07:22:56.443
Jan 17 07:22:56.443: INFO: starting watch
STEP: cluster-wide listing 01/17/23 07:22:56.447
STEP: cluster-wide watching 01/17/23 07:22:56.457
Jan 17 07:22:56.458: INFO: starting watch
STEP: patching 01/17/23 07:22:56.462
STEP: updating 01/17/23 07:22:56.479
Jan 17 07:22:56.501: INFO: waiting for watch events with expected annotations
Jan 17 07:22:56.501: INFO: saw patched and updated annotations
STEP: patching /status 01/17/23 07:22:56.501
STEP: updating /status 01/17/23 07:22:56.524
STEP: get /status 01/17/23 07:22:56.55
STEP: deleting 01/17/23 07:22:56.557
STEP: deleting a collection 01/17/23 07:22:56.602
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 07:22:56.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4532" for this suite. 01/17/23 07:22:56.631
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":172,"skipped":2832,"failed":0}
------------------------------
• [0.302 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:56.356
    Jan 17 07:22:56.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename cronjob 01/17/23 07:22:56.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.399
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/17/23 07:22:56.41
    STEP: creating 01/17/23 07:22:56.411
    STEP: getting 01/17/23 07:22:56.428
    STEP: listing 01/17/23 07:22:56.436
    STEP: watching 01/17/23 07:22:56.443
    Jan 17 07:22:56.443: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 07:22:56.447
    STEP: cluster-wide watching 01/17/23 07:22:56.457
    Jan 17 07:22:56.458: INFO: starting watch
    STEP: patching 01/17/23 07:22:56.462
    STEP: updating 01/17/23 07:22:56.479
    Jan 17 07:22:56.501: INFO: waiting for watch events with expected annotations
    Jan 17 07:22:56.501: INFO: saw patched and updated annotations
    STEP: patching /status 01/17/23 07:22:56.501
    STEP: updating /status 01/17/23 07:22:56.524
    STEP: get /status 01/17/23 07:22:56.55
    STEP: deleting 01/17/23 07:22:56.557
    STEP: deleting a collection 01/17/23 07:22:56.602
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 07:22:56.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4532" for this suite. 01/17/23 07:22:56.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:56.661
Jan 17 07:22:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename watch 01/17/23 07:22:56.662
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.7
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/17/23 07:22:56.716
STEP: modifying the configmap once 01/17/23 07:22:56.742
STEP: modifying the configmap a second time 01/17/23 07:22:56.773
STEP: deleting the configmap 01/17/23 07:22:56.831
STEP: creating a watch on configmaps from the resource version returned by the first update 01/17/23 07:22:56.881
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/17/23 07:22:56.887
Jan 17 07:22:56.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8963  0aa2bd27-3f45-4f9a-8a2d-01554d177754 23009 0 2023-01-17 07:22:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 07:22:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 07:22:56.888: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8963  0aa2bd27-3f45-4f9a-8a2d-01554d177754 23010 0 2023-01-17 07:22:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 07:22:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 07:22:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8963" for this suite. 01/17/23 07:22:56.902
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":173,"skipped":2839,"failed":0}
------------------------------
• [0.288 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:56.661
    Jan 17 07:22:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename watch 01/17/23 07:22:56.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:56.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:56.7
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/17/23 07:22:56.716
    STEP: modifying the configmap once 01/17/23 07:22:56.742
    STEP: modifying the configmap a second time 01/17/23 07:22:56.773
    STEP: deleting the configmap 01/17/23 07:22:56.831
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/17/23 07:22:56.881
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/17/23 07:22:56.887
    Jan 17 07:22:56.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8963  0aa2bd27-3f45-4f9a-8a2d-01554d177754 23009 0 2023-01-17 07:22:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 07:22:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 07:22:56.888: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8963  0aa2bd27-3f45-4f9a-8a2d-01554d177754 23010 0 2023-01-17 07:22:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 07:22:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 07:22:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8963" for this suite. 01/17/23 07:22:56.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:22:56.963
Jan 17 07:22:56.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 07:22:56.964
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:57.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:57.015
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5959 01/17/23 07:22:57.028
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-5959 01/17/23 07:22:57.045
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5959 01/17/23 07:22:57.066
Jan 17 07:22:57.074: INFO: Found 0 stateful pods, waiting for 1
Jan 17 07:23:07.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/17/23 07:23:07.082
Jan 17 07:23:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:23:07.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:23:07.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:23:07.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:23:07.410: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 07:23:17.420: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:23:17.420: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:23:17.490: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 07:23:17.490: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
Jan 17 07:23:17.490: INFO: 
Jan 17 07:23:17.490: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 17 07:23:18.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97639794s
Jan 17 07:23:19.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.967642606s
Jan 17 07:23:20.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955602609s
Jan 17 07:23:21.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940097719s
Jan 17 07:23:22.550: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933380204s
Jan 17 07:23:23.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921304903s
Jan 17 07:23:24.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.915379218s
Jan 17 07:23:25.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.902056793s
Jan 17 07:23:26.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 894.162768ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5959 01/17/23 07:23:27.588
Jan 17 07:23:27.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:23:27.855: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:23:27.855: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:23:27.855: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:23:27.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:23:28.158: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 07:23:28.158: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:23:28.158: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:23:28.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:23:28.413: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 07:23:28.413: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:23:28.413: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:23:28.419: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 17 07:23:38.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:23:38.427: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:23:38.427: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/17/23 07:23:38.427
Jan 17 07:23:38.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:23:38.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:23:38.727: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:23:38.727: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:23:38.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:23:39.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:23:39.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:23:39.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:23:39.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:23:39.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:23:39.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:23:39.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:23:39.446: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:23:39.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 17 07:23:49.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:23:49.478: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:23:49.478: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:23:49.515: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 07:23:49.515: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
Jan 17 07:23:49.515: INFO: ss-1  cluster125-w73dz53kvqes-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
Jan 17 07:23:49.515: INFO: ss-2  cluster125-w73dz53kvqes-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
Jan 17 07:23:49.515: INFO: 
Jan 17 07:23:49.515: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 17 07:23:50.521: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 07:23:50.522: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
Jan 17 07:23:50.522: INFO: ss-1  cluster125-w73dz53kvqes-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
Jan 17 07:23:50.522: INFO: 
Jan 17 07:23:50.522: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 17 07:23:51.529: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.978433109s
Jan 17 07:23:52.534: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.972265594s
Jan 17 07:23:53.561: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966995186s
Jan 17 07:23:54.567: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.940265701s
Jan 17 07:23:55.572: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.934072399s
Jan 17 07:23:56.581: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.928950008s
Jan 17 07:23:57.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.920387348s
Jan 17 07:23:58.599: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.002471ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5959 01/17/23 07:23:59.599
Jan 17 07:23:59.613: INFO: Scaling statefulset ss to 0
Jan 17 07:23:59.638: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 07:23:59.643: INFO: Deleting all statefulset in ns statefulset-5959
Jan 17 07:23:59.654: INFO: Scaling statefulset ss to 0
Jan 17 07:23:59.676: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:23:59.681: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 07:23:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5959" for this suite. 01/17/23 07:23:59.742
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":174,"skipped":2952,"failed":0}
------------------------------
• [SLOW TEST] [62.795 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:22:56.963
    Jan 17 07:22:56.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 07:22:56.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:22:57.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:22:57.015
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5959 01/17/23 07:22:57.028
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-5959 01/17/23 07:22:57.045
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5959 01/17/23 07:22:57.066
    Jan 17 07:22:57.074: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 07:23:07.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/17/23 07:23:07.082
    Jan 17 07:23:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:23:07.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:23:07.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:23:07.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:23:07.410: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 17 07:23:17.420: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:23:17.420: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:23:17.490: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 17 07:23:17.490: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
    Jan 17 07:23:17.490: INFO: 
    Jan 17 07:23:17.490: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 17 07:23:18.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97639794s
    Jan 17 07:23:19.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.967642606s
    Jan 17 07:23:20.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955602609s
    Jan 17 07:23:21.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940097719s
    Jan 17 07:23:22.550: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933380204s
    Jan 17 07:23:23.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921304903s
    Jan 17 07:23:24.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.915379218s
    Jan 17 07:23:25.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.902056793s
    Jan 17 07:23:26.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 894.162768ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5959 01/17/23 07:23:27.588
    Jan 17 07:23:27.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:23:27.855: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:23:27.855: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:23:27.855: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:23:27.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:23:28.158: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 17 07:23:28.158: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:23:28.158: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:23:28.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:23:28.413: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 17 07:23:28.413: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:23:28.413: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:23:28.419: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 17 07:23:38.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:23:38.427: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:23:38.427: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/17/23 07:23:38.427
    Jan 17 07:23:38.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:23:38.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:23:38.727: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:23:38.727: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:23:38.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:23:39.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:23:39.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:23:39.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:23:39.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-5959 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:23:39.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:23:39.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:23:39.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:23:39.446: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:23:39.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 17 07:23:49.478: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:23:49.478: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:23:49.478: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:23:49.515: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 17 07:23:49.515: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
    Jan 17 07:23:49.515: INFO: ss-1  cluster125-w73dz53kvqes-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
    Jan 17 07:23:49.515: INFO: ss-2  cluster125-w73dz53kvqes-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
    Jan 17 07:23:49.515: INFO: 
    Jan 17 07:23:49.515: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 17 07:23:50.521: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Jan 17 07:23:50.522: INFO: ss-0  cluster125-w73dz53kvqes-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:22:57 +0000 UTC  }]
    Jan 17 07:23:50.522: INFO: ss-1  cluster125-w73dz53kvqes-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 07:23:17 +0000 UTC  }]
    Jan 17 07:23:50.522: INFO: 
    Jan 17 07:23:50.522: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 17 07:23:51.529: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.978433109s
    Jan 17 07:23:52.534: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.972265594s
    Jan 17 07:23:53.561: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966995186s
    Jan 17 07:23:54.567: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.940265701s
    Jan 17 07:23:55.572: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.934072399s
    Jan 17 07:23:56.581: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.928950008s
    Jan 17 07:23:57.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.920387348s
    Jan 17 07:23:58.599: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.002471ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5959 01/17/23 07:23:59.599
    Jan 17 07:23:59.613: INFO: Scaling statefulset ss to 0
    Jan 17 07:23:59.638: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 07:23:59.643: INFO: Deleting all statefulset in ns statefulset-5959
    Jan 17 07:23:59.654: INFO: Scaling statefulset ss to 0
    Jan 17 07:23:59.676: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:23:59.681: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 07:23:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5959" for this suite. 01/17/23 07:23:59.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:23:59.759
Jan 17 07:23:59.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:23:59.761
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:23:59.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:23:59.806
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 07:23:59.818
Jan 17 07:23:59.845: INFO: Waiting up to 5m0s for pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf" in namespace "emptydir-8607" to be "Succeeded or Failed"
Jan 17 07:23:59.859: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.318876ms
Jan 17 07:24:01.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02097124s
Jan 17 07:24:03.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020968759s
STEP: Saw pod success 01/17/23 07:24:03.866
Jan 17 07:24:03.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf" satisfied condition "Succeeded or Failed"
Jan 17 07:24:03.871: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf container test-container: <nil>
STEP: delete the pod 01/17/23 07:24:03.951
Jan 17 07:24:03.980: INFO: Waiting for pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf to disappear
Jan 17 07:24:03.987: INFO: Pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:24:03.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8607" for this suite. 01/17/23 07:24:03.995
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":175,"skipped":2962,"failed":0}
------------------------------
• [4.251 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:23:59.759
    Jan 17 07:23:59.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:23:59.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:23:59.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:23:59.806
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 07:23:59.818
    Jan 17 07:23:59.845: INFO: Waiting up to 5m0s for pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf" in namespace "emptydir-8607" to be "Succeeded or Failed"
    Jan 17 07:23:59.859: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.318876ms
    Jan 17 07:24:01.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02097124s
    Jan 17 07:24:03.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020968759s
    STEP: Saw pod success 01/17/23 07:24:03.866
    Jan 17 07:24:03.866: INFO: Pod "pod-cd74c9bb-1833-475a-a925-ba932d682ebf" satisfied condition "Succeeded or Failed"
    Jan 17 07:24:03.871: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf container test-container: <nil>
    STEP: delete the pod 01/17/23 07:24:03.951
    Jan 17 07:24:03.980: INFO: Waiting for pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf to disappear
    Jan 17 07:24:03.987: INFO: Pod pod-cd74c9bb-1833-475a-a925-ba932d682ebf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:24:03.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8607" for this suite. 01/17/23 07:24:03.995
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:24:04.011
Jan 17 07:24:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 07:24:04.014
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:04.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:04.054
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/17/23 07:24:04.067
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3571;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3571;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +notcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_tcp@PTR;sleep 1; done
 01/17/23 07:24:04.118
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3571;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3571;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +notcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_tcp@PTR;sleep 1; done
 01/17/23 07:24:04.118
STEP: creating a pod to probe DNS 01/17/23 07:24:04.119
STEP: submitting the pod to kubernetes 01/17/23 07:24:04.119
Jan 17 07:24:04.161: INFO: Waiting up to 15m0s for pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716" in namespace "dns-3571" to be "running"
Jan 17 07:24:04.169: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Pending", Reason="", readiness=false. Elapsed: 7.929509ms
Jan 17 07:24:06.175: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014498349s
Jan 17 07:24:08.176: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Running", Reason="", readiness=true. Elapsed: 4.015336205s
Jan 17 07:24:08.176: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716" satisfied condition "running"
STEP: retrieving the pod 01/17/23 07:24:08.176
STEP: looking for the results for each expected name from probers 01/17/23 07:24:08.181
Jan 17 07:24:08.191: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.197: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.208: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.213: INFO: Unable to read wheezy_udp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.269: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.273: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.278: INFO: Unable to read jessie_udp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.287: INFO: Unable to read jessie_udp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
Jan 17 07:24:08.334: INFO: Lookups using dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3571 wheezy_tcp@dns-test-service.dns-3571 wheezy_udp@dns-test-service.dns-3571.svc wheezy_tcp@dns-test-service.dns-3571.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3571 jessie_tcp@dns-test-service.dns-3571 jessie_udp@dns-test-service.dns-3571.svc jessie_tcp@dns-test-service.dns-3571.svc]

Jan 17 07:24:13.537: INFO: DNS probes using dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716 succeeded

STEP: deleting the pod 01/17/23 07:24:13.537
STEP: deleting the test service 01/17/23 07:24:13.602
STEP: deleting the test headless service 01/17/23 07:24:13.666
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 07:24:13.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3571" for this suite. 01/17/23 07:24:13.704
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":176,"skipped":2965,"failed":0}
------------------------------
• [SLOW TEST] [9.707 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:24:04.011
    Jan 17 07:24:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 07:24:04.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:04.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:04.054
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/17/23 07:24:04.067
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3571;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3571;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +notcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_tcp@PTR;sleep 1; done
     01/17/23 07:24:04.118
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3571;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3571;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3571.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3571.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3571.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3571.svc;check="$$(dig +notcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.112.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.112.227_tcp@PTR;sleep 1; done
     01/17/23 07:24:04.118
    STEP: creating a pod to probe DNS 01/17/23 07:24:04.119
    STEP: submitting the pod to kubernetes 01/17/23 07:24:04.119
    Jan 17 07:24:04.161: INFO: Waiting up to 15m0s for pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716" in namespace "dns-3571" to be "running"
    Jan 17 07:24:04.169: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Pending", Reason="", readiness=false. Elapsed: 7.929509ms
    Jan 17 07:24:06.175: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014498349s
    Jan 17 07:24:08.176: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716": Phase="Running", Reason="", readiness=true. Elapsed: 4.015336205s
    Jan 17 07:24:08.176: INFO: Pod "dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 07:24:08.176
    STEP: looking for the results for each expected name from probers 01/17/23 07:24:08.181
    Jan 17 07:24:08.191: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.197: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.208: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.213: INFO: Unable to read wheezy_udp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.269: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.273: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.278: INFO: Unable to read jessie_udp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-3571 from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.287: INFO: Unable to read jessie_udp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-3571.svc from pod dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716: the server could not find the requested resource (get pods dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716)
    Jan 17 07:24:08.334: INFO: Lookups using dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3571 wheezy_tcp@dns-test-service.dns-3571 wheezy_udp@dns-test-service.dns-3571.svc wheezy_tcp@dns-test-service.dns-3571.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3571 jessie_tcp@dns-test-service.dns-3571 jessie_udp@dns-test-service.dns-3571.svc jessie_tcp@dns-test-service.dns-3571.svc]

    Jan 17 07:24:13.537: INFO: DNS probes using dns-3571/dns-test-c667bc90-3380-48dd-9cf1-bd3faa7e8716 succeeded

    STEP: deleting the pod 01/17/23 07:24:13.537
    STEP: deleting the test service 01/17/23 07:24:13.602
    STEP: deleting the test headless service 01/17/23 07:24:13.666
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 07:24:13.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3571" for this suite. 01/17/23 07:24:13.704
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:24:13.719
Jan 17 07:24:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-pred 01/17/23 07:24:13.72
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:13.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:13.76
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 07:24:13.768: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 07:24:13.782: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 07:24:13.798: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
Jan 17 07:24:13.827: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:24:13.827: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:24:13.827: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:24:13.827: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 07:24:13.827: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 07:24:13.827: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 07:24:13.827: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:24:13.827: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:24:13.827: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 07:24:13.827: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 07:24:13.827: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.827: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:24:13.827: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:24:13.827: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
Jan 17 07:24:13.847: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:24:13.847: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:24:13.847: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:24:13.847: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:24:13.847: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:24:13.847: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 07:24:13.847: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container e2e ready: true, restart count 0
Jan 17 07:24:13.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:24:13.847: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:24:13.847: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:24:13.847: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
Jan 17 07:24:13.870: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:24:13.870: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:24:13.870: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:24:13.870: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container grafana ready: true, restart count 0
Jan 17 07:24:13.870: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 07:24:13.870: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 07:24:13.870: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 07:24:13.870: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:24:13.870: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:24:13.870: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:24:13.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:24:13.870: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-0 01/17/23 07:24:13.924
STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-1 01/17/23 07:24:13.961
STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-2 01/17/23 07:24:13.994
Jan 17 07:24:14.044: INFO: Pod calico-node-hkr25 requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod calico-node-hzzhh requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod calico-node-vhk97 requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-2k9x8 requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-qwhhm requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-z4flm requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod kube-dns-autoscaler-6587b74c7-4phrz requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod magnum-grafana-78479bf475-9z4zt requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod magnum-kube-state-metrics-56f56475f7-rtd9b requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod magnum-metrics-server-6f78bdfdcc-4lxk5 requesting resource cpu=100m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-cbq9j requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-chbc7 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-mzh62 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod npd-b2q92 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod npd-ktdk4 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod npd-rh6l2 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod prometheus-magnum-kube-prometheus-sta-prometheus-0 requesting resource cpu=356m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.044: INFO: Pod sonobuoy requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod sonobuoy-e2e-job-edbb0f4e85d944f9 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
STEP: Starting Pods to consume most of the cluster CPU. 01/17/23 07:24:14.044
Jan 17 07:24:14.045: INFO: Creating a pod which consumes cpu=2242m on Node cluster125-w73dz53kvqes-node-0
Jan 17 07:24:14.113: INFO: Creating a pod which consumes cpu=2576m on Node cluster125-w73dz53kvqes-node-1
Jan 17 07:24:14.165: INFO: Creating a pod which consumes cpu=2576m on Node cluster125-w73dz53kvqes-node-2
Jan 17 07:24:14.195: INFO: Waiting up to 5m0s for pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42" in namespace "sched-pred-9584" to be "running"
Jan 17 07:24:14.216: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42": Phase="Pending", Reason="", readiness=false. Elapsed: 21.357213ms
Jan 17 07:24:16.223: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42": Phase="Running", Reason="", readiness=true. Elapsed: 2.028312023s
Jan 17 07:24:16.223: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42" satisfied condition "running"
Jan 17 07:24:16.223: INFO: Waiting up to 5m0s for pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314" in namespace "sched-pred-9584" to be "running"
Jan 17 07:24:16.230: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314": Phase="Pending", Reason="", readiness=false. Elapsed: 6.691626ms
Jan 17 07:24:18.248: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314": Phase="Running", Reason="", readiness=true. Elapsed: 2.024926593s
Jan 17 07:24:18.248: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314" satisfied condition "running"
Jan 17 07:24:18.248: INFO: Waiting up to 5m0s for pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0" in namespace "sched-pred-9584" to be "running"
Jan 17 07:24:18.254: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.336172ms
Jan 17 07:24:20.271: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022588457s
Jan 17 07:24:22.261: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013373954s
Jan 17 07:24:24.260: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Running", Reason="", readiness=true. Elapsed: 6.011876051s
Jan 17 07:24:24.260: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/17/23 07:24:24.26
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cad24dd75d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314 to cluster125-w73dz53kvqes-node-1] 01/17/23 07:24:24.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb0ddc23b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 07:24:24.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb118c2338], Reason = [Created], Message = [Created container filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314] 01/17/23 07:24:24.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb1a440a2f], Reason = [Started], Message = [Started container filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314] 01/17/23 07:24:24.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cacf56caa3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42 to cluster125-w73dz53kvqes-node-0] 01/17/23 07:24:24.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cafa9de0fd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cafd2b3681], Reason = [Created], Message = [Created container filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cb02486140], Reason = [Started], Message = [Started container filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07cad32baa4e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0 to cluster125-w73dz53kvqes-node-2] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07cb000899e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccb3360516], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 7.301028997s] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccb5c082e6], Reason = [Created], Message = [Created container filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccbb6ed757], Reason = [Started], Message = [Started container filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0] 01/17/23 07:24:24.268
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b07cd2bdc1d3d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 01/17/23 07:24:24.305
STEP: removing the label node off the node cluster125-w73dz53kvqes-node-0 01/17/23 07:24:25.293
STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.329
STEP: removing the label node off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:24:25.337
STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.37
STEP: removing the label node off the node cluster125-w73dz53kvqes-node-2 01/17/23 07:24:25.379
STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.426
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:24:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9584" for this suite. 01/17/23 07:24:25.442
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":177,"skipped":2966,"failed":0}
------------------------------
• [SLOW TEST] [11.759 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:24:13.719
    Jan 17 07:24:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-pred 01/17/23 07:24:13.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:13.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:13.76
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 07:24:13.768: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 07:24:13.782: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 07:24:13.798: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
    Jan 17 07:24:13.827: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.827: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:24:13.827: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
    Jan 17 07:24:13.847: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:24:13.847: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
    Jan 17 07:24:13.870: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container grafana ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:24:13.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:24:13.870: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-0 01/17/23 07:24:13.924
    STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-1 01/17/23 07:24:13.961
    STEP: verifying the node has the label node cluster125-w73dz53kvqes-node-2 01/17/23 07:24:13.994
    Jan 17 07:24:14.044: INFO: Pod calico-node-hkr25 requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod calico-node-hzzhh requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod calico-node-vhk97 requesting resource cpu=250m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-2k9x8 requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-qwhhm requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod csi-cinder-nodeplugin-z4flm requesting resource cpu=50m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod kube-dns-autoscaler-6587b74c7-4phrz requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod magnum-grafana-78479bf475-9z4zt requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod magnum-kube-state-metrics-56f56475f7-rtd9b requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod magnum-metrics-server-6f78bdfdcc-4lxk5 requesting resource cpu=100m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-cbq9j requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-chbc7 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod magnum-prometheus-node-exporter-mzh62 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod npd-b2q92 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod npd-ktdk4 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod npd-rh6l2 requesting resource cpu=20m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod prometheus-magnum-kube-prometheus-sta-prometheus-0 requesting resource cpu=356m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.044: INFO: Pod sonobuoy requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod sonobuoy-e2e-job-edbb0f4e85d944f9 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr requesting resource cpu=0m on Node cluster125-w73dz53kvqes-node-0
    STEP: Starting Pods to consume most of the cluster CPU. 01/17/23 07:24:14.044
    Jan 17 07:24:14.045: INFO: Creating a pod which consumes cpu=2242m on Node cluster125-w73dz53kvqes-node-0
    Jan 17 07:24:14.113: INFO: Creating a pod which consumes cpu=2576m on Node cluster125-w73dz53kvqes-node-1
    Jan 17 07:24:14.165: INFO: Creating a pod which consumes cpu=2576m on Node cluster125-w73dz53kvqes-node-2
    Jan 17 07:24:14.195: INFO: Waiting up to 5m0s for pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42" in namespace "sched-pred-9584" to be "running"
    Jan 17 07:24:14.216: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42": Phase="Pending", Reason="", readiness=false. Elapsed: 21.357213ms
    Jan 17 07:24:16.223: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42": Phase="Running", Reason="", readiness=true. Elapsed: 2.028312023s
    Jan 17 07:24:16.223: INFO: Pod "filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42" satisfied condition "running"
    Jan 17 07:24:16.223: INFO: Waiting up to 5m0s for pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314" in namespace "sched-pred-9584" to be "running"
    Jan 17 07:24:16.230: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314": Phase="Pending", Reason="", readiness=false. Elapsed: 6.691626ms
    Jan 17 07:24:18.248: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314": Phase="Running", Reason="", readiness=true. Elapsed: 2.024926593s
    Jan 17 07:24:18.248: INFO: Pod "filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314" satisfied condition "running"
    Jan 17 07:24:18.248: INFO: Waiting up to 5m0s for pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0" in namespace "sched-pred-9584" to be "running"
    Jan 17 07:24:18.254: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.336172ms
    Jan 17 07:24:20.271: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022588457s
    Jan 17 07:24:22.261: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013373954s
    Jan 17 07:24:24.260: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0": Phase="Running", Reason="", readiness=true. Elapsed: 6.011876051s
    Jan 17 07:24:24.260: INFO: Pod "filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/17/23 07:24:24.26
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cad24dd75d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314 to cluster125-w73dz53kvqes-node-1] 01/17/23 07:24:24.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb0ddc23b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 07:24:24.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb118c2338], Reason = [Created], Message = [Created container filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314] 01/17/23 07:24:24.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314.173b07cb1a440a2f], Reason = [Started], Message = [Started container filler-pod-19c83ced-08e7-4c27-9b23-19359a58c314] 01/17/23 07:24:24.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cacf56caa3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42 to cluster125-w73dz53kvqes-node-0] 01/17/23 07:24:24.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cafa9de0fd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cafd2b3681], Reason = [Created], Message = [Created container filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42.173b07cb02486140], Reason = [Started], Message = [Started container filler-pod-1ea506e8-34fa-4d40-b5ea-8ed669421a42] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07cad32baa4e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9584/filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0 to cluster125-w73dz53kvqes-node-2] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07cb000899e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccb3360516], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 7.301028997s] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccb5c082e6], Reason = [Created], Message = [Created container filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0.173b07ccbb6ed757], Reason = [Started], Message = [Started container filler-pod-7dc675fa-3f6d-4e32-83ca-39801bb223e0] 01/17/23 07:24:24.268
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b07cd2bdc1d3d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 01/17/23 07:24:24.305
    STEP: removing the label node off the node cluster125-w73dz53kvqes-node-0 01/17/23 07:24:25.293
    STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.329
    STEP: removing the label node off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:24:25.337
    STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.37
    STEP: removing the label node off the node cluster125-w73dz53kvqes-node-2 01/17/23 07:24:25.379
    STEP: verifying the node doesn't have the label node 01/17/23 07:24:25.426
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:24:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9584" for this suite. 01/17/23 07:24:25.442
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:24:25.48
Jan 17 07:24:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:24:25.482
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:25.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:25.57
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-6312f26d-aa25-4d59-ae9d-e2936bb61f1d 01/17/23 07:24:25.582
STEP: Creating a pod to test consume secrets 01/17/23 07:24:25.608
Jan 17 07:24:25.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b" in namespace "projected-1837" to be "Succeeded or Failed"
Jan 17 07:24:25.647: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131286ms
Jan 17 07:24:27.652: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022577057s
Jan 17 07:24:29.653: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023520625s
STEP: Saw pod success 01/17/23 07:24:29.653
Jan 17 07:24:29.654: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b" satisfied condition "Succeeded or Failed"
Jan 17 07:24:29.659: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-0 pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:24:29.726
Jan 17 07:24:29.778: INFO: Waiting for pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b to disappear
Jan 17 07:24:29.786: INFO: Pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 07:24:29.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1837" for this suite. 01/17/23 07:24:29.8
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":2967,"failed":0}
------------------------------
• [4.335 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:24:25.48
    Jan 17 07:24:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:24:25.482
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:25.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:25.57
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-6312f26d-aa25-4d59-ae9d-e2936bb61f1d 01/17/23 07:24:25.582
    STEP: Creating a pod to test consume secrets 01/17/23 07:24:25.608
    Jan 17 07:24:25.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b" in namespace "projected-1837" to be "Succeeded or Failed"
    Jan 17 07:24:25.647: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131286ms
    Jan 17 07:24:27.652: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022577057s
    Jan 17 07:24:29.653: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023520625s
    STEP: Saw pod success 01/17/23 07:24:29.653
    Jan 17 07:24:29.654: INFO: Pod "pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b" satisfied condition "Succeeded or Failed"
    Jan 17 07:24:29.659: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-0 pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:24:29.726
    Jan 17 07:24:29.778: INFO: Waiting for pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b to disappear
    Jan 17 07:24:29.786: INFO: Pod pod-projected-secrets-97e40de0-3935-4892-8d03-a50b47db057b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 07:24:29.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1837" for this suite. 01/17/23 07:24:29.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:24:29.821
Jan 17 07:24:29.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 07:24:29.824
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:29.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:29.883
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/17/23 07:24:29.892
STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:24:30.439
Jan 17 07:24:30.470: INFO: Pod name wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60: Found 0 pods out of 5
Jan 17 07:24:35.487: INFO: Pod name wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 07:24:35.487
Jan 17 07:24:35.487: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:35.495: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.685462ms
Jan 17 07:24:37.505: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018562499s
Jan 17 07:24:39.504: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017557089s
Jan 17 07:24:41.515: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028012291s
Jan 17 07:24:43.519: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032387095s
Jan 17 07:24:45.510: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Running", Reason="", readiness=true. Elapsed: 10.02306318s
Jan 17 07:24:45.510: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7" satisfied condition "running"
Jan 17 07:24:45.510: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:45.519: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8": Phase="Running", Reason="", readiness=true. Elapsed: 9.461199ms
Jan 17 07:24:45.520: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8" satisfied condition "running"
Jan 17 07:24:45.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:45.528: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x": Phase="Running", Reason="", readiness=true. Elapsed: 8.335503ms
Jan 17 07:24:45.528: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x" satisfied condition "running"
Jan 17 07:24:45.528: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:45.534: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946": Phase="Running", Reason="", readiness=true. Elapsed: 6.084125ms
Jan 17 07:24:45.534: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946" satisfied condition "running"
Jan 17 07:24:45.534: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:45.569: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46": Phase="Running", Reason="", readiness=true. Elapsed: 34.215129ms
Jan 17 07:24:45.569: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:24:45.569
Jan 17 07:24:45.648: INFO: Deleting ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 took: 20.395396ms
Jan 17 07:24:45.754: INFO: Terminating ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 pods took: 106.306281ms
STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:24:49.27
Jan 17 07:24:49.340: INFO: Pod name wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6: Found 3 pods out of 5
Jan 17 07:24:54.352: INFO: Pod name wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 07:24:54.352
Jan 17 07:24:54.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:24:54.361: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.223099ms
Jan 17 07:24:56.368: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01496487s
Jan 17 07:24:58.369: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016149191s
Jan 17 07:25:00.369: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016483503s
Jan 17 07:25:02.371: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018727049s
Jan 17 07:25:04.367: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Running", Reason="", readiness=true. Elapsed: 10.014800507s
Jan 17 07:25:04.367: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn" satisfied condition "running"
Jan 17 07:25:04.367: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:04.373: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v": Phase="Running", Reason="", readiness=true. Elapsed: 5.539848ms
Jan 17 07:25:04.373: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v" satisfied condition "running"
Jan 17 07:25:04.373: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:04.378: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8": Phase="Running", Reason="", readiness=true. Elapsed: 5.05979ms
Jan 17 07:25:04.378: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8" satisfied condition "running"
Jan 17 07:25:04.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:04.384: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b": Phase="Running", Reason="", readiness=true. Elapsed: 5.893551ms
Jan 17 07:25:04.384: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b" satisfied condition "running"
Jan 17 07:25:04.384: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:04.390: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m": Phase="Running", Reason="", readiness=true. Elapsed: 5.466554ms
Jan 17 07:25:04.390: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:25:04.39
Jan 17 07:25:04.466: INFO: Deleting ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 took: 18.812602ms
Jan 17 07:25:04.567: INFO: Terminating ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 pods took: 100.810748ms
STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:25:07.678
Jan 17 07:25:07.712: INFO: Pod name wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680: Found 0 pods out of 5
Jan 17 07:25:12.721: INFO: Pod name wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 07:25:12.721
Jan 17 07:25:12.721: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:12.727: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689278ms
Jan 17 07:25:14.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618725s
Jan 17 07:25:16.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012977652s
Jan 17 07:25:18.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012276531s
Jan 17 07:25:20.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013326836s
Jan 17 07:25:22.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Running", Reason="", readiness=true. Elapsed: 10.011613304s
Jan 17 07:25:22.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929" satisfied condition "running"
Jan 17 07:25:22.733: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:22.738: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql": Phase="Running", Reason="", readiness=true. Elapsed: 5.240215ms
Jan 17 07:25:22.738: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql" satisfied condition "running"
Jan 17 07:25:22.738: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:22.746: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq": Phase="Running", Reason="", readiness=true. Elapsed: 7.745094ms
Jan 17 07:25:22.746: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq" satisfied condition "running"
Jan 17 07:25:22.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:22.755: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr": Phase="Running", Reason="", readiness=true. Elapsed: 9.301584ms
Jan 17 07:25:22.755: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr" satisfied condition "running"
Jan 17 07:25:22.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m" in namespace "emptydir-wrapper-7100" to be "running"
Jan 17 07:25:22.768: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m": Phase="Running", Reason="", readiness=true. Elapsed: 12.97897ms
Jan 17 07:25:22.769: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:25:22.769
Jan 17 07:25:22.847: INFO: Deleting ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 took: 15.639334ms
Jan 17 07:25:22.948: INFO: Terminating ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 pods took: 100.859531ms
STEP: Cleaning up the configMaps 01/17/23 07:25:25.749
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 17 07:25:26.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7100" for this suite. 01/17/23 07:25:26.509
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":179,"skipped":3005,"failed":0}
------------------------------
• [SLOW TEST] [56.703 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:24:29.821
    Jan 17 07:24:29.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 07:24:29.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:24:29.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:24:29.883
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/17/23 07:24:29.892
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:24:30.439
    Jan 17 07:24:30.470: INFO: Pod name wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60: Found 0 pods out of 5
    Jan 17 07:24:35.487: INFO: Pod name wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 07:24:35.487
    Jan 17 07:24:35.487: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:35.495: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.685462ms
    Jan 17 07:24:37.505: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018562499s
    Jan 17 07:24:39.504: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017557089s
    Jan 17 07:24:41.515: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028012291s
    Jan 17 07:24:43.519: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032387095s
    Jan 17 07:24:45.510: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7": Phase="Running", Reason="", readiness=true. Elapsed: 10.02306318s
    Jan 17 07:24:45.510: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-5p7j7" satisfied condition "running"
    Jan 17 07:24:45.510: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:45.519: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8": Phase="Running", Reason="", readiness=true. Elapsed: 9.461199ms
    Jan 17 07:24:45.520: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-8krf8" satisfied condition "running"
    Jan 17 07:24:45.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:45.528: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x": Phase="Running", Reason="", readiness=true. Elapsed: 8.335503ms
    Jan 17 07:24:45.528: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-frr7x" satisfied condition "running"
    Jan 17 07:24:45.528: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:45.534: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946": Phase="Running", Reason="", readiness=true. Elapsed: 6.084125ms
    Jan 17 07:24:45.534: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-rv946" satisfied condition "running"
    Jan 17 07:24:45.534: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:45.569: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46": Phase="Running", Reason="", readiness=true. Elapsed: 34.215129ms
    Jan 17 07:24:45.569: INFO: Pod "wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60-s4z46" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:24:45.569
    Jan 17 07:24:45.648: INFO: Deleting ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 took: 20.395396ms
    Jan 17 07:24:45.754: INFO: Terminating ReplicationController wrapped-volume-race-66df2f12-88b0-463b-a3b9-8967bb74ea60 pods took: 106.306281ms
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:24:49.27
    Jan 17 07:24:49.340: INFO: Pod name wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6: Found 3 pods out of 5
    Jan 17 07:24:54.352: INFO: Pod name wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 07:24:54.352
    Jan 17 07:24:54.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:24:54.361: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.223099ms
    Jan 17 07:24:56.368: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01496487s
    Jan 17 07:24:58.369: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016149191s
    Jan 17 07:25:00.369: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016483503s
    Jan 17 07:25:02.371: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018727049s
    Jan 17 07:25:04.367: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn": Phase="Running", Reason="", readiness=true. Elapsed: 10.014800507s
    Jan 17 07:25:04.367: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-542qn" satisfied condition "running"
    Jan 17 07:25:04.367: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:04.373: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v": Phase="Running", Reason="", readiness=true. Elapsed: 5.539848ms
    Jan 17 07:25:04.373: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-7xr4v" satisfied condition "running"
    Jan 17 07:25:04.373: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:04.378: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8": Phase="Running", Reason="", readiness=true. Elapsed: 5.05979ms
    Jan 17 07:25:04.378: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-rnjg8" satisfied condition "running"
    Jan 17 07:25:04.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:04.384: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b": Phase="Running", Reason="", readiness=true. Elapsed: 5.893551ms
    Jan 17 07:25:04.384: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-vnb2b" satisfied condition "running"
    Jan 17 07:25:04.384: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:04.390: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m": Phase="Running", Reason="", readiness=true. Elapsed: 5.466554ms
    Jan 17 07:25:04.390: INFO: Pod "wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6-ww66m" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:25:04.39
    Jan 17 07:25:04.466: INFO: Deleting ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 took: 18.812602ms
    Jan 17 07:25:04.567: INFO: Terminating ReplicationController wrapped-volume-race-7ac42ff8-39b8-4e53-a36b-6a6a921bf2e6 pods took: 100.810748ms
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 07:25:07.678
    Jan 17 07:25:07.712: INFO: Pod name wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680: Found 0 pods out of 5
    Jan 17 07:25:12.721: INFO: Pod name wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 07:25:12.721
    Jan 17 07:25:12.721: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:12.727: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689278ms
    Jan 17 07:25:14.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618725s
    Jan 17 07:25:16.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012977652s
    Jan 17 07:25:18.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012276531s
    Jan 17 07:25:20.734: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013326836s
    Jan 17 07:25:22.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929": Phase="Running", Reason="", readiness=true. Elapsed: 10.011613304s
    Jan 17 07:25:22.733: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bp929" satisfied condition "running"
    Jan 17 07:25:22.733: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:22.738: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql": Phase="Running", Reason="", readiness=true. Elapsed: 5.240215ms
    Jan 17 07:25:22.738: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-bqrql" satisfied condition "running"
    Jan 17 07:25:22.738: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:22.746: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq": Phase="Running", Reason="", readiness=true. Elapsed: 7.745094ms
    Jan 17 07:25:22.746: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-fxddq" satisfied condition "running"
    Jan 17 07:25:22.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:22.755: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr": Phase="Running", Reason="", readiness=true. Elapsed: 9.301584ms
    Jan 17 07:25:22.755: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-k7vtr" satisfied condition "running"
    Jan 17 07:25:22.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m" in namespace "emptydir-wrapper-7100" to be "running"
    Jan 17 07:25:22.768: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m": Phase="Running", Reason="", readiness=true. Elapsed: 12.97897ms
    Jan 17 07:25:22.769: INFO: Pod "wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680-svv4m" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 in namespace emptydir-wrapper-7100, will wait for the garbage collector to delete the pods 01/17/23 07:25:22.769
    Jan 17 07:25:22.847: INFO: Deleting ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 took: 15.639334ms
    Jan 17 07:25:22.948: INFO: Terminating ReplicationController wrapped-volume-race-72221024-e708-4bdb-acb6-76aaa602f680 pods took: 100.859531ms
    STEP: Cleaning up the configMaps 01/17/23 07:25:25.749
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:25:26.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7100" for this suite. 01/17/23 07:25:26.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:26.525
Jan 17 07:25:26.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:25:26.527
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:26.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:26.564
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:25:26.609
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:25:27.241
STEP: Deploying the webhook pod 01/17/23 07:25:27.257
STEP: Wait for the deployment to be ready 01/17/23 07:25:27.286
Jan 17 07:25:27.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:25:29.362
STEP: Verifying the service has paired with the endpoint 01/17/23 07:25:29.39
Jan 17 07:25:30.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/17/23 07:25:30.397
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/17/23 07:25:30.433
STEP: Creating a configMap that should not be mutated 01/17/23 07:25:30.453
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/17/23 07:25:30.475
STEP: Creating a configMap that should be mutated 01/17/23 07:25:30.489
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:25:30.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7095" for this suite. 01/17/23 07:25:30.545
STEP: Destroying namespace "webhook-7095-markers" for this suite. 01/17/23 07:25:30.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":180,"skipped":3015,"failed":0}
------------------------------
• [4.154 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:26.525
    Jan 17 07:25:26.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:25:26.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:26.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:26.564
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:25:26.609
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:25:27.241
    STEP: Deploying the webhook pod 01/17/23 07:25:27.257
    STEP: Wait for the deployment to be ready 01/17/23 07:25:27.286
    Jan 17 07:25:27.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:25:29.362
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:25:29.39
    Jan 17 07:25:30.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/17/23 07:25:30.397
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/17/23 07:25:30.433
    STEP: Creating a configMap that should not be mutated 01/17/23 07:25:30.453
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/17/23 07:25:30.475
    STEP: Creating a configMap that should be mutated 01/17/23 07:25:30.489
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:25:30.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7095" for this suite. 01/17/23 07:25:30.545
    STEP: Destroying namespace "webhook-7095-markers" for this suite. 01/17/23 07:25:30.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:30.681
Jan 17 07:25:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:25:30.683
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:30.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:30.737
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-c314eb11-46db-4556-b840-a07a56e5f4a0 01/17/23 07:25:30.751
STEP: Creating a pod to test consume secrets 01/17/23 07:25:30.781
Jan 17 07:25:30.817: INFO: Waiting up to 5m0s for pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7" in namespace "secrets-1478" to be "Succeeded or Failed"
Jan 17 07:25:30.844: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.599675ms
Jan 17 07:25:32.870: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052768302s
Jan 17 07:25:34.851: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033251735s
STEP: Saw pod success 01/17/23 07:25:34.851
Jan 17 07:25:34.851: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7" satisfied condition "Succeeded or Failed"
Jan 17 07:25:34.856: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:25:34.923
Jan 17 07:25:34.951: INFO: Waiting for pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 to disappear
Jan 17 07:25:34.957: INFO: Pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:25:34.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1478" for this suite. 01/17/23 07:25:34.963
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":181,"skipped":3027,"failed":0}
------------------------------
• [4.304 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:30.681
    Jan 17 07:25:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:25:30.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:30.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:30.737
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-c314eb11-46db-4556-b840-a07a56e5f4a0 01/17/23 07:25:30.751
    STEP: Creating a pod to test consume secrets 01/17/23 07:25:30.781
    Jan 17 07:25:30.817: INFO: Waiting up to 5m0s for pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7" in namespace "secrets-1478" to be "Succeeded or Failed"
    Jan 17 07:25:30.844: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.599675ms
    Jan 17 07:25:32.870: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052768302s
    Jan 17 07:25:34.851: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033251735s
    STEP: Saw pod success 01/17/23 07:25:34.851
    Jan 17 07:25:34.851: INFO: Pod "pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7" satisfied condition "Succeeded or Failed"
    Jan 17 07:25:34.856: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:25:34.923
    Jan 17 07:25:34.951: INFO: Waiting for pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 to disappear
    Jan 17 07:25:34.957: INFO: Pod pod-secrets-c6e46451-b2ab-4471-b72b-f74ff24152c7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:25:34.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1478" for this suite. 01/17/23 07:25:34.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:34.985
Jan 17 07:25:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:25:34.986
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:35.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:35.021
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 17 07:25:35.030: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 17 07:25:35.051: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 07:25:40.059: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 07:25:40.059
Jan 17 07:25:40.059: INFO: Creating deployment "test-rolling-update-deployment"
Jan 17 07:25:40.069: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 17 07:25:40.085: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 17 07:25:42.110: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 17 07:25:42.117: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:25:42.152: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7747  ac254346-7cdf-441e-8731-8cfd3b0fb433 25016 1 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006828ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:25:40 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-17 07:25:41 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 07:25:42.158: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7747  1e31162a-f9be-476b-af91-6ea4598042d4 25005 1 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ac254346-7cdf-441e-8731-8cfd3b0fb433 0xc003aff757 0xc003aff758}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac254346-7cdf-441e-8731-8cfd3b0fb433\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aff808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:25:42.158: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 17 07:25:42.159: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7747  e09735ca-dbd2-4442-9f1f-e595d5d39f70 25015 2 2023-01-17 07:25:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ac254346-7cdf-441e-8731-8cfd3b0fb433 0xc003aff627 0xc003aff628}] [] [{e2e.test Update apps/v1 2023-01-17 07:25:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac254346-7cdf-441e-8731-8cfd3b0fb433\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003aff6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:25:42.165: INFO: Pod "test-rolling-update-deployment-78f575d8ff-ftbkr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-ftbkr test-rolling-update-deployment-78f575d8ff- deployment-7747  8d545f3c-e774-4d4c-a8be-b846e9b326f9 25004 0 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:25fcf9739418a94b003ce02353d4425cfda10d8c73790bf72fac97e21b055936 cni.projectcalico.org/podIP:10.100.168.103/32 cni.projectcalico.org/podIPs:10.100.168.103/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 1e31162a-f9be-476b-af91-6ea4598042d4 0xc006829077 0xc006829078}] [] [{Go-http-client Update v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e31162a-f9be-476b-af91-6ea4598042d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2f6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2f6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.103,StartTime:2023-01-17 07:25:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:25:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://bdca2eb87c2c5a003e54a36a382d7c9dec4ef2350a466ccfced634598ca3bef4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:25:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7747" for this suite. 01/17/23 07:25:42.184
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":182,"skipped":3033,"failed":0}
------------------------------
• [SLOW TEST] [7.215 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:34.985
    Jan 17 07:25:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:25:34.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:35.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:35.021
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 17 07:25:35.030: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 17 07:25:35.051: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 07:25:40.059: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 07:25:40.059
    Jan 17 07:25:40.059: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 17 07:25:40.069: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 17 07:25:40.085: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 17 07:25:42.110: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 17 07:25:42.117: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:25:42.152: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7747  ac254346-7cdf-441e-8731-8cfd3b0fb433 25016 1 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006828ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:25:40 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-17 07:25:41 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 07:25:42.158: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7747  1e31162a-f9be-476b-af91-6ea4598042d4 25005 1 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ac254346-7cdf-441e-8731-8cfd3b0fb433 0xc003aff757 0xc003aff758}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac254346-7cdf-441e-8731-8cfd3b0fb433\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aff808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:25:42.158: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 17 07:25:42.159: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7747  e09735ca-dbd2-4442-9f1f-e595d5d39f70 25015 2 2023-01-17 07:25:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ac254346-7cdf-441e-8731-8cfd3b0fb433 0xc003aff627 0xc003aff628}] [] [{e2e.test Update apps/v1 2023-01-17 07:25:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac254346-7cdf-441e-8731-8cfd3b0fb433\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003aff6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:25:42.165: INFO: Pod "test-rolling-update-deployment-78f575d8ff-ftbkr" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-ftbkr test-rolling-update-deployment-78f575d8ff- deployment-7747  8d545f3c-e774-4d4c-a8be-b846e9b326f9 25004 0 2023-01-17 07:25:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:25fcf9739418a94b003ce02353d4425cfda10d8c73790bf72fac97e21b055936 cni.projectcalico.org/podIP:10.100.168.103/32 cni.projectcalico.org/podIPs:10.100.168.103/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 1e31162a-f9be-476b-af91-6ea4598042d4 0xc006829077 0xc006829078}] [] [{Go-http-client Update v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 07:25:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e31162a-f9be-476b-af91-6ea4598042d4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:25:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2f6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2f6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:25:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.103,StartTime:2023-01-17 07:25:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:25:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://bdca2eb87c2c5a003e54a36a382d7c9dec4ef2350a466ccfced634598ca3bef4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:25:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7747" for this suite. 01/17/23 07:25:42.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:42.201
Jan 17 07:25:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:25:42.203
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:42.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:42.25
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:25:42.283
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:25:43.325
STEP: Deploying the webhook pod 01/17/23 07:25:43.337
STEP: Wait for the deployment to be ready 01/17/23 07:25:43.368
Jan 17 07:25:43.416: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:25:45.434
STEP: Verifying the service has paired with the endpoint 01/17/23 07:25:45.473
Jan 17 07:25:46.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 17 07:25:46.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/17/23 07:25:47.035
STEP: Creating a custom resource that should be denied by the webhook 01/17/23 07:25:47.092
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/17/23 07:25:49.164
STEP: Updating the custom resource with disallowed data should be denied 01/17/23 07:25:49.179
STEP: Deleting the custom resource should be denied 01/17/23 07:25:49.212
STEP: Remove the offending key and value from the custom resource data 01/17/23 07:25:49.226
STEP: Deleting the updated custom resource should be successful 01/17/23 07:25:49.243
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:25:49.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4632" for this suite. 01/17/23 07:25:49.806
STEP: Destroying namespace "webhook-4632-markers" for this suite. 01/17/23 07:25:49.825
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":183,"skipped":3044,"failed":0}
------------------------------
• [SLOW TEST] [7.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:42.201
    Jan 17 07:25:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:25:42.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:42.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:42.25
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:25:42.283
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:25:43.325
    STEP: Deploying the webhook pod 01/17/23 07:25:43.337
    STEP: Wait for the deployment to be ready 01/17/23 07:25:43.368
    Jan 17 07:25:43.416: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:25:45.434
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:25:45.473
    Jan 17 07:25:46.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 17 07:25:46.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/17/23 07:25:47.035
    STEP: Creating a custom resource that should be denied by the webhook 01/17/23 07:25:47.092
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/17/23 07:25:49.164
    STEP: Updating the custom resource with disallowed data should be denied 01/17/23 07:25:49.179
    STEP: Deleting the custom resource should be denied 01/17/23 07:25:49.212
    STEP: Remove the offending key and value from the custom resource data 01/17/23 07:25:49.226
    STEP: Deleting the updated custom resource should be successful 01/17/23 07:25:49.243
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:25:49.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4632" for this suite. 01/17/23 07:25:49.806
    STEP: Destroying namespace "webhook-4632-markers" for this suite. 01/17/23 07:25:49.825
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:49.944
Jan 17 07:25:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replication-controller 01/17/23 07:25:49.946
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:50.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:50.051
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/17/23 07:25:50.057
Jan 17 07:25:50.077: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3964" to be "running and ready"
Jan 17 07:25:50.093: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 16.459963ms
Jan 17 07:25:50.093: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:25:52.101: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.023804624s
Jan 17 07:25:52.101: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 17 07:25:52.101: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/17/23 07:25:52.116
STEP: Then the orphan pod is adopted 01/17/23 07:25:52.129
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 07:25:53.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3964" for this suite. 01/17/23 07:25:53.158
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":184,"skipped":3052,"failed":0}
------------------------------
• [3.238 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:49.944
    Jan 17 07:25:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replication-controller 01/17/23 07:25:49.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:50.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:50.051
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/17/23 07:25:50.057
    Jan 17 07:25:50.077: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3964" to be "running and ready"
    Jan 17 07:25:50.093: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 16.459963ms
    Jan 17 07:25:50.093: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:25:52.101: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.023804624s
    Jan 17 07:25:52.101: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 17 07:25:52.101: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/17/23 07:25:52.116
    STEP: Then the orphan pod is adopted 01/17/23 07:25:52.129
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 07:25:53.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3964" for this suite. 01/17/23 07:25:53.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:53.183
Jan 17 07:25:53.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:25:53.185
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:53.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:53.225
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/17/23 07:25:53.231
STEP: submitting the pod to kubernetes 01/17/23 07:25:53.231
Jan 17 07:25:53.247: INFO: Waiting up to 5m0s for pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" in namespace "pods-4841" to be "running and ready"
Jan 17 07:25:53.255: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.210464ms
Jan 17 07:25:53.255: INFO: The phase of Pod pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:25:55.264: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.017205218s
Jan 17 07:25:55.264: INFO: The phase of Pod pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc is Running (Ready = true)
Jan 17 07:25:55.264: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/17/23 07:25:55.271
STEP: updating the pod 01/17/23 07:25:55.278
Jan 17 07:25:55.803: INFO: Successfully updated pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc"
Jan 17 07:25:55.804: INFO: Waiting up to 5m0s for pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" in namespace "pods-4841" to be "running"
Jan 17 07:25:55.808: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Running", Reason="", readiness=true. Elapsed: 4.419316ms
Jan 17 07:25:55.808: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/17/23 07:25:55.808
Jan 17 07:25:55.813: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:25:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4841" for this suite. 01/17/23 07:25:55.824
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":185,"skipped":3059,"failed":0}
------------------------------
• [2.663 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:53.183
    Jan 17 07:25:53.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:25:53.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:53.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:53.225
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/17/23 07:25:53.231
    STEP: submitting the pod to kubernetes 01/17/23 07:25:53.231
    Jan 17 07:25:53.247: INFO: Waiting up to 5m0s for pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" in namespace "pods-4841" to be "running and ready"
    Jan 17 07:25:53.255: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.210464ms
    Jan 17 07:25:53.255: INFO: The phase of Pod pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:25:55.264: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.017205218s
    Jan 17 07:25:55.264: INFO: The phase of Pod pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc is Running (Ready = true)
    Jan 17 07:25:55.264: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/17/23 07:25:55.271
    STEP: updating the pod 01/17/23 07:25:55.278
    Jan 17 07:25:55.803: INFO: Successfully updated pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc"
    Jan 17 07:25:55.804: INFO: Waiting up to 5m0s for pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" in namespace "pods-4841" to be "running"
    Jan 17 07:25:55.808: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc": Phase="Running", Reason="", readiness=true. Elapsed: 4.419316ms
    Jan 17 07:25:55.808: INFO: Pod "pod-update-2e00ea53-bb43-4ccf-9887-5e645323c9cc" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/17/23 07:25:55.808
    Jan 17 07:25:55.813: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:25:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4841" for this suite. 01/17/23 07:25:55.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:25:55.846
Jan 17 07:25:55.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:25:55.848
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:55.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:55.893
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-adb4b7d8-c51a-4d4c-b0d2-074e8b81c62d 01/17/23 07:25:55.901
STEP: Creating a pod to test consume configMaps 01/17/23 07:25:55.911
Jan 17 07:25:55.938: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243" in namespace "configmap-5321" to be "Succeeded or Failed"
Jan 17 07:25:55.952: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Pending", Reason="", readiness=false. Elapsed: 13.790535ms
Jan 17 07:25:57.960: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021995998s
Jan 17 07:25:59.959: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02164832s
STEP: Saw pod success 01/17/23 07:25:59.959
Jan 17 07:25:59.960: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243" satisfied condition "Succeeded or Failed"
Jan 17 07:25:59.965: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:26:00.052
Jan 17 07:26:00.076: INFO: Waiting for pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 to disappear
Jan 17 07:26:00.082: INFO: Pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:26:00.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5321" for this suite. 01/17/23 07:26:00.09
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":186,"skipped":3064,"failed":0}
------------------------------
• [4.258 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:25:55.846
    Jan 17 07:25:55.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:25:55.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:25:55.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:25:55.893
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-adb4b7d8-c51a-4d4c-b0d2-074e8b81c62d 01/17/23 07:25:55.901
    STEP: Creating a pod to test consume configMaps 01/17/23 07:25:55.911
    Jan 17 07:25:55.938: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243" in namespace "configmap-5321" to be "Succeeded or Failed"
    Jan 17 07:25:55.952: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Pending", Reason="", readiness=false. Elapsed: 13.790535ms
    Jan 17 07:25:57.960: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021995998s
    Jan 17 07:25:59.959: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02164832s
    STEP: Saw pod success 01/17/23 07:25:59.959
    Jan 17 07:25:59.960: INFO: Pod "pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243" satisfied condition "Succeeded or Failed"
    Jan 17 07:25:59.965: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:26:00.052
    Jan 17 07:26:00.076: INFO: Waiting for pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 to disappear
    Jan 17 07:26:00.082: INFO: Pod pod-configmaps-8bade6a3-e8e7-47ad-9980-bf6dfa0b7243 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:26:00.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5321" for this suite. 01/17/23 07:26:00.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:00.107
Jan 17 07:26:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sysctl 01/17/23 07:26:00.108
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:00.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:00.149
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/17/23 07:26:00.158
STEP: Watching for error events or started pod 01/17/23 07:26:00.176
STEP: Waiting for pod completion 01/17/23 07:26:02.19
Jan 17 07:26:02.190: INFO: Waiting up to 3m0s for pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f" in namespace "sysctl-4498" to be "completed"
Jan 17 07:26:02.196: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305696ms
Jan 17 07:26:04.203: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0128885s
Jan 17 07:26:04.203: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/17/23 07:26:04.214
STEP: Getting logs from the pod 01/17/23 07:26:04.215
STEP: Checking that the sysctl is actually updated 01/17/23 07:26:04.23
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 07:26:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4498" for this suite. 01/17/23 07:26:04.238
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":187,"skipped":3100,"failed":0}
------------------------------
• [4.145 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:00.107
    Jan 17 07:26:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sysctl 01/17/23 07:26:00.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:00.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:00.149
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/17/23 07:26:00.158
    STEP: Watching for error events or started pod 01/17/23 07:26:00.176
    STEP: Waiting for pod completion 01/17/23 07:26:02.19
    Jan 17 07:26:02.190: INFO: Waiting up to 3m0s for pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f" in namespace "sysctl-4498" to be "completed"
    Jan 17 07:26:02.196: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305696ms
    Jan 17 07:26:04.203: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0128885s
    Jan 17 07:26:04.203: INFO: Pod "sysctl-9000b936-96bc-4091-bd4f-ad89e6be959f" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/17/23 07:26:04.214
    STEP: Getting logs from the pod 01/17/23 07:26:04.215
    STEP: Checking that the sysctl is actually updated 01/17/23 07:26:04.23
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-4498" for this suite. 01/17/23 07:26:04.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:04.256
Jan 17 07:26:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:26:04.259
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:04.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:04.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:26:04.336
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:04.602
STEP: Deploying the webhook pod 01/17/23 07:26:04.619
STEP: Wait for the deployment to be ready 01/17/23 07:26:04.649
Jan 17 07:26:04.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:26:06.711
STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:06.739
Jan 17 07:26:07.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 17 07:26:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3577-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 07:26:08.287
STEP: Creating a custom resource while v1 is storage version 01/17/23 07:26:08.32
STEP: Patching Custom Resource Definition to set v2 as storage 01/17/23 07:26:10.415
STEP: Patching the custom resource while v2 is storage version 01/17/23 07:26:10.442
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:26:11.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6127" for this suite. 01/17/23 07:26:11.088
STEP: Destroying namespace "webhook-6127-markers" for this suite. 01/17/23 07:26:11.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":188,"skipped":3105,"failed":0}
------------------------------
• [SLOW TEST] [6.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:04.256
    Jan 17 07:26:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:26:04.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:04.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:04.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:26:04.336
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:04.602
    STEP: Deploying the webhook pod 01/17/23 07:26:04.619
    STEP: Wait for the deployment to be ready 01/17/23 07:26:04.649
    Jan 17 07:26:04.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:26:06.711
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:06.739
    Jan 17 07:26:07.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 17 07:26:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3577-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 07:26:08.287
    STEP: Creating a custom resource while v1 is storage version 01/17/23 07:26:08.32
    STEP: Patching Custom Resource Definition to set v2 as storage 01/17/23 07:26:10.415
    STEP: Patching the custom resource while v2 is storage version 01/17/23 07:26:10.442
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:11.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6127" for this suite. 01/17/23 07:26:11.088
    STEP: Destroying namespace "webhook-6127-markers" for this suite. 01/17/23 07:26:11.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:11.248
Jan 17 07:26:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename limitrange 01/17/23 07:26:11.251
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:11.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:11.313
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/17/23 07:26:11.326
STEP: Setting up watch 01/17/23 07:26:11.326
STEP: Submitting a LimitRange 01/17/23 07:26:11.436
STEP: Verifying LimitRange creation was observed 01/17/23 07:26:11.446
STEP: Fetching the LimitRange to ensure it has proper values 01/17/23 07:26:11.446
Jan 17 07:26:11.452: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 07:26:11.452: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/17/23 07:26:11.452
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/17/23 07:26:11.469
Jan 17 07:26:11.485: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 07:26:11.485: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/17/23 07:26:11.485
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/17/23 07:26:11.501
Jan 17 07:26:11.521: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 17 07:26:11.521: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/17/23 07:26:11.521
STEP: Failing to create a Pod with more than max resources 01/17/23 07:26:11.524
STEP: Updating a LimitRange 01/17/23 07:26:11.528
STEP: Verifying LimitRange updating is effective 01/17/23 07:26:11.545
STEP: Creating a Pod with less than former min resources 01/17/23 07:26:13.567
STEP: Failing to create a Pod with more than max resources 01/17/23 07:26:13.588
STEP: Deleting a LimitRange 01/17/23 07:26:13.593
STEP: Verifying the LimitRange was deleted 01/17/23 07:26:13.618
Jan 17 07:26:18.626: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/17/23 07:26:18.626
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 17 07:26:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9831" for this suite. 01/17/23 07:26:18.666
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":189,"skipped":3159,"failed":0}
------------------------------
• [SLOW TEST] [7.434 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:11.248
    Jan 17 07:26:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename limitrange 01/17/23 07:26:11.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:11.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:11.313
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/17/23 07:26:11.326
    STEP: Setting up watch 01/17/23 07:26:11.326
    STEP: Submitting a LimitRange 01/17/23 07:26:11.436
    STEP: Verifying LimitRange creation was observed 01/17/23 07:26:11.446
    STEP: Fetching the LimitRange to ensure it has proper values 01/17/23 07:26:11.446
    Jan 17 07:26:11.452: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 17 07:26:11.452: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/17/23 07:26:11.452
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/17/23 07:26:11.469
    Jan 17 07:26:11.485: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 17 07:26:11.485: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/17/23 07:26:11.485
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/17/23 07:26:11.501
    Jan 17 07:26:11.521: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 17 07:26:11.521: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/17/23 07:26:11.521
    STEP: Failing to create a Pod with more than max resources 01/17/23 07:26:11.524
    STEP: Updating a LimitRange 01/17/23 07:26:11.528
    STEP: Verifying LimitRange updating is effective 01/17/23 07:26:11.545
    STEP: Creating a Pod with less than former min resources 01/17/23 07:26:13.567
    STEP: Failing to create a Pod with more than max resources 01/17/23 07:26:13.588
    STEP: Deleting a LimitRange 01/17/23 07:26:13.593
    STEP: Verifying the LimitRange was deleted 01/17/23 07:26:13.618
    Jan 17 07:26:18.626: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/17/23 07:26:18.626
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 17 07:26:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9831" for this suite. 01/17/23 07:26:18.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:18.688
Jan 17 07:26:18.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 07:26:18.689
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:18.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:18.723
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/17/23 07:26:18.73
STEP: Ensuring job reaches completions 01/17/23 07:26:18.741
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 07:26:30.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-495" for this suite. 01/17/23 07:26:30.764
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":190,"skipped":3211,"failed":0}
------------------------------
• [SLOW TEST] [12.098 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:18.688
    Jan 17 07:26:18.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 07:26:18.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:18.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:18.723
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/17/23 07:26:18.73
    STEP: Ensuring job reaches completions 01/17/23 07:26:18.741
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 07:26:30.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-495" for this suite. 01/17/23 07:26:30.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:30.787
Jan 17 07:26:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:26:30.788
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:30.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:30.845
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:26:30.908
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:31.754
STEP: Deploying the webhook pod 01/17/23 07:26:31.781
STEP: Wait for the deployment to be ready 01/17/23 07:26:31.816
Jan 17 07:26:31.835: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:26:33.852
STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:33.899
Jan 17 07:26:34.899: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/17/23 07:26:35.048
STEP: Creating a configMap that should be mutated 01/17/23 07:26:35.082
STEP: Deleting the collection of validation webhooks 01/17/23 07:26:35.147
STEP: Creating a configMap that should not be mutated 01/17/23 07:26:35.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:26:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-581" for this suite. 01/17/23 07:26:35.277
STEP: Destroying namespace "webhook-581-markers" for this suite. 01/17/23 07:26:35.309
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":191,"skipped":3222,"failed":0}
------------------------------
• [4.668 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:30.787
    Jan 17 07:26:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:26:30.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:30.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:30.845
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:26:30.908
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:31.754
    STEP: Deploying the webhook pod 01/17/23 07:26:31.781
    STEP: Wait for the deployment to be ready 01/17/23 07:26:31.816
    Jan 17 07:26:31.835: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:26:33.852
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:33.899
    Jan 17 07:26:34.899: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/17/23 07:26:35.048
    STEP: Creating a configMap that should be mutated 01/17/23 07:26:35.082
    STEP: Deleting the collection of validation webhooks 01/17/23 07:26:35.147
    STEP: Creating a configMap that should not be mutated 01/17/23 07:26:35.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-581" for this suite. 01/17/23 07:26:35.277
    STEP: Destroying namespace "webhook-581-markers" for this suite. 01/17/23 07:26:35.309
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:35.459
Jan 17 07:26:35.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename controllerrevisions 01/17/23 07:26:35.462
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:35.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:35.516
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-6wmhl-daemon-set" 01/17/23 07:26:35.567
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:26:35.581
Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:35.595: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
Jan 17 07:26:35.595: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:36.613: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
Jan 17 07:26:36.614: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:37.614: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 2
Jan 17 07:26:37.614: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
Jan 17 07:26:38.602: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:38.603: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:38.603: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:26:38.609: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 3
Jan 17 07:26:38.609: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-6wmhl-daemon-set
STEP: Confirm DaemonSet "e2e-6wmhl-daemon-set" successfully created with "daemonset-name=e2e-6wmhl-daemon-set" label 01/17/23 07:26:38.615
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-6wmhl-daemon-set" 01/17/23 07:26:38.632
Jan 17 07:26:38.639: INFO: Located ControllerRevision: "e2e-6wmhl-daemon-set-74457c8865"
STEP: Patching ControllerRevision "e2e-6wmhl-daemon-set-74457c8865" 01/17/23 07:26:38.645
Jan 17 07:26:38.662: INFO: e2e-6wmhl-daemon-set-74457c8865 has been patched
STEP: Create a new ControllerRevision 01/17/23 07:26:38.662
Jan 17 07:26:38.675: INFO: Created ControllerRevision: e2e-6wmhl-daemon-set-845b99dbd9
STEP: Confirm that there are two ControllerRevisions 01/17/23 07:26:38.675
Jan 17 07:26:38.675: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 07:26:38.685: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-6wmhl-daemon-set-74457c8865" 01/17/23 07:26:38.685
STEP: Confirm that there is only one ControllerRevision 01/17/23 07:26:38.699
Jan 17 07:26:38.699: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 07:26:38.703: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-6wmhl-daemon-set-845b99dbd9" 01/17/23 07:26:38.712
Jan 17 07:26:38.732: INFO: e2e-6wmhl-daemon-set-845b99dbd9 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/17/23 07:26:38.732
W0117 07:26:38.749373      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/17/23 07:26:38.749
Jan 17 07:26:38.749: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 07:26:39.760: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 07:26:39.766: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-6wmhl-daemon-set-845b99dbd9=updated" 01/17/23 07:26:39.766
STEP: Confirm that there is only one ControllerRevision 01/17/23 07:26:39.78
Jan 17 07:26:39.780: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 07:26:39.785: INFO: Found 1 ControllerRevisions
Jan 17 07:26:39.790: INFO: ControllerRevision "e2e-6wmhl-daemon-set-665fc655d4" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-6wmhl-daemon-set" 01/17/23 07:26:39.798
STEP: deleting DaemonSet.extensions e2e-6wmhl-daemon-set in namespace controllerrevisions-2536, will wait for the garbage collector to delete the pods 01/17/23 07:26:39.798
Jan 17 07:26:39.870: INFO: Deleting DaemonSet.extensions e2e-6wmhl-daemon-set took: 14.379283ms
Jan 17 07:26:39.971: INFO: Terminating DaemonSet.extensions e2e-6wmhl-daemon-set pods took: 101.064121ms
Jan 17 07:26:41.278: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
Jan 17 07:26:41.278: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-6wmhl-daemon-set
Jan 17 07:26:41.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25839"},"items":null}

Jan 17 07:26:41.306: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25839"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:26:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-2536" for this suite. 01/17/23 07:26:41.341
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":192,"skipped":3257,"failed":0}
------------------------------
• [SLOW TEST] [5.903 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:35.459
    Jan 17 07:26:35.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename controllerrevisions 01/17/23 07:26:35.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:35.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:35.516
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-6wmhl-daemon-set" 01/17/23 07:26:35.567
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:26:35.581
    Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:35.589: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:35.595: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
    Jan 17 07:26:35.595: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:36.604: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:36.613: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
    Jan 17 07:26:36.614: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:37.608: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:37.614: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 2
    Jan 17 07:26:37.614: INFO: Node cluster125-w73dz53kvqes-node-1 is running 0 daemon pod, expected 1
    Jan 17 07:26:38.602: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:38.603: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:38.603: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:26:38.609: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 3
    Jan 17 07:26:38.609: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-6wmhl-daemon-set
    STEP: Confirm DaemonSet "e2e-6wmhl-daemon-set" successfully created with "daemonset-name=e2e-6wmhl-daemon-set" label 01/17/23 07:26:38.615
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-6wmhl-daemon-set" 01/17/23 07:26:38.632
    Jan 17 07:26:38.639: INFO: Located ControllerRevision: "e2e-6wmhl-daemon-set-74457c8865"
    STEP: Patching ControllerRevision "e2e-6wmhl-daemon-set-74457c8865" 01/17/23 07:26:38.645
    Jan 17 07:26:38.662: INFO: e2e-6wmhl-daemon-set-74457c8865 has been patched
    STEP: Create a new ControllerRevision 01/17/23 07:26:38.662
    Jan 17 07:26:38.675: INFO: Created ControllerRevision: e2e-6wmhl-daemon-set-845b99dbd9
    STEP: Confirm that there are two ControllerRevisions 01/17/23 07:26:38.675
    Jan 17 07:26:38.675: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 07:26:38.685: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-6wmhl-daemon-set-74457c8865" 01/17/23 07:26:38.685
    STEP: Confirm that there is only one ControllerRevision 01/17/23 07:26:38.699
    Jan 17 07:26:38.699: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 07:26:38.703: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-6wmhl-daemon-set-845b99dbd9" 01/17/23 07:26:38.712
    Jan 17 07:26:38.732: INFO: e2e-6wmhl-daemon-set-845b99dbd9 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/17/23 07:26:38.732
    W0117 07:26:38.749373      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/17/23 07:26:38.749
    Jan 17 07:26:38.749: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 07:26:39.760: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 07:26:39.766: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-6wmhl-daemon-set-845b99dbd9=updated" 01/17/23 07:26:39.766
    STEP: Confirm that there is only one ControllerRevision 01/17/23 07:26:39.78
    Jan 17 07:26:39.780: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 07:26:39.785: INFO: Found 1 ControllerRevisions
    Jan 17 07:26:39.790: INFO: ControllerRevision "e2e-6wmhl-daemon-set-665fc655d4" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-6wmhl-daemon-set" 01/17/23 07:26:39.798
    STEP: deleting DaemonSet.extensions e2e-6wmhl-daemon-set in namespace controllerrevisions-2536, will wait for the garbage collector to delete the pods 01/17/23 07:26:39.798
    Jan 17 07:26:39.870: INFO: Deleting DaemonSet.extensions e2e-6wmhl-daemon-set took: 14.379283ms
    Jan 17 07:26:39.971: INFO: Terminating DaemonSet.extensions e2e-6wmhl-daemon-set pods took: 101.064121ms
    Jan 17 07:26:41.278: INFO: Number of nodes with available pods controlled by daemonset e2e-6wmhl-daemon-set: 0
    Jan 17 07:26:41.278: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-6wmhl-daemon-set
    Jan 17 07:26:41.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25839"},"items":null}

    Jan 17 07:26:41.306: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25839"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-2536" for this suite. 01/17/23 07:26:41.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:41.365
Jan 17 07:26:41.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:26:41.367
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:41.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:41.405
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 17 07:26:41.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:26:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3878" for this suite. 01/17/23 07:26:42.475
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":193,"skipped":3276,"failed":0}
------------------------------
• [1.121 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:41.365
    Jan 17 07:26:41.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:26:41.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:41.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:41.405
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 17 07:26:41.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3878" for this suite. 01/17/23 07:26:42.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:42.493
Jan 17 07:26:42.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:26:42.495
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:42.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:42.533
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:26:42.602
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:43.547
STEP: Deploying the webhook pod 01/17/23 07:26:43.566
STEP: Wait for the deployment to be ready 01/17/23 07:26:43.596
Jan 17 07:26:43.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:26:45.629
STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:45.655
Jan 17 07:26:46.656: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/17/23 07:26:46.662
STEP: create a configmap that should be updated by the webhook 01/17/23 07:26:46.689
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:26:46.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9133" for this suite. 01/17/23 07:26:46.75
STEP: Destroying namespace "webhook-9133-markers" for this suite. 01/17/23 07:26:46.762
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":194,"skipped":3346,"failed":0}
------------------------------
• [4.447 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:42.493
    Jan 17 07:26:42.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:26:42.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:42.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:42.533
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:26:42.602
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:26:43.547
    STEP: Deploying the webhook pod 01/17/23 07:26:43.566
    STEP: Wait for the deployment to be ready 01/17/23 07:26:43.596
    Jan 17 07:26:43.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:26:45.629
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:26:45.655
    Jan 17 07:26:46.656: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/17/23 07:26:46.662
    STEP: create a configmap that should be updated by the webhook 01/17/23 07:26:46.689
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:26:46.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9133" for this suite. 01/17/23 07:26:46.75
    STEP: Destroying namespace "webhook-9133-markers" for this suite. 01/17/23 07:26:46.762
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:46.946
Jan 17 07:26:46.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:26:46.948
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:47.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:47.011
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6771/configmap-test-fe912ed1-5ea5-4c76-81e3-9b6bc0e794f8 01/17/23 07:26:47.022
STEP: Creating a pod to test consume configMaps 01/17/23 07:26:47.037
Jan 17 07:26:47.057: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e" in namespace "configmap-6771" to be "Succeeded or Failed"
Jan 17 07:26:47.080: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.691233ms
Jan 17 07:26:49.089: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.031908535s
Jan 17 07:26:51.089: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Running", Reason="", readiness=false. Elapsed: 4.03235738s
Jan 17 07:26:53.086: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029123193s
STEP: Saw pod success 01/17/23 07:26:53.086
Jan 17 07:26:53.086: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e" satisfied condition "Succeeded or Failed"
Jan 17 07:26:53.092: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e container env-test: <nil>
STEP: delete the pod 01/17/23 07:26:53.107
Jan 17 07:26:53.142: INFO: Waiting for pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e to disappear
Jan 17 07:26:53.155: INFO: Pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:26:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6771" for this suite. 01/17/23 07:26:53.163
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":195,"skipped":3394,"failed":0}
------------------------------
• [SLOW TEST] [6.231 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:46.946
    Jan 17 07:26:46.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:26:46.948
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:47.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:47.011
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6771/configmap-test-fe912ed1-5ea5-4c76-81e3-9b6bc0e794f8 01/17/23 07:26:47.022
    STEP: Creating a pod to test consume configMaps 01/17/23 07:26:47.037
    Jan 17 07:26:47.057: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e" in namespace "configmap-6771" to be "Succeeded or Failed"
    Jan 17 07:26:47.080: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.691233ms
    Jan 17 07:26:49.089: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.031908535s
    Jan 17 07:26:51.089: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Running", Reason="", readiness=false. Elapsed: 4.03235738s
    Jan 17 07:26:53.086: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029123193s
    STEP: Saw pod success 01/17/23 07:26:53.086
    Jan 17 07:26:53.086: INFO: Pod "pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e" satisfied condition "Succeeded or Failed"
    Jan 17 07:26:53.092: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e container env-test: <nil>
    STEP: delete the pod 01/17/23 07:26:53.107
    Jan 17 07:26:53.142: INFO: Waiting for pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e to disappear
    Jan 17 07:26:53.155: INFO: Pod pod-configmaps-3e8f8f45-4648-4649-a557-8cc78157bd9e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:26:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6771" for this suite. 01/17/23 07:26:53.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:26:53.178
Jan 17 07:26:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:26:53.18
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:53.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:53.219
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3362 01/17/23 07:26:53.232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[] 01/17/23 07:26:53.258
Jan 17 07:26:53.297: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3362 01/17/23 07:26:53.297
Jan 17 07:26:53.336: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3362" to be "running and ready"
Jan 17 07:26:53.392: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 56.358016ms
Jan 17 07:26:53.392: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:26:55.409: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.072959944s
Jan 17 07:26:55.409: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 07:26:55.409: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod1:[100]] 01/17/23 07:26:55.42
Jan 17 07:26:55.452: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3362 01/17/23 07:26:55.452
Jan 17 07:26:55.467: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3362" to be "running and ready"
Jan 17 07:26:55.489: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.357418ms
Jan 17 07:26:55.489: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:26:57.495: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028147594s
Jan 17 07:26:57.495: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 07:26:57.495: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod1:[100] pod2:[101]] 01/17/23 07:26:57.5
Jan 17 07:26:57.534: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/17/23 07:26:57.534
Jan 17 07:26:57.534: INFO: Creating new exec pod
Jan 17 07:26:57.562: INFO: Waiting up to 5m0s for pod "execpodccgbs" in namespace "services-3362" to be "running"
Jan 17 07:26:57.577: INFO: Pod "execpodccgbs": Phase="Pending", Reason="", readiness=false. Elapsed: 14.632208ms
Jan 17 07:26:59.588: INFO: Pod "execpodccgbs": Phase="Running", Reason="", readiness=true. Elapsed: 2.025755981s
Jan 17 07:26:59.588: INFO: Pod "execpodccgbs" satisfied condition "running"
Jan 17 07:27:00.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 17 07:27:00.905: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 17 07:27:00.905: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:27:00.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.194.98 80'
Jan 17 07:27:01.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.194.98 80\nConnection to 10.254.194.98 80 port [tcp/http] succeeded!\n"
Jan 17 07:27:01.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:27:01.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 17 07:27:01.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 17 07:27:01.598: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:27:01.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.194.98 81'
Jan 17 07:27:01.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.194.98 81\nConnection to 10.254.194.98 81 port [tcp/*] succeeded!\n"
Jan 17 07:27:01.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3362 01/17/23 07:27:01.924
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod2:[101]] 01/17/23 07:27:01.969
Jan 17 07:27:02.007: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3362 01/17/23 07:27:02.007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[] 01/17/23 07:27:02.039
Jan 17 07:27:02.066: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:27:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3362" for this suite. 01/17/23 07:27:02.122
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":196,"skipped":3399,"failed":0}
------------------------------
• [SLOW TEST] [8.964 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:26:53.178
    Jan 17 07:26:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:26:53.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:26:53.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:26:53.219
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3362 01/17/23 07:26:53.232
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[] 01/17/23 07:26:53.258
    Jan 17 07:26:53.297: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3362 01/17/23 07:26:53.297
    Jan 17 07:26:53.336: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3362" to be "running and ready"
    Jan 17 07:26:53.392: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 56.358016ms
    Jan 17 07:26:53.392: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:26:55.409: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.072959944s
    Jan 17 07:26:55.409: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 07:26:55.409: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod1:[100]] 01/17/23 07:26:55.42
    Jan 17 07:26:55.452: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3362 01/17/23 07:26:55.452
    Jan 17 07:26:55.467: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3362" to be "running and ready"
    Jan 17 07:26:55.489: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.357418ms
    Jan 17 07:26:55.489: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:26:57.495: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028147594s
    Jan 17 07:26:57.495: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 07:26:57.495: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod1:[100] pod2:[101]] 01/17/23 07:26:57.5
    Jan 17 07:26:57.534: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/17/23 07:26:57.534
    Jan 17 07:26:57.534: INFO: Creating new exec pod
    Jan 17 07:26:57.562: INFO: Waiting up to 5m0s for pod "execpodccgbs" in namespace "services-3362" to be "running"
    Jan 17 07:26:57.577: INFO: Pod "execpodccgbs": Phase="Pending", Reason="", readiness=false. Elapsed: 14.632208ms
    Jan 17 07:26:59.588: INFO: Pod "execpodccgbs": Phase="Running", Reason="", readiness=true. Elapsed: 2.025755981s
    Jan 17 07:26:59.588: INFO: Pod "execpodccgbs" satisfied condition "running"
    Jan 17 07:27:00.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 17 07:27:00.905: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 17 07:27:00.905: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:27:00.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.194.98 80'
    Jan 17 07:27:01.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.194.98 80\nConnection to 10.254.194.98 80 port [tcp/http] succeeded!\n"
    Jan 17 07:27:01.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:27:01.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 17 07:27:01.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 17 07:27:01.598: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:27:01.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-3362 exec execpodccgbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.194.98 81'
    Jan 17 07:27:01.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.194.98 81\nConnection to 10.254.194.98 81 port [tcp/*] succeeded!\n"
    Jan 17 07:27:01.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3362 01/17/23 07:27:01.924
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[pod2:[101]] 01/17/23 07:27:01.969
    Jan 17 07:27:02.007: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3362 01/17/23 07:27:02.007
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3362 to expose endpoints map[] 01/17/23 07:27:02.039
    Jan 17 07:27:02.066: INFO: successfully validated that service multi-endpoint-test in namespace services-3362 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:27:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3362" for this suite. 01/17/23 07:27:02.122
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:02.147
Jan 17 07:27:02.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename subpath 01/17/23 07:27:02.161
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:02.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:02.248
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 07:27:02.258
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-pwfh 01/17/23 07:27:02.287
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:27:02.287
Jan 17 07:27:02.309: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pwfh" in namespace "subpath-4325" to be "Succeeded or Failed"
Jan 17 07:27:02.315: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391349ms
Jan 17 07:27:04.324: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014773133s
Jan 17 07:27:06.323: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014209184s
Jan 17 07:27:08.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 6.015962316s
Jan 17 07:27:10.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 8.012973301s
Jan 17 07:27:12.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 10.012434027s
Jan 17 07:27:14.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 12.013417668s
Jan 17 07:27:16.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 14.013176969s
Jan 17 07:27:18.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015753799s
Jan 17 07:27:20.321: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 18.01237936s
Jan 17 07:27:22.321: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 20.012320138s
Jan 17 07:27:24.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 22.016256636s
Jan 17 07:27:26.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=false. Elapsed: 24.012448883s
Jan 17 07:27:28.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013308382s
STEP: Saw pod success 01/17/23 07:27:28.322
Jan 17 07:27:28.323: INFO: Pod "pod-subpath-test-secret-pwfh" satisfied condition "Succeeded or Failed"
Jan 17 07:27:28.328: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-secret-pwfh container test-container-subpath-secret-pwfh: <nil>
STEP: delete the pod 01/17/23 07:27:28.347
Jan 17 07:27:28.373: INFO: Waiting for pod pod-subpath-test-secret-pwfh to disappear
Jan 17 07:27:28.379: INFO: Pod pod-subpath-test-secret-pwfh no longer exists
STEP: Deleting pod pod-subpath-test-secret-pwfh 01/17/23 07:27:28.379
Jan 17 07:27:28.379: INFO: Deleting pod "pod-subpath-test-secret-pwfh" in namespace "subpath-4325"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 07:27:28.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4325" for this suite. 01/17/23 07:27:28.392
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":197,"skipped":3430,"failed":0}
------------------------------
• [SLOW TEST] [26.260 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:02.147
    Jan 17 07:27:02.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename subpath 01/17/23 07:27:02.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:02.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:02.248
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 07:27:02.258
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-pwfh 01/17/23 07:27:02.287
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 07:27:02.287
    Jan 17 07:27:02.309: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pwfh" in namespace "subpath-4325" to be "Succeeded or Failed"
    Jan 17 07:27:02.315: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.391349ms
    Jan 17 07:27:04.324: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014773133s
    Jan 17 07:27:06.323: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014209184s
    Jan 17 07:27:08.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 6.015962316s
    Jan 17 07:27:10.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 8.012973301s
    Jan 17 07:27:12.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 10.012434027s
    Jan 17 07:27:14.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 12.013417668s
    Jan 17 07:27:16.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 14.013176969s
    Jan 17 07:27:18.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015753799s
    Jan 17 07:27:20.321: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 18.01237936s
    Jan 17 07:27:22.321: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 20.012320138s
    Jan 17 07:27:24.325: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=true. Elapsed: 22.016256636s
    Jan 17 07:27:26.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Running", Reason="", readiness=false. Elapsed: 24.012448883s
    Jan 17 07:27:28.322: INFO: Pod "pod-subpath-test-secret-pwfh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013308382s
    STEP: Saw pod success 01/17/23 07:27:28.322
    Jan 17 07:27:28.323: INFO: Pod "pod-subpath-test-secret-pwfh" satisfied condition "Succeeded or Failed"
    Jan 17 07:27:28.328: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-secret-pwfh container test-container-subpath-secret-pwfh: <nil>
    STEP: delete the pod 01/17/23 07:27:28.347
    Jan 17 07:27:28.373: INFO: Waiting for pod pod-subpath-test-secret-pwfh to disappear
    Jan 17 07:27:28.379: INFO: Pod pod-subpath-test-secret-pwfh no longer exists
    STEP: Deleting pod pod-subpath-test-secret-pwfh 01/17/23 07:27:28.379
    Jan 17 07:27:28.379: INFO: Deleting pod "pod-subpath-test-secret-pwfh" in namespace "subpath-4325"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 07:27:28.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4325" for this suite. 01/17/23 07:27:28.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:28.413
Jan 17 07:27:28.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename containers 01/17/23 07:27:28.414
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:28.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:28.455
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/17/23 07:27:28.465
Jan 17 07:27:28.485: INFO: Waiting up to 5m0s for pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc" in namespace "containers-8672" to be "Succeeded or Failed"
Jan 17 07:27:28.491: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.819487ms
Jan 17 07:27:30.501: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016081346s
Jan 17 07:27:32.498: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012893877s
Jan 17 07:27:34.499: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014148318s
STEP: Saw pod success 01/17/23 07:27:34.499
Jan 17 07:27:34.499: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc" satisfied condition "Succeeded or Failed"
Jan 17 07:27:34.506: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:27:34.52
Jan 17 07:27:34.549: INFO: Waiting for pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc to disappear
Jan 17 07:27:34.557: INFO: Pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 07:27:34.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8672" for this suite. 01/17/23 07:27:34.565
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":198,"skipped":3496,"failed":0}
------------------------------
• [SLOW TEST] [6.177 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:28.413
    Jan 17 07:27:28.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename containers 01/17/23 07:27:28.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:28.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:28.455
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/17/23 07:27:28.465
    Jan 17 07:27:28.485: INFO: Waiting up to 5m0s for pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc" in namespace "containers-8672" to be "Succeeded or Failed"
    Jan 17 07:27:28.491: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.819487ms
    Jan 17 07:27:30.501: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016081346s
    Jan 17 07:27:32.498: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012893877s
    Jan 17 07:27:34.499: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014148318s
    STEP: Saw pod success 01/17/23 07:27:34.499
    Jan 17 07:27:34.499: INFO: Pod "client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc" satisfied condition "Succeeded or Failed"
    Jan 17 07:27:34.506: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:27:34.52
    Jan 17 07:27:34.549: INFO: Waiting for pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc to disappear
    Jan 17 07:27:34.557: INFO: Pod client-containers-8386fb65-2210-40a5-82f2-76ef84cb45bc no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 07:27:34.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8672" for this suite. 01/17/23 07:27:34.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:34.59
Jan 17 07:27:34.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:27:34.591
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:34.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:34.651
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 17 07:27:34.664: INFO: Creating simple deployment test-new-deployment
Jan 17 07:27:34.727: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/17/23 07:27:36.767
STEP: updating a scale subresource 01/17/23 07:27:36.774
STEP: verifying the deployment Spec.Replicas was modified 01/17/23 07:27:36.807
STEP: Patch a scale subresource 01/17/23 07:27:36.845
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:27:37.016: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3279  d78e5eb3-4aa6-4476-98fb-7448054b73e9 26313 3 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-17 07:27:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c2ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-17 07:27:36 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:27:36 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 07:27:37.049: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3279  ddc37ef0-badc-4e51-8a85-c2c10342893d 26319 3 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d78e5eb3-4aa6-4476-98fb-7448054b73e9 0xc00308cd87 0xc00308cd88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78e5eb3-4aa6-4476-98fb-7448054b73e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00308cf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:27:37.073: INFO: Pod "test-new-deployment-845c8977d9-hd84c" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-hd84c test-new-deployment-845c8977d9- deployment-3279  f65e519e-16be-439e-9803-afd3c8e38e03 26321 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00308de97 0xc00308de98}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm6j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm6j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:27:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:27:37.073: INFO: Pod "test-new-deployment-845c8977d9-t5nbf" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-t5nbf test-new-deployment-845c8977d9- deployment-3279  d685c63b-6623-467d-96d0-24ccea282f07 26320 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e1d0 0xc00400e1d1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8s8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8s8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:27:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:27:37.074: INFO: Pod "test-new-deployment-845c8977d9-v4mmd" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-v4mmd test-new-deployment-845c8977d9- deployment-3279  a6493d2c-572b-450a-ba66-d68002c08e42 26288 0 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:35f68ba008b518fb6e7b5787f8ca18ac55776c72e9ec25966f3b7c6b1d058d15 cni.projectcalico.org/podIP:10.100.135.22/32 cni.projectcalico.org/podIPs:10.100.135.22/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e3b0 0xc00400e3b1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:27:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgdb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgdb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.22,StartTime:2023-01-17 07:27:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:27:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29ebbb968fabf372bc0f3f4eceadfc04e2b55d94310ba0dd3a44e2737ffbdd02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:27:37.074: INFO: Pod "test-new-deployment-845c8977d9-z2fzq" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-z2fzq test-new-deployment-845c8977d9- deployment-3279  191341c4-0812-4fea-a82d-f1920d6bd742 26303 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e5c0 0xc00400e5c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7sr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7sr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:27:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:27:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3279" for this suite. 01/17/23 07:27:37.082
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":199,"skipped":3501,"failed":0}
------------------------------
• [2.508 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:34.59
    Jan 17 07:27:34.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:27:34.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:34.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:34.651
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 17 07:27:34.664: INFO: Creating simple deployment test-new-deployment
    Jan 17 07:27:34.727: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/17/23 07:27:36.767
    STEP: updating a scale subresource 01/17/23 07:27:36.774
    STEP: verifying the deployment Spec.Replicas was modified 01/17/23 07:27:36.807
    STEP: Patch a scale subresource 01/17/23 07:27:36.845
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:27:37.016: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3279  d78e5eb3-4aa6-4476-98fb-7448054b73e9 26313 3 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-17 07:27:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c2ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-17 07:27:36 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:27:36 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 07:27:37.049: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3279  ddc37ef0-badc-4e51-8a85-c2c10342893d 26319 3 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d78e5eb3-4aa6-4476-98fb-7448054b73e9 0xc00308cd87 0xc00308cd88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78e5eb3-4aa6-4476-98fb-7448054b73e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00308cf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:27:37.073: INFO: Pod "test-new-deployment-845c8977d9-hd84c" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-hd84c test-new-deployment-845c8977d9- deployment-3279  f65e519e-16be-439e-9803-afd3c8e38e03 26321 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00308de97 0xc00308de98}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm6j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm6j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:27:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:27:37.073: INFO: Pod "test-new-deployment-845c8977d9-t5nbf" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-t5nbf test-new-deployment-845c8977d9- deployment-3279  d685c63b-6623-467d-96d0-24ccea282f07 26320 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e1d0 0xc00400e1d1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8s8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8s8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:27:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:27:37.074: INFO: Pod "test-new-deployment-845c8977d9-v4mmd" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-v4mmd test-new-deployment-845c8977d9- deployment-3279  a6493d2c-572b-450a-ba66-d68002c08e42 26288 0 2023-01-17 07:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:35f68ba008b518fb6e7b5787f8ca18ac55776c72e9ec25966f3b7c6b1d058d15 cni.projectcalico.org/podIP:10.100.135.22/32 cni.projectcalico.org/podIPs:10.100.135.22/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e3b0 0xc00400e3b1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:27:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgdb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgdb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.22,StartTime:2023-01-17 07:27:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:27:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29ebbb968fabf372bc0f3f4eceadfc04e2b55d94310ba0dd3a44e2737ffbdd02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:27:37.074: INFO: Pod "test-new-deployment-845c8977d9-z2fzq" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-z2fzq test-new-deployment-845c8977d9- deployment-3279  191341c4-0812-4fea-a82d-f1920d6bd742 26303 0 2023-01-17 07:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ddc37ef0-badc-4e51-8a85-c2c10342893d 0xc00400e5c0 0xc00400e5c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc37ef0-badc-4e51-8a85-c2c10342893d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:27:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7sr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7sr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:27:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:27:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3279" for this suite. 01/17/23 07:27:37.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:37.1
Jan 17 07:27:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:27:37.102
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:37.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:37.146
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-1508a8a1-b904-42c5-a62d-8fd334769886 01/17/23 07:27:37.152
STEP: Creating a pod to test consume secrets 01/17/23 07:27:37.164
Jan 17 07:27:37.181: INFO: Waiting up to 5m0s for pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f" in namespace "secrets-8832" to be "Succeeded or Failed"
Jan 17 07:27:37.197: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.823064ms
Jan 17 07:27:39.204: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023379906s
Jan 17 07:27:41.203: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022341505s
STEP: Saw pod success 01/17/23 07:27:41.203
Jan 17 07:27:41.203: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f" satisfied condition "Succeeded or Failed"
Jan 17 07:27:41.209: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:27:41.222
Jan 17 07:27:41.269: INFO: Waiting for pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f to disappear
Jan 17 07:27:41.275: INFO: Pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:27:41.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8832" for this suite. 01/17/23 07:27:41.284
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":200,"skipped":3507,"failed":0}
------------------------------
• [4.207 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:37.1
    Jan 17 07:27:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:27:37.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:37.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:37.146
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-1508a8a1-b904-42c5-a62d-8fd334769886 01/17/23 07:27:37.152
    STEP: Creating a pod to test consume secrets 01/17/23 07:27:37.164
    Jan 17 07:27:37.181: INFO: Waiting up to 5m0s for pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f" in namespace "secrets-8832" to be "Succeeded or Failed"
    Jan 17 07:27:37.197: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.823064ms
    Jan 17 07:27:39.204: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023379906s
    Jan 17 07:27:41.203: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022341505s
    STEP: Saw pod success 01/17/23 07:27:41.203
    Jan 17 07:27:41.203: INFO: Pod "pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f" satisfied condition "Succeeded or Failed"
    Jan 17 07:27:41.209: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:27:41.222
    Jan 17 07:27:41.269: INFO: Waiting for pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f to disappear
    Jan 17 07:27:41.275: INFO: Pod pod-secrets-89b48dde-d903-4297-8f6a-e5cc8e34296f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:27:41.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8832" for this suite. 01/17/23 07:27:41.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:41.307
Jan 17 07:27:41.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename events 01/17/23 07:27:41.308
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:41.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:41.368
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/17/23 07:27:41.38
STEP: get a list of Events with a label in the current namespace 01/17/23 07:27:41.414
STEP: delete a list of events 01/17/23 07:27:41.42
Jan 17 07:27:41.420: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/17/23 07:27:41.458
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 17 07:27:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8523" for this suite. 01/17/23 07:27:41.472
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":201,"skipped":3515,"failed":0}
------------------------------
• [0.183 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:41.307
    Jan 17 07:27:41.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename events 01/17/23 07:27:41.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:41.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:41.368
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/17/23 07:27:41.38
    STEP: get a list of Events with a label in the current namespace 01/17/23 07:27:41.414
    STEP: delete a list of events 01/17/23 07:27:41.42
    Jan 17 07:27:41.420: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/17/23 07:27:41.458
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 17 07:27:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8523" for this suite. 01/17/23 07:27:41.472
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:41.494
Jan 17 07:27:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:27:41.496
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:41.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:41.547
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 17 07:27:41.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: creating the pod 01/17/23 07:27:41.559
STEP: submitting the pod to kubernetes 01/17/23 07:27:41.559
Jan 17 07:27:41.591: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74" in namespace "pods-613" to be "running and ready"
Jan 17 07:27:41.601: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105877ms
Jan 17 07:27:41.601: INFO: The phase of Pod pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:27:43.624: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74": Phase="Running", Reason="", readiness=true. Elapsed: 2.032880584s
Jan 17 07:27:43.624: INFO: The phase of Pod pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74 is Running (Ready = true)
Jan 17 07:27:43.624: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:27:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-613" for this suite. 01/17/23 07:27:43.672
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":202,"skipped":3518,"failed":0}
------------------------------
• [2.202 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:41.494
    Jan 17 07:27:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:27:41.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:41.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:41.547
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 17 07:27:41.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: creating the pod 01/17/23 07:27:41.559
    STEP: submitting the pod to kubernetes 01/17/23 07:27:41.559
    Jan 17 07:27:41.591: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74" in namespace "pods-613" to be "running and ready"
    Jan 17 07:27:41.601: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105877ms
    Jan 17 07:27:41.601: INFO: The phase of Pod pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:27:43.624: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74": Phase="Running", Reason="", readiness=true. Elapsed: 2.032880584s
    Jan 17 07:27:43.624: INFO: The phase of Pod pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74 is Running (Ready = true)
    Jan 17 07:27:43.624: INFO: Pod "pod-logs-websocket-e1726138-a164-4081-a234-967ece953f74" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:27:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-613" for this suite. 01/17/23 07:27:43.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:27:43.696
Jan 17 07:27:43.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 07:27:43.697
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:43.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:43.737
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f in namespace container-probe-7084 01/17/23 07:27:43.744
Jan 17 07:27:43.762: INFO: Waiting up to 5m0s for pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f" in namespace "container-probe-7084" to be "not pending"
Jan 17 07:27:43.769: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887146ms
Jan 17 07:27:45.776: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013887448s
Jan 17 07:27:45.776: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f" satisfied condition "not pending"
Jan 17 07:27:45.776: INFO: Started pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f in namespace container-probe-7084
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:27:45.776
Jan 17 07:27:45.782: INFO: Initial restart count of pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f is 0
Jan 17 07:28:35.996: INFO: Restart count of pod container-probe-7084/busybox-4558bd3b-9c60-408b-b199-ae5baad3736f is now 1 (50.213863808s elapsed)
STEP: deleting the pod 01/17/23 07:28:35.996
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 07:28:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7084" for this suite. 01/17/23 07:28:36.034
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":203,"skipped":3525,"failed":0}
------------------------------
• [SLOW TEST] [52.356 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:27:43.696
    Jan 17 07:27:43.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 07:27:43.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:27:43.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:27:43.737
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f in namespace container-probe-7084 01/17/23 07:27:43.744
    Jan 17 07:27:43.762: INFO: Waiting up to 5m0s for pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f" in namespace "container-probe-7084" to be "not pending"
    Jan 17 07:27:43.769: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887146ms
    Jan 17 07:27:45.776: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013887448s
    Jan 17 07:27:45.776: INFO: Pod "busybox-4558bd3b-9c60-408b-b199-ae5baad3736f" satisfied condition "not pending"
    Jan 17 07:27:45.776: INFO: Started pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f in namespace container-probe-7084
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:27:45.776
    Jan 17 07:27:45.782: INFO: Initial restart count of pod busybox-4558bd3b-9c60-408b-b199-ae5baad3736f is 0
    Jan 17 07:28:35.996: INFO: Restart count of pod container-probe-7084/busybox-4558bd3b-9c60-408b-b199-ae5baad3736f is now 1 (50.213863808s elapsed)
    STEP: deleting the pod 01/17/23 07:28:35.996
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 07:28:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7084" for this suite. 01/17/23 07:28:36.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:28:36.056
Jan 17 07:28:36.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 07:28:36.057
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:36.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:36.119
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/17/23 07:28:36.128
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/17/23 07:28:36.128
STEP: creating a pod to probe DNS 01/17/23 07:28:36.128
STEP: submitting the pod to kubernetes 01/17/23 07:28:36.128
Jan 17 07:28:36.149: INFO: Waiting up to 15m0s for pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192" in namespace "dns-7460" to be "running"
Jan 17 07:28:36.164: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192": Phase="Pending", Reason="", readiness=false. Elapsed: 14.766889ms
Jan 17 07:28:38.171: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192": Phase="Running", Reason="", readiness=true. Elapsed: 2.021033038s
Jan 17 07:28:38.171: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192" satisfied condition "running"
STEP: retrieving the pod 01/17/23 07:28:38.171
STEP: looking for the results for each expected name from probers 01/17/23 07:28:38.177
Jan 17 07:28:38.207: INFO: DNS probes using dns-7460/dns-test-154ca81e-4206-4c00-95c0-506b26a62192 succeeded

STEP: deleting the pod 01/17/23 07:28:38.207
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 07:28:38.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7460" for this suite. 01/17/23 07:28:38.299
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":204,"skipped":3539,"failed":0}
------------------------------
• [2.265 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:28:36.056
    Jan 17 07:28:36.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 07:28:36.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:36.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:36.119
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/17/23 07:28:36.128
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/17/23 07:28:36.128
    STEP: creating a pod to probe DNS 01/17/23 07:28:36.128
    STEP: submitting the pod to kubernetes 01/17/23 07:28:36.128
    Jan 17 07:28:36.149: INFO: Waiting up to 15m0s for pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192" in namespace "dns-7460" to be "running"
    Jan 17 07:28:36.164: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192": Phase="Pending", Reason="", readiness=false. Elapsed: 14.766889ms
    Jan 17 07:28:38.171: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192": Phase="Running", Reason="", readiness=true. Elapsed: 2.021033038s
    Jan 17 07:28:38.171: INFO: Pod "dns-test-154ca81e-4206-4c00-95c0-506b26a62192" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 07:28:38.171
    STEP: looking for the results for each expected name from probers 01/17/23 07:28:38.177
    Jan 17 07:28:38.207: INFO: DNS probes using dns-7460/dns-test-154ca81e-4206-4c00-95c0-506b26a62192 succeeded

    STEP: deleting the pod 01/17/23 07:28:38.207
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 07:28:38.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7460" for this suite. 01/17/23 07:28:38.299
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:28:38.321
Jan 17 07:28:38.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename conformance-tests 01/17/23 07:28:38.323
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:38.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:38.362
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/17/23 07:28:38.37
Jan 17 07:28:38.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 17 07:28:38.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3943" for this suite. 01/17/23 07:28:38.391
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":205,"skipped":3543,"failed":0}
------------------------------
• [0.081 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:28:38.321
    Jan 17 07:28:38.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename conformance-tests 01/17/23 07:28:38.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:38.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:38.362
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/17/23 07:28:38.37
    Jan 17 07:28:38.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 17 07:28:38.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3943" for this suite. 01/17/23 07:28:38.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:28:38.407
Jan 17 07:28:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:28:38.408
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:38.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:38.448
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-944c6ef5-5709-49c6-bef5-15dc7dd3add9 01/17/23 07:28:38.464
STEP: Creating a pod to test consume configMaps 01/17/23 07:28:38.477
Jan 17 07:28:38.495: INFO: Waiting up to 5m0s for pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b" in namespace "configmap-8152" to be "Succeeded or Failed"
Jan 17 07:28:38.505: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027405ms
Jan 17 07:28:40.512: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017153871s
Jan 17 07:28:42.512: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017133621s
STEP: Saw pod success 01/17/23 07:28:42.512
Jan 17 07:28:42.513: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b" satisfied condition "Succeeded or Failed"
Jan 17 07:28:42.518: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b container configmap-volume-test: <nil>
STEP: delete the pod 01/17/23 07:28:42.527
Jan 17 07:28:42.558: INFO: Waiting for pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b to disappear
Jan 17 07:28:42.565: INFO: Pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:28:42.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8152" for this suite. 01/17/23 07:28:42.572
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":206,"skipped":3581,"failed":0}
------------------------------
• [4.183 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:28:38.407
    Jan 17 07:28:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:28:38.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:38.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:38.448
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-944c6ef5-5709-49c6-bef5-15dc7dd3add9 01/17/23 07:28:38.464
    STEP: Creating a pod to test consume configMaps 01/17/23 07:28:38.477
    Jan 17 07:28:38.495: INFO: Waiting up to 5m0s for pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b" in namespace "configmap-8152" to be "Succeeded or Failed"
    Jan 17 07:28:38.505: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027405ms
    Jan 17 07:28:40.512: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017153871s
    Jan 17 07:28:42.512: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017133621s
    STEP: Saw pod success 01/17/23 07:28:42.512
    Jan 17 07:28:42.513: INFO: Pod "pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b" satisfied condition "Succeeded or Failed"
    Jan 17 07:28:42.518: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b container configmap-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:28:42.527
    Jan 17 07:28:42.558: INFO: Waiting for pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b to disappear
    Jan 17 07:28:42.565: INFO: Pod pod-configmaps-1405dad0-9f0c-41da-b99c-afb8f28b630b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:28:42.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8152" for this suite. 01/17/23 07:28:42.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:28:42.6
Jan 17 07:28:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename certificates 01/17/23 07:28:42.601
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:42.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:42.643
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/17/23 07:28:43.621
STEP: getting /apis/certificates.k8s.io 01/17/23 07:28:43.629
STEP: getting /apis/certificates.k8s.io/v1 01/17/23 07:28:43.632
STEP: creating 01/17/23 07:28:43.635
STEP: getting 01/17/23 07:28:43.672
STEP: listing 01/17/23 07:28:43.679
STEP: watching 01/17/23 07:28:43.686
Jan 17 07:28:43.687: INFO: starting watch
STEP: patching 01/17/23 07:28:43.691
STEP: updating 01/17/23 07:28:43.716
Jan 17 07:28:43.739: INFO: waiting for watch events with expected annotations
Jan 17 07:28:43.739: INFO: saw patched and updated annotations
STEP: getting /approval 01/17/23 07:28:43.739
STEP: patching /approval 01/17/23 07:28:43.748
STEP: updating /approval 01/17/23 07:28:43.766
STEP: getting /status 01/17/23 07:28:43.781
STEP: patching /status 01/17/23 07:28:43.787
STEP: updating /status 01/17/23 07:28:43.811
STEP: deleting 01/17/23 07:28:43.84
STEP: deleting a collection 01/17/23 07:28:43.866
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:28:43.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2920" for this suite. 01/17/23 07:28:43.896
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":207,"skipped":3616,"failed":0}
------------------------------
• [1.316 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:28:42.6
    Jan 17 07:28:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename certificates 01/17/23 07:28:42.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:42.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:42.643
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/17/23 07:28:43.621
    STEP: getting /apis/certificates.k8s.io 01/17/23 07:28:43.629
    STEP: getting /apis/certificates.k8s.io/v1 01/17/23 07:28:43.632
    STEP: creating 01/17/23 07:28:43.635
    STEP: getting 01/17/23 07:28:43.672
    STEP: listing 01/17/23 07:28:43.679
    STEP: watching 01/17/23 07:28:43.686
    Jan 17 07:28:43.687: INFO: starting watch
    STEP: patching 01/17/23 07:28:43.691
    STEP: updating 01/17/23 07:28:43.716
    Jan 17 07:28:43.739: INFO: waiting for watch events with expected annotations
    Jan 17 07:28:43.739: INFO: saw patched and updated annotations
    STEP: getting /approval 01/17/23 07:28:43.739
    STEP: patching /approval 01/17/23 07:28:43.748
    STEP: updating /approval 01/17/23 07:28:43.766
    STEP: getting /status 01/17/23 07:28:43.781
    STEP: patching /status 01/17/23 07:28:43.787
    STEP: updating /status 01/17/23 07:28:43.811
    STEP: deleting 01/17/23 07:28:43.84
    STEP: deleting a collection 01/17/23 07:28:43.866
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:28:43.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2920" for this suite. 01/17/23 07:28:43.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:28:43.918
Jan 17 07:28:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:28:43.92
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:43.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:43.968
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/17/23 07:28:43.976
STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:28:43.99
STEP: Creating a ResourceQuota with not best effort scope 01/17/23 07:28:45.999
STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:28:46.01
STEP: Creating a best-effort pod 01/17/23 07:28:48.019
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/17/23 07:28:48.052
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/17/23 07:28:50.057
STEP: Deleting the pod 01/17/23 07:28:52.063
STEP: Ensuring resource quota status released the pod usage 01/17/23 07:28:52.09
STEP: Creating a not best-effort pod 01/17/23 07:28:54.096
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/17/23 07:28:54.134
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/17/23 07:28:56.141
STEP: Deleting the pod 01/17/23 07:28:58.148
STEP: Ensuring resource quota status released the pod usage 01/17/23 07:28:58.184
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:29:00.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4086" for this suite. 01/17/23 07:29:00.208
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":208,"skipped":3627,"failed":0}
------------------------------
• [SLOW TEST] [16.312 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:28:43.918
    Jan 17 07:28:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:28:43.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:28:43.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:28:43.968
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/17/23 07:28:43.976
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:28:43.99
    STEP: Creating a ResourceQuota with not best effort scope 01/17/23 07:28:45.999
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:28:46.01
    STEP: Creating a best-effort pod 01/17/23 07:28:48.019
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/17/23 07:28:48.052
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/17/23 07:28:50.057
    STEP: Deleting the pod 01/17/23 07:28:52.063
    STEP: Ensuring resource quota status released the pod usage 01/17/23 07:28:52.09
    STEP: Creating a not best-effort pod 01/17/23 07:28:54.096
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/17/23 07:28:54.134
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/17/23 07:28:56.141
    STEP: Deleting the pod 01/17/23 07:28:58.148
    STEP: Ensuring resource quota status released the pod usage 01/17/23 07:28:58.184
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:29:00.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4086" for this suite. 01/17/23 07:29:00.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:00.233
Jan 17 07:29:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:29:00.235
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:00.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:00.285
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8594/secret-test-b31bae5f-fc3d-460a-a5ce-566c5a1cf218 01/17/23 07:29:00.3
STEP: Creating a pod to test consume secrets 01/17/23 07:29:00.315
Jan 17 07:29:00.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7" in namespace "secrets-8594" to be "Succeeded or Failed"
Jan 17 07:29:00.360: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.289295ms
Jan 17 07:29:02.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025138925s
Jan 17 07:29:04.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025086402s
STEP: Saw pod success 01/17/23 07:29:04.366
Jan 17 07:29:04.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7" satisfied condition "Succeeded or Failed"
Jan 17 07:29:04.372: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 container env-test: <nil>
STEP: delete the pod 01/17/23 07:29:04.381
Jan 17 07:29:04.405: INFO: Waiting for pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 to disappear
Jan 17 07:29:04.413: INFO: Pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:29:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8594" for this suite. 01/17/23 07:29:04.419
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":209,"skipped":3658,"failed":0}
------------------------------
• [4.201 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:00.233
    Jan 17 07:29:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:29:00.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:00.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:00.285
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8594/secret-test-b31bae5f-fc3d-460a-a5ce-566c5a1cf218 01/17/23 07:29:00.3
    STEP: Creating a pod to test consume secrets 01/17/23 07:29:00.315
    Jan 17 07:29:00.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7" in namespace "secrets-8594" to be "Succeeded or Failed"
    Jan 17 07:29:00.360: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.289295ms
    Jan 17 07:29:02.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025138925s
    Jan 17 07:29:04.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025086402s
    STEP: Saw pod success 01/17/23 07:29:04.366
    Jan 17 07:29:04.366: INFO: Pod "pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7" satisfied condition "Succeeded or Failed"
    Jan 17 07:29:04.372: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 container env-test: <nil>
    STEP: delete the pod 01/17/23 07:29:04.381
    Jan 17 07:29:04.405: INFO: Waiting for pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 to disappear
    Jan 17 07:29:04.413: INFO: Pod pod-configmaps-014bd42d-0a39-40f9-99cb-fe15033215a7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:29:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8594" for this suite. 01/17/23 07:29:04.419
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:04.434
Jan 17 07:29:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:29:04.436
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:04.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:04.475
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/17/23 07:29:04.485
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/17/23 07:29:04.488
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 07:29:04.488
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/17/23 07:29:04.488
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/17/23 07:29:04.491
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 07:29:04.491
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 07:29:04.494
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:29:04.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4184" for this suite. 01/17/23 07:29:04.5
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":210,"skipped":3660,"failed":0}
------------------------------
• [0.079 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:04.434
    Jan 17 07:29:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 07:29:04.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:04.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:04.475
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/17/23 07:29:04.485
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/17/23 07:29:04.488
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 07:29:04.488
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/17/23 07:29:04.488
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/17/23 07:29:04.491
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 07:29:04.491
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 07:29:04.494
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:29:04.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4184" for this suite. 01/17/23 07:29:04.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:04.515
Jan 17 07:29:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename daemonsets 01/17/23 07:29:04.516
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:04.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:04.558
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 17 07:29:04.631: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:29:04.645
Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:04.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:29:04.689: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:05.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:29:05.713: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:29:06.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:06.701: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:06.701: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:06.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:29:06.706: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/17/23 07:29:06.731
STEP: Check that daemon pods images are updated. 01/17/23 07:29:06.757
Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-cqw5b. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:07.779: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:07.779: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:08.791: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:08.791: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:09.781: INFO: Pod daemon-set-lftbs is not available
Jan 17 07:29:09.781: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:09.781: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:10.782: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:11.781: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:12.813: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:12.813: INFO: Pod daemon-set-zf4mr is not available
Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:13.782: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 07:29:13.782: INFO: Pod daemon-set-zf4mr is not available
Jan 17 07:29:13.793: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:13.794: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:13.794: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.779: INFO: Pod daemon-set-jtv2k is not available
Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/17/23 07:29:15.791
Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:15.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 07:29:15.810: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 07:29:16.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 07:29:16.848: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:29:16.886
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9552, will wait for the garbage collector to delete the pods 01/17/23 07:29:16.888
Jan 17 07:29:16.968: INFO: Deleting DaemonSet.extensions daemon-set took: 19.802554ms
Jan 17 07:29:17.069: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.789103ms
Jan 17 07:29:19.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 07:29:19.076: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 07:29:19.083: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27069"},"items":null}

Jan 17 07:29:19.098: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27069"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:29:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9552" for this suite. 01/17/23 07:29:19.135
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":211,"skipped":3690,"failed":0}
------------------------------
• [SLOW TEST] [14.637 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:04.515
    Jan 17 07:29:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename daemonsets 01/17/23 07:29:04.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:04.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:04.558
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 17 07:29:04.631: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 07:29:04.645
    Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:04.672: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:04.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:29:04.689: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:05.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:05.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:29:05.713: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:29:06.700: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:06.701: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:06.701: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:06.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:29:06.706: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/17/23 07:29:06.731
    STEP: Check that daemon pods images are updated. 01/17/23 07:29:06.757
    Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-cqw5b. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:06.763: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:06.773: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:07.779: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:07.779: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:07.785: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:08.791: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:08.791: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:08.828: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:09.781: INFO: Pod daemon-set-lftbs is not available
    Jan 17 07:29:09.781: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:09.781: INFO: Wrong image for pod: daemon-set-rvz24. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:09.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:10.782: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:10.789: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:11.781: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:11.788: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:12.813: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:12.813: INFO: Pod daemon-set-zf4mr is not available
    Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:12.832: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:13.782: INFO: Wrong image for pod: daemon-set-rjtzt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 07:29:13.782: INFO: Pod daemon-set-zf4mr is not available
    Jan 17 07:29:13.793: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:13.794: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:13.794: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:14.797: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.779: INFO: Pod daemon-set-jtv2k is not available
    Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.791: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/17/23 07:29:15.791
    Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.802: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:15.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 07:29:15.810: INFO: Node cluster125-w73dz53kvqes-node-0 is running 0 daemon pod, expected 1
    Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:16.833: INFO: DaemonSet pods can't tolerate node cluster125-w73dz53kvqes-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 07:29:16.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 07:29:16.848: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 07:29:16.886
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9552, will wait for the garbage collector to delete the pods 01/17/23 07:29:16.888
    Jan 17 07:29:16.968: INFO: Deleting DaemonSet.extensions daemon-set took: 19.802554ms
    Jan 17 07:29:17.069: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.789103ms
    Jan 17 07:29:19.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 07:29:19.076: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 07:29:19.083: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27069"},"items":null}

    Jan 17 07:29:19.098: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27069"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:29:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9552" for this suite. 01/17/23 07:29:19.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:19.153
Jan 17 07:29:19.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:29:19.155
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:19.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:19.2
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 07:29:19.21
Jan 17 07:29:19.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 07:29:19.417: INFO: stderr: ""
Jan 17 07:29:19.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/17/23 07:29:19.417
STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 07:29:24.471
Jan 17 07:29:24.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 get pod e2e-test-httpd-pod -o json'
Jan 17 07:29:24.595: INFO: stderr: ""
Jan 17 07:29:24.596: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f5523399f98c5bc022eab02d2845383afd592b9269debf792e90cd0fed0be8a2\",\n            \"cni.projectcalico.org/podIP\": \"10.100.135.9/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.135.9/32\"\n        },\n        \"creationTimestamp\": \"2023-01-17T07:29:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4196\",\n        \"resourceVersion\": \"27089\",\n        \"uid\": \"d15244bf-7f7f-4f5d-bda2-18c9966cb09f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zmpkx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cluster125-w73dz53kvqes-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zmpkx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://894f54c404c8ddcb4932970375a79f4fa83145c2c336f38d7ebb20ab97667e9a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-17T07:29:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.22\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.135.9\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.135.9\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-17T07:29:19Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/17/23 07:29:24.596
Jan 17 07:29:24.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 replace -f -'
Jan 17 07:29:26.127: INFO: stderr: ""
Jan 17 07:29:26.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/17/23 07:29:26.127
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 17 07:29:26.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 delete pods e2e-test-httpd-pod'
Jan 17 07:29:28.157: INFO: stderr: ""
Jan 17 07:29:28.157: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:29:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4196" for this suite. 01/17/23 07:29:28.165
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":212,"skipped":3695,"failed":0}
------------------------------
• [SLOW TEST] [9.032 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:19.153
    Jan 17 07:29:19.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:29:19.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:19.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:19.2
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 07:29:19.21
    Jan 17 07:29:19.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 17 07:29:19.417: INFO: stderr: ""
    Jan 17 07:29:19.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/17/23 07:29:19.417
    STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 07:29:24.471
    Jan 17 07:29:24.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 get pod e2e-test-httpd-pod -o json'
    Jan 17 07:29:24.595: INFO: stderr: ""
    Jan 17 07:29:24.596: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f5523399f98c5bc022eab02d2845383afd592b9269debf792e90cd0fed0be8a2\",\n            \"cni.projectcalico.org/podIP\": \"10.100.135.9/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.135.9/32\"\n        },\n        \"creationTimestamp\": \"2023-01-17T07:29:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4196\",\n        \"resourceVersion\": \"27089\",\n        \"uid\": \"d15244bf-7f7f-4f5d-bda2-18c9966cb09f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zmpkx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cluster125-w73dz53kvqes-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zmpkx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T07:29:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://894f54c404c8ddcb4932970375a79f4fa83145c2c336f38d7ebb20ab97667e9a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-17T07:29:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.22\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.135.9\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.135.9\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-17T07:29:19Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/17/23 07:29:24.596
    Jan 17 07:29:24.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 replace -f -'
    Jan 17 07:29:26.127: INFO: stderr: ""
    Jan 17 07:29:26.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/17/23 07:29:26.127
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 17 07:29:26.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4196 delete pods e2e-test-httpd-pod'
    Jan 17 07:29:28.157: INFO: stderr: ""
    Jan 17 07:29:28.157: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:29:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4196" for this suite. 01/17/23 07:29:28.165
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:28.186
Jan 17 07:29:28.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:29:28.187
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:28.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:28.23
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/17/23 07:29:28.237
STEP: Creating a ResourceQuota 01/17/23 07:29:33.25
STEP: Ensuring resource quota status is calculated 01/17/23 07:29:33.265
STEP: Creating a Pod that fits quota 01/17/23 07:29:35.272
STEP: Ensuring ResourceQuota status captures the pod usage 01/17/23 07:29:35.311
STEP: Not allowing a pod to be created that exceeds remaining quota 01/17/23 07:29:37.317
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/17/23 07:29:37.32
STEP: Ensuring a pod cannot update its resource requirements 01/17/23 07:29:37.324
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/17/23 07:29:37.334
STEP: Deleting the pod 01/17/23 07:29:39.342
STEP: Ensuring resource quota status released the pod usage 01/17/23 07:29:39.379
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:29:41.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9751" for this suite. 01/17/23 07:29:41.396
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":213,"skipped":3697,"failed":0}
------------------------------
• [SLOW TEST] [13.233 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:28.186
    Jan 17 07:29:28.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:29:28.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:28.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:28.23
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/17/23 07:29:28.237
    STEP: Creating a ResourceQuota 01/17/23 07:29:33.25
    STEP: Ensuring resource quota status is calculated 01/17/23 07:29:33.265
    STEP: Creating a Pod that fits quota 01/17/23 07:29:35.272
    STEP: Ensuring ResourceQuota status captures the pod usage 01/17/23 07:29:35.311
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/17/23 07:29:37.317
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/17/23 07:29:37.32
    STEP: Ensuring a pod cannot update its resource requirements 01/17/23 07:29:37.324
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/17/23 07:29:37.334
    STEP: Deleting the pod 01/17/23 07:29:39.342
    STEP: Ensuring resource quota status released the pod usage 01/17/23 07:29:39.379
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:29:41.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9751" for this suite. 01/17/23 07:29:41.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:41.42
Jan 17 07:29:41.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-pred 01/17/23 07:29:41.423
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:41.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:41.463
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 07:29:41.470: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 07:29:41.481: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 07:29:41.487: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
Jan 17 07:29:41.500: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:29:41.500: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:29:41.500: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:29:41.500: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 07:29:41.500: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 07:29:41.500: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 07:29:41.500: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:29:41.500: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:29:41.500: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 07:29:41.500: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 07:29:41.500: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:29:41.500: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:29:41.500: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
Jan 17 07:29:41.512: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:29:41.512: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:29:41.512: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:29:41.512: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:29:41.512: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:29:41.512: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 07:29:41.512: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container e2e ready: true, restart count 0
Jan 17 07:29:41.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:29:41.512: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:29:41.512: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 07:29:41.512: INFO: 
Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
Jan 17 07:29:41.541: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 07:29:41.541: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 07:29:41.541: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 07:29:41.541: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container grafana ready: true, restart count 0
Jan 17 07:29:41.541: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 07:29:41.541: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 07:29:41.541: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 07:29:41.541: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 07:29:41.541: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 07:29:41.541: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
Jan 17 07:29:41.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 07:29:41.541: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:29:41.542
Jan 17 07:29:41.594: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5870" to be "running"
Jan 17 07:29:41.619: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.879167ms
Jan 17 07:29:43.628: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.034238336s
Jan 17 07:29:43.635: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:29:43.641
STEP: Trying to apply a random label on the found node. 01/17/23 07:29:43.669
STEP: verifying the node has the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b 42 01/17/23 07:29:43.696
STEP: Trying to relaunch the pod, now with labels. 01/17/23 07:29:43.708
Jan 17 07:29:43.718: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5870" to be "not pending"
Jan 17 07:29:43.725: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059353ms
Jan 17 07:29:45.733: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015496241s
Jan 17 07:29:45.734: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:29:45.739
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b 01/17/23 07:29:45.762
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:29:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5870" for this suite. 01/17/23 07:29:45.783
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":214,"skipped":3725,"failed":0}
------------------------------
• [4.379 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:41.42
    Jan 17 07:29:41.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-pred 01/17/23 07:29:41.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:41.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:41.463
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 07:29:41.470: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 07:29:41.481: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 07:29:41.487: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-0 before test
    Jan 17 07:29:41.500: INFO: calico-node-vhk97 from kube-system started at 2023-01-17 06:17:48 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: csi-cinder-nodeplugin-2k9x8 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: kube-dns-autoscaler-6587b74c7-4phrz from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: magnum-kube-prometheus-sta-operator-69d8bc7684-7mqhc from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: magnum-metrics-server-6f78bdfdcc-4lxk5 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: magnum-prometheus-node-exporter-chbc7 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: npd-b2q92 from kube-system started at 2023-01-17 06:18:08 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 06:18:31 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-rg7tr from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:29:41.500: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-1 before test
    Jan 17 07:29:41.512: INFO: calico-node-hzzhh from kube-system started at 2023-01-17 06:18:20 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: csi-cinder-nodeplugin-qwhhm from kube-system started at 2023-01-17 07:07:17 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: magnum-prometheus-node-exporter-mzh62 from kube-system started at 2023-01-17 07:07:17 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: npd-rh6l2 from kube-system started at 2023-01-17 06:18:40 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: sonobuoy from sonobuoy started at 2023-01-17 06:25:53 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: sonobuoy-e2e-job-edbb0f4e85d944f9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-2c7vc from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 07:29:41.512: INFO: 
    Logging pods the apiserver thinks is on node cluster125-w73dz53kvqes-node-2 before test
    Jan 17 07:29:41.541: INFO: calico-node-hkr25 from kube-system started at 2023-01-17 06:17:56 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container calico-node ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: csi-cinder-nodeplugin-z4flm from kube-system started at 2023-01-17 06:18:16 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: magnum-grafana-78479bf475-9z4zt from kube-system started at 2023-01-17 06:18:16 +0000 UTC (3 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container grafana ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: magnum-kube-state-metrics-56f56475f7-rtd9b from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: magnum-prometheus-node-exporter-cbq9j from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: npd-ktdk4 from kube-system started at 2023-01-17 06:18:16 +0000 UTC (1 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: sonobuoy-systemd-logs-daemon-set-24a62d259e4c4076-7pdf9 from sonobuoy started at 2023-01-17 06:26:01 +0000 UTC (2 container statuses recorded)
    Jan 17 07:29:41.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 07:29:41.541: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 07:29:41.542
    Jan 17 07:29:41.594: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5870" to be "running"
    Jan 17 07:29:41.619: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.879167ms
    Jan 17 07:29:43.628: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.034238336s
    Jan 17 07:29:43.635: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 07:29:43.641
    STEP: Trying to apply a random label on the found node. 01/17/23 07:29:43.669
    STEP: verifying the node has the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b 42 01/17/23 07:29:43.696
    STEP: Trying to relaunch the pod, now with labels. 01/17/23 07:29:43.708
    Jan 17 07:29:43.718: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5870" to be "not pending"
    Jan 17 07:29:43.725: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059353ms
    Jan 17 07:29:45.733: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015496241s
    Jan 17 07:29:45.734: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b off the node cluster125-w73dz53kvqes-node-1 01/17/23 07:29:45.739
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1df60e6a-8c24-4ba3-80a8-e7b567cae08b 01/17/23 07:29:45.762
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:29:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5870" for this suite. 01/17/23 07:29:45.783
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:45.801
Jan 17 07:29:45.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename podtemplate 01/17/23 07:29:45.803
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:45.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:45.857
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/17/23 07:29:45.865
STEP: Replace a pod template 01/17/23 07:29:45.878
Jan 17 07:29:45.903: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 07:29:45.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6395" for this suite. 01/17/23 07:29:45.915
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":215,"skipped":3736,"failed":0}
------------------------------
• [0.145 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:45.801
    Jan 17 07:29:45.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename podtemplate 01/17/23 07:29:45.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:45.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:45.857
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/17/23 07:29:45.865
    STEP: Replace a pod template 01/17/23 07:29:45.878
    Jan 17 07:29:45.903: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 07:29:45.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6395" for this suite. 01/17/23 07:29:45.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:45.947
Jan 17 07:29:45.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename init-container 01/17/23 07:29:45.949
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:45.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:46.001
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/17/23 07:29:46.014
Jan 17 07:29:46.014: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 07:29:50.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6435" for this suite. 01/17/23 07:29:50.609
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":216,"skipped":3749,"failed":0}
------------------------------
• [4.677 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:45.947
    Jan 17 07:29:45.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename init-container 01/17/23 07:29:45.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:45.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:46.001
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/17/23 07:29:46.014
    Jan 17 07:29:46.014: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 07:29:50.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6435" for this suite. 01/17/23 07:29:50.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:29:50.626
Jan 17 07:29:50.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:29:50.628
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:50.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:50.682
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-014a61eb-3b3f-4d7e-8b79-3e21106b3a01 01/17/23 07:29:50.71
STEP: Creating configMap with name cm-test-opt-upd-96c60c3c-9345-4bae-9d2a-120e5e5773d4 01/17/23 07:29:50.721
STEP: Creating the pod 01/17/23 07:29:50.733
Jan 17 07:29:50.774: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745" in namespace "projected-6008" to be "running and ready"
Jan 17 07:29:50.790: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Pending", Reason="", readiness=false. Elapsed: 15.489082ms
Jan 17 07:29:50.790: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:29:52.807: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0328017s
Jan 17 07:29:52.807: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:29:54.810: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Running", Reason="", readiness=true. Elapsed: 4.035821822s
Jan 17 07:29:54.810: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Running (Ready = true)
Jan 17 07:29:54.810: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-014a61eb-3b3f-4d7e-8b79-3e21106b3a01 01/17/23 07:29:54.856
STEP: Updating configmap cm-test-opt-upd-96c60c3c-9345-4bae-9d2a-120e5e5773d4 01/17/23 07:29:54.884
STEP: Creating configMap with name cm-test-opt-create-3091529b-495c-4302-add2-de741b8df27e 01/17/23 07:29:54.901
STEP: waiting to observe update in volume 01/17/23 07:29:54.911
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 07:31:07.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6008" for this suite. 01/17/23 07:31:07.513
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":217,"skipped":3760,"failed":0}
------------------------------
• [SLOW TEST] [76.903 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:29:50.626
    Jan 17 07:29:50.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:29:50.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:29:50.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:29:50.682
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-014a61eb-3b3f-4d7e-8b79-3e21106b3a01 01/17/23 07:29:50.71
    STEP: Creating configMap with name cm-test-opt-upd-96c60c3c-9345-4bae-9d2a-120e5e5773d4 01/17/23 07:29:50.721
    STEP: Creating the pod 01/17/23 07:29:50.733
    Jan 17 07:29:50.774: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745" in namespace "projected-6008" to be "running and ready"
    Jan 17 07:29:50.790: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Pending", Reason="", readiness=false. Elapsed: 15.489082ms
    Jan 17 07:29:50.790: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:29:52.807: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0328017s
    Jan 17 07:29:52.807: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:29:54.810: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745": Phase="Running", Reason="", readiness=true. Elapsed: 4.035821822s
    Jan 17 07:29:54.810: INFO: The phase of Pod pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745 is Running (Ready = true)
    Jan 17 07:29:54.810: INFO: Pod "pod-projected-configmaps-c8f76df7-e22f-457e-99bb-94cf539f5745" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-014a61eb-3b3f-4d7e-8b79-3e21106b3a01 01/17/23 07:29:54.856
    STEP: Updating configmap cm-test-opt-upd-96c60c3c-9345-4bae-9d2a-120e5e5773d4 01/17/23 07:29:54.884
    STEP: Creating configMap with name cm-test-opt-create-3091529b-495c-4302-add2-de741b8df27e 01/17/23 07:29:54.901
    STEP: waiting to observe update in volume 01/17/23 07:29:54.911
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 07:31:07.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6008" for this suite. 01/17/23 07:31:07.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:31:07.532
Jan 17 07:31:07.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:31:07.533
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.571
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/17/23 07:31:07.578
STEP: watching for the ServiceAccount to be added 01/17/23 07:31:07.595
STEP: patching the ServiceAccount 01/17/23 07:31:07.598
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/17/23 07:31:07.609
STEP: deleting the ServiceAccount 01/17/23 07:31:07.619
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 07:31:07.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8354" for this suite. 01/17/23 07:31:07.662
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":218,"skipped":3796,"failed":0}
------------------------------
• [0.146 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:31:07.532
    Jan 17 07:31:07.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 07:31:07.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.571
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/17/23 07:31:07.578
    STEP: watching for the ServiceAccount to be added 01/17/23 07:31:07.595
    STEP: patching the ServiceAccount 01/17/23 07:31:07.598
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/17/23 07:31:07.609
    STEP: deleting the ServiceAccount 01/17/23 07:31:07.619
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 07:31:07.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8354" for this suite. 01/17/23 07:31:07.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:31:07.679
Jan 17 07:31:07.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:31:07.682
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.734
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/17/23 07:31:07.743
Jan 17 07:31:07.743: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5588 proxy --unix-socket=/tmp/kubectl-proxy-unix1492713434/test'
STEP: retrieving proxy /api/ output 01/17/23 07:31:07.816
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:31:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5588" for this suite. 01/17/23 07:31:07.824
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":219,"skipped":3818,"failed":0}
------------------------------
• [0.160 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:31:07.679
    Jan 17 07:31:07.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:31:07.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.734
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/17/23 07:31:07.743
    Jan 17 07:31:07.743: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-5588 proxy --unix-socket=/tmp/kubectl-proxy-unix1492713434/test'
    STEP: retrieving proxy /api/ output 01/17/23 07:31:07.816
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:31:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5588" for this suite. 01/17/23 07:31:07.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:31:07.841
Jan 17 07:31:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 07:31:07.843
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.891
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 in namespace container-probe-8791 01/17/23 07:31:07.901
Jan 17 07:31:07.920: INFO: Waiting up to 5m0s for pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101" in namespace "container-probe-8791" to be "not pending"
Jan 17 07:31:07.928: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990327ms
Jan 17 07:31:09.945: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101": Phase="Running", Reason="", readiness=true. Elapsed: 2.024410569s
Jan 17 07:31:09.945: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101" satisfied condition "not pending"
Jan 17 07:31:09.945: INFO: Started pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 in namespace container-probe-8791
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:31:09.945
Jan 17 07:31:09.952: INFO: Initial restart count of pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 is 0
STEP: deleting the pod 01/17/23 07:35:10.92
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 07:35:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8791" for this suite. 01/17/23 07:35:10.969
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":220,"skipped":3835,"failed":0}
------------------------------
• [SLOW TEST] [243.147 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:31:07.841
    Jan 17 07:31:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 07:31:07.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:31:07.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:31:07.891
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 in namespace container-probe-8791 01/17/23 07:31:07.901
    Jan 17 07:31:07.920: INFO: Waiting up to 5m0s for pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101" in namespace "container-probe-8791" to be "not pending"
    Jan 17 07:31:07.928: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990327ms
    Jan 17 07:31:09.945: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101": Phase="Running", Reason="", readiness=true. Elapsed: 2.024410569s
    Jan 17 07:31:09.945: INFO: Pod "liveness-0ca91279-df99-468c-baf9-c4a0c7934101" satisfied condition "not pending"
    Jan 17 07:31:09.945: INFO: Started pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 in namespace container-probe-8791
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:31:09.945
    Jan 17 07:31:09.952: INFO: Initial restart count of pod liveness-0ca91279-df99-468c-baf9-c4a0c7934101 is 0
    STEP: deleting the pod 01/17/23 07:35:10.92
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 07:35:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8791" for this suite. 01/17/23 07:35:10.969
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:35:10.989
Jan 17 07:35:10.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:35:10.991
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:35:11.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:35:11.036
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 07:35:11.075: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 07:36:11.162: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:36:11.169
Jan 17 07:36:11.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 07:36:11.17
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:11.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:11.206
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 17 07:36:11.280: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 17 07:36:11.288: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 17 07:36:11.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8155" for this suite. 01/17/23 07:36:11.352
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:36:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6082" for this suite. 01/17/23 07:36:11.4
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":221,"skipped":3836,"failed":0}
------------------------------
• [SLOW TEST] [60.507 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:35:10.989
    Jan 17 07:35:10.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:35:10.991
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:35:11.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:35:11.036
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 07:35:11.075: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 07:36:11.162: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:36:11.169
    Jan 17 07:36:11.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 07:36:11.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:11.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:11.206
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 17 07:36:11.280: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 17 07:36:11.288: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 17 07:36:11.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8155" for this suite. 01/17/23 07:36:11.352
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:36:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6082" for this suite. 01/17/23 07:36:11.4
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:36:11.496
Jan 17 07:36:11.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:36:11.498
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:11.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:11.536
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:36:11.544
Jan 17 07:36:11.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be" in namespace "projected-7838" to be "Succeeded or Failed"
Jan 17 07:36:11.583: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Pending", Reason="", readiness=false. Elapsed: 11.352735ms
Jan 17 07:36:13.589: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Running", Reason="", readiness=true. Elapsed: 2.017374142s
Jan 17 07:36:15.589: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Running", Reason="", readiness=false. Elapsed: 4.01743392s
Jan 17 07:36:17.590: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017903986s
STEP: Saw pod success 01/17/23 07:36:17.59
Jan 17 07:36:17.590: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be" satisfied condition "Succeeded or Failed"
Jan 17 07:36:17.595: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be container client-container: <nil>
STEP: delete the pod 01/17/23 07:36:17.68
Jan 17 07:36:17.720: INFO: Waiting for pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be to disappear
Jan 17 07:36:17.726: INFO: Pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:36:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7838" for this suite. 01/17/23 07:36:17.736
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":222,"skipped":3841,"failed":0}
------------------------------
• [SLOW TEST] [6.261 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:36:11.496
    Jan 17 07:36:11.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:36:11.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:11.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:11.536
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:36:11.544
    Jan 17 07:36:11.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be" in namespace "projected-7838" to be "Succeeded or Failed"
    Jan 17 07:36:11.583: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Pending", Reason="", readiness=false. Elapsed: 11.352735ms
    Jan 17 07:36:13.589: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Running", Reason="", readiness=true. Elapsed: 2.017374142s
    Jan 17 07:36:15.589: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Running", Reason="", readiness=false. Elapsed: 4.01743392s
    Jan 17 07:36:17.590: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017903986s
    STEP: Saw pod success 01/17/23 07:36:17.59
    Jan 17 07:36:17.590: INFO: Pod "downwardapi-volume-74840683-d838-4927-9920-23809e25a2be" satisfied condition "Succeeded or Failed"
    Jan 17 07:36:17.595: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be container client-container: <nil>
    STEP: delete the pod 01/17/23 07:36:17.68
    Jan 17 07:36:17.720: INFO: Waiting for pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be to disappear
    Jan 17 07:36:17.726: INFO: Pod downwardapi-volume-74840683-d838-4927-9920-23809e25a2be no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:36:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7838" for this suite. 01/17/23 07:36:17.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:36:17.758
Jan 17 07:36:17.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename init-container 01/17/23 07:36:17.76
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:17.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:17.795
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/17/23 07:36:17.804
Jan 17 07:36:17.804: INFO: PodSpec: initContainers in spec.initContainers
Jan 17 07:36:56.843: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-183be273-d8ca-42c4-873b-c07594a27035", GenerateName:"", Namespace:"init-container-1066", SelfLink:"", UID:"0a2feca6-9b25-44bf-9d8c-a0ee15f989f3", ResourceVersion:"28583", Generation:0, CreationTimestamp:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"804611199"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"29631a4e320c64125a5501d8afcb28ae3687bd84403dc57eb5807822d9393827", "cni.projectcalico.org/podIP":"10.100.135.29/32", "cni.projectcalico.org/podIPs":"10.100.135.29/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcab0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcae0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcb28), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-vb5sn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003e13d00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000cd5220), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cluster125-w73dz53kvqes-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004413ab0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000cd52b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000cd52d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000cd52d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000cd52dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000510440), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.22", PodIP:"10.100.135.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.135.29"}}, StartTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000afcbb8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004413b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://25142cf5fa1e99f6b885c589307985f3b8404c392fe7fe7d96622c57cf2961d3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e13d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e13d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000cd5b2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 07:36:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1066" for this suite. 01/17/23 07:36:56.878
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":223,"skipped":3851,"failed":0}
------------------------------
• [SLOW TEST] [39.136 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:36:17.758
    Jan 17 07:36:17.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename init-container 01/17/23 07:36:17.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:17.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:17.795
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/17/23 07:36:17.804
    Jan 17 07:36:17.804: INFO: PodSpec: initContainers in spec.initContainers
    Jan 17 07:36:56.843: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-183be273-d8ca-42c4-873b-c07594a27035", GenerateName:"", Namespace:"init-container-1066", SelfLink:"", UID:"0a2feca6-9b25-44bf-9d8c-a0ee15f989f3", ResourceVersion:"28583", Generation:0, CreationTimestamp:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"804611199"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"29631a4e320c64125a5501d8afcb28ae3687bd84403dc57eb5807822d9393827", "cni.projectcalico.org/podIP":"10.100.135.29/32", "cni.projectcalico.org/podIPs":"10.100.135.29/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcab0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcae0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 7, 36, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000afcb28), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-vb5sn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003e13d00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vb5sn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000cd5220), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cluster125-w73dz53kvqes-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004413ab0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000cd52b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000cd52d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000cd52d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000cd52dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000510440), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.22", PodIP:"10.100.135.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.135.29"}}, StartTime:time.Date(2023, time.January, 17, 7, 36, 17, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000afcbb8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004413b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://25142cf5fa1e99f6b885c589307985f3b8404c392fe7fe7d96622c57cf2961d3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e13d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e13d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000cd5b2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 07:36:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1066" for this suite. 01/17/23 07:36:56.878
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:36:56.895
Jan 17 07:36:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:36:56.896
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:56.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:56.954
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/17/23 07:36:56.965
Jan 17 07:36:56.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 create -f -'
Jan 17 07:36:57.448: INFO: stderr: ""
Jan 17 07:36:57.448: INFO: stdout: "pod/pause created\n"
Jan 17 07:36:57.448: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 17 07:36:57.448: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8612" to be "running and ready"
Jan 17 07:36:57.468: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.160503ms
Jan 17 07:36:57.471: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cluster125-w73dz53kvqes-node-2' to be 'Running' but was 'Pending'
Jan 17 07:36:59.478: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.030464548s
Jan 17 07:36:59.478: INFO: Pod "pause" satisfied condition "running and ready"
Jan 17 07:36:59.478: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/17/23 07:36:59.478
Jan 17 07:36:59.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 label pods pause testing-label=testing-label-value'
Jan 17 07:36:59.649: INFO: stderr: ""
Jan 17 07:36:59.649: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/17/23 07:36:59.65
Jan 17 07:36:59.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pod pause -L testing-label'
Jan 17 07:36:59.846: INFO: stderr: ""
Jan 17 07:36:59.846: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/17/23 07:36:59.846
Jan 17 07:36:59.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 label pods pause testing-label-'
Jan 17 07:37:00.000: INFO: stderr: ""
Jan 17 07:37:00.000: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/17/23 07:37:00
Jan 17 07:37:00.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pod pause -L testing-label'
Jan 17 07:37:00.117: INFO: stderr: ""
Jan 17 07:37:00.117: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/17/23 07:37:00.118
Jan 17 07:37:00.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 delete --grace-period=0 --force -f -'
Jan 17 07:37:00.292: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:37:00.292: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 17 07:37:00.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get rc,svc -l name=pause --no-headers'
Jan 17 07:37:00.413: INFO: stderr: "No resources found in kubectl-8612 namespace.\n"
Jan 17 07:37:00.413: INFO: stdout: ""
Jan 17 07:37:00.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 07:37:00.563: INFO: stderr: ""
Jan 17 07:37:00.563: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:37:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8612" for this suite. 01/17/23 07:37:00.575
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":224,"skipped":3855,"failed":0}
------------------------------
• [3.701 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:36:56.895
    Jan 17 07:36:56.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:36:56.896
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:36:56.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:36:56.954
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/17/23 07:36:56.965
    Jan 17 07:36:56.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 create -f -'
    Jan 17 07:36:57.448: INFO: stderr: ""
    Jan 17 07:36:57.448: INFO: stdout: "pod/pause created\n"
    Jan 17 07:36:57.448: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 17 07:36:57.448: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8612" to be "running and ready"
    Jan 17 07:36:57.468: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.160503ms
    Jan 17 07:36:57.471: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cluster125-w73dz53kvqes-node-2' to be 'Running' but was 'Pending'
    Jan 17 07:36:59.478: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.030464548s
    Jan 17 07:36:59.478: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 17 07:36:59.478: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/17/23 07:36:59.478
    Jan 17 07:36:59.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 label pods pause testing-label=testing-label-value'
    Jan 17 07:36:59.649: INFO: stderr: ""
    Jan 17 07:36:59.649: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/17/23 07:36:59.65
    Jan 17 07:36:59.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pod pause -L testing-label'
    Jan 17 07:36:59.846: INFO: stderr: ""
    Jan 17 07:36:59.846: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/17/23 07:36:59.846
    Jan 17 07:36:59.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 label pods pause testing-label-'
    Jan 17 07:37:00.000: INFO: stderr: ""
    Jan 17 07:37:00.000: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/17/23 07:37:00
    Jan 17 07:37:00.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pod pause -L testing-label'
    Jan 17 07:37:00.117: INFO: stderr: ""
    Jan 17 07:37:00.117: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/17/23 07:37:00.118
    Jan 17 07:37:00.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 delete --grace-period=0 --force -f -'
    Jan 17 07:37:00.292: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:37:00.292: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 17 07:37:00.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get rc,svc -l name=pause --no-headers'
    Jan 17 07:37:00.413: INFO: stderr: "No resources found in kubectl-8612 namespace.\n"
    Jan 17 07:37:00.413: INFO: stdout: ""
    Jan 17 07:37:00.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8612 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 07:37:00.563: INFO: stderr: ""
    Jan 17 07:37:00.563: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:37:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8612" for this suite. 01/17/23 07:37:00.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:00.6
Jan 17 07:37:00.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename lease-test 01/17/23 07:37:00.601
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:00.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:00.635
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 17 07:37:00.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1284" for this suite. 01/17/23 07:37:00.834
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":225,"skipped":3864,"failed":0}
------------------------------
• [0.253 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:00.6
    Jan 17 07:37:00.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename lease-test 01/17/23 07:37:00.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:00.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:00.635
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 17 07:37:00.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-1284" for this suite. 01/17/23 07:37:00.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:00.854
Jan 17 07:37:00.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption 01/17/23 07:37:00.856
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:00.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:00.899
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/17/23 07:37:00.913
STEP: Waiting for the pdb to be processed 01/17/23 07:37:00.928
STEP: First trying to evict a pod which shouldn't be evictable 01/17/23 07:37:00.97
STEP: Waiting for all pods to be running 01/17/23 07:37:00.97
Jan 17 07:37:00.977: INFO: pods: 0 < 3
STEP: locating a running pod 01/17/23 07:37:03.001
STEP: Updating the pdb to allow a pod to be evicted 01/17/23 07:37:03.029
STEP: Waiting for the pdb to be processed 01/17/23 07:37:03.051
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 07:37:03.069
STEP: Waiting for all pods to be running 01/17/23 07:37:03.07
STEP: Waiting for the pdb to observed all healthy pods 01/17/23 07:37:03.078
STEP: Patching the pdb to disallow a pod to be evicted 01/17/23 07:37:03.135
STEP: Waiting for the pdb to be processed 01/17/23 07:37:03.219
STEP: Waiting for all pods to be running 01/17/23 07:37:03.25
Jan 17 07:37:03.292: INFO: running pods: 2 < 3
STEP: locating a running pod 01/17/23 07:37:05.306
STEP: Deleting the pdb to allow a pod to be evicted 01/17/23 07:37:05.324
STEP: Waiting for the pdb to be deleted 01/17/23 07:37:05.341
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 07:37:05.348
STEP: Waiting for all pods to be running 01/17/23 07:37:05.348
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 07:37:05.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-123" for this suite. 01/17/23 07:37:05.403
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":226,"skipped":3873,"failed":0}
------------------------------
• [4.578 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:00.854
    Jan 17 07:37:00.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption 01/17/23 07:37:00.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:00.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:00.899
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/17/23 07:37:00.913
    STEP: Waiting for the pdb to be processed 01/17/23 07:37:00.928
    STEP: First trying to evict a pod which shouldn't be evictable 01/17/23 07:37:00.97
    STEP: Waiting for all pods to be running 01/17/23 07:37:00.97
    Jan 17 07:37:00.977: INFO: pods: 0 < 3
    STEP: locating a running pod 01/17/23 07:37:03.001
    STEP: Updating the pdb to allow a pod to be evicted 01/17/23 07:37:03.029
    STEP: Waiting for the pdb to be processed 01/17/23 07:37:03.051
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 07:37:03.069
    STEP: Waiting for all pods to be running 01/17/23 07:37:03.07
    STEP: Waiting for the pdb to observed all healthy pods 01/17/23 07:37:03.078
    STEP: Patching the pdb to disallow a pod to be evicted 01/17/23 07:37:03.135
    STEP: Waiting for the pdb to be processed 01/17/23 07:37:03.219
    STEP: Waiting for all pods to be running 01/17/23 07:37:03.25
    Jan 17 07:37:03.292: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/17/23 07:37:05.306
    STEP: Deleting the pdb to allow a pod to be evicted 01/17/23 07:37:05.324
    STEP: Waiting for the pdb to be deleted 01/17/23 07:37:05.341
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 07:37:05.348
    STEP: Waiting for all pods to be running 01/17/23 07:37:05.348
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 07:37:05.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-123" for this suite. 01/17/23 07:37:05.403
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:05.433
Jan 17 07:37:05.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-runtime 01/17/23 07:37:05.435
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:05.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:05.496
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/17/23 07:37:05.536
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/17/23 07:37:23.708
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/17/23 07:37:23.714
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/17/23 07:37:23.761
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/17/23 07:37:23.762
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/17/23 07:37:23.851
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/17/23 07:37:27.9
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/17/23 07:37:29.922
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/17/23 07:37:29.936
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/17/23 07:37:29.936
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/17/23 07:37:29.982
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/17/23 07:37:31.016
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/17/23 07:37:34.051
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/17/23 07:37:34.061
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/17/23 07:37:34.061
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 07:37:34.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6293" for this suite. 01/17/23 07:37:34.118
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":227,"skipped":3873,"failed":0}
------------------------------
• [SLOW TEST] [28.697 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:05.433
    Jan 17 07:37:05.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-runtime 01/17/23 07:37:05.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:05.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:05.496
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/17/23 07:37:05.536
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/17/23 07:37:23.708
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/17/23 07:37:23.714
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/17/23 07:37:23.761
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/17/23 07:37:23.762
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/17/23 07:37:23.851
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/17/23 07:37:27.9
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/17/23 07:37:29.922
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/17/23 07:37:29.936
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/17/23 07:37:29.936
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/17/23 07:37:29.982
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/17/23 07:37:31.016
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/17/23 07:37:34.051
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/17/23 07:37:34.061
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/17/23 07:37:34.061
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 07:37:34.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6293" for this suite. 01/17/23 07:37:34.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:34.136
Jan 17 07:37:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:37:34.138
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:34.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:34.181
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-b3603cc5-dec9-4a9d-8f25-76468c244d56 01/17/23 07:37:34.198
STEP: Creating the pod 01/17/23 07:37:34.21
Jan 17 07:37:34.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93" in namespace "configmap-7265" to be "running"
Jan 17 07:37:34.267: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93": Phase="Pending", Reason="", readiness=false. Elapsed: 28.923438ms
Jan 17 07:37:36.274: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93": Phase="Running", Reason="", readiness=false. Elapsed: 2.036125428s
Jan 17 07:37:36.274: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93" satisfied condition "running"
STEP: Waiting for pod with text data 01/17/23 07:37:36.274
STEP: Waiting for pod with binary data 01/17/23 07:37:36.285
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:37:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7265" for this suite. 01/17/23 07:37:36.301
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":228,"skipped":3913,"failed":0}
------------------------------
• [2.177 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:34.136
    Jan 17 07:37:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:37:34.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:34.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:34.181
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-b3603cc5-dec9-4a9d-8f25-76468c244d56 01/17/23 07:37:34.198
    STEP: Creating the pod 01/17/23 07:37:34.21
    Jan 17 07:37:34.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93" in namespace "configmap-7265" to be "running"
    Jan 17 07:37:34.267: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93": Phase="Pending", Reason="", readiness=false. Elapsed: 28.923438ms
    Jan 17 07:37:36.274: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93": Phase="Running", Reason="", readiness=false. Elapsed: 2.036125428s
    Jan 17 07:37:36.274: INFO: Pod "pod-configmaps-088850d0-aa5a-4b2e-a01f-e5c6f31eab93" satisfied condition "running"
    STEP: Waiting for pod with text data 01/17/23 07:37:36.274
    STEP: Waiting for pod with binary data 01/17/23 07:37:36.285
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:37:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7265" for this suite. 01/17/23 07:37:36.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:36.314
Jan 17 07:37:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename discovery 01/17/23 07:37:36.315
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:36.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:36.353
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/17/23 07:37:36.366
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 17 07:37:36.940: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 17 07:37:36.945: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 17 07:37:36.945: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 17 07:37:36.945: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 17 07:37:36.945: INFO: Checking APIGroup: apps
Jan 17 07:37:36.949: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 17 07:37:36.957: INFO: Versions found [{apps/v1 v1}]
Jan 17 07:37:36.957: INFO: apps/v1 matches apps/v1
Jan 17 07:37:36.958: INFO: Checking APIGroup: events.k8s.io
Jan 17 07:37:36.966: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 17 07:37:36.966: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 17 07:37:36.966: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 17 07:37:36.966: INFO: Checking APIGroup: authentication.k8s.io
Jan 17 07:37:36.979: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 17 07:37:36.979: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 17 07:37:36.979: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 17 07:37:36.979: INFO: Checking APIGroup: authorization.k8s.io
Jan 17 07:37:36.984: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 17 07:37:36.984: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 17 07:37:36.984: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 17 07:37:36.984: INFO: Checking APIGroup: autoscaling
Jan 17 07:37:36.987: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 17 07:37:36.987: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 17 07:37:36.987: INFO: autoscaling/v2 matches autoscaling/v2
Jan 17 07:37:36.987: INFO: Checking APIGroup: batch
Jan 17 07:37:36.990: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 17 07:37:36.990: INFO: Versions found [{batch/v1 v1}]
Jan 17 07:37:36.990: INFO: batch/v1 matches batch/v1
Jan 17 07:37:36.990: INFO: Checking APIGroup: certificates.k8s.io
Jan 17 07:37:36.995: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 17 07:37:36.995: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 17 07:37:36.995: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 17 07:37:36.995: INFO: Checking APIGroup: networking.k8s.io
Jan 17 07:37:36.998: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 17 07:37:36.998: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Jan 17 07:37:36.998: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 17 07:37:36.998: INFO: Checking APIGroup: policy
Jan 17 07:37:37.001: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 17 07:37:37.001: INFO: Versions found [{policy/v1 v1}]
Jan 17 07:37:37.001: INFO: policy/v1 matches policy/v1
Jan 17 07:37:37.001: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 17 07:37:37.005: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 17 07:37:37.005: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 17 07:37:37.005: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 17 07:37:37.005: INFO: Checking APIGroup: storage.k8s.io
Jan 17 07:37:37.010: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 17 07:37:37.010: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 17 07:37:37.010: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 17 07:37:37.010: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 17 07:37:37.015: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 17 07:37:37.015: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 17 07:37:37.015: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 17 07:37:37.015: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 17 07:37:37.021: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 17 07:37:37.021: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 17 07:37:37.021: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 17 07:37:37.021: INFO: Checking APIGroup: scheduling.k8s.io
Jan 17 07:37:37.025: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 17 07:37:37.025: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 17 07:37:37.025: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 17 07:37:37.025: INFO: Checking APIGroup: coordination.k8s.io
Jan 17 07:37:37.030: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 17 07:37:37.030: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 17 07:37:37.030: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 17 07:37:37.030: INFO: Checking APIGroup: node.k8s.io
Jan 17 07:37:37.033: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 17 07:37:37.033: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 17 07:37:37.033: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 17 07:37:37.033: INFO: Checking APIGroup: discovery.k8s.io
Jan 17 07:37:37.037: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 17 07:37:37.037: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 17 07:37:37.037: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 17 07:37:37.037: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 17 07:37:37.040: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 07:37:37.040: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 17 07:37:37.040: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 07:37:37.040: INFO: Checking APIGroup: internal.apiserver.k8s.io
Jan 17 07:37:37.043: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Jan 17 07:37:37.043: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jan 17 07:37:37.043: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Jan 17 07:37:37.043: INFO: Checking APIGroup: crd.projectcalico.org
Jan 17 07:37:37.046: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 17 07:37:37.046: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 17 07:37:37.046: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 17 07:37:37.046: INFO: Checking APIGroup: monitoring.coreos.com
Jan 17 07:37:37.049: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 17 07:37:37.049: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 17 07:37:37.049: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 17 07:37:37.049: INFO: Checking APIGroup: metrics.k8s.io
Jan 17 07:37:37.053: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 17 07:37:37.053: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 17 07:37:37.053: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 17 07:37:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-46" for this suite. 01/17/23 07:37:37.06
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":229,"skipped":3924,"failed":0}
------------------------------
• [0.771 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:36.314
    Jan 17 07:37:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename discovery 01/17/23 07:37:36.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:36.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:36.353
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/17/23 07:37:36.366
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 17 07:37:36.940: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 17 07:37:36.945: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 17 07:37:36.945: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 17 07:37:36.945: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 17 07:37:36.945: INFO: Checking APIGroup: apps
    Jan 17 07:37:36.949: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 17 07:37:36.957: INFO: Versions found [{apps/v1 v1}]
    Jan 17 07:37:36.957: INFO: apps/v1 matches apps/v1
    Jan 17 07:37:36.958: INFO: Checking APIGroup: events.k8s.io
    Jan 17 07:37:36.966: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 17 07:37:36.966: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 17 07:37:36.966: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 17 07:37:36.966: INFO: Checking APIGroup: authentication.k8s.io
    Jan 17 07:37:36.979: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 17 07:37:36.979: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 17 07:37:36.979: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 17 07:37:36.979: INFO: Checking APIGroup: authorization.k8s.io
    Jan 17 07:37:36.984: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 17 07:37:36.984: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 17 07:37:36.984: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 17 07:37:36.984: INFO: Checking APIGroup: autoscaling
    Jan 17 07:37:36.987: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 17 07:37:36.987: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 17 07:37:36.987: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 17 07:37:36.987: INFO: Checking APIGroup: batch
    Jan 17 07:37:36.990: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 17 07:37:36.990: INFO: Versions found [{batch/v1 v1}]
    Jan 17 07:37:36.990: INFO: batch/v1 matches batch/v1
    Jan 17 07:37:36.990: INFO: Checking APIGroup: certificates.k8s.io
    Jan 17 07:37:36.995: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 17 07:37:36.995: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 17 07:37:36.995: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 17 07:37:36.995: INFO: Checking APIGroup: networking.k8s.io
    Jan 17 07:37:36.998: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 17 07:37:36.998: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Jan 17 07:37:36.998: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 17 07:37:36.998: INFO: Checking APIGroup: policy
    Jan 17 07:37:37.001: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 17 07:37:37.001: INFO: Versions found [{policy/v1 v1}]
    Jan 17 07:37:37.001: INFO: policy/v1 matches policy/v1
    Jan 17 07:37:37.001: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 17 07:37:37.005: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 17 07:37:37.005: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 17 07:37:37.005: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 17 07:37:37.005: INFO: Checking APIGroup: storage.k8s.io
    Jan 17 07:37:37.010: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 17 07:37:37.010: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 17 07:37:37.010: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 17 07:37:37.010: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 17 07:37:37.015: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 17 07:37:37.015: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 17 07:37:37.015: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 17 07:37:37.015: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 17 07:37:37.021: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 17 07:37:37.021: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 17 07:37:37.021: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 17 07:37:37.021: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 17 07:37:37.025: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 17 07:37:37.025: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 17 07:37:37.025: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 17 07:37:37.025: INFO: Checking APIGroup: coordination.k8s.io
    Jan 17 07:37:37.030: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 17 07:37:37.030: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 17 07:37:37.030: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 17 07:37:37.030: INFO: Checking APIGroup: node.k8s.io
    Jan 17 07:37:37.033: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 17 07:37:37.033: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 17 07:37:37.033: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 17 07:37:37.033: INFO: Checking APIGroup: discovery.k8s.io
    Jan 17 07:37:37.037: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 17 07:37:37.037: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 17 07:37:37.037: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 17 07:37:37.037: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 17 07:37:37.040: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 17 07:37:37.040: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 17 07:37:37.040: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 17 07:37:37.040: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Jan 17 07:37:37.043: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Jan 17 07:37:37.043: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Jan 17 07:37:37.043: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Jan 17 07:37:37.043: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 17 07:37:37.046: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 17 07:37:37.046: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 17 07:37:37.046: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 17 07:37:37.046: INFO: Checking APIGroup: monitoring.coreos.com
    Jan 17 07:37:37.049: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Jan 17 07:37:37.049: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Jan 17 07:37:37.049: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Jan 17 07:37:37.049: INFO: Checking APIGroup: metrics.k8s.io
    Jan 17 07:37:37.053: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 17 07:37:37.053: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 17 07:37:37.053: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 17 07:37:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-46" for this suite. 01/17/23 07:37:37.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:37.086
Jan 17 07:37:37.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context-test 01/17/23 07:37:37.087
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:37.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:37.152
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 17 07:37:37.185: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5" in namespace "security-context-test-9416" to be "Succeeded or Failed"
Jan 17 07:37:37.194: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.345698ms
Jan 17 07:37:39.209: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024154035s
Jan 17 07:37:41.201: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015519952s
Jan 17 07:37:41.201: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5" satisfied condition "Succeeded or Failed"
Jan 17 07:37:41.278: INFO: Got logs for pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 07:37:41.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9416" for this suite. 01/17/23 07:37:41.286
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":230,"skipped":3938,"failed":0}
------------------------------
• [4.217 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:37.086
    Jan 17 07:37:37.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context-test 01/17/23 07:37:37.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:37.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:37.152
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 17 07:37:37.185: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5" in namespace "security-context-test-9416" to be "Succeeded or Failed"
    Jan 17 07:37:37.194: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.345698ms
    Jan 17 07:37:39.209: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024154035s
    Jan 17 07:37:41.201: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015519952s
    Jan 17 07:37:41.201: INFO: Pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5" satisfied condition "Succeeded or Failed"
    Jan 17 07:37:41.278: INFO: Got logs for pod "busybox-privileged-false-e6dceb8c-6f09-4d98-9c4e-bd21c93eecc5": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 07:37:41.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9416" for this suite. 01/17/23 07:37:41.286
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:41.303
Jan 17 07:37:41.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:37:41.306
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:41.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:41.362
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/17/23 07:37:41.371
STEP: getting /apis/node.k8s.io 01/17/23 07:37:41.377
STEP: getting /apis/node.k8s.io/v1 01/17/23 07:37:41.38
STEP: creating 01/17/23 07:37:41.382
STEP: watching 01/17/23 07:37:41.421
Jan 17 07:37:41.421: INFO: starting watch
STEP: getting 01/17/23 07:37:41.438
STEP: listing 01/17/23 07:37:41.447
STEP: patching 01/17/23 07:37:41.453
STEP: updating 01/17/23 07:37:41.465
Jan 17 07:37:41.479: INFO: waiting for watch events with expected annotations
STEP: deleting 01/17/23 07:37:41.479
STEP: deleting a collection 01/17/23 07:37:41.523
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 07:37:41.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8095" for this suite. 01/17/23 07:37:41.605
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":231,"skipped":3942,"failed":0}
------------------------------
• [0.335 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:41.303
    Jan 17 07:37:41.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 07:37:41.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:41.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:41.362
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/17/23 07:37:41.371
    STEP: getting /apis/node.k8s.io 01/17/23 07:37:41.377
    STEP: getting /apis/node.k8s.io/v1 01/17/23 07:37:41.38
    STEP: creating 01/17/23 07:37:41.382
    STEP: watching 01/17/23 07:37:41.421
    Jan 17 07:37:41.421: INFO: starting watch
    STEP: getting 01/17/23 07:37:41.438
    STEP: listing 01/17/23 07:37:41.447
    STEP: patching 01/17/23 07:37:41.453
    STEP: updating 01/17/23 07:37:41.465
    Jan 17 07:37:41.479: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/17/23 07:37:41.479
    STEP: deleting a collection 01/17/23 07:37:41.523
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 07:37:41.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8095" for this suite. 01/17/23 07:37:41.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:41.642
Jan 17 07:37:41.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename prestop 01/17/23 07:37:41.643
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:41.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:41.745
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-8369 01/17/23 07:37:41.764
STEP: Waiting for pods to come up. 01/17/23 07:37:41.788
Jan 17 07:37:41.788: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8369" to be "running"
Jan 17 07:37:41.820: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 32.479849ms
Jan 17 07:37:43.829: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.041112298s
Jan 17 07:37:43.829: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-8369 01/17/23 07:37:43.834
Jan 17 07:37:43.845: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8369" to be "running"
Jan 17 07:37:43.862: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 16.494609ms
Jan 17 07:37:45.869: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.024147882s
Jan 17 07:37:45.869: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/17/23 07:37:45.869
Jan 17 07:37:50.900: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/17/23 07:37:50.9
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 17 07:37:50.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8369" for this suite. 01/17/23 07:37:50.967
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":232,"skipped":3966,"failed":0}
------------------------------
• [SLOW TEST] [9.349 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:41.642
    Jan 17 07:37:41.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename prestop 01/17/23 07:37:41.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:41.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:41.745
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-8369 01/17/23 07:37:41.764
    STEP: Waiting for pods to come up. 01/17/23 07:37:41.788
    Jan 17 07:37:41.788: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8369" to be "running"
    Jan 17 07:37:41.820: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 32.479849ms
    Jan 17 07:37:43.829: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.041112298s
    Jan 17 07:37:43.829: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-8369 01/17/23 07:37:43.834
    Jan 17 07:37:43.845: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8369" to be "running"
    Jan 17 07:37:43.862: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 16.494609ms
    Jan 17 07:37:45.869: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.024147882s
    Jan 17 07:37:45.869: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/17/23 07:37:45.869
    Jan 17 07:37:50.900: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/17/23 07:37:50.9
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 17 07:37:50.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-8369" for this suite. 01/17/23 07:37:50.967
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:50.991
Jan 17 07:37:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:37:50.993
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:51.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:51.061
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:37:51.075
Jan 17 07:37:51.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee" in namespace "projected-8942" to be "Succeeded or Failed"
Jan 17 07:37:51.146: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.397336ms
Jan 17 07:37:53.153: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021433132s
Jan 17 07:37:55.169: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037928396s
STEP: Saw pod success 01/17/23 07:37:55.17
Jan 17 07:37:55.170: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee" satisfied condition "Succeeded or Failed"
Jan 17 07:37:55.176: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee container client-container: <nil>
STEP: delete the pod 01/17/23 07:37:55.195
Jan 17 07:37:55.231: INFO: Waiting for pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee to disappear
Jan 17 07:37:55.237: INFO: Pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:37:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8942" for this suite. 01/17/23 07:37:55.244
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":3967,"failed":0}
------------------------------
• [4.270 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:50.991
    Jan 17 07:37:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:37:50.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:51.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:51.061
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:37:51.075
    Jan 17 07:37:51.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee" in namespace "projected-8942" to be "Succeeded or Failed"
    Jan 17 07:37:51.146: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.397336ms
    Jan 17 07:37:53.153: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021433132s
    Jan 17 07:37:55.169: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037928396s
    STEP: Saw pod success 01/17/23 07:37:55.17
    Jan 17 07:37:55.170: INFO: Pod "downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee" satisfied condition "Succeeded or Failed"
    Jan 17 07:37:55.176: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee container client-container: <nil>
    STEP: delete the pod 01/17/23 07:37:55.195
    Jan 17 07:37:55.231: INFO: Waiting for pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee to disappear
    Jan 17 07:37:55.237: INFO: Pod downwardapi-volume-6116904f-21f7-4a93-bf46-ea08e6ac35ee no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:37:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8942" for this suite. 01/17/23 07:37:55.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:55.266
Jan 17 07:37:55.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 07:37:55.267
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:55.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:55.306
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/17/23 07:37:55.313
STEP: Ensure pods equal to paralellism count is attached to the job 01/17/23 07:37:55.328
STEP: patching /status 01/17/23 07:37:57.339
STEP: updating /status 01/17/23 07:37:57.36
STEP: get /status 01/17/23 07:37:57.382
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 07:37:57.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1917" for this suite. 01/17/23 07:37:57.395
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":234,"skipped":4011,"failed":0}
------------------------------
• [2.162 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:55.266
    Jan 17 07:37:55.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 07:37:55.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:55.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:55.306
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/17/23 07:37:55.313
    STEP: Ensure pods equal to paralellism count is attached to the job 01/17/23 07:37:55.328
    STEP: patching /status 01/17/23 07:37:57.339
    STEP: updating /status 01/17/23 07:37:57.36
    STEP: get /status 01/17/23 07:37:57.382
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 07:37:57.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1917" for this suite. 01/17/23 07:37:57.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:37:57.452
Jan 17 07:37:57.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:37:57.453
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:57.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:57.497
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-c8712ba4-b936-4d7d-9fe2-86eafd59d55f 01/17/23 07:37:57.508
STEP: Creating a pod to test consume configMaps 01/17/23 07:37:57.522
Jan 17 07:37:57.573: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6" in namespace "projected-5507" to be "Succeeded or Failed"
Jan 17 07:37:57.596: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.06133ms
Jan 17 07:37:59.604: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030215421s
Jan 17 07:38:01.605: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031653354s
STEP: Saw pod success 01/17/23 07:38:01.605
Jan 17 07:38:01.605: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6" satisfied condition "Succeeded or Failed"
Jan 17 07:38:01.612: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:38:01.643
Jan 17 07:38:01.672: INFO: Waiting for pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 to disappear
Jan 17 07:38:01.710: INFO: Pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 07:38:01.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5507" for this suite. 01/17/23 07:38:01.725
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":235,"skipped":4122,"failed":0}
------------------------------
• [4.301 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:37:57.452
    Jan 17 07:37:57.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:37:57.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:37:57.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:37:57.497
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-c8712ba4-b936-4d7d-9fe2-86eafd59d55f 01/17/23 07:37:57.508
    STEP: Creating a pod to test consume configMaps 01/17/23 07:37:57.522
    Jan 17 07:37:57.573: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6" in namespace "projected-5507" to be "Succeeded or Failed"
    Jan 17 07:37:57.596: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.06133ms
    Jan 17 07:37:59.604: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030215421s
    Jan 17 07:38:01.605: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031653354s
    STEP: Saw pod success 01/17/23 07:38:01.605
    Jan 17 07:38:01.605: INFO: Pod "pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6" satisfied condition "Succeeded or Failed"
    Jan 17 07:38:01.612: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:38:01.643
    Jan 17 07:38:01.672: INFO: Waiting for pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 to disappear
    Jan 17 07:38:01.710: INFO: Pod pod-projected-configmaps-026b521c-ba8b-4e68-bc66-fa1359f629a6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 07:38:01.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5507" for this suite. 01/17/23 07:38:01.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:01.754
Jan 17 07:38:01.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 07:38:01.755
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:01.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:01.836
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/17/23 07:38:01.847
STEP: Verify that the required pods have come up 01/17/23 07:38:01.861
Jan 17 07:38:01.873: INFO: Pod name sample-pod: Found 1 pods out of 3
Jan 17 07:38:06.879: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/17/23 07:38:06.879
Jan 17 07:38:06.886: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/17/23 07:38:06.886
STEP: DeleteCollection of the ReplicaSets 01/17/23 07:38:06.916
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/17/23 07:38:06.955
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 07:38:07.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9647" for this suite. 01/17/23 07:38:07.099
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":236,"skipped":4131,"failed":0}
------------------------------
• [SLOW TEST] [5.396 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:01.754
    Jan 17 07:38:01.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 07:38:01.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:01.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:01.836
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/17/23 07:38:01.847
    STEP: Verify that the required pods have come up 01/17/23 07:38:01.861
    Jan 17 07:38:01.873: INFO: Pod name sample-pod: Found 1 pods out of 3
    Jan 17 07:38:06.879: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/17/23 07:38:06.879
    Jan 17 07:38:06.886: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/17/23 07:38:06.886
    STEP: DeleteCollection of the ReplicaSets 01/17/23 07:38:06.916
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/17/23 07:38:06.955
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 07:38:07.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9647" for this suite. 01/17/23 07:38:07.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:07.151
Jan 17 07:38:07.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename podtemplate 01/17/23 07:38:07.152
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:07.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:07.268
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/17/23 07:38:07.283
Jan 17 07:38:07.319: INFO: created test-podtemplate-1
Jan 17 07:38:07.330: INFO: created test-podtemplate-2
Jan 17 07:38:07.343: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/17/23 07:38:07.343
STEP: delete collection of pod templates 01/17/23 07:38:07.35
Jan 17 07:38:07.351: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/17/23 07:38:07.415
Jan 17 07:38:07.416: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 07:38:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2537" for this suite. 01/17/23 07:38:07.427
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":237,"skipped":4160,"failed":0}
------------------------------
• [0.296 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:07.151
    Jan 17 07:38:07.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename podtemplate 01/17/23 07:38:07.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:07.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:07.268
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/17/23 07:38:07.283
    Jan 17 07:38:07.319: INFO: created test-podtemplate-1
    Jan 17 07:38:07.330: INFO: created test-podtemplate-2
    Jan 17 07:38:07.343: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/17/23 07:38:07.343
    STEP: delete collection of pod templates 01/17/23 07:38:07.35
    Jan 17 07:38:07.351: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/17/23 07:38:07.415
    Jan 17 07:38:07.416: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 07:38:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2537" for this suite. 01/17/23 07:38:07.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:07.452
Jan 17 07:38:07.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:38:07.453
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:07.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:07.5
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-c99e7b09-b489-4251-90f3-97e6f8881dd1 01/17/23 07:38:07.511
STEP: Creating a pod to test consume secrets 01/17/23 07:38:07.522
Jan 17 07:38:07.539: INFO: Waiting up to 5m0s for pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2" in namespace "secrets-2885" to be "Succeeded or Failed"
Jan 17 07:38:07.558: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.462065ms
Jan 17 07:38:09.582: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043679175s
Jan 17 07:38:11.581: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042152568s
STEP: Saw pod success 01/17/23 07:38:11.581
Jan 17 07:38:11.581: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2" satisfied condition "Succeeded or Failed"
Jan 17 07:38:11.588: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:38:11.614
Jan 17 07:38:11.648: INFO: Waiting for pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 to disappear
Jan 17 07:38:11.655: INFO: Pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:38:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2885" for this suite. 01/17/23 07:38:11.671
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":238,"skipped":4201,"failed":0}
------------------------------
• [4.239 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:07.452
    Jan 17 07:38:07.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:38:07.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:07.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:07.5
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-c99e7b09-b489-4251-90f3-97e6f8881dd1 01/17/23 07:38:07.511
    STEP: Creating a pod to test consume secrets 01/17/23 07:38:07.522
    Jan 17 07:38:07.539: INFO: Waiting up to 5m0s for pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2" in namespace "secrets-2885" to be "Succeeded or Failed"
    Jan 17 07:38:07.558: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.462065ms
    Jan 17 07:38:09.582: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043679175s
    Jan 17 07:38:11.581: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042152568s
    STEP: Saw pod success 01/17/23 07:38:11.581
    Jan 17 07:38:11.581: INFO: Pod "pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2" satisfied condition "Succeeded or Failed"
    Jan 17 07:38:11.588: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:38:11.614
    Jan 17 07:38:11.648: INFO: Waiting for pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 to disappear
    Jan 17 07:38:11.655: INFO: Pod pod-secrets-9cdae016-bced-4528-b70b-7ac865fd1cc2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:38:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2885" for this suite. 01/17/23 07:38:11.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:11.692
Jan 17 07:38:11.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:38:11.693
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:11.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:11.729
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/17/23 07:38:11.736
Jan 17 07:38:11.736: INFO: Creating e2e-svc-a-8lrxh
Jan 17 07:38:11.761: INFO: Creating e2e-svc-b-mlm2g
Jan 17 07:38:11.805: INFO: Creating e2e-svc-c-42mbp
STEP: deleting service collection 01/17/23 07:38:11.848
Jan 17 07:38:11.914: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:38:11.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1444" for this suite. 01/17/23 07:38:11.923
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":239,"skipped":4216,"failed":0}
------------------------------
• [0.249 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:11.692
    Jan 17 07:38:11.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:38:11.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:11.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:11.729
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/17/23 07:38:11.736
    Jan 17 07:38:11.736: INFO: Creating e2e-svc-a-8lrxh
    Jan 17 07:38:11.761: INFO: Creating e2e-svc-b-mlm2g
    Jan 17 07:38:11.805: INFO: Creating e2e-svc-c-42mbp
    STEP: deleting service collection 01/17/23 07:38:11.848
    Jan 17 07:38:11.914: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:38:11.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1444" for this suite. 01/17/23 07:38:11.923
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:11.949
Jan 17 07:38:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:38:11.951
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:11.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:11.982
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-9762 01/17/23 07:38:11.988
Jan 17 07:38:12.008: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9762" to be "running and ready"
Jan 17 07:38:12.035: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 27.00393ms
Jan 17 07:38:12.035: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:38:14.044: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.035700705s
Jan 17 07:38:14.044: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 07:38:14.044: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 17 07:38:14.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 07:38:14.351: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 17 07:38:14.351: INFO: stdout: "iptables"
Jan 17 07:38:14.351: INFO: proxyMode: iptables
Jan 17 07:38:14.379: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 07:38:14.384: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9762 01/17/23 07:38:14.384
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9762 01/17/23 07:38:14.415
I0117 07:38:14.433088      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9762, replica count: 3
I0117 07:38:17.484533      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:38:17.514: INFO: Creating new exec pod
Jan 17 07:38:17.526: INFO: Waiting up to 5m0s for pod "execpod-affinityxx224" in namespace "services-9762" to be "running"
Jan 17 07:38:17.540: INFO: Pod "execpod-affinityxx224": Phase="Pending", Reason="", readiness=false. Elapsed: 13.194662ms
Jan 17 07:38:19.548: INFO: Pod "execpod-affinityxx224": Phase="Running", Reason="", readiness=true. Elapsed: 2.021018061s
Jan 17 07:38:19.548: INFO: Pod "execpod-affinityxx224" satisfied condition "running"
Jan 17 07:38:20.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 17 07:38:20.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 07:38:20.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:38:20.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.57.213 80'
Jan 17 07:38:21.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.57.213 80\nConnection to 10.254.57.213 80 port [tcp/http] succeeded!\n"
Jan 17 07:38:21.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:38:21.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30030'
Jan 17 07:38:21.542: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30030\nConnection to 10.0.0.16 30030 port [tcp/*] succeeded!\n"
Jan 17 07:38:21.542: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:38:21.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30030'
Jan 17 07:38:21.883: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30030\nConnection to 10.0.0.22 30030 port [tcp/*] succeeded!\n"
Jan 17 07:38:21.883: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:38:21.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30030/ ; done'
Jan 17 07:38:22.242: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
Jan 17 07:38:22.242: INFO: stdout: "\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf"
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
Jan 17 07:38:22.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30030/'
Jan 17 07:38:22.524: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
Jan 17 07:38:22.524: INFO: stdout: "affinity-nodeport-timeout-tgqpf"
Jan 17 07:38:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30030/'
Jan 17 07:38:42.829: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
Jan 17 07:38:42.829: INFO: stdout: "affinity-nodeport-timeout-xtjnb"
Jan 17 07:38:42.829: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9762, will wait for the garbage collector to delete the pods 01/17/23 07:38:42.867
Jan 17 07:38:42.945: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.419355ms
Jan 17 07:38:43.047: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 102.30765ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:38:45.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9762" for this suite. 01/17/23 07:38:45.619
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":240,"skipped":4267,"failed":0}
------------------------------
• [SLOW TEST] [33.684 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:11.949
    Jan 17 07:38:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:38:11.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:11.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:11.982
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-9762 01/17/23 07:38:11.988
    Jan 17 07:38:12.008: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9762" to be "running and ready"
    Jan 17 07:38:12.035: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 27.00393ms
    Jan 17 07:38:12.035: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:38:14.044: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.035700705s
    Jan 17 07:38:14.044: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 17 07:38:14.044: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 17 07:38:14.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 17 07:38:14.351: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 17 07:38:14.351: INFO: stdout: "iptables"
    Jan 17 07:38:14.351: INFO: proxyMode: iptables
    Jan 17 07:38:14.379: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 17 07:38:14.384: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-9762 01/17/23 07:38:14.384
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-9762 01/17/23 07:38:14.415
    I0117 07:38:14.433088      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9762, replica count: 3
    I0117 07:38:17.484533      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:38:17.514: INFO: Creating new exec pod
    Jan 17 07:38:17.526: INFO: Waiting up to 5m0s for pod "execpod-affinityxx224" in namespace "services-9762" to be "running"
    Jan 17 07:38:17.540: INFO: Pod "execpod-affinityxx224": Phase="Pending", Reason="", readiness=false. Elapsed: 13.194662ms
    Jan 17 07:38:19.548: INFO: Pod "execpod-affinityxx224": Phase="Running", Reason="", readiness=true. Elapsed: 2.021018061s
    Jan 17 07:38:19.548: INFO: Pod "execpod-affinityxx224" satisfied condition "running"
    Jan 17 07:38:20.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 17 07:38:20.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 17 07:38:20.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:38:20.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.57.213 80'
    Jan 17 07:38:21.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.57.213 80\nConnection to 10.254.57.213 80 port [tcp/http] succeeded!\n"
    Jan 17 07:38:21.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:38:21.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30030'
    Jan 17 07:38:21.542: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30030\nConnection to 10.0.0.16 30030 port [tcp/*] succeeded!\n"
    Jan 17 07:38:21.542: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:38:21.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30030'
    Jan 17 07:38:21.883: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30030\nConnection to 10.0.0.22 30030 port [tcp/*] succeeded!\n"
    Jan 17 07:38:21.883: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:38:21.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30030/ ; done'
    Jan 17 07:38:22.242: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
    Jan 17 07:38:22.242: INFO: stdout: "\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf\naffinity-nodeport-timeout-tgqpf"
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.242: INFO: Received response from host: affinity-nodeport-timeout-tgqpf
    Jan 17 07:38:22.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30030/'
    Jan 17 07:38:22.524: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
    Jan 17 07:38:22.524: INFO: stdout: "affinity-nodeport-timeout-tgqpf"
    Jan 17 07:38:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-9762 exec execpod-affinityxx224 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30030/'
    Jan 17 07:38:42.829: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30030/\n"
    Jan 17 07:38:42.829: INFO: stdout: "affinity-nodeport-timeout-xtjnb"
    Jan 17 07:38:42.829: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9762, will wait for the garbage collector to delete the pods 01/17/23 07:38:42.867
    Jan 17 07:38:42.945: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.419355ms
    Jan 17 07:38:43.047: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 102.30765ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:38:45.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9762" for this suite. 01/17/23 07:38:45.619
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:45.636
Jan 17 07:38:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:38:45.638
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:45.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:45.696
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:38:45.711
Jan 17 07:38:45.730: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1155" to be "running and ready"
Jan 17 07:38:45.741: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.7757ms
Jan 17 07:38:45.741: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:38:47.748: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017285525s
Jan 17 07:38:47.748: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 07:38:47.748: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/17/23 07:38:47.753
Jan 17 07:38:47.761: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1155" to be "running and ready"
Jan 17 07:38:47.783: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 21.253181ms
Jan 17 07:38:47.783: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:38:49.797: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035453686s
Jan 17 07:38:49.797: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 17 07:38:49.797: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/17/23 07:38:49.808
Jan 17 07:38:49.827: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 07:38:49.833: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 07:38:51.833: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 07:38:51.842: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 07:38:53.835: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 07:38:53.840: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/17/23 07:38:53.84
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 07:38:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1155" for this suite. 01/17/23 07:38:53.862
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":241,"skipped":4267,"failed":0}
------------------------------
• [SLOW TEST] [8.245 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:45.636
    Jan 17 07:38:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:38:45.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:45.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:45.696
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:38:45.711
    Jan 17 07:38:45.730: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1155" to be "running and ready"
    Jan 17 07:38:45.741: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.7757ms
    Jan 17 07:38:45.741: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:38:47.748: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.017285525s
    Jan 17 07:38:47.748: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 07:38:47.748: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/17/23 07:38:47.753
    Jan 17 07:38:47.761: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1155" to be "running and ready"
    Jan 17 07:38:47.783: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 21.253181ms
    Jan 17 07:38:47.783: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:38:49.797: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035453686s
    Jan 17 07:38:49.797: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 17 07:38:49.797: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/17/23 07:38:49.808
    Jan 17 07:38:49.827: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 07:38:49.833: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 17 07:38:51.833: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 07:38:51.842: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 17 07:38:53.835: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 07:38:53.840: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/17/23 07:38:53.84
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 07:38:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1155" for this suite. 01/17/23 07:38:53.862
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:53.882
Jan 17 07:38:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:38:53.886
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:53.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:53.925
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 07:38:53.934
Jan 17 07:38:53.952: INFO: Waiting up to 5m0s for pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd" in namespace "emptydir-5092" to be "Succeeded or Failed"
Jan 17 07:38:53.968: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.831317ms
Jan 17 07:38:55.973: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020878311s
Jan 17 07:38:57.982: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03043325s
STEP: Saw pod success 01/17/23 07:38:57.982
Jan 17 07:38:57.983: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd" satisfied condition "Succeeded or Failed"
Jan 17 07:38:57.991: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd container test-container: <nil>
STEP: delete the pod 01/17/23 07:38:58.002
Jan 17 07:38:58.031: INFO: Waiting for pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd to disappear
Jan 17 07:38:58.037: INFO: Pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:38:58.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5092" for this suite. 01/17/23 07:38:58.048
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":242,"skipped":4270,"failed":0}
------------------------------
• [4.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:53.882
    Jan 17 07:38:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:38:53.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:53.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:53.925
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 07:38:53.934
    Jan 17 07:38:53.952: INFO: Waiting up to 5m0s for pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd" in namespace "emptydir-5092" to be "Succeeded or Failed"
    Jan 17 07:38:53.968: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.831317ms
    Jan 17 07:38:55.973: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020878311s
    Jan 17 07:38:57.982: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03043325s
    STEP: Saw pod success 01/17/23 07:38:57.982
    Jan 17 07:38:57.983: INFO: Pod "pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd" satisfied condition "Succeeded or Failed"
    Jan 17 07:38:57.991: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd container test-container: <nil>
    STEP: delete the pod 01/17/23 07:38:58.002
    Jan 17 07:38:58.031: INFO: Waiting for pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd to disappear
    Jan 17 07:38:58.037: INFO: Pod pod-547c7053-a2aa-48f7-8ff1-a86a43c151bd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:38:58.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5092" for this suite. 01/17/23 07:38:58.048
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:38:58.064
Jan 17 07:38:58.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename cronjob 01/17/23 07:38:58.066
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:58.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:58.121
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/17/23 07:38:58.128
STEP: Ensuring a job is scheduled 01/17/23 07:38:58.146
STEP: Ensuring exactly one is scheduled 01/17/23 07:39:00.162
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 07:39:00.172
STEP: Ensuring the job is replaced with a new one 01/17/23 07:39:00.182
STEP: Removing cronjob 01/17/23 07:40:00.203
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 07:40:00.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5432" for this suite. 01/17/23 07:40:00.251
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":243,"skipped":4271,"failed":0}
------------------------------
• [SLOW TEST] [62.224 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:38:58.064
    Jan 17 07:38:58.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename cronjob 01/17/23 07:38:58.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:38:58.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:38:58.121
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/17/23 07:38:58.128
    STEP: Ensuring a job is scheduled 01/17/23 07:38:58.146
    STEP: Ensuring exactly one is scheduled 01/17/23 07:39:00.162
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 07:39:00.172
    STEP: Ensuring the job is replaced with a new one 01/17/23 07:39:00.182
    STEP: Removing cronjob 01/17/23 07:40:00.203
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 07:40:00.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5432" for this suite. 01/17/23 07:40:00.251
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:00.292
Jan 17 07:40:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:40:00.296
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:00.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:00.338
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:40:00.35
Jan 17 07:40:00.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789" in namespace "downward-api-6003" to be "Succeeded or Failed"
Jan 17 07:40:00.393: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Pending", Reason="", readiness=false. Elapsed: 16.329741ms
Jan 17 07:40:02.400: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023377752s
Jan 17 07:40:04.402: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025254433s
STEP: Saw pod success 01/17/23 07:40:04.402
Jan 17 07:40:04.402: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789" satisfied condition "Succeeded or Failed"
Jan 17 07:40:04.406: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 container client-container: <nil>
STEP: delete the pod 01/17/23 07:40:04.416
Jan 17 07:40:04.440: INFO: Waiting for pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 to disappear
Jan 17 07:40:04.450: INFO: Pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:40:04.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6003" for this suite. 01/17/23 07:40:04.457
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":244,"skipped":4274,"failed":0}
------------------------------
• [4.176 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:00.292
    Jan 17 07:40:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:40:00.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:00.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:00.338
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:40:00.35
    Jan 17 07:40:00.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789" in namespace "downward-api-6003" to be "Succeeded or Failed"
    Jan 17 07:40:00.393: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Pending", Reason="", readiness=false. Elapsed: 16.329741ms
    Jan 17 07:40:02.400: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023377752s
    Jan 17 07:40:04.402: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025254433s
    STEP: Saw pod success 01/17/23 07:40:04.402
    Jan 17 07:40:04.402: INFO: Pod "downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789" satisfied condition "Succeeded or Failed"
    Jan 17 07:40:04.406: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 container client-container: <nil>
    STEP: delete the pod 01/17/23 07:40:04.416
    Jan 17 07:40:04.440: INFO: Waiting for pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 to disappear
    Jan 17 07:40:04.450: INFO: Pod downwardapi-volume-afca9d05-8c73-4d79-9352-6cd3ee4f1789 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:40:04.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6003" for this suite. 01/17/23 07:40:04.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:04.47
Jan 17 07:40:04.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:40:04.471
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:04.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:04.512
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/17/23 07:40:04.522
Jan 17 07:40:04.544: INFO: Waiting up to 5m0s for pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45" in namespace "downward-api-8204" to be "Succeeded or Failed"
Jan 17 07:40:04.562: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Pending", Reason="", readiness=false. Elapsed: 17.720536ms
Jan 17 07:40:06.569: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025144804s
Jan 17 07:40:08.572: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028271964s
STEP: Saw pod success 01/17/23 07:40:08.572
Jan 17 07:40:08.573: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45" satisfied condition "Succeeded or Failed"
Jan 17 07:40:08.578: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 container dapi-container: <nil>
STEP: delete the pod 01/17/23 07:40:08.6
Jan 17 07:40:08.636: INFO: Waiting for pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 to disappear
Jan 17 07:40:08.643: INFO: Pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 07:40:08.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8204" for this suite. 01/17/23 07:40:08.652
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":245,"skipped":4301,"failed":0}
------------------------------
• [4.203 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:04.47
    Jan 17 07:40:04.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:40:04.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:04.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:04.512
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/17/23 07:40:04.522
    Jan 17 07:40:04.544: INFO: Waiting up to 5m0s for pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45" in namespace "downward-api-8204" to be "Succeeded or Failed"
    Jan 17 07:40:04.562: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Pending", Reason="", readiness=false. Elapsed: 17.720536ms
    Jan 17 07:40:06.569: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025144804s
    Jan 17 07:40:08.572: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028271964s
    STEP: Saw pod success 01/17/23 07:40:08.572
    Jan 17 07:40:08.573: INFO: Pod "downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45" satisfied condition "Succeeded or Failed"
    Jan 17 07:40:08.578: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 07:40:08.6
    Jan 17 07:40:08.636: INFO: Waiting for pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 to disappear
    Jan 17 07:40:08.643: INFO: Pod downward-api-7d6dcaf7-adf3-4166-bf41-2ea366682f45 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 07:40:08.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8204" for this suite. 01/17/23 07:40:08.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:08.676
Jan 17 07:40:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 07:40:08.677
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:08.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:08.714
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/17/23 07:40:08.728
Jan 17 07:40:08.749: INFO: Waiting up to 5m0s for pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e" in namespace "var-expansion-508" to be "Succeeded or Failed"
Jan 17 07:40:08.768: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.6874ms
Jan 17 07:40:10.773: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024652787s
Jan 17 07:40:12.775: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025953327s
Jan 17 07:40:14.777: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02793118s
STEP: Saw pod success 01/17/23 07:40:14.777
Jan 17 07:40:14.777: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e" satisfied condition "Succeeded or Failed"
Jan 17 07:40:14.788: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e container dapi-container: <nil>
STEP: delete the pod 01/17/23 07:40:14.807
Jan 17 07:40:14.833: INFO: Waiting for pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e to disappear
Jan 17 07:40:14.840: INFO: Pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 07:40:14.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-508" for this suite. 01/17/23 07:40:14.85
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":246,"skipped":4341,"failed":0}
------------------------------
• [SLOW TEST] [6.189 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:08.676
    Jan 17 07:40:08.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 07:40:08.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:08.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:08.714
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/17/23 07:40:08.728
    Jan 17 07:40:08.749: INFO: Waiting up to 5m0s for pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e" in namespace "var-expansion-508" to be "Succeeded or Failed"
    Jan 17 07:40:08.768: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.6874ms
    Jan 17 07:40:10.773: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024652787s
    Jan 17 07:40:12.775: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025953327s
    Jan 17 07:40:14.777: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02793118s
    STEP: Saw pod success 01/17/23 07:40:14.777
    Jan 17 07:40:14.777: INFO: Pod "var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e" satisfied condition "Succeeded or Failed"
    Jan 17 07:40:14.788: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e container dapi-container: <nil>
    STEP: delete the pod 01/17/23 07:40:14.807
    Jan 17 07:40:14.833: INFO: Waiting for pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e to disappear
    Jan 17 07:40:14.840: INFO: Pod var-expansion-bbee0985-b12c-4e4f-94c8-6f081ca8989e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 07:40:14.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-508" for this suite. 01/17/23 07:40:14.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:14.865
Jan 17 07:40:14.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename endpointslice 01/17/23 07:40:14.868
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:14.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:14.907
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 17 07:40:14.943: INFO: Endpoints addresses: [10.0.0.20 10.0.0.4 10.0.0.9] , ports: [6443]
Jan 17 07:40:14.943: INFO: EndpointSlices addresses: [10.0.0.20 10.0.0.4 10.0.0.9] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 07:40:14.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5751" for this suite. 01/17/23 07:40:14.954
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":247,"skipped":4347,"failed":0}
------------------------------
• [0.100 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:14.865
    Jan 17 07:40:14.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename endpointslice 01/17/23 07:40:14.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:14.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:14.907
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 17 07:40:14.943: INFO: Endpoints addresses: [10.0.0.20 10.0.0.4 10.0.0.9] , ports: [6443]
    Jan 17 07:40:14.943: INFO: EndpointSlices addresses: [10.0.0.20 10.0.0.4 10.0.0.9] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 07:40:14.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5751" for this suite. 01/17/23 07:40:14.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:14.967
Jan 17 07:40:14.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 07:40:14.969
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:15.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:15.029
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-532 01/17/23 07:40:15.036
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/17/23 07:40:15.051
STEP: Creating pod with conflicting port in namespace statefulset-532 01/17/23 07:40:15.07
STEP: Waiting until pod test-pod will start running in namespace statefulset-532 01/17/23 07:40:15.092
Jan 17 07:40:15.092: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-532" to be "running"
Jan 17 07:40:15.106: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.997249ms
Jan 17 07:40:17.113: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021051347s
Jan 17 07:40:17.113: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-532 01/17/23 07:40:17.113
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-532 01/17/23 07:40:17.131
Jan 17 07:40:17.155: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Pending. Waiting for statefulset controller to delete.
Jan 17 07:40:17.194: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 07:40:17.224: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 07:40:17.224: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-532
STEP: Removing pod with conflicting port in namespace statefulset-532 01/17/23 07:40:17.224
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-532 and will be in running state 01/17/23 07:40:17.281
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 07:40:19.317: INFO: Deleting all statefulset in ns statefulset-532
Jan 17 07:40:19.336: INFO: Scaling statefulset ss to 0
Jan 17 07:40:29.379: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:40:29.386: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 07:40:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-532" for this suite. 01/17/23 07:40:29.446
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":248,"skipped":4365,"failed":0}
------------------------------
• [SLOW TEST] [14.503 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:14.967
    Jan 17 07:40:14.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 07:40:14.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:15.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:15.029
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-532 01/17/23 07:40:15.036
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/17/23 07:40:15.051
    STEP: Creating pod with conflicting port in namespace statefulset-532 01/17/23 07:40:15.07
    STEP: Waiting until pod test-pod will start running in namespace statefulset-532 01/17/23 07:40:15.092
    Jan 17 07:40:15.092: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-532" to be "running"
    Jan 17 07:40:15.106: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.997249ms
    Jan 17 07:40:17.113: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021051347s
    Jan 17 07:40:17.113: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-532 01/17/23 07:40:17.113
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-532 01/17/23 07:40:17.131
    Jan 17 07:40:17.155: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 17 07:40:17.194: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 17 07:40:17.224: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 44791bdb-9ea8-4e96-9e52-7ca2e5cf616a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 17 07:40:17.224: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-532
    STEP: Removing pod with conflicting port in namespace statefulset-532 01/17/23 07:40:17.224
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-532 and will be in running state 01/17/23 07:40:17.281
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 07:40:19.317: INFO: Deleting all statefulset in ns statefulset-532
    Jan 17 07:40:19.336: INFO: Scaling statefulset ss to 0
    Jan 17 07:40:29.379: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:40:29.386: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 07:40:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-532" for this suite. 01/17/23 07:40:29.446
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:29.47
Jan 17 07:40:29.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:40:29.472
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:29.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:29.53
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 17 07:40:29.539: INFO: Creating deployment "webserver-deployment"
Jan 17 07:40:29.577: INFO: Waiting for observed generation 1
Jan 17 07:40:31.606: INFO: Waiting for all required pods to come up
Jan 17 07:40:31.628: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/17/23 07:40:31.628
Jan 17 07:40:31.628: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zz95v" in namespace "deployment-8109" to be "running"
Jan 17 07:40:31.631: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9ktxd" in namespace "deployment-8109" to be "running"
Jan 17 07:40:31.632: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j2lpr" in namespace "deployment-8109" to be "running"
Jan 17 07:40:31.632: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l2wch" in namespace "deployment-8109" to be "running"
Jan 17 07:40:31.643: INFO: Pod "webserver-deployment-845c8977d9-9ktxd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.400978ms
Jan 17 07:40:31.648: INFO: Pod "webserver-deployment-845c8977d9-zz95v": Phase="Pending", Reason="", readiness=false. Elapsed: 20.131296ms
Jan 17 07:40:31.649: INFO: Pod "webserver-deployment-845c8977d9-j2lpr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.202772ms
Jan 17 07:40:31.654: INFO: Pod "webserver-deployment-845c8977d9-l2wch": Phase="Pending", Reason="", readiness=false. Elapsed: 21.510917ms
Jan 17 07:40:33.652: INFO: Pod "webserver-deployment-845c8977d9-9ktxd": Phase="Running", Reason="", readiness=true. Elapsed: 2.020721047s
Jan 17 07:40:33.652: INFO: Pod "webserver-deployment-845c8977d9-9ktxd" satisfied condition "running"
Jan 17 07:40:33.658: INFO: Pod "webserver-deployment-845c8977d9-j2lpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.025970932s
Jan 17 07:40:33.658: INFO: Pod "webserver-deployment-845c8977d9-j2lpr" satisfied condition "running"
Jan 17 07:40:33.659: INFO: Pod "webserver-deployment-845c8977d9-zz95v": Phase="Running", Reason="", readiness=true. Elapsed: 2.030503061s
Jan 17 07:40:33.659: INFO: Pod "webserver-deployment-845c8977d9-zz95v" satisfied condition "running"
Jan 17 07:40:33.665: INFO: Pod "webserver-deployment-845c8977d9-l2wch": Phase="Running", Reason="", readiness=true. Elapsed: 2.03265882s
Jan 17 07:40:33.665: INFO: Pod "webserver-deployment-845c8977d9-l2wch" satisfied condition "running"
Jan 17 07:40:33.665: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 17 07:40:33.678: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 17 07:40:33.698: INFO: Updating deployment webserver-deployment
Jan 17 07:40:33.698: INFO: Waiting for observed generation 2
Jan 17 07:40:35.715: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 17 07:40:35.720: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 17 07:40:35.724: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 07:40:35.742: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 17 07:40:35.742: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 17 07:40:35.754: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 07:40:35.766: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 17 07:40:35.766: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 17 07:40:35.785: INFO: Updating deployment webserver-deployment
Jan 17 07:40:35.785: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 17 07:40:35.800: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 17 07:40:35.819: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:40:35.879: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8109  7e8852d9-8b87-4a24-acd7-78f4287a692f 30606 3 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067e2c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-17 07:40:33 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:40:35 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 17 07:40:35.946: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8109  c354eb87-beb2-42ab-a684-e3adf1f876c3 30601 3 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7e8852d9-8b87-4a24-acd7-78f4287a692f 0xc0038fdf37 0xc0038fdf38}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e8852d9-8b87-4a24-acd7-78f4287a692f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a38078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:40:35.946: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 17 07:40:35.946: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8109  91d42e39-73ca-4428-8216-bf5e1d760b13 30598 3 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7e8852d9-8b87-4a24-acd7-78f4287a692f 0xc006a380d7 0xc006a380d8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e8852d9-8b87-4a24-acd7-78f4287a692f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a38178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:40:36.005: INFO: Pod "webserver-deployment-69b7448995-2978j" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2978j webserver-deployment-69b7448995- deployment-8109  766f33b7-e782-4d39-af02-64c78f41ac63 30650 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38627 0xc006a38628}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rvsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rvsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.006: INFO: Pod "webserver-deployment-69b7448995-69htl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-69htl webserver-deployment-69b7448995- deployment-8109  2a2cbde8-bf62-4026-a677-ff5572046521 30573 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:85f22a18ce7bb2de498c12c92b09b0b57006afb1e59b4de0da1577be7b128e11 cni.projectcalico.org/podIP:10.100.168.123/32 cni.projectcalico.org/podIPs:10.100.168.123/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38790 0xc006a38791}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xp8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xp8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.006: INFO: Pod "webserver-deployment-69b7448995-7j8ls" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7j8ls webserver-deployment-69b7448995- deployment-8109  a27686cc-6035-419c-893d-1ca833e88737 30548 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:488c96e102ce543f745197dfe82e23ff1a803768f2cf4e4ebfcedcb8b8355c0e cni.projectcalico.org/podIP:10.100.206.21/32 cni.projectcalico.org/podIPs:10.100.206.21/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a389b0 0xc006a389b1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ndg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ndg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.008: INFO: Pod "webserver-deployment-69b7448995-bkwrv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bkwrv webserver-deployment-69b7448995- deployment-8109  1f90a15c-d7c6-4949-8d87-d7eb63a871ea 30621 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38bb0 0xc006a38bb1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7kkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7kkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.008: INFO: Pod "webserver-deployment-69b7448995-j2mvq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-j2mvq webserver-deployment-69b7448995- deployment-8109  0a6a9861-b401-441f-a0e3-426c298ed47f 30651 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38d10 0xc006a38d11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hzgvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hzgvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-jjcgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jjcgb webserver-deployment-69b7448995- deployment-8109  4513da73-02f0-46f9-bb01-da70e933f487 30655 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38e70 0xc006a38e71}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gwhz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gwhz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-l9f8h" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-l9f8h webserver-deployment-69b7448995- deployment-8109  9ace6eae-7196-448a-b532-37e55bdccd13 30562 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:775cd21129af6bce50dd552f9b20c4198ccda27cd1d40c0574c3966208a1432c cni.projectcalico.org/podIP:10.100.168.81/32 cni.projectcalico.org/podIPs:10.100.168.81/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38ff0 0xc006a38ff1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr977,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr977,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-pm5n2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pm5n2 webserver-deployment-69b7448995- deployment-8109  541912b7-0b6c-4708-b806-0ed49ff1485b 30567 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8e656e8f0e8fcfb12c0513771a9e80d65d0a3e027530bce6b39bf0ea92d54d3d cni.projectcalico.org/podIP:10.100.135.58/32 cni.projectcalico.org/podIPs:10.100.135.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39210 0xc006a39211}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6976,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6976,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-pn4kg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pn4kg webserver-deployment-69b7448995- deployment-8109  26240359-1aef-4b3f-9557-63f3769f5cd6 30644 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39410 0xc006a39411}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdxfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdxfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-qhpph" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qhpph webserver-deployment-69b7448995- deployment-8109  1f4d76eb-8a66-4235-938a-6b4d51cddf64 30579 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56527504a62bd8da60245f691ed850eb7b5f00bbf81dc7640ad957630350dde2 cni.projectcalico.org/podIP:10.100.135.55/32 cni.projectcalico.org/podIPs:10.100.135.55/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39587 0xc006a39588}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdm25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdm25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-qwv9t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qwv9t webserver-deployment-69b7448995- deployment-8109  98cfc999-f7f5-409d-9cb2-c5709c90fab6 30652 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39780 0xc006a39781}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d76ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d76ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-zmnq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zmnq4 webserver-deployment-69b7448995- deployment-8109  063bc1db-6b55-44b9-a248-5b296167bec4 30642 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a398e0 0xc006a398e1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftsq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftsq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-zvw5c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zvw5c webserver-deployment-69b7448995- deployment-8109  8fee8672-331a-440c-836c-f95c91e54e04 30634 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39ab0 0xc006a39ab1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5pxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5pxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-845c8977d9-46h5k" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-46h5k webserver-deployment-845c8977d9- deployment-8109  295ffd32-9b01-4e8d-a8cb-a246d69e672d 30463 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5d5bea1d20a04c311ddba48b63cdfada6fc1d96d0a6c4122c895179f420f3989 cni.projectcalico.org/podIP:10.100.206.19/32 cni.projectcalico.org/podIPs:10.100.206.19/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc006a39ca0 0xc006a39ca1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6ld9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6ld9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.19,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eacad416c9f9c7bca886ef1087d14e223b058f4b336860b67c04e8e2e7bf435f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-487jb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-487jb webserver-deployment-845c8977d9- deployment-8109  d7fa825d-772d-4f2f-93bd-4cc9371846c5 30451 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:770b11aa6a6ec201033122978f698697324c73ac0ab9cec26af66e8eee55d594 cni.projectcalico.org/podIP:10.100.168.77/32 cni.projectcalico.org/podIPs:10.100.168.77/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc006a39ec0 0xc006a39ec1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l89w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l89w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.77,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://338e77804ba474eb143584a259c3de5be35517eca4f6a4cfd2628b0f6bbfb620,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-52z27" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-52z27 webserver-deployment-845c8977d9- deployment-8109  9aa882b0-5081-434a-8945-d30d73738da6 30469 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4ea64f41cb17bc71496205dc21df76ccdcc80f4e4d6ebd243829f0392d7c6f9e cni.projectcalico.org/podIP:10.100.206.18/32 cni.projectcalico.org/podIPs:10.100.206.18/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc0006582c0 0xc0006582c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xh5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xh5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.18,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://09af2f5230ae64542ed0d05464caba96d7e4a034f8da4d48ca283b6dac6c82ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-9bz5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9bz5f webserver-deployment-845c8977d9- deployment-8109  02fc5f66-751e-4835-8469-c8dae6399378 30647 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000658580 0xc000658581}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lff6f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lff6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-9ktxd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9ktxd webserver-deployment-845c8977d9- deployment-8109  3eacedb2-222d-4731-a7d8-16ea507048ba 30479 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7de1e37441d6c85f44650cc3c088e45c00b94a5f5c6d8fb226eabd7183fe4aef cni.projectcalico.org/podIP:10.100.135.51/32 cni.projectcalico.org/podIPs:10.100.135.51/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000658940 0xc000658941}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjf7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjf7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.51,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9a3563ce3e5b447a50e648852a0835e66db77eb0143d6d569aa62002e4c207b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-bg99q" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bg99q webserver-deployment-845c8977d9- deployment-8109  ba84856b-eec2-4813-8c27-fff1e8eb6514 30656 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc0006591d0 0xc0006591d1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9dtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9dtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.012: INFO: Pod "webserver-deployment-845c8977d9-cwww5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cwww5 webserver-deployment-845c8977d9- deployment-8109  2c3cb176-24dd-4a40-a00c-6141e58b2e1d 30658 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659650 0xc000659651}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l82jw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l82jw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.012: INFO: Pod "webserver-deployment-845c8977d9-fvjbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fvjbz webserver-deployment-845c8977d9- deployment-8109  1a55e7ee-c9d9-4850-88c1-63e3061bc04d 30640 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659830 0xc000659831}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp2qp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp2qp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-ggxd7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ggxd7 webserver-deployment-845c8977d9- deployment-8109  8453ae50-ebb1-4c71-b99b-414a031d6fe9 30452 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2c428dcac3e759020305f3f8c9383cffa7d260a314cf29a3e2c0e083d777256b cni.projectcalico.org/podIP:10.100.168.79/32 cni.projectcalico.org/podIPs:10.100.168.79/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659a10 0xc000659a11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsltz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsltz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.79,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://255b924a4fe35e5f81bc83f4a1b6987919416b59c7b42e759226118ca6af8d13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-hgk2w" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hgk2w webserver-deployment-845c8977d9- deployment-8109  0f2aa524-ebef-4006-b928-2274634c0ed2 30613 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659c10 0xc000659c11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vt2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vt2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-j2lpr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2lpr webserver-deployment-845c8977d9- deployment-8109  e565980e-a0cb-48fc-b9f9-68f2aab8d92a 30482 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:20c85b25b1ab49853f57c54c968b5e4bb1e85bea7075f54ba1b357876126babc cni.projectcalico.org/podIP:10.100.135.49/32 cni.projectcalico.org/podIPs:10.100.135.49/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659df0 0xc000659df1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqklr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqklr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.49,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bbeb2e6a730eab5255c0606c6688f0b316bdc55a1dc72bee2d50456dbd0d7ce3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-jlr8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jlr8z webserver-deployment-845c8977d9- deployment-8109  61aec38b-6d3a-4223-b909-fc5358262a97 30639 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac070 0xc004eac071}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x5r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x5r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-mlr4j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mlr4j webserver-deployment-845c8977d9- deployment-8109  1dc07220-79cc-4714-992b-db18c60d716c 30457 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6735bcc503b85b3bfd37c728bcc9070ec8f7df5db6d0df46354857ae2b65cf7c cni.projectcalico.org/podIP:10.100.168.127/32 cni.projectcalico.org/podIPs:10.100.168.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac1c0 0xc004eac1c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kg4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kg4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.127,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1d24ba44a35366297d073b20597c8dc640803adbbeda75f42f566cb0405c7264,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-mw89w" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mw89w webserver-deployment-845c8977d9- deployment-8109  0ab1ca19-820a-4aa4-b423-7c251c2558fb 30648 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac3c0 0xc004eac3c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlk6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlk6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-p9gcc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-p9gcc webserver-deployment-845c8977d9- deployment-8109  b2670b18-7fbe-4a4d-a630-d1cf0bb45230 30657 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac540 0xc004eac541}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftwgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftwgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-pkgfn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pkgfn webserver-deployment-845c8977d9- deployment-8109  72e547c6-36fd-4d31-adc7-bfeaa5cf6145 30638 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac690 0xc004eac691}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd2mz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd2mz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-sb7nx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sb7nx webserver-deployment-845c8977d9- deployment-8109  3fd324ab-77bc-42e8-9688-f2accb68fdae 30465 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:41a6d58cf72504944fbef38689943fd47751f6e50a1c5f2ee9fb561e5881b38f cni.projectcalico.org/podIP:10.100.206.20/32 cni.projectcalico.org/podIPs:10.100.206.20/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac810 0xc004eac811}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kwhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kwhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.20,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4e328ee6b02ab870100250ec90bd18389972981ae06a8841380ac5c00ed52835,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-vnf7c" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vnf7c webserver-deployment-845c8977d9- deployment-8109  bf6c440a-7a1d-4b48-8de4-b1ceda1eeeb4 30636 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eaca10 0xc004eaca11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67gq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67gq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-x9cst" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x9cst webserver-deployment-845c8977d9- deployment-8109  ff7910fa-cb24-49df-8a2d-c01b031a0a46 30654 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eacbc0 0xc004eacbc1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bzxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bzxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-zbkmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zbkmt webserver-deployment-845c8977d9- deployment-8109  96d30ec5-549f-44ea-bd54-12310790a79a 30653 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eacd10 0xc004eacd11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xz8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xz8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:40:36.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8109" for this suite. 01/17/23 07:40:36.049
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":249,"skipped":4365,"failed":0}
------------------------------
• [SLOW TEST] [6.609 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:29.47
    Jan 17 07:40:29.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:40:29.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:29.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:29.53
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 17 07:40:29.539: INFO: Creating deployment "webserver-deployment"
    Jan 17 07:40:29.577: INFO: Waiting for observed generation 1
    Jan 17 07:40:31.606: INFO: Waiting for all required pods to come up
    Jan 17 07:40:31.628: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/17/23 07:40:31.628
    Jan 17 07:40:31.628: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zz95v" in namespace "deployment-8109" to be "running"
    Jan 17 07:40:31.631: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9ktxd" in namespace "deployment-8109" to be "running"
    Jan 17 07:40:31.632: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j2lpr" in namespace "deployment-8109" to be "running"
    Jan 17 07:40:31.632: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l2wch" in namespace "deployment-8109" to be "running"
    Jan 17 07:40:31.643: INFO: Pod "webserver-deployment-845c8977d9-9ktxd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.400978ms
    Jan 17 07:40:31.648: INFO: Pod "webserver-deployment-845c8977d9-zz95v": Phase="Pending", Reason="", readiness=false. Elapsed: 20.131296ms
    Jan 17 07:40:31.649: INFO: Pod "webserver-deployment-845c8977d9-j2lpr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.202772ms
    Jan 17 07:40:31.654: INFO: Pod "webserver-deployment-845c8977d9-l2wch": Phase="Pending", Reason="", readiness=false. Elapsed: 21.510917ms
    Jan 17 07:40:33.652: INFO: Pod "webserver-deployment-845c8977d9-9ktxd": Phase="Running", Reason="", readiness=true. Elapsed: 2.020721047s
    Jan 17 07:40:33.652: INFO: Pod "webserver-deployment-845c8977d9-9ktxd" satisfied condition "running"
    Jan 17 07:40:33.658: INFO: Pod "webserver-deployment-845c8977d9-j2lpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.025970932s
    Jan 17 07:40:33.658: INFO: Pod "webserver-deployment-845c8977d9-j2lpr" satisfied condition "running"
    Jan 17 07:40:33.659: INFO: Pod "webserver-deployment-845c8977d9-zz95v": Phase="Running", Reason="", readiness=true. Elapsed: 2.030503061s
    Jan 17 07:40:33.659: INFO: Pod "webserver-deployment-845c8977d9-zz95v" satisfied condition "running"
    Jan 17 07:40:33.665: INFO: Pod "webserver-deployment-845c8977d9-l2wch": Phase="Running", Reason="", readiness=true. Elapsed: 2.03265882s
    Jan 17 07:40:33.665: INFO: Pod "webserver-deployment-845c8977d9-l2wch" satisfied condition "running"
    Jan 17 07:40:33.665: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 17 07:40:33.678: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 17 07:40:33.698: INFO: Updating deployment webserver-deployment
    Jan 17 07:40:33.698: INFO: Waiting for observed generation 2
    Jan 17 07:40:35.715: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 17 07:40:35.720: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 17 07:40:35.724: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 07:40:35.742: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 17 07:40:35.742: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 17 07:40:35.754: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 07:40:35.766: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 17 07:40:35.766: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 17 07:40:35.785: INFO: Updating deployment webserver-deployment
    Jan 17 07:40:35.785: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 07:40:35.800: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 17 07:40:35.819: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:40:35.879: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8109  7e8852d9-8b87-4a24-acd7-78f4287a692f 30606 3 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067e2c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-17 07:40:33 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 07:40:35 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 17 07:40:35.946: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8109  c354eb87-beb2-42ab-a684-e3adf1f876c3 30601 3 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7e8852d9-8b87-4a24-acd7-78f4287a692f 0xc0038fdf37 0xc0038fdf38}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e8852d9-8b87-4a24-acd7-78f4287a692f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a38078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:40:35.946: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 17 07:40:35.946: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8109  91d42e39-73ca-4428-8216-bf5e1d760b13 30598 3 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7e8852d9-8b87-4a24-acd7-78f4287a692f 0xc006a380d7 0xc006a380d8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e8852d9-8b87-4a24-acd7-78f4287a692f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a38178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:40:36.005: INFO: Pod "webserver-deployment-69b7448995-2978j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2978j webserver-deployment-69b7448995- deployment-8109  766f33b7-e782-4d39-af02-64c78f41ac63 30650 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38627 0xc006a38628}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rvsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rvsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.006: INFO: Pod "webserver-deployment-69b7448995-69htl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-69htl webserver-deployment-69b7448995- deployment-8109  2a2cbde8-bf62-4026-a677-ff5572046521 30573 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:85f22a18ce7bb2de498c12c92b09b0b57006afb1e59b4de0da1577be7b128e11 cni.projectcalico.org/podIP:10.100.168.123/32 cni.projectcalico.org/podIPs:10.100.168.123/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38790 0xc006a38791}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xp8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xp8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.006: INFO: Pod "webserver-deployment-69b7448995-7j8ls" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7j8ls webserver-deployment-69b7448995- deployment-8109  a27686cc-6035-419c-893d-1ca833e88737 30548 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:488c96e102ce543f745197dfe82e23ff1a803768f2cf4e4ebfcedcb8b8355c0e cni.projectcalico.org/podIP:10.100.206.21/32 cni.projectcalico.org/podIPs:10.100.206.21/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a389b0 0xc006a389b1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ndg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ndg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.008: INFO: Pod "webserver-deployment-69b7448995-bkwrv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bkwrv webserver-deployment-69b7448995- deployment-8109  1f90a15c-d7c6-4949-8d87-d7eb63a871ea 30621 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38bb0 0xc006a38bb1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7kkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7kkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.008: INFO: Pod "webserver-deployment-69b7448995-j2mvq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-j2mvq webserver-deployment-69b7448995- deployment-8109  0a6a9861-b401-441f-a0e3-426c298ed47f 30651 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38d10 0xc006a38d11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hzgvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hzgvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-jjcgb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jjcgb webserver-deployment-69b7448995- deployment-8109  4513da73-02f0-46f9-bb01-da70e933f487 30655 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38e70 0xc006a38e71}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gwhz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gwhz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-l9f8h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-l9f8h webserver-deployment-69b7448995- deployment-8109  9ace6eae-7196-448a-b532-37e55bdccd13 30562 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:775cd21129af6bce50dd552f9b20c4198ccda27cd1d40c0574c3966208a1432c cni.projectcalico.org/podIP:10.100.168.81/32 cni.projectcalico.org/podIPs:10.100.168.81/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a38ff0 0xc006a38ff1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr977,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr977,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-pm5n2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pm5n2 webserver-deployment-69b7448995- deployment-8109  541912b7-0b6c-4708-b806-0ed49ff1485b 30567 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8e656e8f0e8fcfb12c0513771a9e80d65d0a3e027530bce6b39bf0ea92d54d3d cni.projectcalico.org/podIP:10.100.135.58/32 cni.projectcalico.org/podIPs:10.100.135.58/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39210 0xc006a39211}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6976,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6976,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.009: INFO: Pod "webserver-deployment-69b7448995-pn4kg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pn4kg webserver-deployment-69b7448995- deployment-8109  26240359-1aef-4b3f-9557-63f3769f5cd6 30644 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39410 0xc006a39411}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdxfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdxfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-qhpph" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qhpph webserver-deployment-69b7448995- deployment-8109  1f4d76eb-8a66-4235-938a-6b4d51cddf64 30579 0 2023-01-17 07:40:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56527504a62bd8da60245f691ed850eb7b5f00bbf81dc7640ad957630350dde2 cni.projectcalico.org/podIP:10.100.135.55/32 cni.projectcalico.org/podIPs:10.100.135.55/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39587 0xc006a39588}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 07:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdm25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdm25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-qwv9t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qwv9t webserver-deployment-69b7448995- deployment-8109  98cfc999-f7f5-409d-9cb2-c5709c90fab6 30652 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39780 0xc006a39781}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d76ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d76ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-zmnq4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zmnq4 webserver-deployment-69b7448995- deployment-8109  063bc1db-6b55-44b9-a248-5b296167bec4 30642 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a398e0 0xc006a398e1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftsq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftsq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-69b7448995-zvw5c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zvw5c webserver-deployment-69b7448995- deployment-8109  8fee8672-331a-440c-836c-f95c91e54e04 30634 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c354eb87-beb2-42ab-a684-e3adf1f876c3 0xc006a39ab0 0xc006a39ab1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c354eb87-beb2-42ab-a684-e3adf1f876c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5pxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5pxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.010: INFO: Pod "webserver-deployment-845c8977d9-46h5k" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-46h5k webserver-deployment-845c8977d9- deployment-8109  295ffd32-9b01-4e8d-a8cb-a246d69e672d 30463 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5d5bea1d20a04c311ddba48b63cdfada6fc1d96d0a6c4122c895179f420f3989 cni.projectcalico.org/podIP:10.100.206.19/32 cni.projectcalico.org/podIPs:10.100.206.19/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc006a39ca0 0xc006a39ca1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6ld9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6ld9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.19,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://eacad416c9f9c7bca886ef1087d14e223b058f4b336860b67c04e8e2e7bf435f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-487jb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-487jb webserver-deployment-845c8977d9- deployment-8109  d7fa825d-772d-4f2f-93bd-4cc9371846c5 30451 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:770b11aa6a6ec201033122978f698697324c73ac0ab9cec26af66e8eee55d594 cni.projectcalico.org/podIP:10.100.168.77/32 cni.projectcalico.org/podIPs:10.100.168.77/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc006a39ec0 0xc006a39ec1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l89w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l89w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.77,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://338e77804ba474eb143584a259c3de5be35517eca4f6a4cfd2628b0f6bbfb620,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-52z27" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-52z27 webserver-deployment-845c8977d9- deployment-8109  9aa882b0-5081-434a-8945-d30d73738da6 30469 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4ea64f41cb17bc71496205dc21df76ccdcc80f4e4d6ebd243829f0392d7c6f9e cni.projectcalico.org/podIP:10.100.206.18/32 cni.projectcalico.org/podIPs:10.100.206.18/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc0006582c0 0xc0006582c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xh5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xh5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.18,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://09af2f5230ae64542ed0d05464caba96d7e4a034f8da4d48ca283b6dac6c82ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-9bz5f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9bz5f webserver-deployment-845c8977d9- deployment-8109  02fc5f66-751e-4835-8469-c8dae6399378 30647 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000658580 0xc000658581}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lff6f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lff6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-9ktxd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9ktxd webserver-deployment-845c8977d9- deployment-8109  3eacedb2-222d-4731-a7d8-16ea507048ba 30479 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7de1e37441d6c85f44650cc3c088e45c00b94a5f5c6d8fb226eabd7183fe4aef cni.projectcalico.org/podIP:10.100.135.51/32 cni.projectcalico.org/podIPs:10.100.135.51/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000658940 0xc000658941}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjf7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjf7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.51,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9a3563ce3e5b447a50e648852a0835e66db77eb0143d6d569aa62002e4c207b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.011: INFO: Pod "webserver-deployment-845c8977d9-bg99q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bg99q webserver-deployment-845c8977d9- deployment-8109  ba84856b-eec2-4813-8c27-fff1e8eb6514 30656 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc0006591d0 0xc0006591d1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9dtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9dtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.012: INFO: Pod "webserver-deployment-845c8977d9-cwww5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cwww5 webserver-deployment-845c8977d9- deployment-8109  2c3cb176-24dd-4a40-a00c-6141e58b2e1d 30658 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659650 0xc000659651}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l82jw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l82jw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.012: INFO: Pod "webserver-deployment-845c8977d9-fvjbz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fvjbz webserver-deployment-845c8977d9- deployment-8109  1a55e7ee-c9d9-4850-88c1-63e3061bc04d 30640 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659830 0xc000659831}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp2qp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp2qp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-ggxd7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ggxd7 webserver-deployment-845c8977d9- deployment-8109  8453ae50-ebb1-4c71-b99b-414a031d6fe9 30452 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2c428dcac3e759020305f3f8c9383cffa7d260a314cf29a3e2c0e083d777256b cni.projectcalico.org/podIP:10.100.168.79/32 cni.projectcalico.org/podIPs:10.100.168.79/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659a10 0xc000659a11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsltz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsltz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.79,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://255b924a4fe35e5f81bc83f4a1b6987919416b59c7b42e759226118ca6af8d13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-hgk2w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hgk2w webserver-deployment-845c8977d9- deployment-8109  0f2aa524-ebef-4006-b928-2274634c0ed2 30613 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659c10 0xc000659c11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vt2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vt2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.013: INFO: Pod "webserver-deployment-845c8977d9-j2lpr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2lpr webserver-deployment-845c8977d9- deployment-8109  e565980e-a0cb-48fc-b9f9-68f2aab8d92a 30482 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:20c85b25b1ab49853f57c54c968b5e4bb1e85bea7075f54ba1b357876126babc cni.projectcalico.org/podIP:10.100.135.49/32 cni.projectcalico.org/podIPs:10.100.135.49/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc000659df0 0xc000659df1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.135.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqklr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqklr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:10.100.135.49,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bbeb2e6a730eab5255c0606c6688f0b316bdc55a1dc72bee2d50456dbd0d7ce3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.135.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-jlr8z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jlr8z webserver-deployment-845c8977d9- deployment-8109  61aec38b-6d3a-4223-b909-fc5358262a97 30639 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac070 0xc004eac071}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x5r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x5r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-mlr4j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mlr4j webserver-deployment-845c8977d9- deployment-8109  1dc07220-79cc-4714-992b-db18c60d716c 30457 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6735bcc503b85b3bfd37c728bcc9070ec8f7df5db6d0df46354857ae2b65cf7c cni.projectcalico.org/podIP:10.100.168.127/32 cni.projectcalico.org/podIPs:10.100.168.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac1c0 0xc004eac1c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kg4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kg4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.127,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1d24ba44a35366297d073b20597c8dc640803adbbeda75f42f566cb0405c7264,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-mw89w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mw89w webserver-deployment-845c8977d9- deployment-8109  0ab1ca19-820a-4aa4-b423-7c251c2558fb 30648 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac3c0 0xc004eac3c1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlk6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlk6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-p9gcc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-p9gcc webserver-deployment-845c8977d9- deployment-8109  b2670b18-7fbe-4a4d-a630-d1cf0bb45230 30657 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac540 0xc004eac541}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftwgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftwgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.014: INFO: Pod "webserver-deployment-845c8977d9-pkgfn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pkgfn webserver-deployment-845c8977d9- deployment-8109  72e547c6-36fd-4d31-adc7-bfeaa5cf6145 30638 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac690 0xc004eac691}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd2mz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd2mz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-sb7nx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sb7nx webserver-deployment-845c8977d9- deployment-8109  3fd324ab-77bc-42e8-9688-f2accb68fdae 30465 0 2023-01-17 07:40:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:41a6d58cf72504944fbef38689943fd47751f6e50a1c5f2ee9fb561e5881b38f cni.projectcalico.org/podIP:10.100.206.20/32 cni.projectcalico.org/podIPs:10.100.206.20/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eac810 0xc004eac811}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 07:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 07:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.206.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kwhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kwhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.206.20,StartTime:2023-01-17 07:40:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:40:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4e328ee6b02ab870100250ec90bd18389972981ae06a8841380ac5c00ed52835,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.206.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-vnf7c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vnf7c webserver-deployment-845c8977d9- deployment-8109  bf6c440a-7a1d-4b48-8de4-b1ceda1eeeb4 30636 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eaca10 0xc004eaca11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67gq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67gq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.22,PodIP:,StartTime:2023-01-17 07:40:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-x9cst" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x9cst webserver-deployment-845c8977d9- deployment-8109  ff7910fa-cb24-49df-8a2d-c01b031a0a46 30654 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eacbc0 0xc004eacbc1}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bzxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bzxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:40:36.015: INFO: Pod "webserver-deployment-845c8977d9-zbkmt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zbkmt webserver-deployment-845c8977d9- deployment-8109  96d30ec5-549f-44ea-bd54-12310790a79a 30653 0 2023-01-17 07:40:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91d42e39-73ca-4428-8216-bf5e1d760b13 0xc004eacd10 0xc004eacd11}] [] [{kube-controller-manager Update v1 2023-01-17 07:40:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91d42e39-73ca-4428-8216-bf5e1d760b13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xz8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xz8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:40:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:40:36.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8109" for this suite. 01/17/23 07:40:36.049
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:36.09
Jan 17 07:40:36.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:40:36.095
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:36.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:36.148
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-247/configmap-test-630b044c-db6b-4bd2-87d2-c83bda546a9e 01/17/23 07:40:36.17
STEP: Creating a pod to test consume configMaps 01/17/23 07:40:36.182
Jan 17 07:40:36.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e" in namespace "configmap-247" to be "Succeeded or Failed"
Jan 17 07:40:36.234: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.863447ms
Jan 17 07:40:38.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023010271s
Jan 17 07:40:40.242: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024790765s
Jan 17 07:40:42.307: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089514078s
Jan 17 07:40:44.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023125279s
STEP: Saw pod success 01/17/23 07:40:44.24
Jan 17 07:40:44.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e" satisfied condition "Succeeded or Failed"
Jan 17 07:40:44.250: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e container env-test: <nil>
STEP: delete the pod 01/17/23 07:40:44.265
Jan 17 07:40:44.290: INFO: Waiting for pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e to disappear
Jan 17 07:40:44.298: INFO: Pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:40:44.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-247" for this suite. 01/17/23 07:40:44.304
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":250,"skipped":4368,"failed":0}
------------------------------
• [SLOW TEST] [8.225 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:36.09
    Jan 17 07:40:36.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:40:36.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:36.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:36.148
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-247/configmap-test-630b044c-db6b-4bd2-87d2-c83bda546a9e 01/17/23 07:40:36.17
    STEP: Creating a pod to test consume configMaps 01/17/23 07:40:36.182
    Jan 17 07:40:36.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e" in namespace "configmap-247" to be "Succeeded or Failed"
    Jan 17 07:40:36.234: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.863447ms
    Jan 17 07:40:38.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023010271s
    Jan 17 07:40:40.242: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024790765s
    Jan 17 07:40:42.307: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089514078s
    Jan 17 07:40:44.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023125279s
    STEP: Saw pod success 01/17/23 07:40:44.24
    Jan 17 07:40:44.240: INFO: Pod "pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e" satisfied condition "Succeeded or Failed"
    Jan 17 07:40:44.250: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e container env-test: <nil>
    STEP: delete the pod 01/17/23 07:40:44.265
    Jan 17 07:40:44.290: INFO: Waiting for pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e to disappear
    Jan 17 07:40:44.298: INFO: Pod pod-configmaps-4c8f3c7b-4af6-4637-85a8-3c23ac82a29e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:40:44.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-247" for this suite. 01/17/23 07:40:44.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:44.316
Jan 17 07:40:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:40:44.317
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:44.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:44.376
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/17/23 07:40:44.391
STEP: waiting for Deployment to be created 01/17/23 07:40:44.401
STEP: waiting for all Replicas to be Ready 01/17/23 07:40:44.405
Jan 17 07:40:44.409: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.409: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.432: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.432: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.461: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.461: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.501: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:44.501: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 07:40:46.510: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 07:40:46.510: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 07:40:49.666: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/17/23 07:40:49.666
W0117 07:40:49.698081      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 07:40:49.702: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/17/23 07:40:49.702
Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.720: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.720: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.769: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.769: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:49.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:49.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:49.821: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:49.821: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:51.675: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:51.675: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:51.721: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
STEP: listing Deployments 01/17/23 07:40:51.721
Jan 17 07:40:51.732: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/17/23 07:40:51.733
Jan 17 07:40:51.766: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/17/23 07:40:51.766
Jan 17 07:40:51.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:51.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:51.852: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:51.863: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:51.890: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:53.386: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:53.468: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:53.474: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 07:40:54.415: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/17/23 07:40:54.479
STEP: fetching the DeploymentStatus 01/17/23 07:40:54.493
Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 3
STEP: deleting the Deployment 01/17/23 07:40:54.507
Jan 17 07:40:54.562: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
Jan 17 07:40:54.563: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:40:54.579: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:40:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5473" for this suite. 01/17/23 07:40:54.629
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":251,"skipped":4379,"failed":0}
------------------------------
• [SLOW TEST] [10.335 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:44.316
    Jan 17 07:40:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:40:44.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:44.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:44.376
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/17/23 07:40:44.391
    STEP: waiting for Deployment to be created 01/17/23 07:40:44.401
    STEP: waiting for all Replicas to be Ready 01/17/23 07:40:44.405
    Jan 17 07:40:44.409: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.409: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.432: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.432: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.461: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.461: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.501: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:44.501: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 07:40:46.510: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 17 07:40:46.510: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 17 07:40:49.666: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/17/23 07:40:49.666
    W0117 07:40:49.698081      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 07:40:49.702: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/17/23 07:40:49.702
    Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.704: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 0
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.705: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.720: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.720: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.769: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.769: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:49.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:49.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:49.821: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:49.821: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:51.675: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:51.675: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:51.721: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    STEP: listing Deployments 01/17/23 07:40:51.721
    Jan 17 07:40:51.732: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/17/23 07:40:51.733
    Jan 17 07:40:51.766: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/17/23 07:40:51.766
    Jan 17 07:40:51.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:51.794: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:51.852: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:51.863: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:51.890: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:53.386: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:53.468: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:53.474: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 07:40:54.415: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/17/23 07:40:54.479
    STEP: fetching the DeploymentStatus 01/17/23 07:40:54.493
    Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:54.506: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 1
    Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 2
    Jan 17 07:40:54.507: INFO: observed Deployment test-deployment in namespace deployment-5473 with ReadyReplicas 3
    STEP: deleting the Deployment 01/17/23 07:40:54.507
    Jan 17 07:40:54.562: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    Jan 17 07:40:54.563: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:40:54.579: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:40:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5473" for this suite. 01/17/23 07:40:54.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:40:54.654
Jan 17 07:40:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:40:54.655
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:54.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:54.707
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 07:40:54.806: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 07:41:54.875: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/17/23 07:41:54.881
Jan 17 07:41:54.931: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 07:41:54.953: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 07:41:55.008: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 07:41:55.031: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 07:41:55.083: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 07:41:55.114: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/17/23 07:41:55.115
Jan 17 07:41:55.115: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:41:55.128: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.325836ms
Jan 17 07:41:57.140: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025523709s
Jan 17 07:41:59.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021701663s
Jan 17 07:42:01.134: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019389038s
Jan 17 07:42:03.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.028094541s
Jan 17 07:42:03.143: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 17 07:42:03.143: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:03.154: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.931159ms
Jan 17 07:42:03.154: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:42:03.154: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:03.166: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.410099ms
Jan 17 07:42:05.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018749754s
Jan 17 07:42:05.173: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:42:05.173: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:05.180: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.033588ms
Jan 17 07:42:05.180: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:42:05.180: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:05.188: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.57058ms
Jan 17 07:42:05.188: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:42:05.188: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:05.194: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.823089ms
Jan 17 07:42:05.194: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/17/23 07:42:05.194
Jan 17 07:42:05.220: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-863" to be "running"
Jan 17 07:42:05.238: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.982981ms
Jan 17 07:42:07.255: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035106394s
Jan 17 07:42:09.245: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024675816s
Jan 17 07:42:09.245: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:42:09.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-863" for this suite. 01/17/23 07:42:09.299
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":252,"skipped":4390,"failed":0}
------------------------------
• [SLOW TEST] [74.763 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:40:54.654
    Jan 17 07:40:54.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:40:54.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:40:54.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:40:54.707
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 07:40:54.806: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 07:41:54.875: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/17/23 07:41:54.881
    Jan 17 07:41:54.931: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 17 07:41:54.953: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 17 07:41:55.008: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 17 07:41:55.031: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 17 07:41:55.083: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 17 07:41:55.114: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/17/23 07:41:55.115
    Jan 17 07:41:55.115: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:41:55.128: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.325836ms
    Jan 17 07:41:57.140: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025523709s
    Jan 17 07:41:59.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021701663s
    Jan 17 07:42:01.134: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019389038s
    Jan 17 07:42:03.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.028094541s
    Jan 17 07:42:03.143: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 17 07:42:03.143: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:03.154: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.931159ms
    Jan 17 07:42:03.154: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:42:03.154: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:03.166: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.410099ms
    Jan 17 07:42:05.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018749754s
    Jan 17 07:42:05.173: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:42:05.173: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:05.180: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.033588ms
    Jan 17 07:42:05.180: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:42:05.180: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:05.188: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.57058ms
    Jan 17 07:42:05.188: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:42:05.188: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:05.194: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.823089ms
    Jan 17 07:42:05.194: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/17/23 07:42:05.194
    Jan 17 07:42:05.220: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-863" to be "running"
    Jan 17 07:42:05.238: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.982981ms
    Jan 17 07:42:07.255: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035106394s
    Jan 17 07:42:09.245: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024675816s
    Jan 17 07:42:09.245: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:42:09.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-863" for this suite. 01/17/23 07:42:09.299
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:09.42
Jan 17 07:42:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 07:42:09.422
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:09.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:09.46
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/17/23 07:42:09.469
STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 07:42:09.484
STEP: delete the deployment 01/17/23 07:42:10.012
STEP: wait for all rs to be garbage collected 01/17/23 07:42:10.026
STEP: expected 0 rs, got 1 rs 01/17/23 07:42:10.038
STEP: expected 0 pods, got 2 pods 01/17/23 07:42:10.051
STEP: Gathering metrics 01/17/23 07:42:10.564
W0117 07:42:10.601113      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 07:42:10.601: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 07:42:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-978" for this suite. 01/17/23 07:42:10.613
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":253,"skipped":4399,"failed":0}
------------------------------
• [1.209 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:09.42
    Jan 17 07:42:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 07:42:09.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:09.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:09.46
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/17/23 07:42:09.469
    STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 07:42:09.484
    STEP: delete the deployment 01/17/23 07:42:10.012
    STEP: wait for all rs to be garbage collected 01/17/23 07:42:10.026
    STEP: expected 0 rs, got 1 rs 01/17/23 07:42:10.038
    STEP: expected 0 pods, got 2 pods 01/17/23 07:42:10.051
    STEP: Gathering metrics 01/17/23 07:42:10.564
    W0117 07:42:10.601113      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 07:42:10.601: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 07:42:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-978" for this suite. 01/17/23 07:42:10.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:10.63
Jan 17 07:42:10.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:42:10.631
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:10.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:10.687
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/17/23 07:42:10.699
STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:42:10.713
STEP: Creating a ResourceQuota with not terminating scope 01/17/23 07:42:12.721
STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:42:12.754
STEP: Creating a long running pod 01/17/23 07:42:14.764
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/17/23 07:42:14.812
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/17/23 07:42:16.829
STEP: Deleting the pod 01/17/23 07:42:18.835
STEP: Ensuring resource quota status released the pod usage 01/17/23 07:42:18.87
STEP: Creating a terminating pod 01/17/23 07:42:20.876
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/17/23 07:42:20.897
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/17/23 07:42:22.905
STEP: Deleting the pod 01/17/23 07:42:24.912
STEP: Ensuring resource quota status released the pod usage 01/17/23 07:42:24.94
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:42:26.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1736" for this suite. 01/17/23 07:42:26.956
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":254,"skipped":4423,"failed":0}
------------------------------
• [SLOW TEST] [16.340 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:10.63
    Jan 17 07:42:10.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:42:10.631
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:10.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:10.687
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/17/23 07:42:10.699
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:42:10.713
    STEP: Creating a ResourceQuota with not terminating scope 01/17/23 07:42:12.721
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 07:42:12.754
    STEP: Creating a long running pod 01/17/23 07:42:14.764
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/17/23 07:42:14.812
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/17/23 07:42:16.829
    STEP: Deleting the pod 01/17/23 07:42:18.835
    STEP: Ensuring resource quota status released the pod usage 01/17/23 07:42:18.87
    STEP: Creating a terminating pod 01/17/23 07:42:20.876
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/17/23 07:42:20.897
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/17/23 07:42:22.905
    STEP: Deleting the pod 01/17/23 07:42:24.912
    STEP: Ensuring resource quota status released the pod usage 01/17/23 07:42:24.94
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:42:26.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1736" for this suite. 01/17/23 07:42:26.956
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:26.971
Jan 17 07:42:26.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:42:26.972
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:27.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:27.02
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:42:27.031
Jan 17 07:42:27.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e" in namespace "downward-api-2681" to be "Succeeded or Failed"
Jan 17 07:42:27.070: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.750142ms
Jan 17 07:42:29.077: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021684557s
Jan 17 07:42:31.090: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034804567s
STEP: Saw pod success 01/17/23 07:42:31.09
Jan 17 07:42:31.091: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e" satisfied condition "Succeeded or Failed"
Jan 17 07:42:31.097: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e container client-container: <nil>
STEP: delete the pod 01/17/23 07:42:31.182
Jan 17 07:42:31.212: INFO: Waiting for pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e to disappear
Jan 17 07:42:31.219: INFO: Pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:42:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2681" for this suite. 01/17/23 07:42:31.226
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":255,"skipped":4426,"failed":0}
------------------------------
• [4.269 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:26.971
    Jan 17 07:42:26.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:42:26.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:27.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:27.02
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:42:27.031
    Jan 17 07:42:27.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e" in namespace "downward-api-2681" to be "Succeeded or Failed"
    Jan 17 07:42:27.070: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.750142ms
    Jan 17 07:42:29.077: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021684557s
    Jan 17 07:42:31.090: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034804567s
    STEP: Saw pod success 01/17/23 07:42:31.09
    Jan 17 07:42:31.091: INFO: Pod "downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e" satisfied condition "Succeeded or Failed"
    Jan 17 07:42:31.097: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e container client-container: <nil>
    STEP: delete the pod 01/17/23 07:42:31.182
    Jan 17 07:42:31.212: INFO: Waiting for pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e to disappear
    Jan 17 07:42:31.219: INFO: Pod downwardapi-volume-d6941a80-5426-467f-aa42-d9661f077b8e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:42:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2681" for this suite. 01/17/23 07:42:31.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:31.24
Jan 17 07:42:31.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:42:31.242
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.286
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/17/23 07:42:31.308
STEP: Getting a ResourceQuota 01/17/23 07:42:31.322
STEP: Updating a ResourceQuota 01/17/23 07:42:31.334
STEP: Verifying a ResourceQuota was modified 01/17/23 07:42:31.346
STEP: Deleting a ResourceQuota 01/17/23 07:42:31.354
STEP: Verifying the deleted ResourceQuota 01/17/23 07:42:31.371
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:42:31.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8454" for this suite. 01/17/23 07:42:31.383
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":256,"skipped":4433,"failed":0}
------------------------------
• [0.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:31.24
    Jan 17 07:42:31.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:42:31.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.286
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/17/23 07:42:31.308
    STEP: Getting a ResourceQuota 01/17/23 07:42:31.322
    STEP: Updating a ResourceQuota 01/17/23 07:42:31.334
    STEP: Verifying a ResourceQuota was modified 01/17/23 07:42:31.346
    STEP: Deleting a ResourceQuota 01/17/23 07:42:31.354
    STEP: Verifying the deleted ResourceQuota 01/17/23 07:42:31.371
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:42:31.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8454" for this suite. 01/17/23 07:42:31.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:31.409
Jan 17 07:42:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:42:31.412
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.457
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/17/23 07:42:31.465
STEP: submitting the pod to kubernetes 01/17/23 07:42:31.465
STEP: verifying QOS class is set on the pod 01/17/23 07:42:31.488
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 17 07:42:31.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9896" for this suite. 01/17/23 07:42:31.513
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":257,"skipped":4456,"failed":0}
------------------------------
• [0.121 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:31.409
    Jan 17 07:42:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:42:31.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.457
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/17/23 07:42:31.465
    STEP: submitting the pod to kubernetes 01/17/23 07:42:31.465
    STEP: verifying QOS class is set on the pod 01/17/23 07:42:31.488
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 17 07:42:31.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9896" for this suite. 01/17/23 07:42:31.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:42:31.532
Jan 17 07:42:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 07:42:31.537
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.578
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2829 01/17/23 07:42:31.586
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/17/23 07:42:31.6
STEP: Creating stateful set ss in namespace statefulset-2829 01/17/23 07:42:31.621
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2829 01/17/23 07:42:31.639
Jan 17 07:42:31.648: INFO: Found 0 stateful pods, waiting for 1
Jan 17 07:42:41.654: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/17/23 07:42:41.654
Jan 17 07:42:41.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:42:41.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:42:41.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:42:41.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:42:41.932: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 07:42:51.938: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:42:51.938: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:42:51.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999518s
Jan 17 07:42:52.978: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994406414s
Jan 17 07:42:53.984: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985920193s
Jan 17 07:42:54.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980499225s
Jan 17 07:42:56.008: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974941281s
Jan 17 07:42:57.015: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956615098s
Jan 17 07:42:58.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948903447s
Jan 17 07:42:59.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.942025923s
Jan 17 07:43:00.036: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936366268s
Jan 17 07:43:01.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.177523ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2829 01/17/23 07:43:02.043
Jan 17 07:43:02.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:43:02.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:43:02.388: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:43:02.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:43:02.393: INFO: Found 1 stateful pods, waiting for 3
Jan 17 07:43:12.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:43:12.402: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 07:43:12.402: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/17/23 07:43:12.402
STEP: Scale down will halt with unhealthy stateful pod 01/17/23 07:43:12.403
Jan 17 07:43:12.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:43:12.686: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:43:12.686: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:43:12.686: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:43:12.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:43:12.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:43:12.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:43:12.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:43:12.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 07:43:13.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 07:43:13.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 07:43:13.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 07:43:13.203: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:43:13.209: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 17 07:43:23.221: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:43:23.221: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:43:23.221: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 07:43:23.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999605s
Jan 17 07:43:24.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986572601s
Jan 17 07:43:25.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981323461s
Jan 17 07:43:26.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974443646s
Jan 17 07:43:27.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969317271s
Jan 17 07:43:28.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963088534s
Jan 17 07:43:29.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95702076s
Jan 17 07:43:30.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949595796s
Jan 17 07:43:31.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.943059785s
Jan 17 07:43:32.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.995661ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2829 01/17/23 07:43:33.321
Jan 17 07:43:33.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:43:33.584: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:43:33.584: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:43:33.584: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:43:33.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:43:33.865: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:43:33.865: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:43:33.865: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:43:33.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 07:43:34.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 07:43:34.147: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 07:43:34.147: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 07:43:34.147: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/17/23 07:43:44.186
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 07:43:44.186: INFO: Deleting all statefulset in ns statefulset-2829
Jan 17 07:43:44.192: INFO: Scaling statefulset ss to 0
Jan 17 07:43:44.210: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:43:44.216: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 07:43:44.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2829" for this suite. 01/17/23 07:43:44.254
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":258,"skipped":4488,"failed":0}
------------------------------
• [SLOW TEST] [72.738 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:42:31.532
    Jan 17 07:42:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 07:42:31.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:42:31.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:42:31.578
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2829 01/17/23 07:42:31.586
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/17/23 07:42:31.6
    STEP: Creating stateful set ss in namespace statefulset-2829 01/17/23 07:42:31.621
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2829 01/17/23 07:42:31.639
    Jan 17 07:42:31.648: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 07:42:41.654: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/17/23 07:42:41.654
    Jan 17 07:42:41.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:42:41.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:42:41.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:42:41.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:42:41.932: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 17 07:42:51.938: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:42:51.938: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:42:51.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999518s
    Jan 17 07:42:52.978: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994406414s
    Jan 17 07:42:53.984: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985920193s
    Jan 17 07:42:54.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980499225s
    Jan 17 07:42:56.008: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974941281s
    Jan 17 07:42:57.015: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956615098s
    Jan 17 07:42:58.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948903447s
    Jan 17 07:42:59.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.942025923s
    Jan 17 07:43:00.036: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.936366268s
    Jan 17 07:43:01.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.177523ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2829 01/17/23 07:43:02.043
    Jan 17 07:43:02.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:43:02.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:43:02.388: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:43:02.388: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:43:02.393: INFO: Found 1 stateful pods, waiting for 3
    Jan 17 07:43:12.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:43:12.402: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 07:43:12.402: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/17/23 07:43:12.402
    STEP: Scale down will halt with unhealthy stateful pod 01/17/23 07:43:12.403
    Jan 17 07:43:12.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:43:12.686: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:43:12.686: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:43:12.686: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:43:12.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:43:12.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:43:12.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:43:12.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:43:12.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 07:43:13.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 07:43:13.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 07:43:13.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 07:43:13.203: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:43:13.209: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 17 07:43:23.221: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:43:23.221: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:43:23.221: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 07:43:23.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999605s
    Jan 17 07:43:24.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986572601s
    Jan 17 07:43:25.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981323461s
    Jan 17 07:43:26.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974443646s
    Jan 17 07:43:27.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969317271s
    Jan 17 07:43:28.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963088534s
    Jan 17 07:43:29.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95702076s
    Jan 17 07:43:30.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949595796s
    Jan 17 07:43:31.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.943059785s
    Jan 17 07:43:32.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.995661ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2829 01/17/23 07:43:33.321
    Jan 17 07:43:33.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:43:33.584: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:43:33.584: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:43:33.584: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:43:33.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:43:33.865: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:43:33.865: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:43:33.865: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:43:33.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=statefulset-2829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 07:43:34.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 07:43:34.147: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 07:43:34.147: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 07:43:34.147: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/17/23 07:43:44.186
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 07:43:44.186: INFO: Deleting all statefulset in ns statefulset-2829
    Jan 17 07:43:44.192: INFO: Scaling statefulset ss to 0
    Jan 17 07:43:44.210: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:43:44.216: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 07:43:44.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2829" for this suite. 01/17/23 07:43:44.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:43:44.274
Jan 17 07:43:44.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:43:44.276
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:43:44.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:43:44.318
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 07:43:44.354: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 07:44:44.411: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/17/23 07:44:44.417
Jan 17 07:44:44.452: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 07:44:44.470: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 07:44:44.522: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 07:44:44.547: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 07:44:44.599: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 07:44:44.617: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/17/23 07:44:44.617
Jan 17 07:44:44.617: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:44.628: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.157568ms
Jan 17 07:44:46.636: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018928972s
Jan 17 07:44:46.636: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 17 07:44:46.636: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:46.644: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.845717ms
Jan 17 07:44:46.644: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:44:46.644: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:46.650: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.957352ms
Jan 17 07:44:46.651: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:44:46.651: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:46.657: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.021899ms
Jan 17 07:44:46.657: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:44:46.657: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:46.666: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.687698ms
Jan 17 07:44:46.666: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 07:44:46.666: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
Jan 17 07:44:46.673: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.658859ms
Jan 17 07:44:46.673: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/17/23 07:44:46.673
Jan 17 07:44:46.697: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 17 07:44:46.708: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.432928ms
Jan 17 07:44:48.713: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016211455s
Jan 17 07:44:50.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017357873s
Jan 17 07:44:52.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017000441s
Jan 17 07:44:54.719: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.022054004s
Jan 17 07:44:54.719: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:44:54.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7207" for this suite. 01/17/23 07:44:54.789
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":259,"skipped":4504,"failed":0}
------------------------------
• [SLOW TEST] [70.649 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:43:44.274
    Jan 17 07:43:44.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 07:43:44.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:43:44.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:43:44.318
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 07:43:44.354: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 07:44:44.411: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/17/23 07:44:44.417
    Jan 17 07:44:44.452: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 17 07:44:44.470: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 17 07:44:44.522: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 17 07:44:44.547: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 17 07:44:44.599: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 17 07:44:44.617: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/17/23 07:44:44.617
    Jan 17 07:44:44.617: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:44.628: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.157568ms
    Jan 17 07:44:46.636: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018928972s
    Jan 17 07:44:46.636: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 17 07:44:46.636: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:46.644: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.845717ms
    Jan 17 07:44:46.644: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:44:46.644: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:46.650: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.957352ms
    Jan 17 07:44:46.651: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:44:46.651: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:46.657: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.021899ms
    Jan 17 07:44:46.657: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:44:46.657: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:46.666: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.687698ms
    Jan 17 07:44:46.666: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 07:44:46.666: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7207" to be "running"
    Jan 17 07:44:46.673: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.658859ms
    Jan 17 07:44:46.673: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/17/23 07:44:46.673
    Jan 17 07:44:46.697: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 17 07:44:46.708: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.432928ms
    Jan 17 07:44:48.713: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016211455s
    Jan 17 07:44:50.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017357873s
    Jan 17 07:44:52.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017000441s
    Jan 17 07:44:54.719: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.022054004s
    Jan 17 07:44:54.719: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:44:54.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7207" for this suite. 01/17/23 07:44:54.789
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:44:54.924
Jan 17 07:44:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:44:54.926
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:44:54.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:44:54.967
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:44:54.974
Jan 17 07:44:54.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c" in namespace "downward-api-5128" to be "Succeeded or Failed"
Jan 17 07:44:55.006: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.829894ms
Jan 17 07:44:57.012: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501185s
Jan 17 07:44:59.024: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030256517s
STEP: Saw pod success 01/17/23 07:44:59.024
Jan 17 07:44:59.024: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c" satisfied condition "Succeeded or Failed"
Jan 17 07:44:59.030: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c container client-container: <nil>
STEP: delete the pod 01/17/23 07:44:59.114
Jan 17 07:44:59.145: INFO: Waiting for pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c to disappear
Jan 17 07:44:59.152: INFO: Pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:44:59.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5128" for this suite. 01/17/23 07:44:59.159
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":260,"skipped":4504,"failed":0}
------------------------------
• [4.251 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:44:54.924
    Jan 17 07:44:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:44:54.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:44:54.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:44:54.967
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:44:54.974
    Jan 17 07:44:54.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c" in namespace "downward-api-5128" to be "Succeeded or Failed"
    Jan 17 07:44:55.006: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.829894ms
    Jan 17 07:44:57.012: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501185s
    Jan 17 07:44:59.024: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030256517s
    STEP: Saw pod success 01/17/23 07:44:59.024
    Jan 17 07:44:59.024: INFO: Pod "downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c" satisfied condition "Succeeded or Failed"
    Jan 17 07:44:59.030: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c container client-container: <nil>
    STEP: delete the pod 01/17/23 07:44:59.114
    Jan 17 07:44:59.145: INFO: Waiting for pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c to disappear
    Jan 17 07:44:59.152: INFO: Pod downwardapi-volume-d86a424d-2e4f-45b1-b352-924d64a5bb5c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:44:59.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5128" for this suite. 01/17/23 07:44:59.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:44:59.176
Jan 17 07:44:59.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-webhook 01/17/23 07:44:59.178
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:44:59.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:44:59.224
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/17/23 07:44:59.232
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 07:44:59.94
STEP: Deploying the custom resource conversion webhook pod 01/17/23 07:44:59.956
STEP: Wait for the deployment to be ready 01/17/23 07:44:59.984
Jan 17 07:45:00.022: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 17 07:45:02.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 44, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/17/23 07:45:04.055
STEP: Verifying the service has paired with the endpoint 01/17/23 07:45:04.09
Jan 17 07:45:05.091: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 17 07:45:05.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Creating a v1 custom resource 01/17/23 07:45:07.738
STEP: v2 custom resource should be converted 01/17/23 07:45:07.755
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:45:08.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3377" for this suite. 01/17/23 07:45:08.314
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":261,"skipped":4516,"failed":0}
------------------------------
• [SLOW TEST] [9.338 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:44:59.176
    Jan 17 07:44:59.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-webhook 01/17/23 07:44:59.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:44:59.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:44:59.224
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/17/23 07:44:59.232
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 07:44:59.94
    STEP: Deploying the custom resource conversion webhook pod 01/17/23 07:44:59.956
    STEP: Wait for the deployment to be ready 01/17/23 07:44:59.984
    Jan 17 07:45:00.022: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 17 07:45:02.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 44, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/17/23 07:45:04.055
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:45:04.09
    Jan 17 07:45:05.091: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 17 07:45:05.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Creating a v1 custom resource 01/17/23 07:45:07.738
    STEP: v2 custom resource should be converted 01/17/23 07:45:07.755
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:45:08.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3377" for this suite. 01/17/23 07:45:08.314
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:45:08.518
Jan 17 07:45:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:45:08.521
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:08.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:08.638
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 17 07:45:08.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7730 version'
Jan 17 07:45:08.856: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 17 07:45:08.856: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:45:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7730" for this suite. 01/17/23 07:45:08.865
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":262,"skipped":4584,"failed":0}
------------------------------
• [0.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:45:08.518
    Jan 17 07:45:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:45:08.521
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:08.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:08.638
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 17 07:45:08.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7730 version'
    Jan 17 07:45:08.856: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 17 07:45:08.856: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:45:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7730" for this suite. 01/17/23 07:45:08.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:45:08.882
Jan 17 07:45:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename containers 01/17/23 07:45:08.884
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:08.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:08.97
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 17 07:45:09.002: INFO: Waiting up to 5m0s for pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7" in namespace "containers-8226" to be "running"
Jan 17 07:45:09.038: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 35.232358ms
Jan 17 07:45:11.049: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.046469081s
Jan 17 07:45:11.049: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 07:45:11.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8226" for this suite. 01/17/23 07:45:11.076
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":263,"skipped":4623,"failed":0}
------------------------------
• [2.210 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:45:08.882
    Jan 17 07:45:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename containers 01/17/23 07:45:08.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:08.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:08.97
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 17 07:45:09.002: INFO: Waiting up to 5m0s for pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7" in namespace "containers-8226" to be "running"
    Jan 17 07:45:09.038: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 35.232358ms
    Jan 17 07:45:11.049: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.046469081s
    Jan 17 07:45:11.049: INFO: Pod "client-containers-d6759213-bf83-48c0-9d11-e03f3ed5fbf7" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 07:45:11.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8226" for this suite. 01/17/23 07:45:11.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:45:11.103
Jan 17 07:45:11.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:45:11.104
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:11.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:11.151
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/17/23 07:45:11.164
Jan 17 07:45:11.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 create -f -'
Jan 17 07:45:12.583: INFO: stderr: ""
Jan 17 07:45:12.583: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/17/23 07:45:12.583
Jan 17 07:45:12.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 diff -f -'
Jan 17 07:45:14.708: INFO: rc: 1
Jan 17 07:45:14.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 delete -f -'
Jan 17 07:45:14.835: INFO: stderr: ""
Jan 17 07:45:14.835: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:45:14.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7264" for this suite. 01/17/23 07:45:14.846
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":264,"skipped":4760,"failed":0}
------------------------------
• [3.762 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:45:11.103
    Jan 17 07:45:11.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:45:11.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:11.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:11.151
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/17/23 07:45:11.164
    Jan 17 07:45:11.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 create -f -'
    Jan 17 07:45:12.583: INFO: stderr: ""
    Jan 17 07:45:12.583: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/17/23 07:45:12.583
    Jan 17 07:45:12.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 diff -f -'
    Jan 17 07:45:14.708: INFO: rc: 1
    Jan 17 07:45:14.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7264 delete -f -'
    Jan 17 07:45:14.835: INFO: stderr: ""
    Jan 17 07:45:14.835: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:45:14.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7264" for this suite. 01/17/23 07:45:14.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:45:14.867
Jan 17 07:45:14.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context 01/17/23 07:45:14.869
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:14.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:14.911
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 07:45:14.918
Jan 17 07:45:14.943: INFO: Waiting up to 5m0s for pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633" in namespace "security-context-605" to be "Succeeded or Failed"
Jan 17 07:45:14.952: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Pending", Reason="", readiness=false. Elapsed: 8.442187ms
Jan 17 07:45:16.984: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0402549s
Jan 17 07:45:18.958: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014207319s
STEP: Saw pod success 01/17/23 07:45:18.958
Jan 17 07:45:18.958: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633" satisfied condition "Succeeded or Failed"
Jan 17 07:45:18.964: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 container test-container: <nil>
STEP: delete the pod 01/17/23 07:45:19.04
Jan 17 07:45:19.066: INFO: Waiting for pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 to disappear
Jan 17 07:45:19.072: INFO: Pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 07:45:19.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-605" for this suite. 01/17/23 07:45:19.079
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":265,"skipped":4771,"failed":0}
------------------------------
• [4.228 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:45:14.867
    Jan 17 07:45:14.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context 01/17/23 07:45:14.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:14.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:14.911
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 07:45:14.918
    Jan 17 07:45:14.943: INFO: Waiting up to 5m0s for pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633" in namespace "security-context-605" to be "Succeeded or Failed"
    Jan 17 07:45:14.952: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Pending", Reason="", readiness=false. Elapsed: 8.442187ms
    Jan 17 07:45:16.984: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0402549s
    Jan 17 07:45:18.958: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014207319s
    STEP: Saw pod success 01/17/23 07:45:18.958
    Jan 17 07:45:18.958: INFO: Pod "security-context-fbe63646-7b02-42a1-9dd4-6af131442633" satisfied condition "Succeeded or Failed"
    Jan 17 07:45:18.964: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:45:19.04
    Jan 17 07:45:19.066: INFO: Waiting for pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 to disappear
    Jan 17 07:45:19.072: INFO: Pod security-context-fbe63646-7b02-42a1-9dd4-6af131442633 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 07:45:19.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-605" for this suite. 01/17/23 07:45:19.079
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:45:19.095
Jan 17 07:45:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 07:45:19.096
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:19.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:19.132
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 in namespace container-probe-9735 01/17/23 07:45:19.139
Jan 17 07:45:19.164: INFO: Waiting up to 5m0s for pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83" in namespace "container-probe-9735" to be "not pending"
Jan 17 07:45:19.178: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83": Phase="Pending", Reason="", readiness=false. Elapsed: 14.733976ms
Jan 17 07:45:21.185: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83": Phase="Running", Reason="", readiness=true. Elapsed: 2.021084375s
Jan 17 07:45:21.185: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83" satisfied condition "not pending"
Jan 17 07:45:21.185: INFO: Started pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 in namespace container-probe-9735
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:45:21.186
Jan 17 07:45:21.198: INFO: Initial restart count of pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is 0
Jan 17 07:45:41.417: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 1 (20.219044453s elapsed)
Jan 17 07:46:01.505: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 2 (40.30747058s elapsed)
Jan 17 07:46:21.602: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 3 (1m0.404669896s elapsed)
Jan 17 07:46:41.692: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 4 (1m20.494427706s elapsed)
Jan 17 07:47:49.935: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 5 (2m28.737009368s elapsed)
STEP: deleting the pod 01/17/23 07:47:49.935
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 07:47:49.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9735" for this suite. 01/17/23 07:47:49.981
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":266,"skipped":4774,"failed":0}
------------------------------
• [SLOW TEST] [150.903 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:45:19.095
    Jan 17 07:45:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 07:45:19.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:45:19.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:45:19.132
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 in namespace container-probe-9735 01/17/23 07:45:19.139
    Jan 17 07:45:19.164: INFO: Waiting up to 5m0s for pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83" in namespace "container-probe-9735" to be "not pending"
    Jan 17 07:45:19.178: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83": Phase="Pending", Reason="", readiness=false. Elapsed: 14.733976ms
    Jan 17 07:45:21.185: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83": Phase="Running", Reason="", readiness=true. Elapsed: 2.021084375s
    Jan 17 07:45:21.185: INFO: Pod "liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83" satisfied condition "not pending"
    Jan 17 07:45:21.185: INFO: Started pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 in namespace container-probe-9735
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 07:45:21.186
    Jan 17 07:45:21.198: INFO: Initial restart count of pod liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is 0
    Jan 17 07:45:41.417: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 1 (20.219044453s elapsed)
    Jan 17 07:46:01.505: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 2 (40.30747058s elapsed)
    Jan 17 07:46:21.602: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 3 (1m0.404669896s elapsed)
    Jan 17 07:46:41.692: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 4 (1m20.494427706s elapsed)
    Jan 17 07:47:49.935: INFO: Restart count of pod container-probe-9735/liveness-70b96ef4-084b-4526-9cf3-1bfc5ba4bd83 is now 5 (2m28.737009368s elapsed)
    STEP: deleting the pod 01/17/23 07:47:49.935
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 07:47:49.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9735" for this suite. 01/17/23 07:47:49.981
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:47:49.999
Jan 17 07:47:49.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:47:50.001
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:47:50.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:47:50.047
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/17/23 07:47:50.057
Jan 17 07:47:50.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 17 07:47:50.241: INFO: stderr: ""
Jan 17 07:47:50.241: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/17/23 07:47:50.241
Jan 17 07:47:50.241: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 17 07:47:50.242: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8541" to be "running and ready, or succeeded"
Jan 17 07:47:50.263: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 21.207701ms
Jan 17 07:47:50.263: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cluster125-w73dz53kvqes-node-1' to be 'Running' but was 'Pending'
Jan 17 07:47:52.271: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029859338s
Jan 17 07:47:52.271: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 17 07:47:52.271: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/17/23 07:47:52.271
Jan 17 07:47:52.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator'
Jan 17 07:47:52.507: INFO: stderr: ""
Jan 17 07:47:52.507: INFO: stdout: "I0117 07:47:51.345268       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z5 261\nI0117 07:47:51.545377       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9z2 227\nI0117 07:47:51.746105       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/h86 211\nI0117 07:47:51.945400       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/7zg 588\nI0117 07:47:52.146013       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/4d2 358\nI0117 07:47:52.345861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/nvg 225\n"
STEP: limiting log lines 01/17/23 07:47:52.507
Jan 17 07:47:52.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --tail=1'
Jan 17 07:47:52.768: INFO: stderr: ""
Jan 17 07:47:52.768: INFO: stdout: "I0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\n"
Jan 17 07:47:52.768: INFO: got output "I0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\n"
STEP: limiting log bytes 01/17/23 07:47:52.768
Jan 17 07:47:52.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --limit-bytes=1'
Jan 17 07:47:52.947: INFO: stderr: ""
Jan 17 07:47:52.947: INFO: stdout: "I"
Jan 17 07:47:52.947: INFO: got output "I"
STEP: exposing timestamps 01/17/23 07:47:52.947
Jan 17 07:47:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 17 07:47:53.078: INFO: stderr: ""
Jan 17 07:47:53.078: INFO: stdout: "2023-01-17T07:47:52.945867972Z I0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\n"
Jan 17 07:47:53.078: INFO: got output "2023-01-17T07:47:52.945867972Z I0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\n"
STEP: restricting to a time range 01/17/23 07:47:53.078
Jan 17 07:47:55.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --since=1s'
Jan 17 07:47:55.757: INFO: stderr: ""
Jan 17 07:47:55.757: INFO: stdout: "I0117 07:47:54.946151       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/cth 267\nI0117 07:47:55.145413       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4w8 355\nI0117 07:47:55.345816       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jmh 273\nI0117 07:47:55.546267       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/95v 452\nI0117 07:47:55.745713       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/x4n 304\n"
Jan 17 07:47:55.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --since=24h'
Jan 17 07:47:55.887: INFO: stderr: ""
Jan 17 07:47:55.887: INFO: stdout: "I0117 07:47:51.345268       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z5 261\nI0117 07:47:51.545377       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9z2 227\nI0117 07:47:51.746105       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/h86 211\nI0117 07:47:51.945400       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/7zg 588\nI0117 07:47:52.146013       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/4d2 358\nI0117 07:47:52.345861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/nvg 225\nI0117 07:47:52.547689       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/42zk 357\nI0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\nI0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\nI0117 07:47:53.145720       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/zs42 475\nI0117 07:47:53.346122       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/jsg 407\nI0117 07:47:53.545457       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/cvmt 468\nI0117 07:47:53.745868       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/b7xb 295\nI0117 07:47:53.946254       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/h26 540\nI0117 07:47:54.145637       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/2hl 284\nI0117 07:47:54.346044       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/6pl 343\nI0117 07:47:54.545325       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mk8z 472\nI0117 07:47:54.745752       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/68h6 367\nI0117 07:47:54.946151       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/cth 267\nI0117 07:47:55.145413       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4w8 355\nI0117 07:47:55.345816       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jmh 273\nI0117 07:47:55.546267       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/95v 452\nI0117 07:47:55.745713       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/x4n 304\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 17 07:47:55.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 delete pod logs-generator'
Jan 17 07:47:57.045: INFO: stderr: ""
Jan 17 07:47:57.045: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:47:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8541" for this suite. 01/17/23 07:47:57.063
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":267,"skipped":4774,"failed":0}
------------------------------
• [SLOW TEST] [7.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:47:49.999
    Jan 17 07:47:49.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:47:50.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:47:50.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:47:50.047
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/17/23 07:47:50.057
    Jan 17 07:47:50.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 17 07:47:50.241: INFO: stderr: ""
    Jan 17 07:47:50.241: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/17/23 07:47:50.241
    Jan 17 07:47:50.241: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 17 07:47:50.242: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8541" to be "running and ready, or succeeded"
    Jan 17 07:47:50.263: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 21.207701ms
    Jan 17 07:47:50.263: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cluster125-w73dz53kvqes-node-1' to be 'Running' but was 'Pending'
    Jan 17 07:47:52.271: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029859338s
    Jan 17 07:47:52.271: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 17 07:47:52.271: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/17/23 07:47:52.271
    Jan 17 07:47:52.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator'
    Jan 17 07:47:52.507: INFO: stderr: ""
    Jan 17 07:47:52.507: INFO: stdout: "I0117 07:47:51.345268       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z5 261\nI0117 07:47:51.545377       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9z2 227\nI0117 07:47:51.746105       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/h86 211\nI0117 07:47:51.945400       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/7zg 588\nI0117 07:47:52.146013       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/4d2 358\nI0117 07:47:52.345861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/nvg 225\n"
    STEP: limiting log lines 01/17/23 07:47:52.507
    Jan 17 07:47:52.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --tail=1'
    Jan 17 07:47:52.768: INFO: stderr: ""
    Jan 17 07:47:52.768: INFO: stdout: "I0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\n"
    Jan 17 07:47:52.768: INFO: got output "I0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\n"
    STEP: limiting log bytes 01/17/23 07:47:52.768
    Jan 17 07:47:52.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --limit-bytes=1'
    Jan 17 07:47:52.947: INFO: stderr: ""
    Jan 17 07:47:52.947: INFO: stdout: "I"
    Jan 17 07:47:52.947: INFO: got output "I"
    STEP: exposing timestamps 01/17/23 07:47:52.947
    Jan 17 07:47:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 17 07:47:53.078: INFO: stderr: ""
    Jan 17 07:47:53.078: INFO: stdout: "2023-01-17T07:47:52.945867972Z I0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\n"
    Jan 17 07:47:53.078: INFO: got output "2023-01-17T07:47:52.945867972Z I0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\n"
    STEP: restricting to a time range 01/17/23 07:47:53.078
    Jan 17 07:47:55.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --since=1s'
    Jan 17 07:47:55.757: INFO: stderr: ""
    Jan 17 07:47:55.757: INFO: stdout: "I0117 07:47:54.946151       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/cth 267\nI0117 07:47:55.145413       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4w8 355\nI0117 07:47:55.345816       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jmh 273\nI0117 07:47:55.546267       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/95v 452\nI0117 07:47:55.745713       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/x4n 304\n"
    Jan 17 07:47:55.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 logs logs-generator logs-generator --since=24h'
    Jan 17 07:47:55.887: INFO: stderr: ""
    Jan 17 07:47:55.887: INFO: stdout: "I0117 07:47:51.345268       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5z5 261\nI0117 07:47:51.545377       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/9z2 227\nI0117 07:47:51.746105       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/h86 211\nI0117 07:47:51.945400       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/7zg 588\nI0117 07:47:52.146013       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/4d2 358\nI0117 07:47:52.345861       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/nvg 225\nI0117 07:47:52.547689       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/42zk 357\nI0117 07:47:52.746088       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6np5 243\nI0117 07:47:52.945364       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/6tl4 591\nI0117 07:47:53.145720       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/zs42 475\nI0117 07:47:53.346122       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/jsg 407\nI0117 07:47:53.545457       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/cvmt 468\nI0117 07:47:53.745868       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/b7xb 295\nI0117 07:47:53.946254       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/h26 540\nI0117 07:47:54.145637       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/2hl 284\nI0117 07:47:54.346044       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/6pl 343\nI0117 07:47:54.545325       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mk8z 472\nI0117 07:47:54.745752       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/68h6 367\nI0117 07:47:54.946151       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/cth 267\nI0117 07:47:55.145413       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4w8 355\nI0117 07:47:55.345816       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jmh 273\nI0117 07:47:55.546267       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/95v 452\nI0117 07:47:55.745713       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/x4n 304\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 17 07:47:55.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-8541 delete pod logs-generator'
    Jan 17 07:47:57.045: INFO: stderr: ""
    Jan 17 07:47:57.045: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:47:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8541" for this suite. 01/17/23 07:47:57.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:47:57.091
Jan 17 07:47:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svc-latency 01/17/23 07:47:57.102
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:47:57.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:47:57.16
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 17 07:47:57.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7286 01/17/23 07:47:57.169
I0117 07:47:57.190496      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7286, replica count: 1
I0117 07:47:58.241373      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 07:47:59.242294      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:47:59.371: INFO: Created: latency-svc-z2xzz
Jan 17 07:47:59.392: INFO: Got endpoints: latency-svc-z2xzz [49.735382ms]
Jan 17 07:47:59.448: INFO: Created: latency-svc-nrj57
Jan 17 07:47:59.461: INFO: Got endpoints: latency-svc-nrj57 [67.73929ms]
Jan 17 07:47:59.469: INFO: Created: latency-svc-bc85l
Jan 17 07:47:59.493: INFO: Got endpoints: latency-svc-bc85l [99.318199ms]
Jan 17 07:47:59.510: INFO: Created: latency-svc-mgtnz
Jan 17 07:47:59.515: INFO: Got endpoints: latency-svc-mgtnz [122.031891ms]
Jan 17 07:47:59.527: INFO: Created: latency-svc-dr49f
Jan 17 07:47:59.538: INFO: Got endpoints: latency-svc-dr49f [144.874599ms]
Jan 17 07:47:59.558: INFO: Created: latency-svc-vxnmp
Jan 17 07:47:59.587: INFO: Got endpoints: latency-svc-vxnmp [193.849085ms]
Jan 17 07:47:59.600: INFO: Created: latency-svc-xn2bg
Jan 17 07:47:59.603: INFO: Got endpoints: latency-svc-xn2bg [210.373155ms]
Jan 17 07:47:59.642: INFO: Created: latency-svc-f6mmm
Jan 17 07:47:59.653: INFO: Got endpoints: latency-svc-f6mmm [259.297713ms]
Jan 17 07:47:59.680: INFO: Created: latency-svc-7sbws
Jan 17 07:47:59.696: INFO: Got endpoints: latency-svc-7sbws [301.822505ms]
Jan 17 07:47:59.722: INFO: Created: latency-svc-kqqtk
Jan 17 07:47:59.746: INFO: Got endpoints: latency-svc-kqqtk [352.570488ms]
Jan 17 07:47:59.753: INFO: Created: latency-svc-gc9g9
Jan 17 07:47:59.769: INFO: Got endpoints: latency-svc-gc9g9 [375.589251ms]
Jan 17 07:47:59.801: INFO: Created: latency-svc-jg4hk
Jan 17 07:47:59.810: INFO: Got endpoints: latency-svc-jg4hk [416.082172ms]
Jan 17 07:47:59.839: INFO: Created: latency-svc-9fh6l
Jan 17 07:47:59.841: INFO: Got endpoints: latency-svc-9fh6l [447.203886ms]
Jan 17 07:47:59.883: INFO: Created: latency-svc-jz6bl
Jan 17 07:47:59.892: INFO: Got endpoints: latency-svc-jz6bl [498.273469ms]
Jan 17 07:47:59.904: INFO: Created: latency-svc-9jvq6
Jan 17 07:47:59.918: INFO: Got endpoints: latency-svc-9jvq6 [524.034332ms]
Jan 17 07:47:59.928: INFO: Created: latency-svc-62ptf
Jan 17 07:47:59.940: INFO: Got endpoints: latency-svc-62ptf [546.034406ms]
Jan 17 07:47:59.949: INFO: Created: latency-svc-bqlj7
Jan 17 07:47:59.976: INFO: Got endpoints: latency-svc-bqlj7 [515.257077ms]
Jan 17 07:47:59.989: INFO: Created: latency-svc-t4btb
Jan 17 07:48:00.004: INFO: Got endpoints: latency-svc-t4btb [511.40613ms]
Jan 17 07:48:00.014: INFO: Created: latency-svc-dtwks
Jan 17 07:48:00.027: INFO: Got endpoints: latency-svc-dtwks [512.265495ms]
Jan 17 07:48:00.038: INFO: Created: latency-svc-lxtt6
Jan 17 07:48:00.055: INFO: Got endpoints: latency-svc-lxtt6 [516.867348ms]
Jan 17 07:48:00.067: INFO: Created: latency-svc-rdg77
Jan 17 07:48:00.076: INFO: Got endpoints: latency-svc-rdg77 [489.305159ms]
Jan 17 07:48:00.089: INFO: Created: latency-svc-tvpr9
Jan 17 07:48:00.107: INFO: Got endpoints: latency-svc-tvpr9 [503.819132ms]
Jan 17 07:48:00.116: INFO: Created: latency-svc-8bj2x
Jan 17 07:48:00.132: INFO: Got endpoints: latency-svc-8bj2x [478.120797ms]
Jan 17 07:48:00.165: INFO: Created: latency-svc-kvxr8
Jan 17 07:48:00.165: INFO: Got endpoints: latency-svc-kvxr8 [469.318669ms]
Jan 17 07:48:00.186: INFO: Created: latency-svc-rblvr
Jan 17 07:48:00.198: INFO: Got endpoints: latency-svc-rblvr [452.216402ms]
Jan 17 07:48:00.208: INFO: Created: latency-svc-7tfvh
Jan 17 07:48:00.221: INFO: Got endpoints: latency-svc-7tfvh [451.941299ms]
Jan 17 07:48:00.249: INFO: Created: latency-svc-dlfl5
Jan 17 07:48:00.249: INFO: Got endpoints: latency-svc-dlfl5 [439.043001ms]
Jan 17 07:48:00.320: INFO: Created: latency-svc-z8ncr
Jan 17 07:48:00.339: INFO: Got endpoints: latency-svc-z8ncr [497.780517ms]
Jan 17 07:48:00.349: INFO: Created: latency-svc-44gwv
Jan 17 07:48:00.359: INFO: Got endpoints: latency-svc-44gwv [467.411812ms]
Jan 17 07:48:00.392: INFO: Created: latency-svc-xd4d2
Jan 17 07:48:00.404: INFO: Got endpoints: latency-svc-xd4d2 [486.033096ms]
Jan 17 07:48:00.437: INFO: Created: latency-svc-bq46c
Jan 17 07:48:00.437: INFO: Got endpoints: latency-svc-bq46c [497.590046ms]
Jan 17 07:48:00.474: INFO: Created: latency-svc-bns5x
Jan 17 07:48:00.474: INFO: Got endpoints: latency-svc-bns5x [498.074836ms]
Jan 17 07:48:00.490: INFO: Created: latency-svc-6ql55
Jan 17 07:48:00.507: INFO: Got endpoints: latency-svc-6ql55 [502.708556ms]
Jan 17 07:48:00.522: INFO: Created: latency-svc-9ps54
Jan 17 07:48:00.524: INFO: Got endpoints: latency-svc-9ps54 [496.999935ms]
Jan 17 07:48:00.559: INFO: Created: latency-svc-l7b96
Jan 17 07:48:00.566: INFO: Got endpoints: latency-svc-l7b96 [511.167176ms]
Jan 17 07:48:00.588: INFO: Created: latency-svc-dgz2x
Jan 17 07:48:00.593: INFO: Got endpoints: latency-svc-dgz2x [516.674131ms]
Jan 17 07:48:00.614: INFO: Created: latency-svc-h8stk
Jan 17 07:48:00.654: INFO: Got endpoints: latency-svc-h8stk [546.968443ms]
Jan 17 07:48:00.655: INFO: Created: latency-svc-dt957
Jan 17 07:48:00.673: INFO: Got endpoints: latency-svc-dt957 [541.302843ms]
Jan 17 07:48:00.733: INFO: Created: latency-svc-d5wf4
Jan 17 07:48:00.733: INFO: Created: latency-svc-2sqrm
Jan 17 07:48:00.733: INFO: Got endpoints: latency-svc-d5wf4 [567.917138ms]
Jan 17 07:48:00.764: INFO: Got endpoints: latency-svc-2sqrm [565.482461ms]
Jan 17 07:48:00.781: INFO: Created: latency-svc-89q67
Jan 17 07:48:00.825: INFO: Got endpoints: latency-svc-89q67 [604.026507ms]
Jan 17 07:48:00.837: INFO: Created: latency-svc-vdbbm
Jan 17 07:48:00.858: INFO: Got endpoints: latency-svc-vdbbm [609.636127ms]
Jan 17 07:48:00.876: INFO: Created: latency-svc-dmbbn
Jan 17 07:48:00.893: INFO: Got endpoints: latency-svc-dmbbn [553.701861ms]
Jan 17 07:48:00.921: INFO: Created: latency-svc-q8hhx
Jan 17 07:48:00.922: INFO: Created: latency-svc-kmstr
Jan 17 07:48:00.933: INFO: Got endpoints: latency-svc-q8hhx [528.514111ms]
Jan 17 07:48:00.938: INFO: Got endpoints: latency-svc-kmstr [579.324598ms]
Jan 17 07:48:00.950: INFO: Created: latency-svc-jvjmp
Jan 17 07:48:00.973: INFO: Got endpoints: latency-svc-jvjmp [535.803253ms]
Jan 17 07:48:01.005: INFO: Created: latency-svc-jpxdd
Jan 17 07:48:01.024: INFO: Got endpoints: latency-svc-jpxdd [516.778669ms]
Jan 17 07:48:01.042: INFO: Created: latency-svc-6xbzf
Jan 17 07:48:01.051: INFO: Got endpoints: latency-svc-6xbzf [526.434866ms]
Jan 17 07:48:01.062: INFO: Created: latency-svc-hlskb
Jan 17 07:48:01.078: INFO: Got endpoints: latency-svc-hlskb [511.602655ms]
Jan 17 07:48:01.100: INFO: Created: latency-svc-4whp8
Jan 17 07:48:01.124: INFO: Got endpoints: latency-svc-4whp8 [530.530215ms]
Jan 17 07:48:01.148: INFO: Created: latency-svc-44b5x
Jan 17 07:48:01.164: INFO: Got endpoints: latency-svc-44b5x [510.654873ms]
Jan 17 07:48:01.174: INFO: Created: latency-svc-5jsx7
Jan 17 07:48:01.195: INFO: Got endpoints: latency-svc-5jsx7 [521.658384ms]
Jan 17 07:48:01.236: INFO: Created: latency-svc-2jhsh
Jan 17 07:48:01.239: INFO: Got endpoints: latency-svc-2jhsh [504.971835ms]
Jan 17 07:48:01.268: INFO: Created: latency-svc-n2c59
Jan 17 07:48:01.289: INFO: Got endpoints: latency-svc-n2c59 [524.905553ms]
Jan 17 07:48:01.297: INFO: Created: latency-svc-dpv6m
Jan 17 07:48:01.311: INFO: Got endpoints: latency-svc-dpv6m [485.01555ms]
Jan 17 07:48:01.342: INFO: Created: latency-svc-2wrqs
Jan 17 07:48:01.354: INFO: Got endpoints: latency-svc-2wrqs [495.774969ms]
Jan 17 07:48:01.360: INFO: Created: latency-svc-fcz27
Jan 17 07:48:01.370: INFO: Got endpoints: latency-svc-fcz27 [476.789483ms]
Jan 17 07:48:01.377: INFO: Created: latency-svc-kxpz6
Jan 17 07:48:01.429: INFO: Got endpoints: latency-svc-kxpz6 [496.135947ms]
Jan 17 07:48:01.512: INFO: Created: latency-svc-zrlfw
Jan 17 07:48:01.515: INFO: Got endpoints: latency-svc-zrlfw [576.480964ms]
Jan 17 07:48:01.519: INFO: Created: latency-svc-z75dk
Jan 17 07:48:01.519: INFO: Got endpoints: latency-svc-z75dk [1.044553537s]
Jan 17 07:48:01.535: INFO: Created: latency-svc-r5xb6
Jan 17 07:48:01.590: INFO: Got endpoints: latency-svc-r5xb6 [616.740848ms]
Jan 17 07:48:01.630: INFO: Created: latency-svc-pxd2r
Jan 17 07:48:01.646: INFO: Got endpoints: latency-svc-pxd2r [622.464027ms]
Jan 17 07:48:01.668: INFO: Created: latency-svc-f2rh4
Jan 17 07:48:01.674: INFO: Got endpoints: latency-svc-f2rh4 [623.065894ms]
Jan 17 07:48:01.710: INFO: Created: latency-svc-kbs9k
Jan 17 07:48:01.722: INFO: Got endpoints: latency-svc-kbs9k [644.012129ms]
Jan 17 07:48:01.739: INFO: Created: latency-svc-4thgn
Jan 17 07:48:01.752: INFO: Got endpoints: latency-svc-4thgn [628.327827ms]
Jan 17 07:48:01.773: INFO: Created: latency-svc-bmps5
Jan 17 07:48:01.780: INFO: Got endpoints: latency-svc-bmps5 [615.613146ms]
Jan 17 07:48:01.806: INFO: Created: latency-svc-2j8wr
Jan 17 07:48:01.820: INFO: Got endpoints: latency-svc-2j8wr [625.676725ms]
Jan 17 07:48:01.825: INFO: Created: latency-svc-8kw42
Jan 17 07:48:01.840: INFO: Got endpoints: latency-svc-8kw42 [601.946056ms]
Jan 17 07:48:01.849: INFO: Created: latency-svc-rhgkn
Jan 17 07:48:01.864: INFO: Got endpoints: latency-svc-rhgkn [575.331619ms]
Jan 17 07:48:01.877: INFO: Created: latency-svc-grnxp
Jan 17 07:48:01.890: INFO: Got endpoints: latency-svc-grnxp [579.069288ms]
Jan 17 07:48:01.917: INFO: Created: latency-svc-7nd6s
Jan 17 07:48:01.920: INFO: Got endpoints: latency-svc-7nd6s [566.015852ms]
Jan 17 07:48:01.961: INFO: Created: latency-svc-kgqn5
Jan 17 07:48:01.985: INFO: Got endpoints: latency-svc-kgqn5 [556.17413ms]
Jan 17 07:48:01.999: INFO: Created: latency-svc-qdmxp
Jan 17 07:48:02.009: INFO: Got endpoints: latency-svc-qdmxp [493.718504ms]
Jan 17 07:48:02.043: INFO: Created: latency-svc-fttf6
Jan 17 07:48:02.047: INFO: Got endpoints: latency-svc-fttf6 [528.06297ms]
Jan 17 07:48:02.072: INFO: Created: latency-svc-j98pk
Jan 17 07:48:02.089: INFO: Got endpoints: latency-svc-j98pk [498.729586ms]
Jan 17 07:48:02.119: INFO: Created: latency-svc-79rbp
Jan 17 07:48:02.153: INFO: Got endpoints: latency-svc-79rbp [506.752363ms]
Jan 17 07:48:02.165: INFO: Created: latency-svc-2h8cf
Jan 17 07:48:02.174: INFO: Got endpoints: latency-svc-2h8cf [500.351784ms]
Jan 17 07:48:02.186: INFO: Created: latency-svc-fqjnk
Jan 17 07:48:02.197: INFO: Got endpoints: latency-svc-fqjnk [474.273331ms]
Jan 17 07:48:02.223: INFO: Created: latency-svc-lwhxf
Jan 17 07:48:02.241: INFO: Got endpoints: latency-svc-lwhxf [489.2607ms]
Jan 17 07:48:02.252: INFO: Created: latency-svc-gj7cd
Jan 17 07:48:02.268: INFO: Got endpoints: latency-svc-gj7cd [487.912411ms]
Jan 17 07:48:02.284: INFO: Created: latency-svc-v597n
Jan 17 07:48:02.317: INFO: Got endpoints: latency-svc-v597n [496.366109ms]
Jan 17 07:48:02.320: INFO: Created: latency-svc-qt2xm
Jan 17 07:48:02.323: INFO: Got endpoints: latency-svc-qt2xm [482.111796ms]
Jan 17 07:48:02.363: INFO: Created: latency-svc-ft74h
Jan 17 07:48:02.367: INFO: Got endpoints: latency-svc-ft74h [502.472715ms]
Jan 17 07:48:02.389: INFO: Created: latency-svc-749dv
Jan 17 07:48:02.406: INFO: Created: latency-svc-rsljj
Jan 17 07:48:02.407: INFO: Created: latency-svc-fhpp4
Jan 17 07:48:02.409: INFO: Got endpoints: latency-svc-749dv [518.860661ms]
Jan 17 07:48:02.434: INFO: Created: latency-svc-m2qdd
Jan 17 07:48:02.442: INFO: Got endpoints: latency-svc-fhpp4 [1.072789984s]
Jan 17 07:48:02.443: INFO: Got endpoints: latency-svc-rsljj [522.97998ms]
Jan 17 07:48:02.472: INFO: Got endpoints: latency-svc-m2qdd [486.757119ms]
Jan 17 07:48:02.483: INFO: Created: latency-svc-6cxmp
Jan 17 07:48:02.496: INFO: Got endpoints: latency-svc-6cxmp [486.599754ms]
Jan 17 07:48:02.511: INFO: Created: latency-svc-9sng4
Jan 17 07:48:02.528: INFO: Got endpoints: latency-svc-9sng4 [480.630945ms]
Jan 17 07:48:02.551: INFO: Created: latency-svc-cnrmv
Jan 17 07:48:02.560: INFO: Got endpoints: latency-svc-cnrmv [471.174455ms]
Jan 17 07:48:02.580: INFO: Created: latency-svc-4jbcg
Jan 17 07:48:02.596: INFO: Got endpoints: latency-svc-4jbcg [443.334803ms]
Jan 17 07:48:02.614: INFO: Created: latency-svc-44knj
Jan 17 07:48:02.629: INFO: Got endpoints: latency-svc-44knj [454.771965ms]
Jan 17 07:48:02.645: INFO: Created: latency-svc-92nhw
Jan 17 07:48:02.654: INFO: Got endpoints: latency-svc-92nhw [457.191464ms]
Jan 17 07:48:02.672: INFO: Created: latency-svc-ttk6h
Jan 17 07:48:02.690: INFO: Got endpoints: latency-svc-ttk6h [448.255783ms]
Jan 17 07:48:02.697: INFO: Created: latency-svc-sww7q
Jan 17 07:48:02.699: INFO: Got endpoints: latency-svc-sww7q [430.993632ms]
Jan 17 07:48:02.719: INFO: Created: latency-svc-77627
Jan 17 07:48:02.728: INFO: Got endpoints: latency-svc-77627 [411.131424ms]
Jan 17 07:48:02.744: INFO: Created: latency-svc-g45p6
Jan 17 07:48:02.753: INFO: Got endpoints: latency-svc-g45p6 [430.042868ms]
Jan 17 07:48:02.770: INFO: Created: latency-svc-kjhjg
Jan 17 07:48:02.786: INFO: Created: latency-svc-858gf
Jan 17 07:48:02.808: INFO: Got endpoints: latency-svc-kjhjg [441.164965ms]
Jan 17 07:48:02.836: INFO: Created: latency-svc-lhwfz
Jan 17 07:48:02.864: INFO: Got endpoints: latency-svc-858gf [455.514327ms]
Jan 17 07:48:02.902: INFO: Created: latency-svc-gmhgx
Jan 17 07:48:02.922: INFO: Got endpoints: latency-svc-lhwfz [479.989389ms]
Jan 17 07:48:02.961: INFO: Got endpoints: latency-svc-gmhgx [517.708298ms]
Jan 17 07:48:02.981: INFO: Created: latency-svc-qdcx5
Jan 17 07:48:03.007: INFO: Got endpoints: latency-svc-qdcx5 [534.232086ms]
Jan 17 07:48:03.044: INFO: Created: latency-svc-c6prz
Jan 17 07:48:03.062: INFO: Got endpoints: latency-svc-c6prz [566.43814ms]
Jan 17 07:48:03.079: INFO: Created: latency-svc-gxrxl
Jan 17 07:48:03.112: INFO: Created: latency-svc-t6jf8
Jan 17 07:48:03.120: INFO: Got endpoints: latency-svc-gxrxl [592.129799ms]
Jan 17 07:48:03.163: INFO: Created: latency-svc-796zr
Jan 17 07:48:03.165: INFO: Got endpoints: latency-svc-t6jf8 [604.072468ms]
Jan 17 07:48:03.185: INFO: Created: latency-svc-prg8b
Jan 17 07:48:03.205: INFO: Got endpoints: latency-svc-796zr [608.660627ms]
Jan 17 07:48:03.218: INFO: Created: latency-svc-5m56d
Jan 17 07:48:03.264: INFO: Got endpoints: latency-svc-prg8b [634.901253ms]
Jan 17 07:48:03.269: INFO: Created: latency-svc-h7gkb
Jan 17 07:48:03.289: INFO: Created: latency-svc-pdtsw
Jan 17 07:48:03.303: INFO: Got endpoints: latency-svc-5m56d [648.917479ms]
Jan 17 07:48:03.321: INFO: Created: latency-svc-kx2px
Jan 17 07:48:03.347: INFO: Created: latency-svc-rlrv9
Jan 17 07:48:03.374: INFO: Got endpoints: latency-svc-h7gkb [684.102367ms]
Jan 17 07:48:03.382: INFO: Created: latency-svc-xh6gt
Jan 17 07:48:03.415: INFO: Got endpoints: latency-svc-pdtsw [686.791946ms]
Jan 17 07:48:03.422: INFO: Created: latency-svc-8bww6
Jan 17 07:48:03.442: INFO: Created: latency-svc-q6j4b
Jan 17 07:48:03.458: INFO: Got endpoints: latency-svc-kx2px [705.591066ms]
Jan 17 07:48:03.468: INFO: Created: latency-svc-q2d87
Jan 17 07:48:03.491: INFO: Created: latency-svc-tx8dv
Jan 17 07:48:03.511: INFO: Got endpoints: latency-svc-rlrv9 [703.434375ms]
Jan 17 07:48:03.518: INFO: Created: latency-svc-jcmvk
Jan 17 07:48:03.538: INFO: Created: latency-svc-b847q
Jan 17 07:48:03.554: INFO: Got endpoints: latency-svc-xh6gt [689.554507ms]
Jan 17 07:48:03.561: INFO: Created: latency-svc-hlgbx
Jan 17 07:48:03.598: INFO: Created: latency-svc-d9f9d
Jan 17 07:48:03.599: INFO: Created: latency-svc-qh76t
Jan 17 07:48:03.600: INFO: Got endpoints: latency-svc-8bww6 [677.201445ms]
Jan 17 07:48:03.612: INFO: Created: latency-svc-vpnkv
Jan 17 07:48:03.627: INFO: Created: latency-svc-pkzbm
Jan 17 07:48:03.648: INFO: Created: latency-svc-6tk5d
Jan 17 07:48:03.663: INFO: Got endpoints: latency-svc-q6j4b [701.809329ms]
Jan 17 07:48:03.674: INFO: Created: latency-svc-zztk4
Jan 17 07:48:03.706: INFO: Created: latency-svc-kpjz9
Jan 17 07:48:03.709: INFO: Got endpoints: latency-svc-q2d87 [701.605549ms]
Jan 17 07:48:03.732: INFO: Created: latency-svc-7lhss
Jan 17 07:48:03.746: INFO: Created: latency-svc-q7bln
Jan 17 07:48:03.756: INFO: Got endpoints: latency-svc-tx8dv [693.611026ms]
Jan 17 07:48:03.765: INFO: Created: latency-svc-c5xrt
Jan 17 07:48:03.800: INFO: Created: latency-svc-r97dr
Jan 17 07:48:03.812: INFO: Got endpoints: latency-svc-jcmvk [691.895746ms]
Jan 17 07:48:03.823: INFO: Created: latency-svc-29q92
Jan 17 07:48:03.842: INFO: Created: latency-svc-62bjr
Jan 17 07:48:03.853: INFO: Got endpoints: latency-svc-b847q [688.081433ms]
Jan 17 07:48:03.873: INFO: Created: latency-svc-2kmmf
Jan 17 07:48:03.900: INFO: Got endpoints: latency-svc-hlgbx [694.411504ms]
Jan 17 07:48:03.926: INFO: Created: latency-svc-67swb
Jan 17 07:48:03.950: INFO: Got endpoints: latency-svc-d9f9d [1.251125785s]
Jan 17 07:48:03.973: INFO: Created: latency-svc-7fhpg
Jan 17 07:48:04.002: INFO: Got endpoints: latency-svc-qh76t [737.89167ms]
Jan 17 07:48:04.035: INFO: Created: latency-svc-r7m2m
Jan 17 07:48:04.049: INFO: Got endpoints: latency-svc-vpnkv [745.045697ms]
Jan 17 07:48:04.076: INFO: Created: latency-svc-ckmjk
Jan 17 07:48:04.101: INFO: Got endpoints: latency-svc-pkzbm [726.898593ms]
Jan 17 07:48:04.124: INFO: Created: latency-svc-jcsrh
Jan 17 07:48:04.153: INFO: Got endpoints: latency-svc-6tk5d [738.134602ms]
Jan 17 07:48:04.180: INFO: Created: latency-svc-49prp
Jan 17 07:48:04.205: INFO: Got endpoints: latency-svc-zztk4 [746.221984ms]
Jan 17 07:48:04.226: INFO: Created: latency-svc-22nnh
Jan 17 07:48:04.253: INFO: Got endpoints: latency-svc-kpjz9 [741.841866ms]
Jan 17 07:48:04.284: INFO: Created: latency-svc-9bm5m
Jan 17 07:48:04.300: INFO: Got endpoints: latency-svc-7lhss [746.016404ms]
Jan 17 07:48:04.325: INFO: Created: latency-svc-292t4
Jan 17 07:48:04.351: INFO: Got endpoints: latency-svc-q7bln [750.77173ms]
Jan 17 07:48:04.378: INFO: Created: latency-svc-lctqf
Jan 17 07:48:04.408: INFO: Got endpoints: latency-svc-c5xrt [745.020497ms]
Jan 17 07:48:04.464: INFO: Created: latency-svc-vbn4s
Jan 17 07:48:04.476: INFO: Got endpoints: latency-svc-r97dr [767.183209ms]
Jan 17 07:48:04.508: INFO: Got endpoints: latency-svc-29q92 [751.986069ms]
Jan 17 07:48:04.527: INFO: Created: latency-svc-ncs4n
Jan 17 07:48:04.553: INFO: Got endpoints: latency-svc-62bjr [740.94606ms]
Jan 17 07:48:04.560: INFO: Created: latency-svc-4bxzr
Jan 17 07:48:04.587: INFO: Created: latency-svc-p9wp8
Jan 17 07:48:04.601: INFO: Got endpoints: latency-svc-2kmmf [748.456404ms]
Jan 17 07:48:04.628: INFO: Created: latency-svc-q8jkl
Jan 17 07:48:04.654: INFO: Got endpoints: latency-svc-67swb [754.019206ms]
Jan 17 07:48:04.684: INFO: Created: latency-svc-ftskk
Jan 17 07:48:04.709: INFO: Got endpoints: latency-svc-7fhpg [758.984966ms]
Jan 17 07:48:04.730: INFO: Created: latency-svc-s5ptk
Jan 17 07:48:04.753: INFO: Got endpoints: latency-svc-r7m2m [751.121507ms]
Jan 17 07:48:04.803: INFO: Created: latency-svc-kphrf
Jan 17 07:48:04.813: INFO: Got endpoints: latency-svc-ckmjk [764.280603ms]
Jan 17 07:48:04.860: INFO: Got endpoints: latency-svc-jcsrh [759.468165ms]
Jan 17 07:48:04.860: INFO: Created: latency-svc-kvd4h
Jan 17 07:48:04.883: INFO: Created: latency-svc-jltdx
Jan 17 07:48:04.905: INFO: Got endpoints: latency-svc-49prp [751.444498ms]
Jan 17 07:48:04.928: INFO: Created: latency-svc-8q6rn
Jan 17 07:48:04.955: INFO: Got endpoints: latency-svc-22nnh [750.402982ms]
Jan 17 07:48:04.976: INFO: Created: latency-svc-6dhmg
Jan 17 07:48:04.999: INFO: Got endpoints: latency-svc-9bm5m [745.201435ms]
Jan 17 07:48:05.023: INFO: Created: latency-svc-jbdx9
Jan 17 07:48:05.052: INFO: Got endpoints: latency-svc-292t4 [752.233127ms]
Jan 17 07:48:05.075: INFO: Created: latency-svc-9h7zh
Jan 17 07:48:05.106: INFO: Got endpoints: latency-svc-lctqf [755.523729ms]
Jan 17 07:48:05.127: INFO: Created: latency-svc-mk6b5
Jan 17 07:48:05.154: INFO: Got endpoints: latency-svc-vbn4s [745.723061ms]
Jan 17 07:48:05.175: INFO: Created: latency-svc-44xgb
Jan 17 07:48:05.205: INFO: Got endpoints: latency-svc-ncs4n [728.495848ms]
Jan 17 07:48:05.238: INFO: Created: latency-svc-w2gn8
Jan 17 07:48:05.253: INFO: Got endpoints: latency-svc-4bxzr [744.002457ms]
Jan 17 07:48:05.277: INFO: Created: latency-svc-lhxgv
Jan 17 07:48:05.305: INFO: Got endpoints: latency-svc-p9wp8 [751.332218ms]
Jan 17 07:48:05.330: INFO: Created: latency-svc-bftv9
Jan 17 07:48:05.350: INFO: Got endpoints: latency-svc-q8jkl [748.963881ms]
Jan 17 07:48:05.383: INFO: Created: latency-svc-txc95
Jan 17 07:48:05.413: INFO: Got endpoints: latency-svc-ftskk [759.638214ms]
Jan 17 07:48:05.438: INFO: Created: latency-svc-mjmqw
Jan 17 07:48:05.460: INFO: Got endpoints: latency-svc-s5ptk [750.916262ms]
Jan 17 07:48:05.484: INFO: Created: latency-svc-xtgjg
Jan 17 07:48:05.508: INFO: Got endpoints: latency-svc-kphrf [754.657558ms]
Jan 17 07:48:05.533: INFO: Created: latency-svc-sbt6c
Jan 17 07:48:05.577: INFO: Got endpoints: latency-svc-kvd4h [763.520131ms]
Jan 17 07:48:05.603: INFO: Got endpoints: latency-svc-jltdx [742.520865ms]
Jan 17 07:48:05.611: INFO: Created: latency-svc-4jv96
Jan 17 07:48:05.630: INFO: Created: latency-svc-w447t
Jan 17 07:48:05.662: INFO: Got endpoints: latency-svc-8q6rn [757.449092ms]
Jan 17 07:48:05.686: INFO: Created: latency-svc-f74lb
Jan 17 07:48:05.702: INFO: Got endpoints: latency-svc-6dhmg [747.009549ms]
Jan 17 07:48:05.727: INFO: Created: latency-svc-lv74h
Jan 17 07:48:05.754: INFO: Got endpoints: latency-svc-jbdx9 [755.000114ms]
Jan 17 07:48:05.780: INFO: Created: latency-svc-54dpf
Jan 17 07:48:05.800: INFO: Got endpoints: latency-svc-9h7zh [747.976765ms]
Jan 17 07:48:05.824: INFO: Created: latency-svc-l6x52
Jan 17 07:48:05.856: INFO: Got endpoints: latency-svc-mk6b5 [749.282259ms]
Jan 17 07:48:05.879: INFO: Created: latency-svc-lj7r2
Jan 17 07:48:05.905: INFO: Got endpoints: latency-svc-44xgb [751.065884ms]
Jan 17 07:48:05.929: INFO: Created: latency-svc-nm5vq
Jan 17 07:48:05.959: INFO: Got endpoints: latency-svc-w2gn8 [754.690826ms]
Jan 17 07:48:05.994: INFO: Created: latency-svc-jf576
Jan 17 07:48:06.012: INFO: Got endpoints: latency-svc-lhxgv [758.983605ms]
Jan 17 07:48:06.047: INFO: Created: latency-svc-kshhm
Jan 17 07:48:06.058: INFO: Got endpoints: latency-svc-bftv9 [752.975489ms]
Jan 17 07:48:06.093: INFO: Created: latency-svc-7scpd
Jan 17 07:48:06.101: INFO: Got endpoints: latency-svc-txc95 [751.014318ms]
Jan 17 07:48:06.140: INFO: Created: latency-svc-ng5p7
Jan 17 07:48:06.156: INFO: Got endpoints: latency-svc-mjmqw [742.531175ms]
Jan 17 07:48:06.180: INFO: Created: latency-svc-h6f2c
Jan 17 07:48:06.206: INFO: Got endpoints: latency-svc-xtgjg [745.941792ms]
Jan 17 07:48:06.237: INFO: Created: latency-svc-znk5q
Jan 17 07:48:06.256: INFO: Got endpoints: latency-svc-sbt6c [747.574003ms]
Jan 17 07:48:06.290: INFO: Created: latency-svc-qhsml
Jan 17 07:48:06.304: INFO: Got endpoints: latency-svc-4jv96 [727.197518ms]
Jan 17 07:48:06.341: INFO: Created: latency-svc-45mbl
Jan 17 07:48:06.352: INFO: Got endpoints: latency-svc-w447t [749.088552ms]
Jan 17 07:48:06.374: INFO: Created: latency-svc-4f9zj
Jan 17 07:48:06.408: INFO: Got endpoints: latency-svc-f74lb [745.426313ms]
Jan 17 07:48:06.433: INFO: Created: latency-svc-mjx2x
Jan 17 07:48:06.450: INFO: Got endpoints: latency-svc-lv74h [747.83001ms]
Jan 17 07:48:06.469: INFO: Created: latency-svc-vppqm
Jan 17 07:48:06.505: INFO: Got endpoints: latency-svc-54dpf [751.179193ms]
Jan 17 07:48:06.533: INFO: Created: latency-svc-62vvk
Jan 17 07:48:06.555: INFO: Got endpoints: latency-svc-l6x52 [754.336385ms]
Jan 17 07:48:06.580: INFO: Created: latency-svc-hr4gf
Jan 17 07:48:06.604: INFO: Got endpoints: latency-svc-lj7r2 [748.38042ms]
Jan 17 07:48:06.632: INFO: Created: latency-svc-jngpg
Jan 17 07:48:06.661: INFO: Got endpoints: latency-svc-nm5vq [755.470507ms]
Jan 17 07:48:06.686: INFO: Created: latency-svc-spnpb
Jan 17 07:48:06.701: INFO: Got endpoints: latency-svc-jf576 [741.908695ms]
Jan 17 07:48:06.735: INFO: Created: latency-svc-dshzg
Jan 17 07:48:06.751: INFO: Got endpoints: latency-svc-kshhm [739.731497ms]
Jan 17 07:48:06.775: INFO: Created: latency-svc-vxskz
Jan 17 07:48:06.802: INFO: Got endpoints: latency-svc-7scpd [743.93126ms]
Jan 17 07:48:06.827: INFO: Created: latency-svc-54wh4
Jan 17 07:48:06.852: INFO: Got endpoints: latency-svc-ng5p7 [750.090696ms]
Jan 17 07:48:06.887: INFO: Created: latency-svc-vcjbm
Jan 17 07:48:06.910: INFO: Got endpoints: latency-svc-h6f2c [753.595165ms]
Jan 17 07:48:06.937: INFO: Created: latency-svc-plmbr
Jan 17 07:48:06.958: INFO: Got endpoints: latency-svc-znk5q [751.327037ms]
Jan 17 07:48:06.981: INFO: Created: latency-svc-p689b
Jan 17 07:48:06.999: INFO: Got endpoints: latency-svc-qhsml [743.141853ms]
Jan 17 07:48:07.032: INFO: Created: latency-svc-gb6lc
Jan 17 07:48:07.057: INFO: Got endpoints: latency-svc-45mbl [753.344339ms]
Jan 17 07:48:07.084: INFO: Created: latency-svc-cxt9p
Jan 17 07:48:07.103: INFO: Got endpoints: latency-svc-4f9zj [750.508981ms]
Jan 17 07:48:07.124: INFO: Created: latency-svc-ctdzr
Jan 17 07:48:07.160: INFO: Got endpoints: latency-svc-mjx2x [751.962765ms]
Jan 17 07:48:07.182: INFO: Created: latency-svc-qmtmv
Jan 17 07:48:07.203: INFO: Got endpoints: latency-svc-vppqm [752.668538ms]
Jan 17 07:48:07.226: INFO: Created: latency-svc-mjww9
Jan 17 07:48:07.253: INFO: Got endpoints: latency-svc-62vvk [748.335901ms]
Jan 17 07:48:07.301: INFO: Got endpoints: latency-svc-hr4gf [746.002635ms]
Jan 17 07:48:07.352: INFO: Got endpoints: latency-svc-jngpg [748.084619ms]
Jan 17 07:48:07.402: INFO: Got endpoints: latency-svc-spnpb [740.954723ms]
Jan 17 07:48:07.450: INFO: Got endpoints: latency-svc-dshzg [748.538091ms]
Jan 17 07:48:07.503: INFO: Got endpoints: latency-svc-vxskz [751.16771ms]
Jan 17 07:48:07.562: INFO: Got endpoints: latency-svc-54wh4 [759.751714ms]
Jan 17 07:48:07.604: INFO: Got endpoints: latency-svc-vcjbm [752.521722ms]
Jan 17 07:48:07.650: INFO: Got endpoints: latency-svc-plmbr [739.990509ms]
Jan 17 07:48:07.700: INFO: Got endpoints: latency-svc-p689b [742.04829ms]
Jan 17 07:48:07.763: INFO: Got endpoints: latency-svc-gb6lc [764.262651ms]
Jan 17 07:48:07.804: INFO: Got endpoints: latency-svc-cxt9p [746.61792ms]
Jan 17 07:48:07.850: INFO: Got endpoints: latency-svc-ctdzr [747.781416ms]
Jan 17 07:48:07.903: INFO: Got endpoints: latency-svc-qmtmv [742.759576ms]
Jan 17 07:48:07.953: INFO: Got endpoints: latency-svc-mjww9 [749.540187ms]
Jan 17 07:48:07.953: INFO: Latencies: [67.73929ms 99.318199ms 122.031891ms 144.874599ms 193.849085ms 210.373155ms 259.297713ms 301.822505ms 352.570488ms 375.589251ms 411.131424ms 416.082172ms 430.042868ms 430.993632ms 439.043001ms 441.164965ms 443.334803ms 447.203886ms 448.255783ms 451.941299ms 452.216402ms 454.771965ms 455.514327ms 457.191464ms 467.411812ms 469.318669ms 471.174455ms 474.273331ms 476.789483ms 478.120797ms 479.989389ms 480.630945ms 482.111796ms 485.01555ms 486.033096ms 486.599754ms 486.757119ms 487.912411ms 489.2607ms 489.305159ms 493.718504ms 495.774969ms 496.135947ms 496.366109ms 496.999935ms 497.590046ms 497.780517ms 498.074836ms 498.273469ms 498.729586ms 500.351784ms 502.472715ms 502.708556ms 503.819132ms 504.971835ms 506.752363ms 510.654873ms 511.167176ms 511.40613ms 511.602655ms 512.265495ms 515.257077ms 516.674131ms 516.778669ms 516.867348ms 517.708298ms 518.860661ms 521.658384ms 522.97998ms 524.034332ms 524.905553ms 526.434866ms 528.06297ms 528.514111ms 530.530215ms 534.232086ms 535.803253ms 541.302843ms 546.034406ms 546.968443ms 553.701861ms 556.17413ms 565.482461ms 566.015852ms 566.43814ms 567.917138ms 575.331619ms 576.480964ms 579.069288ms 579.324598ms 592.129799ms 601.946056ms 604.026507ms 604.072468ms 608.660627ms 609.636127ms 615.613146ms 616.740848ms 622.464027ms 623.065894ms 625.676725ms 628.327827ms 634.901253ms 644.012129ms 648.917479ms 677.201445ms 684.102367ms 686.791946ms 688.081433ms 689.554507ms 691.895746ms 693.611026ms 694.411504ms 701.605549ms 701.809329ms 703.434375ms 705.591066ms 726.898593ms 727.197518ms 728.495848ms 737.89167ms 738.134602ms 739.731497ms 739.990509ms 740.94606ms 740.954723ms 741.841866ms 741.908695ms 742.04829ms 742.520865ms 742.531175ms 742.759576ms 743.141853ms 743.93126ms 744.002457ms 745.020497ms 745.045697ms 745.201435ms 745.426313ms 745.723061ms 745.941792ms 746.002635ms 746.016404ms 746.221984ms 746.61792ms 747.009549ms 747.574003ms 747.781416ms 747.83001ms 747.976765ms 748.084619ms 748.335901ms 748.38042ms 748.456404ms 748.538091ms 748.963881ms 749.088552ms 749.282259ms 749.540187ms 750.090696ms 750.402982ms 750.508981ms 750.77173ms 750.916262ms 751.014318ms 751.065884ms 751.121507ms 751.16771ms 751.179193ms 751.327037ms 751.332218ms 751.444498ms 751.962765ms 751.986069ms 752.233127ms 752.521722ms 752.668538ms 752.975489ms 753.344339ms 753.595165ms 754.019206ms 754.336385ms 754.657558ms 754.690826ms 755.000114ms 755.470507ms 755.523729ms 757.449092ms 758.983605ms 758.984966ms 759.468165ms 759.638214ms 759.751714ms 763.520131ms 764.262651ms 764.280603ms 767.183209ms 1.044553537s 1.072789984s 1.251125785s]
Jan 17 07:48:07.953: INFO: 50 %ile: 625.676725ms
Jan 17 07:48:07.953: INFO: 90 %ile: 754.019206ms
Jan 17 07:48:07.953: INFO: 99 %ile: 1.072789984s
Jan 17 07:48:07.953: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 17 07:48:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7286" for this suite. 01/17/23 07:48:07.965
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":268,"skipped":4806,"failed":0}
------------------------------
• [SLOW TEST] [10.886 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:47:57.091
    Jan 17 07:47:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svc-latency 01/17/23 07:47:57.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:47:57.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:47:57.16
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 17 07:47:57.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7286 01/17/23 07:47:57.169
    I0117 07:47:57.190496      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7286, replica count: 1
    I0117 07:47:58.241373      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 07:47:59.242294      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:47:59.371: INFO: Created: latency-svc-z2xzz
    Jan 17 07:47:59.392: INFO: Got endpoints: latency-svc-z2xzz [49.735382ms]
    Jan 17 07:47:59.448: INFO: Created: latency-svc-nrj57
    Jan 17 07:47:59.461: INFO: Got endpoints: latency-svc-nrj57 [67.73929ms]
    Jan 17 07:47:59.469: INFO: Created: latency-svc-bc85l
    Jan 17 07:47:59.493: INFO: Got endpoints: latency-svc-bc85l [99.318199ms]
    Jan 17 07:47:59.510: INFO: Created: latency-svc-mgtnz
    Jan 17 07:47:59.515: INFO: Got endpoints: latency-svc-mgtnz [122.031891ms]
    Jan 17 07:47:59.527: INFO: Created: latency-svc-dr49f
    Jan 17 07:47:59.538: INFO: Got endpoints: latency-svc-dr49f [144.874599ms]
    Jan 17 07:47:59.558: INFO: Created: latency-svc-vxnmp
    Jan 17 07:47:59.587: INFO: Got endpoints: latency-svc-vxnmp [193.849085ms]
    Jan 17 07:47:59.600: INFO: Created: latency-svc-xn2bg
    Jan 17 07:47:59.603: INFO: Got endpoints: latency-svc-xn2bg [210.373155ms]
    Jan 17 07:47:59.642: INFO: Created: latency-svc-f6mmm
    Jan 17 07:47:59.653: INFO: Got endpoints: latency-svc-f6mmm [259.297713ms]
    Jan 17 07:47:59.680: INFO: Created: latency-svc-7sbws
    Jan 17 07:47:59.696: INFO: Got endpoints: latency-svc-7sbws [301.822505ms]
    Jan 17 07:47:59.722: INFO: Created: latency-svc-kqqtk
    Jan 17 07:47:59.746: INFO: Got endpoints: latency-svc-kqqtk [352.570488ms]
    Jan 17 07:47:59.753: INFO: Created: latency-svc-gc9g9
    Jan 17 07:47:59.769: INFO: Got endpoints: latency-svc-gc9g9 [375.589251ms]
    Jan 17 07:47:59.801: INFO: Created: latency-svc-jg4hk
    Jan 17 07:47:59.810: INFO: Got endpoints: latency-svc-jg4hk [416.082172ms]
    Jan 17 07:47:59.839: INFO: Created: latency-svc-9fh6l
    Jan 17 07:47:59.841: INFO: Got endpoints: latency-svc-9fh6l [447.203886ms]
    Jan 17 07:47:59.883: INFO: Created: latency-svc-jz6bl
    Jan 17 07:47:59.892: INFO: Got endpoints: latency-svc-jz6bl [498.273469ms]
    Jan 17 07:47:59.904: INFO: Created: latency-svc-9jvq6
    Jan 17 07:47:59.918: INFO: Got endpoints: latency-svc-9jvq6 [524.034332ms]
    Jan 17 07:47:59.928: INFO: Created: latency-svc-62ptf
    Jan 17 07:47:59.940: INFO: Got endpoints: latency-svc-62ptf [546.034406ms]
    Jan 17 07:47:59.949: INFO: Created: latency-svc-bqlj7
    Jan 17 07:47:59.976: INFO: Got endpoints: latency-svc-bqlj7 [515.257077ms]
    Jan 17 07:47:59.989: INFO: Created: latency-svc-t4btb
    Jan 17 07:48:00.004: INFO: Got endpoints: latency-svc-t4btb [511.40613ms]
    Jan 17 07:48:00.014: INFO: Created: latency-svc-dtwks
    Jan 17 07:48:00.027: INFO: Got endpoints: latency-svc-dtwks [512.265495ms]
    Jan 17 07:48:00.038: INFO: Created: latency-svc-lxtt6
    Jan 17 07:48:00.055: INFO: Got endpoints: latency-svc-lxtt6 [516.867348ms]
    Jan 17 07:48:00.067: INFO: Created: latency-svc-rdg77
    Jan 17 07:48:00.076: INFO: Got endpoints: latency-svc-rdg77 [489.305159ms]
    Jan 17 07:48:00.089: INFO: Created: latency-svc-tvpr9
    Jan 17 07:48:00.107: INFO: Got endpoints: latency-svc-tvpr9 [503.819132ms]
    Jan 17 07:48:00.116: INFO: Created: latency-svc-8bj2x
    Jan 17 07:48:00.132: INFO: Got endpoints: latency-svc-8bj2x [478.120797ms]
    Jan 17 07:48:00.165: INFO: Created: latency-svc-kvxr8
    Jan 17 07:48:00.165: INFO: Got endpoints: latency-svc-kvxr8 [469.318669ms]
    Jan 17 07:48:00.186: INFO: Created: latency-svc-rblvr
    Jan 17 07:48:00.198: INFO: Got endpoints: latency-svc-rblvr [452.216402ms]
    Jan 17 07:48:00.208: INFO: Created: latency-svc-7tfvh
    Jan 17 07:48:00.221: INFO: Got endpoints: latency-svc-7tfvh [451.941299ms]
    Jan 17 07:48:00.249: INFO: Created: latency-svc-dlfl5
    Jan 17 07:48:00.249: INFO: Got endpoints: latency-svc-dlfl5 [439.043001ms]
    Jan 17 07:48:00.320: INFO: Created: latency-svc-z8ncr
    Jan 17 07:48:00.339: INFO: Got endpoints: latency-svc-z8ncr [497.780517ms]
    Jan 17 07:48:00.349: INFO: Created: latency-svc-44gwv
    Jan 17 07:48:00.359: INFO: Got endpoints: latency-svc-44gwv [467.411812ms]
    Jan 17 07:48:00.392: INFO: Created: latency-svc-xd4d2
    Jan 17 07:48:00.404: INFO: Got endpoints: latency-svc-xd4d2 [486.033096ms]
    Jan 17 07:48:00.437: INFO: Created: latency-svc-bq46c
    Jan 17 07:48:00.437: INFO: Got endpoints: latency-svc-bq46c [497.590046ms]
    Jan 17 07:48:00.474: INFO: Created: latency-svc-bns5x
    Jan 17 07:48:00.474: INFO: Got endpoints: latency-svc-bns5x [498.074836ms]
    Jan 17 07:48:00.490: INFO: Created: latency-svc-6ql55
    Jan 17 07:48:00.507: INFO: Got endpoints: latency-svc-6ql55 [502.708556ms]
    Jan 17 07:48:00.522: INFO: Created: latency-svc-9ps54
    Jan 17 07:48:00.524: INFO: Got endpoints: latency-svc-9ps54 [496.999935ms]
    Jan 17 07:48:00.559: INFO: Created: latency-svc-l7b96
    Jan 17 07:48:00.566: INFO: Got endpoints: latency-svc-l7b96 [511.167176ms]
    Jan 17 07:48:00.588: INFO: Created: latency-svc-dgz2x
    Jan 17 07:48:00.593: INFO: Got endpoints: latency-svc-dgz2x [516.674131ms]
    Jan 17 07:48:00.614: INFO: Created: latency-svc-h8stk
    Jan 17 07:48:00.654: INFO: Got endpoints: latency-svc-h8stk [546.968443ms]
    Jan 17 07:48:00.655: INFO: Created: latency-svc-dt957
    Jan 17 07:48:00.673: INFO: Got endpoints: latency-svc-dt957 [541.302843ms]
    Jan 17 07:48:00.733: INFO: Created: latency-svc-d5wf4
    Jan 17 07:48:00.733: INFO: Created: latency-svc-2sqrm
    Jan 17 07:48:00.733: INFO: Got endpoints: latency-svc-d5wf4 [567.917138ms]
    Jan 17 07:48:00.764: INFO: Got endpoints: latency-svc-2sqrm [565.482461ms]
    Jan 17 07:48:00.781: INFO: Created: latency-svc-89q67
    Jan 17 07:48:00.825: INFO: Got endpoints: latency-svc-89q67 [604.026507ms]
    Jan 17 07:48:00.837: INFO: Created: latency-svc-vdbbm
    Jan 17 07:48:00.858: INFO: Got endpoints: latency-svc-vdbbm [609.636127ms]
    Jan 17 07:48:00.876: INFO: Created: latency-svc-dmbbn
    Jan 17 07:48:00.893: INFO: Got endpoints: latency-svc-dmbbn [553.701861ms]
    Jan 17 07:48:00.921: INFO: Created: latency-svc-q8hhx
    Jan 17 07:48:00.922: INFO: Created: latency-svc-kmstr
    Jan 17 07:48:00.933: INFO: Got endpoints: latency-svc-q8hhx [528.514111ms]
    Jan 17 07:48:00.938: INFO: Got endpoints: latency-svc-kmstr [579.324598ms]
    Jan 17 07:48:00.950: INFO: Created: latency-svc-jvjmp
    Jan 17 07:48:00.973: INFO: Got endpoints: latency-svc-jvjmp [535.803253ms]
    Jan 17 07:48:01.005: INFO: Created: latency-svc-jpxdd
    Jan 17 07:48:01.024: INFO: Got endpoints: latency-svc-jpxdd [516.778669ms]
    Jan 17 07:48:01.042: INFO: Created: latency-svc-6xbzf
    Jan 17 07:48:01.051: INFO: Got endpoints: latency-svc-6xbzf [526.434866ms]
    Jan 17 07:48:01.062: INFO: Created: latency-svc-hlskb
    Jan 17 07:48:01.078: INFO: Got endpoints: latency-svc-hlskb [511.602655ms]
    Jan 17 07:48:01.100: INFO: Created: latency-svc-4whp8
    Jan 17 07:48:01.124: INFO: Got endpoints: latency-svc-4whp8 [530.530215ms]
    Jan 17 07:48:01.148: INFO: Created: latency-svc-44b5x
    Jan 17 07:48:01.164: INFO: Got endpoints: latency-svc-44b5x [510.654873ms]
    Jan 17 07:48:01.174: INFO: Created: latency-svc-5jsx7
    Jan 17 07:48:01.195: INFO: Got endpoints: latency-svc-5jsx7 [521.658384ms]
    Jan 17 07:48:01.236: INFO: Created: latency-svc-2jhsh
    Jan 17 07:48:01.239: INFO: Got endpoints: latency-svc-2jhsh [504.971835ms]
    Jan 17 07:48:01.268: INFO: Created: latency-svc-n2c59
    Jan 17 07:48:01.289: INFO: Got endpoints: latency-svc-n2c59 [524.905553ms]
    Jan 17 07:48:01.297: INFO: Created: latency-svc-dpv6m
    Jan 17 07:48:01.311: INFO: Got endpoints: latency-svc-dpv6m [485.01555ms]
    Jan 17 07:48:01.342: INFO: Created: latency-svc-2wrqs
    Jan 17 07:48:01.354: INFO: Got endpoints: latency-svc-2wrqs [495.774969ms]
    Jan 17 07:48:01.360: INFO: Created: latency-svc-fcz27
    Jan 17 07:48:01.370: INFO: Got endpoints: latency-svc-fcz27 [476.789483ms]
    Jan 17 07:48:01.377: INFO: Created: latency-svc-kxpz6
    Jan 17 07:48:01.429: INFO: Got endpoints: latency-svc-kxpz6 [496.135947ms]
    Jan 17 07:48:01.512: INFO: Created: latency-svc-zrlfw
    Jan 17 07:48:01.515: INFO: Got endpoints: latency-svc-zrlfw [576.480964ms]
    Jan 17 07:48:01.519: INFO: Created: latency-svc-z75dk
    Jan 17 07:48:01.519: INFO: Got endpoints: latency-svc-z75dk [1.044553537s]
    Jan 17 07:48:01.535: INFO: Created: latency-svc-r5xb6
    Jan 17 07:48:01.590: INFO: Got endpoints: latency-svc-r5xb6 [616.740848ms]
    Jan 17 07:48:01.630: INFO: Created: latency-svc-pxd2r
    Jan 17 07:48:01.646: INFO: Got endpoints: latency-svc-pxd2r [622.464027ms]
    Jan 17 07:48:01.668: INFO: Created: latency-svc-f2rh4
    Jan 17 07:48:01.674: INFO: Got endpoints: latency-svc-f2rh4 [623.065894ms]
    Jan 17 07:48:01.710: INFO: Created: latency-svc-kbs9k
    Jan 17 07:48:01.722: INFO: Got endpoints: latency-svc-kbs9k [644.012129ms]
    Jan 17 07:48:01.739: INFO: Created: latency-svc-4thgn
    Jan 17 07:48:01.752: INFO: Got endpoints: latency-svc-4thgn [628.327827ms]
    Jan 17 07:48:01.773: INFO: Created: latency-svc-bmps5
    Jan 17 07:48:01.780: INFO: Got endpoints: latency-svc-bmps5 [615.613146ms]
    Jan 17 07:48:01.806: INFO: Created: latency-svc-2j8wr
    Jan 17 07:48:01.820: INFO: Got endpoints: latency-svc-2j8wr [625.676725ms]
    Jan 17 07:48:01.825: INFO: Created: latency-svc-8kw42
    Jan 17 07:48:01.840: INFO: Got endpoints: latency-svc-8kw42 [601.946056ms]
    Jan 17 07:48:01.849: INFO: Created: latency-svc-rhgkn
    Jan 17 07:48:01.864: INFO: Got endpoints: latency-svc-rhgkn [575.331619ms]
    Jan 17 07:48:01.877: INFO: Created: latency-svc-grnxp
    Jan 17 07:48:01.890: INFO: Got endpoints: latency-svc-grnxp [579.069288ms]
    Jan 17 07:48:01.917: INFO: Created: latency-svc-7nd6s
    Jan 17 07:48:01.920: INFO: Got endpoints: latency-svc-7nd6s [566.015852ms]
    Jan 17 07:48:01.961: INFO: Created: latency-svc-kgqn5
    Jan 17 07:48:01.985: INFO: Got endpoints: latency-svc-kgqn5 [556.17413ms]
    Jan 17 07:48:01.999: INFO: Created: latency-svc-qdmxp
    Jan 17 07:48:02.009: INFO: Got endpoints: latency-svc-qdmxp [493.718504ms]
    Jan 17 07:48:02.043: INFO: Created: latency-svc-fttf6
    Jan 17 07:48:02.047: INFO: Got endpoints: latency-svc-fttf6 [528.06297ms]
    Jan 17 07:48:02.072: INFO: Created: latency-svc-j98pk
    Jan 17 07:48:02.089: INFO: Got endpoints: latency-svc-j98pk [498.729586ms]
    Jan 17 07:48:02.119: INFO: Created: latency-svc-79rbp
    Jan 17 07:48:02.153: INFO: Got endpoints: latency-svc-79rbp [506.752363ms]
    Jan 17 07:48:02.165: INFO: Created: latency-svc-2h8cf
    Jan 17 07:48:02.174: INFO: Got endpoints: latency-svc-2h8cf [500.351784ms]
    Jan 17 07:48:02.186: INFO: Created: latency-svc-fqjnk
    Jan 17 07:48:02.197: INFO: Got endpoints: latency-svc-fqjnk [474.273331ms]
    Jan 17 07:48:02.223: INFO: Created: latency-svc-lwhxf
    Jan 17 07:48:02.241: INFO: Got endpoints: latency-svc-lwhxf [489.2607ms]
    Jan 17 07:48:02.252: INFO: Created: latency-svc-gj7cd
    Jan 17 07:48:02.268: INFO: Got endpoints: latency-svc-gj7cd [487.912411ms]
    Jan 17 07:48:02.284: INFO: Created: latency-svc-v597n
    Jan 17 07:48:02.317: INFO: Got endpoints: latency-svc-v597n [496.366109ms]
    Jan 17 07:48:02.320: INFO: Created: latency-svc-qt2xm
    Jan 17 07:48:02.323: INFO: Got endpoints: latency-svc-qt2xm [482.111796ms]
    Jan 17 07:48:02.363: INFO: Created: latency-svc-ft74h
    Jan 17 07:48:02.367: INFO: Got endpoints: latency-svc-ft74h [502.472715ms]
    Jan 17 07:48:02.389: INFO: Created: latency-svc-749dv
    Jan 17 07:48:02.406: INFO: Created: latency-svc-rsljj
    Jan 17 07:48:02.407: INFO: Created: latency-svc-fhpp4
    Jan 17 07:48:02.409: INFO: Got endpoints: latency-svc-749dv [518.860661ms]
    Jan 17 07:48:02.434: INFO: Created: latency-svc-m2qdd
    Jan 17 07:48:02.442: INFO: Got endpoints: latency-svc-fhpp4 [1.072789984s]
    Jan 17 07:48:02.443: INFO: Got endpoints: latency-svc-rsljj [522.97998ms]
    Jan 17 07:48:02.472: INFO: Got endpoints: latency-svc-m2qdd [486.757119ms]
    Jan 17 07:48:02.483: INFO: Created: latency-svc-6cxmp
    Jan 17 07:48:02.496: INFO: Got endpoints: latency-svc-6cxmp [486.599754ms]
    Jan 17 07:48:02.511: INFO: Created: latency-svc-9sng4
    Jan 17 07:48:02.528: INFO: Got endpoints: latency-svc-9sng4 [480.630945ms]
    Jan 17 07:48:02.551: INFO: Created: latency-svc-cnrmv
    Jan 17 07:48:02.560: INFO: Got endpoints: latency-svc-cnrmv [471.174455ms]
    Jan 17 07:48:02.580: INFO: Created: latency-svc-4jbcg
    Jan 17 07:48:02.596: INFO: Got endpoints: latency-svc-4jbcg [443.334803ms]
    Jan 17 07:48:02.614: INFO: Created: latency-svc-44knj
    Jan 17 07:48:02.629: INFO: Got endpoints: latency-svc-44knj [454.771965ms]
    Jan 17 07:48:02.645: INFO: Created: latency-svc-92nhw
    Jan 17 07:48:02.654: INFO: Got endpoints: latency-svc-92nhw [457.191464ms]
    Jan 17 07:48:02.672: INFO: Created: latency-svc-ttk6h
    Jan 17 07:48:02.690: INFO: Got endpoints: latency-svc-ttk6h [448.255783ms]
    Jan 17 07:48:02.697: INFO: Created: latency-svc-sww7q
    Jan 17 07:48:02.699: INFO: Got endpoints: latency-svc-sww7q [430.993632ms]
    Jan 17 07:48:02.719: INFO: Created: latency-svc-77627
    Jan 17 07:48:02.728: INFO: Got endpoints: latency-svc-77627 [411.131424ms]
    Jan 17 07:48:02.744: INFO: Created: latency-svc-g45p6
    Jan 17 07:48:02.753: INFO: Got endpoints: latency-svc-g45p6 [430.042868ms]
    Jan 17 07:48:02.770: INFO: Created: latency-svc-kjhjg
    Jan 17 07:48:02.786: INFO: Created: latency-svc-858gf
    Jan 17 07:48:02.808: INFO: Got endpoints: latency-svc-kjhjg [441.164965ms]
    Jan 17 07:48:02.836: INFO: Created: latency-svc-lhwfz
    Jan 17 07:48:02.864: INFO: Got endpoints: latency-svc-858gf [455.514327ms]
    Jan 17 07:48:02.902: INFO: Created: latency-svc-gmhgx
    Jan 17 07:48:02.922: INFO: Got endpoints: latency-svc-lhwfz [479.989389ms]
    Jan 17 07:48:02.961: INFO: Got endpoints: latency-svc-gmhgx [517.708298ms]
    Jan 17 07:48:02.981: INFO: Created: latency-svc-qdcx5
    Jan 17 07:48:03.007: INFO: Got endpoints: latency-svc-qdcx5 [534.232086ms]
    Jan 17 07:48:03.044: INFO: Created: latency-svc-c6prz
    Jan 17 07:48:03.062: INFO: Got endpoints: latency-svc-c6prz [566.43814ms]
    Jan 17 07:48:03.079: INFO: Created: latency-svc-gxrxl
    Jan 17 07:48:03.112: INFO: Created: latency-svc-t6jf8
    Jan 17 07:48:03.120: INFO: Got endpoints: latency-svc-gxrxl [592.129799ms]
    Jan 17 07:48:03.163: INFO: Created: latency-svc-796zr
    Jan 17 07:48:03.165: INFO: Got endpoints: latency-svc-t6jf8 [604.072468ms]
    Jan 17 07:48:03.185: INFO: Created: latency-svc-prg8b
    Jan 17 07:48:03.205: INFO: Got endpoints: latency-svc-796zr [608.660627ms]
    Jan 17 07:48:03.218: INFO: Created: latency-svc-5m56d
    Jan 17 07:48:03.264: INFO: Got endpoints: latency-svc-prg8b [634.901253ms]
    Jan 17 07:48:03.269: INFO: Created: latency-svc-h7gkb
    Jan 17 07:48:03.289: INFO: Created: latency-svc-pdtsw
    Jan 17 07:48:03.303: INFO: Got endpoints: latency-svc-5m56d [648.917479ms]
    Jan 17 07:48:03.321: INFO: Created: latency-svc-kx2px
    Jan 17 07:48:03.347: INFO: Created: latency-svc-rlrv9
    Jan 17 07:48:03.374: INFO: Got endpoints: latency-svc-h7gkb [684.102367ms]
    Jan 17 07:48:03.382: INFO: Created: latency-svc-xh6gt
    Jan 17 07:48:03.415: INFO: Got endpoints: latency-svc-pdtsw [686.791946ms]
    Jan 17 07:48:03.422: INFO: Created: latency-svc-8bww6
    Jan 17 07:48:03.442: INFO: Created: latency-svc-q6j4b
    Jan 17 07:48:03.458: INFO: Got endpoints: latency-svc-kx2px [705.591066ms]
    Jan 17 07:48:03.468: INFO: Created: latency-svc-q2d87
    Jan 17 07:48:03.491: INFO: Created: latency-svc-tx8dv
    Jan 17 07:48:03.511: INFO: Got endpoints: latency-svc-rlrv9 [703.434375ms]
    Jan 17 07:48:03.518: INFO: Created: latency-svc-jcmvk
    Jan 17 07:48:03.538: INFO: Created: latency-svc-b847q
    Jan 17 07:48:03.554: INFO: Got endpoints: latency-svc-xh6gt [689.554507ms]
    Jan 17 07:48:03.561: INFO: Created: latency-svc-hlgbx
    Jan 17 07:48:03.598: INFO: Created: latency-svc-d9f9d
    Jan 17 07:48:03.599: INFO: Created: latency-svc-qh76t
    Jan 17 07:48:03.600: INFO: Got endpoints: latency-svc-8bww6 [677.201445ms]
    Jan 17 07:48:03.612: INFO: Created: latency-svc-vpnkv
    Jan 17 07:48:03.627: INFO: Created: latency-svc-pkzbm
    Jan 17 07:48:03.648: INFO: Created: latency-svc-6tk5d
    Jan 17 07:48:03.663: INFO: Got endpoints: latency-svc-q6j4b [701.809329ms]
    Jan 17 07:48:03.674: INFO: Created: latency-svc-zztk4
    Jan 17 07:48:03.706: INFO: Created: latency-svc-kpjz9
    Jan 17 07:48:03.709: INFO: Got endpoints: latency-svc-q2d87 [701.605549ms]
    Jan 17 07:48:03.732: INFO: Created: latency-svc-7lhss
    Jan 17 07:48:03.746: INFO: Created: latency-svc-q7bln
    Jan 17 07:48:03.756: INFO: Got endpoints: latency-svc-tx8dv [693.611026ms]
    Jan 17 07:48:03.765: INFO: Created: latency-svc-c5xrt
    Jan 17 07:48:03.800: INFO: Created: latency-svc-r97dr
    Jan 17 07:48:03.812: INFO: Got endpoints: latency-svc-jcmvk [691.895746ms]
    Jan 17 07:48:03.823: INFO: Created: latency-svc-29q92
    Jan 17 07:48:03.842: INFO: Created: latency-svc-62bjr
    Jan 17 07:48:03.853: INFO: Got endpoints: latency-svc-b847q [688.081433ms]
    Jan 17 07:48:03.873: INFO: Created: latency-svc-2kmmf
    Jan 17 07:48:03.900: INFO: Got endpoints: latency-svc-hlgbx [694.411504ms]
    Jan 17 07:48:03.926: INFO: Created: latency-svc-67swb
    Jan 17 07:48:03.950: INFO: Got endpoints: latency-svc-d9f9d [1.251125785s]
    Jan 17 07:48:03.973: INFO: Created: latency-svc-7fhpg
    Jan 17 07:48:04.002: INFO: Got endpoints: latency-svc-qh76t [737.89167ms]
    Jan 17 07:48:04.035: INFO: Created: latency-svc-r7m2m
    Jan 17 07:48:04.049: INFO: Got endpoints: latency-svc-vpnkv [745.045697ms]
    Jan 17 07:48:04.076: INFO: Created: latency-svc-ckmjk
    Jan 17 07:48:04.101: INFO: Got endpoints: latency-svc-pkzbm [726.898593ms]
    Jan 17 07:48:04.124: INFO: Created: latency-svc-jcsrh
    Jan 17 07:48:04.153: INFO: Got endpoints: latency-svc-6tk5d [738.134602ms]
    Jan 17 07:48:04.180: INFO: Created: latency-svc-49prp
    Jan 17 07:48:04.205: INFO: Got endpoints: latency-svc-zztk4 [746.221984ms]
    Jan 17 07:48:04.226: INFO: Created: latency-svc-22nnh
    Jan 17 07:48:04.253: INFO: Got endpoints: latency-svc-kpjz9 [741.841866ms]
    Jan 17 07:48:04.284: INFO: Created: latency-svc-9bm5m
    Jan 17 07:48:04.300: INFO: Got endpoints: latency-svc-7lhss [746.016404ms]
    Jan 17 07:48:04.325: INFO: Created: latency-svc-292t4
    Jan 17 07:48:04.351: INFO: Got endpoints: latency-svc-q7bln [750.77173ms]
    Jan 17 07:48:04.378: INFO: Created: latency-svc-lctqf
    Jan 17 07:48:04.408: INFO: Got endpoints: latency-svc-c5xrt [745.020497ms]
    Jan 17 07:48:04.464: INFO: Created: latency-svc-vbn4s
    Jan 17 07:48:04.476: INFO: Got endpoints: latency-svc-r97dr [767.183209ms]
    Jan 17 07:48:04.508: INFO: Got endpoints: latency-svc-29q92 [751.986069ms]
    Jan 17 07:48:04.527: INFO: Created: latency-svc-ncs4n
    Jan 17 07:48:04.553: INFO: Got endpoints: latency-svc-62bjr [740.94606ms]
    Jan 17 07:48:04.560: INFO: Created: latency-svc-4bxzr
    Jan 17 07:48:04.587: INFO: Created: latency-svc-p9wp8
    Jan 17 07:48:04.601: INFO: Got endpoints: latency-svc-2kmmf [748.456404ms]
    Jan 17 07:48:04.628: INFO: Created: latency-svc-q8jkl
    Jan 17 07:48:04.654: INFO: Got endpoints: latency-svc-67swb [754.019206ms]
    Jan 17 07:48:04.684: INFO: Created: latency-svc-ftskk
    Jan 17 07:48:04.709: INFO: Got endpoints: latency-svc-7fhpg [758.984966ms]
    Jan 17 07:48:04.730: INFO: Created: latency-svc-s5ptk
    Jan 17 07:48:04.753: INFO: Got endpoints: latency-svc-r7m2m [751.121507ms]
    Jan 17 07:48:04.803: INFO: Created: latency-svc-kphrf
    Jan 17 07:48:04.813: INFO: Got endpoints: latency-svc-ckmjk [764.280603ms]
    Jan 17 07:48:04.860: INFO: Got endpoints: latency-svc-jcsrh [759.468165ms]
    Jan 17 07:48:04.860: INFO: Created: latency-svc-kvd4h
    Jan 17 07:48:04.883: INFO: Created: latency-svc-jltdx
    Jan 17 07:48:04.905: INFO: Got endpoints: latency-svc-49prp [751.444498ms]
    Jan 17 07:48:04.928: INFO: Created: latency-svc-8q6rn
    Jan 17 07:48:04.955: INFO: Got endpoints: latency-svc-22nnh [750.402982ms]
    Jan 17 07:48:04.976: INFO: Created: latency-svc-6dhmg
    Jan 17 07:48:04.999: INFO: Got endpoints: latency-svc-9bm5m [745.201435ms]
    Jan 17 07:48:05.023: INFO: Created: latency-svc-jbdx9
    Jan 17 07:48:05.052: INFO: Got endpoints: latency-svc-292t4 [752.233127ms]
    Jan 17 07:48:05.075: INFO: Created: latency-svc-9h7zh
    Jan 17 07:48:05.106: INFO: Got endpoints: latency-svc-lctqf [755.523729ms]
    Jan 17 07:48:05.127: INFO: Created: latency-svc-mk6b5
    Jan 17 07:48:05.154: INFO: Got endpoints: latency-svc-vbn4s [745.723061ms]
    Jan 17 07:48:05.175: INFO: Created: latency-svc-44xgb
    Jan 17 07:48:05.205: INFO: Got endpoints: latency-svc-ncs4n [728.495848ms]
    Jan 17 07:48:05.238: INFO: Created: latency-svc-w2gn8
    Jan 17 07:48:05.253: INFO: Got endpoints: latency-svc-4bxzr [744.002457ms]
    Jan 17 07:48:05.277: INFO: Created: latency-svc-lhxgv
    Jan 17 07:48:05.305: INFO: Got endpoints: latency-svc-p9wp8 [751.332218ms]
    Jan 17 07:48:05.330: INFO: Created: latency-svc-bftv9
    Jan 17 07:48:05.350: INFO: Got endpoints: latency-svc-q8jkl [748.963881ms]
    Jan 17 07:48:05.383: INFO: Created: latency-svc-txc95
    Jan 17 07:48:05.413: INFO: Got endpoints: latency-svc-ftskk [759.638214ms]
    Jan 17 07:48:05.438: INFO: Created: latency-svc-mjmqw
    Jan 17 07:48:05.460: INFO: Got endpoints: latency-svc-s5ptk [750.916262ms]
    Jan 17 07:48:05.484: INFO: Created: latency-svc-xtgjg
    Jan 17 07:48:05.508: INFO: Got endpoints: latency-svc-kphrf [754.657558ms]
    Jan 17 07:48:05.533: INFO: Created: latency-svc-sbt6c
    Jan 17 07:48:05.577: INFO: Got endpoints: latency-svc-kvd4h [763.520131ms]
    Jan 17 07:48:05.603: INFO: Got endpoints: latency-svc-jltdx [742.520865ms]
    Jan 17 07:48:05.611: INFO: Created: latency-svc-4jv96
    Jan 17 07:48:05.630: INFO: Created: latency-svc-w447t
    Jan 17 07:48:05.662: INFO: Got endpoints: latency-svc-8q6rn [757.449092ms]
    Jan 17 07:48:05.686: INFO: Created: latency-svc-f74lb
    Jan 17 07:48:05.702: INFO: Got endpoints: latency-svc-6dhmg [747.009549ms]
    Jan 17 07:48:05.727: INFO: Created: latency-svc-lv74h
    Jan 17 07:48:05.754: INFO: Got endpoints: latency-svc-jbdx9 [755.000114ms]
    Jan 17 07:48:05.780: INFO: Created: latency-svc-54dpf
    Jan 17 07:48:05.800: INFO: Got endpoints: latency-svc-9h7zh [747.976765ms]
    Jan 17 07:48:05.824: INFO: Created: latency-svc-l6x52
    Jan 17 07:48:05.856: INFO: Got endpoints: latency-svc-mk6b5 [749.282259ms]
    Jan 17 07:48:05.879: INFO: Created: latency-svc-lj7r2
    Jan 17 07:48:05.905: INFO: Got endpoints: latency-svc-44xgb [751.065884ms]
    Jan 17 07:48:05.929: INFO: Created: latency-svc-nm5vq
    Jan 17 07:48:05.959: INFO: Got endpoints: latency-svc-w2gn8 [754.690826ms]
    Jan 17 07:48:05.994: INFO: Created: latency-svc-jf576
    Jan 17 07:48:06.012: INFO: Got endpoints: latency-svc-lhxgv [758.983605ms]
    Jan 17 07:48:06.047: INFO: Created: latency-svc-kshhm
    Jan 17 07:48:06.058: INFO: Got endpoints: latency-svc-bftv9 [752.975489ms]
    Jan 17 07:48:06.093: INFO: Created: latency-svc-7scpd
    Jan 17 07:48:06.101: INFO: Got endpoints: latency-svc-txc95 [751.014318ms]
    Jan 17 07:48:06.140: INFO: Created: latency-svc-ng5p7
    Jan 17 07:48:06.156: INFO: Got endpoints: latency-svc-mjmqw [742.531175ms]
    Jan 17 07:48:06.180: INFO: Created: latency-svc-h6f2c
    Jan 17 07:48:06.206: INFO: Got endpoints: latency-svc-xtgjg [745.941792ms]
    Jan 17 07:48:06.237: INFO: Created: latency-svc-znk5q
    Jan 17 07:48:06.256: INFO: Got endpoints: latency-svc-sbt6c [747.574003ms]
    Jan 17 07:48:06.290: INFO: Created: latency-svc-qhsml
    Jan 17 07:48:06.304: INFO: Got endpoints: latency-svc-4jv96 [727.197518ms]
    Jan 17 07:48:06.341: INFO: Created: latency-svc-45mbl
    Jan 17 07:48:06.352: INFO: Got endpoints: latency-svc-w447t [749.088552ms]
    Jan 17 07:48:06.374: INFO: Created: latency-svc-4f9zj
    Jan 17 07:48:06.408: INFO: Got endpoints: latency-svc-f74lb [745.426313ms]
    Jan 17 07:48:06.433: INFO: Created: latency-svc-mjx2x
    Jan 17 07:48:06.450: INFO: Got endpoints: latency-svc-lv74h [747.83001ms]
    Jan 17 07:48:06.469: INFO: Created: latency-svc-vppqm
    Jan 17 07:48:06.505: INFO: Got endpoints: latency-svc-54dpf [751.179193ms]
    Jan 17 07:48:06.533: INFO: Created: latency-svc-62vvk
    Jan 17 07:48:06.555: INFO: Got endpoints: latency-svc-l6x52 [754.336385ms]
    Jan 17 07:48:06.580: INFO: Created: latency-svc-hr4gf
    Jan 17 07:48:06.604: INFO: Got endpoints: latency-svc-lj7r2 [748.38042ms]
    Jan 17 07:48:06.632: INFO: Created: latency-svc-jngpg
    Jan 17 07:48:06.661: INFO: Got endpoints: latency-svc-nm5vq [755.470507ms]
    Jan 17 07:48:06.686: INFO: Created: latency-svc-spnpb
    Jan 17 07:48:06.701: INFO: Got endpoints: latency-svc-jf576 [741.908695ms]
    Jan 17 07:48:06.735: INFO: Created: latency-svc-dshzg
    Jan 17 07:48:06.751: INFO: Got endpoints: latency-svc-kshhm [739.731497ms]
    Jan 17 07:48:06.775: INFO: Created: latency-svc-vxskz
    Jan 17 07:48:06.802: INFO: Got endpoints: latency-svc-7scpd [743.93126ms]
    Jan 17 07:48:06.827: INFO: Created: latency-svc-54wh4
    Jan 17 07:48:06.852: INFO: Got endpoints: latency-svc-ng5p7 [750.090696ms]
    Jan 17 07:48:06.887: INFO: Created: latency-svc-vcjbm
    Jan 17 07:48:06.910: INFO: Got endpoints: latency-svc-h6f2c [753.595165ms]
    Jan 17 07:48:06.937: INFO: Created: latency-svc-plmbr
    Jan 17 07:48:06.958: INFO: Got endpoints: latency-svc-znk5q [751.327037ms]
    Jan 17 07:48:06.981: INFO: Created: latency-svc-p689b
    Jan 17 07:48:06.999: INFO: Got endpoints: latency-svc-qhsml [743.141853ms]
    Jan 17 07:48:07.032: INFO: Created: latency-svc-gb6lc
    Jan 17 07:48:07.057: INFO: Got endpoints: latency-svc-45mbl [753.344339ms]
    Jan 17 07:48:07.084: INFO: Created: latency-svc-cxt9p
    Jan 17 07:48:07.103: INFO: Got endpoints: latency-svc-4f9zj [750.508981ms]
    Jan 17 07:48:07.124: INFO: Created: latency-svc-ctdzr
    Jan 17 07:48:07.160: INFO: Got endpoints: latency-svc-mjx2x [751.962765ms]
    Jan 17 07:48:07.182: INFO: Created: latency-svc-qmtmv
    Jan 17 07:48:07.203: INFO: Got endpoints: latency-svc-vppqm [752.668538ms]
    Jan 17 07:48:07.226: INFO: Created: latency-svc-mjww9
    Jan 17 07:48:07.253: INFO: Got endpoints: latency-svc-62vvk [748.335901ms]
    Jan 17 07:48:07.301: INFO: Got endpoints: latency-svc-hr4gf [746.002635ms]
    Jan 17 07:48:07.352: INFO: Got endpoints: latency-svc-jngpg [748.084619ms]
    Jan 17 07:48:07.402: INFO: Got endpoints: latency-svc-spnpb [740.954723ms]
    Jan 17 07:48:07.450: INFO: Got endpoints: latency-svc-dshzg [748.538091ms]
    Jan 17 07:48:07.503: INFO: Got endpoints: latency-svc-vxskz [751.16771ms]
    Jan 17 07:48:07.562: INFO: Got endpoints: latency-svc-54wh4 [759.751714ms]
    Jan 17 07:48:07.604: INFO: Got endpoints: latency-svc-vcjbm [752.521722ms]
    Jan 17 07:48:07.650: INFO: Got endpoints: latency-svc-plmbr [739.990509ms]
    Jan 17 07:48:07.700: INFO: Got endpoints: latency-svc-p689b [742.04829ms]
    Jan 17 07:48:07.763: INFO: Got endpoints: latency-svc-gb6lc [764.262651ms]
    Jan 17 07:48:07.804: INFO: Got endpoints: latency-svc-cxt9p [746.61792ms]
    Jan 17 07:48:07.850: INFO: Got endpoints: latency-svc-ctdzr [747.781416ms]
    Jan 17 07:48:07.903: INFO: Got endpoints: latency-svc-qmtmv [742.759576ms]
    Jan 17 07:48:07.953: INFO: Got endpoints: latency-svc-mjww9 [749.540187ms]
    Jan 17 07:48:07.953: INFO: Latencies: [67.73929ms 99.318199ms 122.031891ms 144.874599ms 193.849085ms 210.373155ms 259.297713ms 301.822505ms 352.570488ms 375.589251ms 411.131424ms 416.082172ms 430.042868ms 430.993632ms 439.043001ms 441.164965ms 443.334803ms 447.203886ms 448.255783ms 451.941299ms 452.216402ms 454.771965ms 455.514327ms 457.191464ms 467.411812ms 469.318669ms 471.174455ms 474.273331ms 476.789483ms 478.120797ms 479.989389ms 480.630945ms 482.111796ms 485.01555ms 486.033096ms 486.599754ms 486.757119ms 487.912411ms 489.2607ms 489.305159ms 493.718504ms 495.774969ms 496.135947ms 496.366109ms 496.999935ms 497.590046ms 497.780517ms 498.074836ms 498.273469ms 498.729586ms 500.351784ms 502.472715ms 502.708556ms 503.819132ms 504.971835ms 506.752363ms 510.654873ms 511.167176ms 511.40613ms 511.602655ms 512.265495ms 515.257077ms 516.674131ms 516.778669ms 516.867348ms 517.708298ms 518.860661ms 521.658384ms 522.97998ms 524.034332ms 524.905553ms 526.434866ms 528.06297ms 528.514111ms 530.530215ms 534.232086ms 535.803253ms 541.302843ms 546.034406ms 546.968443ms 553.701861ms 556.17413ms 565.482461ms 566.015852ms 566.43814ms 567.917138ms 575.331619ms 576.480964ms 579.069288ms 579.324598ms 592.129799ms 601.946056ms 604.026507ms 604.072468ms 608.660627ms 609.636127ms 615.613146ms 616.740848ms 622.464027ms 623.065894ms 625.676725ms 628.327827ms 634.901253ms 644.012129ms 648.917479ms 677.201445ms 684.102367ms 686.791946ms 688.081433ms 689.554507ms 691.895746ms 693.611026ms 694.411504ms 701.605549ms 701.809329ms 703.434375ms 705.591066ms 726.898593ms 727.197518ms 728.495848ms 737.89167ms 738.134602ms 739.731497ms 739.990509ms 740.94606ms 740.954723ms 741.841866ms 741.908695ms 742.04829ms 742.520865ms 742.531175ms 742.759576ms 743.141853ms 743.93126ms 744.002457ms 745.020497ms 745.045697ms 745.201435ms 745.426313ms 745.723061ms 745.941792ms 746.002635ms 746.016404ms 746.221984ms 746.61792ms 747.009549ms 747.574003ms 747.781416ms 747.83001ms 747.976765ms 748.084619ms 748.335901ms 748.38042ms 748.456404ms 748.538091ms 748.963881ms 749.088552ms 749.282259ms 749.540187ms 750.090696ms 750.402982ms 750.508981ms 750.77173ms 750.916262ms 751.014318ms 751.065884ms 751.121507ms 751.16771ms 751.179193ms 751.327037ms 751.332218ms 751.444498ms 751.962765ms 751.986069ms 752.233127ms 752.521722ms 752.668538ms 752.975489ms 753.344339ms 753.595165ms 754.019206ms 754.336385ms 754.657558ms 754.690826ms 755.000114ms 755.470507ms 755.523729ms 757.449092ms 758.983605ms 758.984966ms 759.468165ms 759.638214ms 759.751714ms 763.520131ms 764.262651ms 764.280603ms 767.183209ms 1.044553537s 1.072789984s 1.251125785s]
    Jan 17 07:48:07.953: INFO: 50 %ile: 625.676725ms
    Jan 17 07:48:07.953: INFO: 90 %ile: 754.019206ms
    Jan 17 07:48:07.953: INFO: 99 %ile: 1.072789984s
    Jan 17 07:48:07.953: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 17 07:48:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-7286" for this suite. 01/17/23 07:48:07.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:48:07.979
Jan 17 07:48:07.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:48:07.981
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:08.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:08.016
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:48:08.022
Jan 17 07:48:08.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216" in namespace "projected-8420" to be "Succeeded or Failed"
Jan 17 07:48:08.061: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619666ms
Jan 17 07:48:10.070: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017879657s
Jan 17 07:48:12.079: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027350041s
Jan 17 07:48:14.073: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020575262s
STEP: Saw pod success 01/17/23 07:48:14.073
Jan 17 07:48:14.073: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216" satisfied condition "Succeeded or Failed"
Jan 17 07:48:14.078: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 container client-container: <nil>
STEP: delete the pod 01/17/23 07:48:14.129
Jan 17 07:48:14.202: INFO: Waiting for pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 to disappear
Jan 17 07:48:14.221: INFO: Pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:48:14.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8420" for this suite. 01/17/23 07:48:14.246
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":269,"skipped":4821,"failed":0}
------------------------------
• [SLOW TEST] [6.292 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:48:07.979
    Jan 17 07:48:07.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:48:07.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:08.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:08.016
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:48:08.022
    Jan 17 07:48:08.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216" in namespace "projected-8420" to be "Succeeded or Failed"
    Jan 17 07:48:08.061: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619666ms
    Jan 17 07:48:10.070: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017879657s
    Jan 17 07:48:12.079: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027350041s
    Jan 17 07:48:14.073: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020575262s
    STEP: Saw pod success 01/17/23 07:48:14.073
    Jan 17 07:48:14.073: INFO: Pod "downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216" satisfied condition "Succeeded or Failed"
    Jan 17 07:48:14.078: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 container client-container: <nil>
    STEP: delete the pod 01/17/23 07:48:14.129
    Jan 17 07:48:14.202: INFO: Waiting for pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 to disappear
    Jan 17 07:48:14.221: INFO: Pod downwardapi-volume-c7b551b1-496a-471a-b386-818d86195216 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:48:14.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8420" for this suite. 01/17/23 07:48:14.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:48:14.275
Jan 17 07:48:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:48:14.276
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:14.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:14.353
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 07:48:14.365
Jan 17 07:48:14.390: INFO: Waiting up to 5m0s for pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be" in namespace "emptydir-5146" to be "Succeeded or Failed"
Jan 17 07:48:14.412: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 22.055909ms
Jan 17 07:48:16.431: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040981499s
Jan 17 07:48:18.430: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040279413s
Jan 17 07:48:20.421: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031566117s
STEP: Saw pod success 01/17/23 07:48:20.421
Jan 17 07:48:20.422: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be" satisfied condition "Succeeded or Failed"
Jan 17 07:48:20.427: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be container test-container: <nil>
STEP: delete the pod 01/17/23 07:48:20.497
Jan 17 07:48:20.525: INFO: Waiting for pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be to disappear
Jan 17 07:48:20.531: INFO: Pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:48:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5146" for this suite. 01/17/23 07:48:20.542
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":270,"skipped":4894,"failed":0}
------------------------------
• [SLOW TEST] [6.301 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:48:14.275
    Jan 17 07:48:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:48:14.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:14.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:14.353
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 07:48:14.365
    Jan 17 07:48:14.390: INFO: Waiting up to 5m0s for pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be" in namespace "emptydir-5146" to be "Succeeded or Failed"
    Jan 17 07:48:14.412: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 22.055909ms
    Jan 17 07:48:16.431: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040981499s
    Jan 17 07:48:18.430: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040279413s
    Jan 17 07:48:20.421: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031566117s
    STEP: Saw pod success 01/17/23 07:48:20.421
    Jan 17 07:48:20.422: INFO: Pod "pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be" satisfied condition "Succeeded or Failed"
    Jan 17 07:48:20.427: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be container test-container: <nil>
    STEP: delete the pod 01/17/23 07:48:20.497
    Jan 17 07:48:20.525: INFO: Waiting for pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be to disappear
    Jan 17 07:48:20.531: INFO: Pod pod-9bd8c457-5136-46ac-95f4-ca2b1e7a36be no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:48:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5146" for this suite. 01/17/23 07:48:20.542
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:48:20.576
Jan 17 07:48:20.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename endpointslice 01/17/23 07:48:20.578
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:20.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:20.634
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 07:48:24.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2660" for this suite. 01/17/23 07:48:24.821
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":271,"skipped":4896,"failed":0}
------------------------------
• [4.296 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:48:20.576
    Jan 17 07:48:20.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename endpointslice 01/17/23 07:48:20.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:20.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:20.634
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 07:48:24.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2660" for this suite. 01/17/23 07:48:24.821
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:48:24.874
Jan 17 07:48:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:48:24.877
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:24.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:24.921
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:48:25.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9386" for this suite. 01/17/23 07:48:25.012
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":272,"skipped":4896,"failed":0}
------------------------------
• [0.151 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:48:24.874
    Jan 17 07:48:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:48:24.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:24.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:24.921
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:48:25.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9386" for this suite. 01/17/23 07:48:25.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:48:25.029
Jan 17 07:48:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:48:25.03
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:25.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:25.066
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-abb00a3e-8f13-40c6-9492-90342b245112 01/17/23 07:48:25.082
STEP: Creating the pod 01/17/23 07:48:25.092
Jan 17 07:48:25.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90" in namespace "configmap-3998" to be "running and ready"
Jan 17 07:48:25.118: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01992ms
Jan 17 07:48:25.118: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:48:27.126: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015861068s
Jan 17 07:48:27.126: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:48:29.127: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Running", Reason="", readiness=true. Elapsed: 4.016541756s
Jan 17 07:48:29.127: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Running (Ready = true)
Jan 17 07:48:29.127: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-abb00a3e-8f13-40c6-9492-90342b245112 01/17/23 07:48:29.153
STEP: waiting to observe update in volume 01/17/23 07:48:29.164
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:49:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3998" for this suite. 01/17/23 07:49:45.781
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":273,"skipped":4925,"failed":0}
------------------------------
• [SLOW TEST] [80.772 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:48:25.029
    Jan 17 07:48:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:48:25.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:48:25.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:48:25.066
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-abb00a3e-8f13-40c6-9492-90342b245112 01/17/23 07:48:25.082
    STEP: Creating the pod 01/17/23 07:48:25.092
    Jan 17 07:48:25.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90" in namespace "configmap-3998" to be "running and ready"
    Jan 17 07:48:25.118: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01992ms
    Jan 17 07:48:25.118: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:48:27.126: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015861068s
    Jan 17 07:48:27.126: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:48:29.127: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90": Phase="Running", Reason="", readiness=true. Elapsed: 4.016541756s
    Jan 17 07:48:29.127: INFO: The phase of Pod pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90 is Running (Ready = true)
    Jan 17 07:48:29.127: INFO: Pod "pod-configmaps-9d6694a7-93df-4a6b-bbc5-4aee93d17f90" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-abb00a3e-8f13-40c6-9492-90342b245112 01/17/23 07:48:29.153
    STEP: waiting to observe update in volume 01/17/23 07:48:29.164
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:49:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3998" for this suite. 01/17/23 07:49:45.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:49:45.807
Jan 17 07:49:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubelet-test 01/17/23 07:49:45.809
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:49:45.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:49:45.846
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/17/23 07:49:45.882
Jan 17 07:49:45.882: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4" in namespace "kubelet-test-1164" to be "completed"
Jan 17 07:49:45.897: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.900125ms
Jan 17 07:49:47.905: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02312497s
Jan 17 07:49:49.907: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025245319s
Jan 17 07:49:49.907: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 07:49:49.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1164" for this suite. 01/17/23 07:49:49.924
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":274,"skipped":4950,"failed":0}
------------------------------
• [4.129 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:49:45.807
    Jan 17 07:49:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 07:49:45.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:49:45.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:49:45.846
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/17/23 07:49:45.882
    Jan 17 07:49:45.882: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4" in namespace "kubelet-test-1164" to be "completed"
    Jan 17 07:49:45.897: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.900125ms
    Jan 17 07:49:47.905: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02312497s
    Jan 17 07:49:49.907: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025245319s
    Jan 17 07:49:49.907: INFO: Pod "agnhost-host-aliases993d462f-1dc8-46f2-8564-72be945aefe4" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 07:49:49.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1164" for this suite. 01/17/23 07:49:49.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:49:49.937
Jan 17 07:49:49.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:49:49.939
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:49:49.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:49:49.981
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:49:50.02
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:49:51.136
STEP: Deploying the webhook pod 01/17/23 07:49:51.162
STEP: Wait for the deployment to be ready 01/17/23 07:49:51.216
Jan 17 07:49:51.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/17/23 07:49:53.285
STEP: Verifying the service has paired with the endpoint 01/17/23 07:49:53.316
Jan 17 07:49:54.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/17/23 07:49:54.323
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:54.323
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/17/23 07:49:54.355
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/17/23 07:49:55.378
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:55.379
STEP: Having no error when timeout is longer than webhook latency 01/17/23 07:49:56.452
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:56.452
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/17/23 07:50:01.535
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:50:01.536
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:50:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8345" for this suite. 01/17/23 07:50:06.631
STEP: Destroying namespace "webhook-8345-markers" for this suite. 01/17/23 07:50:06.649
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":275,"skipped":4977,"failed":0}
------------------------------
• [SLOW TEST] [16.848 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:49:49.937
    Jan 17 07:49:49.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:49:49.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:49:49.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:49:49.981
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:49:50.02
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:49:51.136
    STEP: Deploying the webhook pod 01/17/23 07:49:51.162
    STEP: Wait for the deployment to be ready 01/17/23 07:49:51.216
    Jan 17 07:49:51.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 49, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/17/23 07:49:53.285
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:49:53.316
    Jan 17 07:49:54.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/17/23 07:49:54.323
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:54.323
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/17/23 07:49:54.355
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/17/23 07:49:55.378
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:55.379
    STEP: Having no error when timeout is longer than webhook latency 01/17/23 07:49:56.452
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:49:56.452
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/17/23 07:50:01.535
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 07:50:01.536
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:50:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8345" for this suite. 01/17/23 07:50:06.631
    STEP: Destroying namespace "webhook-8345-markers" for this suite. 01/17/23 07:50:06.649
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:06.788
Jan 17 07:50:06.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:50:06.79
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:06.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:06.854
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/17/23 07:50:06.89
Jan 17 07:50:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 create -f -'
Jan 17 07:50:08.476: INFO: stderr: ""
Jan 17 07:50:08.476: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:08.476
Jan 17 07:50:08.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:50:08.630: INFO: stderr: ""
Jan 17 07:50:08.630: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-gmh7v "
Jan 17 07:50:08.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:08.782: INFO: stderr: ""
Jan 17 07:50:08.782: INFO: stdout: ""
Jan 17 07:50:08.782: INFO: update-demo-nautilus-6vxmq is created but not running
Jan 17 07:50:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:50:13.915: INFO: stderr: ""
Jan 17 07:50:13.915: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-gmh7v "
Jan 17 07:50:13.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:14.031: INFO: stderr: ""
Jan 17 07:50:14.031: INFO: stdout: "true"
Jan 17 07:50:14.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:50:14.183: INFO: stderr: ""
Jan 17 07:50:14.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:50:14.183: INFO: validating pod update-demo-nautilus-6vxmq
Jan 17 07:50:14.196: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:50:14.196: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:50:14.196: INFO: update-demo-nautilus-6vxmq is verified up and running
Jan 17 07:50:14.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-gmh7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:14.328: INFO: stderr: ""
Jan 17 07:50:14.328: INFO: stdout: "true"
Jan 17 07:50:14.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-gmh7v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:50:14.443: INFO: stderr: ""
Jan 17 07:50:14.443: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:50:14.443: INFO: validating pod update-demo-nautilus-gmh7v
Jan 17 07:50:14.450: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:50:14.450: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:50:14.450: INFO: update-demo-nautilus-gmh7v is verified up and running
STEP: scaling down the replication controller 01/17/23 07:50:14.45
Jan 17 07:50:14.452: INFO: scanned /root for discovery docs: <nil>
Jan 17 07:50:14.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 17 07:50:15.656: INFO: stderr: ""
Jan 17 07:50:15.656: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:15.656
Jan 17 07:50:15.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:50:15.768: INFO: stderr: ""
Jan 17 07:50:15.768: INFO: stdout: "update-demo-nautilus-6vxmq "
Jan 17 07:50:15.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:15.907: INFO: stderr: ""
Jan 17 07:50:15.907: INFO: stdout: "true"
Jan 17 07:50:15.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:50:16.040: INFO: stderr: ""
Jan 17 07:50:16.040: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:50:16.040: INFO: validating pod update-demo-nautilus-6vxmq
Jan 17 07:50:16.046: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:50:16.046: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:50:16.047: INFO: update-demo-nautilus-6vxmq is verified up and running
STEP: scaling up the replication controller 01/17/23 07:50:16.047
Jan 17 07:50:16.049: INFO: scanned /root for discovery docs: <nil>
Jan 17 07:50:16.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 17 07:50:17.223: INFO: stderr: ""
Jan 17 07:50:17.223: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:17.223
Jan 17 07:50:17.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 07:50:17.411: INFO: stderr: ""
Jan 17 07:50:17.411: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-kkxxq "
Jan 17 07:50:17.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:17.556: INFO: stderr: ""
Jan 17 07:50:17.556: INFO: stdout: "true"
Jan 17 07:50:17.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:50:17.686: INFO: stderr: ""
Jan 17 07:50:17.686: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:50:17.686: INFO: validating pod update-demo-nautilus-6vxmq
Jan 17 07:50:17.692: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:50:17.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:50:17.692: INFO: update-demo-nautilus-6vxmq is verified up and running
Jan 17 07:50:17.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-kkxxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 07:50:17.840: INFO: stderr: ""
Jan 17 07:50:17.841: INFO: stdout: "true"
Jan 17 07:50:17.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-kkxxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 07:50:17.967: INFO: stderr: ""
Jan 17 07:50:17.967: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 07:50:17.967: INFO: validating pod update-demo-nautilus-kkxxq
Jan 17 07:50:17.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 07:50:17.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 07:50:17.975: INFO: update-demo-nautilus-kkxxq is verified up and running
STEP: using delete to clean up resources 01/17/23 07:50:17.975
Jan 17 07:50:17.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 delete --grace-period=0 --force -f -'
Jan 17 07:50:18.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 07:50:18.098: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 07:50:18.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get rc,svc -l name=update-demo --no-headers'
Jan 17 07:50:18.235: INFO: stderr: "No resources found in kubectl-4469 namespace.\n"
Jan 17 07:50:18.235: INFO: stdout: ""
Jan 17 07:50:18.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 07:50:18.415: INFO: stderr: ""
Jan 17 07:50:18.415: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:50:18.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4469" for this suite. 01/17/23 07:50:18.424
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":276,"skipped":4998,"failed":0}
------------------------------
• [SLOW TEST] [11.648 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:06.788
    Jan 17 07:50:06.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:50:06.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:06.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:06.854
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/17/23 07:50:06.89
    Jan 17 07:50:06.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 create -f -'
    Jan 17 07:50:08.476: INFO: stderr: ""
    Jan 17 07:50:08.476: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:08.476
    Jan 17 07:50:08.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:50:08.630: INFO: stderr: ""
    Jan 17 07:50:08.630: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-gmh7v "
    Jan 17 07:50:08.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:08.782: INFO: stderr: ""
    Jan 17 07:50:08.782: INFO: stdout: ""
    Jan 17 07:50:08.782: INFO: update-demo-nautilus-6vxmq is created but not running
    Jan 17 07:50:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:50:13.915: INFO: stderr: ""
    Jan 17 07:50:13.915: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-gmh7v "
    Jan 17 07:50:13.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:14.031: INFO: stderr: ""
    Jan 17 07:50:14.031: INFO: stdout: "true"
    Jan 17 07:50:14.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:50:14.183: INFO: stderr: ""
    Jan 17 07:50:14.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:50:14.183: INFO: validating pod update-demo-nautilus-6vxmq
    Jan 17 07:50:14.196: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:50:14.196: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:50:14.196: INFO: update-demo-nautilus-6vxmq is verified up and running
    Jan 17 07:50:14.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-gmh7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:14.328: INFO: stderr: ""
    Jan 17 07:50:14.328: INFO: stdout: "true"
    Jan 17 07:50:14.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-gmh7v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:50:14.443: INFO: stderr: ""
    Jan 17 07:50:14.443: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:50:14.443: INFO: validating pod update-demo-nautilus-gmh7v
    Jan 17 07:50:14.450: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:50:14.450: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:50:14.450: INFO: update-demo-nautilus-gmh7v is verified up and running
    STEP: scaling down the replication controller 01/17/23 07:50:14.45
    Jan 17 07:50:14.452: INFO: scanned /root for discovery docs: <nil>
    Jan 17 07:50:14.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 17 07:50:15.656: INFO: stderr: ""
    Jan 17 07:50:15.656: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:15.656
    Jan 17 07:50:15.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:50:15.768: INFO: stderr: ""
    Jan 17 07:50:15.768: INFO: stdout: "update-demo-nautilus-6vxmq "
    Jan 17 07:50:15.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:15.907: INFO: stderr: ""
    Jan 17 07:50:15.907: INFO: stdout: "true"
    Jan 17 07:50:15.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:50:16.040: INFO: stderr: ""
    Jan 17 07:50:16.040: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:50:16.040: INFO: validating pod update-demo-nautilus-6vxmq
    Jan 17 07:50:16.046: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:50:16.046: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:50:16.047: INFO: update-demo-nautilus-6vxmq is verified up and running
    STEP: scaling up the replication controller 01/17/23 07:50:16.047
    Jan 17 07:50:16.049: INFO: scanned /root for discovery docs: <nil>
    Jan 17 07:50:16.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 17 07:50:17.223: INFO: stderr: ""
    Jan 17 07:50:17.223: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 07:50:17.223
    Jan 17 07:50:17.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 07:50:17.411: INFO: stderr: ""
    Jan 17 07:50:17.411: INFO: stdout: "update-demo-nautilus-6vxmq update-demo-nautilus-kkxxq "
    Jan 17 07:50:17.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:17.556: INFO: stderr: ""
    Jan 17 07:50:17.556: INFO: stdout: "true"
    Jan 17 07:50:17.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-6vxmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:50:17.686: INFO: stderr: ""
    Jan 17 07:50:17.686: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:50:17.686: INFO: validating pod update-demo-nautilus-6vxmq
    Jan 17 07:50:17.692: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:50:17.692: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:50:17.692: INFO: update-demo-nautilus-6vxmq is verified up and running
    Jan 17 07:50:17.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-kkxxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 07:50:17.840: INFO: stderr: ""
    Jan 17 07:50:17.841: INFO: stdout: "true"
    Jan 17 07:50:17.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods update-demo-nautilus-kkxxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 07:50:17.967: INFO: stderr: ""
    Jan 17 07:50:17.967: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 07:50:17.967: INFO: validating pod update-demo-nautilus-kkxxq
    Jan 17 07:50:17.975: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 07:50:17.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 07:50:17.975: INFO: update-demo-nautilus-kkxxq is verified up and running
    STEP: using delete to clean up resources 01/17/23 07:50:17.975
    Jan 17 07:50:17.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 delete --grace-period=0 --force -f -'
    Jan 17 07:50:18.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 07:50:18.098: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 17 07:50:18.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get rc,svc -l name=update-demo --no-headers'
    Jan 17 07:50:18.235: INFO: stderr: "No resources found in kubectl-4469 namespace.\n"
    Jan 17 07:50:18.235: INFO: stdout: ""
    Jan 17 07:50:18.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-4469 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 07:50:18.415: INFO: stderr: ""
    Jan 17 07:50:18.415: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:50:18.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4469" for this suite. 01/17/23 07:50:18.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:18.445
Jan 17 07:50:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:50:18.447
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:18.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:18.493
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/17/23 07:50:18.501
Jan 17 07:50:18.518: INFO: Waiting up to 5m0s for pod "pod-k44sb" in namespace "pods-2041" to be "running"
Jan 17 07:50:18.527: INFO: Pod "pod-k44sb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.425624ms
Jan 17 07:50:20.534: INFO: Pod "pod-k44sb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015814562s
Jan 17 07:50:22.535: INFO: Pod "pod-k44sb": Phase="Running", Reason="", readiness=true. Elapsed: 4.016569793s
Jan 17 07:50:22.535: INFO: Pod "pod-k44sb" satisfied condition "running"
STEP: patching /status 01/17/23 07:50:22.535
Jan 17 07:50:22.555: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:50:22.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2041" for this suite. 01/17/23 07:50:22.562
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":277,"skipped":5059,"failed":0}
------------------------------
• [4.136 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:18.445
    Jan 17 07:50:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:50:18.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:18.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:18.493
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/17/23 07:50:18.501
    Jan 17 07:50:18.518: INFO: Waiting up to 5m0s for pod "pod-k44sb" in namespace "pods-2041" to be "running"
    Jan 17 07:50:18.527: INFO: Pod "pod-k44sb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.425624ms
    Jan 17 07:50:20.534: INFO: Pod "pod-k44sb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015814562s
    Jan 17 07:50:22.535: INFO: Pod "pod-k44sb": Phase="Running", Reason="", readiness=true. Elapsed: 4.016569793s
    Jan 17 07:50:22.535: INFO: Pod "pod-k44sb" satisfied condition "running"
    STEP: patching /status 01/17/23 07:50:22.535
    Jan 17 07:50:22.555: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:50:22.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2041" for this suite. 01/17/23 07:50:22.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:22.584
Jan 17 07:50:22.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:50:22.587
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:22.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:22.63
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/17/23 07:50:22.639
Jan 17 07:50:22.657: INFO: Waiting up to 5m0s for pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126" in namespace "downward-api-8457" to be "Succeeded or Failed"
Jan 17 07:50:22.682: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Pending", Reason="", readiness=false. Elapsed: 24.829235ms
Jan 17 07:50:24.690: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033343387s
Jan 17 07:50:26.691: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033819213s
STEP: Saw pod success 01/17/23 07:50:26.691
Jan 17 07:50:26.691: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126" satisfied condition "Succeeded or Failed"
Jan 17 07:50:26.697: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 container dapi-container: <nil>
STEP: delete the pod 01/17/23 07:50:26.716
Jan 17 07:50:26.746: INFO: Waiting for pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 to disappear
Jan 17 07:50:26.752: INFO: Pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 07:50:26.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8457" for this suite. 01/17/23 07:50:26.76
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":278,"skipped":5081,"failed":0}
------------------------------
• [4.194 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:22.584
    Jan 17 07:50:22.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:50:22.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:22.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:22.63
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/17/23 07:50:22.639
    Jan 17 07:50:22.657: INFO: Waiting up to 5m0s for pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126" in namespace "downward-api-8457" to be "Succeeded or Failed"
    Jan 17 07:50:22.682: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Pending", Reason="", readiness=false. Elapsed: 24.829235ms
    Jan 17 07:50:24.690: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033343387s
    Jan 17 07:50:26.691: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033819213s
    STEP: Saw pod success 01/17/23 07:50:26.691
    Jan 17 07:50:26.691: INFO: Pod "downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126" satisfied condition "Succeeded or Failed"
    Jan 17 07:50:26.697: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 07:50:26.716
    Jan 17 07:50:26.746: INFO: Waiting for pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 to disappear
    Jan 17 07:50:26.752: INFO: Pod downward-api-69a3d8a1-28c0-48a4-9fce-c5ab1fb0e126 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 07:50:26.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8457" for this suite. 01/17/23 07:50:26.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:26.779
Jan 17 07:50:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:50:26.781
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:26.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:26.816
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-6864 01/17/23 07:50:26.825
STEP: creating service affinity-nodeport in namespace services-6864 01/17/23 07:50:26.825
STEP: creating replication controller affinity-nodeport in namespace services-6864 01/17/23 07:50:26.859
I0117 07:50:26.883701      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6864, replica count: 3
I0117 07:50:29.937439      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:50:29.983: INFO: Creating new exec pod
Jan 17 07:50:30.006: INFO: Waiting up to 5m0s for pod "execpod-affinityzbdj8" in namespace "services-6864" to be "running"
Jan 17 07:50:30.022: INFO: Pod "execpod-affinityzbdj8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.187155ms
Jan 17 07:50:32.029: INFO: Pod "execpod-affinityzbdj8": Phase="Running", Reason="", readiness=true. Elapsed: 2.023349636s
Jan 17 07:50:32.029: INFO: Pod "execpod-affinityzbdj8" satisfied condition "running"
Jan 17 07:50:33.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 17 07:50:33.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 17 07:50:33.370: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:33.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.170.70 80'
Jan 17 07:50:33.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.170.70 80\nConnection to 10.254.170.70 80 port [tcp/http] succeeded!\n"
Jan 17 07:50:33.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:33.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30818'
Jan 17 07:50:34.005: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30818\nConnection to 10.0.0.22 30818 port [tcp/*] succeeded!\n"
Jan 17 07:50:34.005: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:34.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30818'
Jan 17 07:50:34.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30818\nConnection to 10.0.0.16 30818 port [tcp/*] succeeded!\n"
Jan 17 07:50:34.315: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:34.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30818/ ; done'
Jan 17 07:50:34.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n"
Jan 17 07:50:34.739: INFO: stdout: "\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl"
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
Jan 17 07:50:34.740: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6864, will wait for the garbage collector to delete the pods 01/17/23 07:50:34.781
Jan 17 07:50:34.865: INFO: Deleting ReplicationController affinity-nodeport took: 14.866738ms
Jan 17 07:50:34.967: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.768652ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:50:37.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6864" for this suite. 01/17/23 07:50:37.089
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":279,"skipped":5089,"failed":0}
------------------------------
• [SLOW TEST] [10.325 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:26.779
    Jan 17 07:50:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:50:26.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:26.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:26.816
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-6864 01/17/23 07:50:26.825
    STEP: creating service affinity-nodeport in namespace services-6864 01/17/23 07:50:26.825
    STEP: creating replication controller affinity-nodeport in namespace services-6864 01/17/23 07:50:26.859
    I0117 07:50:26.883701      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6864, replica count: 3
    I0117 07:50:29.937439      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:50:29.983: INFO: Creating new exec pod
    Jan 17 07:50:30.006: INFO: Waiting up to 5m0s for pod "execpod-affinityzbdj8" in namespace "services-6864" to be "running"
    Jan 17 07:50:30.022: INFO: Pod "execpod-affinityzbdj8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.187155ms
    Jan 17 07:50:32.029: INFO: Pod "execpod-affinityzbdj8": Phase="Running", Reason="", readiness=true. Elapsed: 2.023349636s
    Jan 17 07:50:32.029: INFO: Pod "execpod-affinityzbdj8" satisfied condition "running"
    Jan 17 07:50:33.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 17 07:50:33.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 17 07:50:33.370: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:33.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.170.70 80'
    Jan 17 07:50:33.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.170.70 80\nConnection to 10.254.170.70 80 port [tcp/http] succeeded!\n"
    Jan 17 07:50:33.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:33.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30818'
    Jan 17 07:50:34.005: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30818\nConnection to 10.0.0.22 30818 port [tcp/*] succeeded!\n"
    Jan 17 07:50:34.005: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:34.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30818'
    Jan 17 07:50:34.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30818\nConnection to 10.0.0.16 30818 port [tcp/*] succeeded!\n"
    Jan 17 07:50:34.315: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:34.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-6864 exec execpod-affinityzbdj8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30818/ ; done'
    Jan 17 07:50:34.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30818/\n"
    Jan 17 07:50:34.739: INFO: stdout: "\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl\naffinity-nodeport-x79jl"
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.739: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Received response from host: affinity-nodeport-x79jl
    Jan 17 07:50:34.740: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6864, will wait for the garbage collector to delete the pods 01/17/23 07:50:34.781
    Jan 17 07:50:34.865: INFO: Deleting ReplicationController affinity-nodeport took: 14.866738ms
    Jan 17 07:50:34.967: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.768652ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:50:37.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6864" for this suite. 01/17/23 07:50:37.089
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:37.105
Jan 17 07:50:37.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-webhook 01/17/23 07:50:37.107
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:37.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:37.157
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/17/23 07:50:37.168
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 07:50:37.75
STEP: Deploying the custom resource conversion webhook pod 01/17/23 07:50:37.767
STEP: Wait for the deployment to be ready 01/17/23 07:50:37.795
Jan 17 07:50:37.821: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 07:50:39.844
STEP: Verifying the service has paired with the endpoint 01/17/23 07:50:39.866
Jan 17 07:50:40.867: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 17 07:50:40.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Creating a v1 custom resource 01/17/23 07:50:43.505
STEP: Create a v2 custom resource 01/17/23 07:50:43.542
STEP: List CRs in v1 01/17/23 07:50:43.609
STEP: List CRs in v2 01/17/23 07:50:43.617
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:50:44.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3130" for this suite. 01/17/23 07:50:44.169
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":280,"skipped":5092,"failed":0}
------------------------------
• [SLOW TEST] [7.231 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:37.105
    Jan 17 07:50:37.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-webhook 01/17/23 07:50:37.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:37.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:37.157
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/17/23 07:50:37.168
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 07:50:37.75
    STEP: Deploying the custom resource conversion webhook pod 01/17/23 07:50:37.767
    STEP: Wait for the deployment to be ready 01/17/23 07:50:37.795
    Jan 17 07:50:37.821: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 07:50:39.844
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:50:39.866
    Jan 17 07:50:40.867: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 17 07:50:40.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Creating a v1 custom resource 01/17/23 07:50:43.505
    STEP: Create a v2 custom resource 01/17/23 07:50:43.542
    STEP: List CRs in v1 01/17/23 07:50:43.609
    STEP: List CRs in v2 01/17/23 07:50:43.617
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:50:44.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3130" for this suite. 01/17/23 07:50:44.169
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:44.336
Jan 17 07:50:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 07:50:44.337
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:44.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:44.427
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-7934 01/17/23 07:50:44.44
STEP: creating service affinity-clusterip-transition in namespace services-7934 01/17/23 07:50:44.44
STEP: creating replication controller affinity-clusterip-transition in namespace services-7934 01/17/23 07:50:44.494
I0117 07:50:44.510077      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7934, replica count: 3
I0117 07:50:47.566233      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 07:50:50.566428      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:50:50.576: INFO: Creating new exec pod
Jan 17 07:50:50.600: INFO: Waiting up to 5m0s for pod "execpod-affinityqhnzr" in namespace "services-7934" to be "running"
Jan 17 07:50:50.617: INFO: Pod "execpod-affinityqhnzr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.203633ms
Jan 17 07:50:52.627: INFO: Pod "execpod-affinityqhnzr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026139209s
Jan 17 07:50:54.623: INFO: Pod "execpod-affinityqhnzr": Phase="Running", Reason="", readiness=true. Elapsed: 4.022237106s
Jan 17 07:50:54.623: INFO: Pod "execpod-affinityqhnzr" satisfied condition "running"
Jan 17 07:50:55.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 17 07:50:55.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 17 07:50:55.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:55.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.122.129 80'
Jan 17 07:50:56.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.122.129 80\nConnection to 10.254.122.129 80 port [tcp/http] succeeded!\n"
Jan 17 07:50:56.170: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 07:50:56.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.122.129:80/ ; done'
Jan 17 07:50:56.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n"
Jan 17 07:50:56.614: INFO: stdout: "\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-zs2q9"
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-bsr9l
Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-zs2q9
Jan 17 07:50:56.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.122.129:80/ ; done'
Jan 17 07:50:57.118: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n"
Jan 17 07:50:57.118: INFO: stdout: "\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r"
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
Jan 17 07:50:57.118: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7934, will wait for the garbage collector to delete the pods 01/17/23 07:50:57.155
Jan 17 07:50:57.224: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.666036ms
Jan 17 07:50:57.328: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 104.11683ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 07:50:59.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7934" for this suite. 01/17/23 07:50:59.318
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":281,"skipped":5093,"failed":0}
------------------------------
• [SLOW TEST] [14.999 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:44.336
    Jan 17 07:50:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 07:50:44.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:44.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:44.427
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-7934 01/17/23 07:50:44.44
    STEP: creating service affinity-clusterip-transition in namespace services-7934 01/17/23 07:50:44.44
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7934 01/17/23 07:50:44.494
    I0117 07:50:44.510077      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7934, replica count: 3
    I0117 07:50:47.566233      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 07:50:50.566428      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:50:50.576: INFO: Creating new exec pod
    Jan 17 07:50:50.600: INFO: Waiting up to 5m0s for pod "execpod-affinityqhnzr" in namespace "services-7934" to be "running"
    Jan 17 07:50:50.617: INFO: Pod "execpod-affinityqhnzr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.203633ms
    Jan 17 07:50:52.627: INFO: Pod "execpod-affinityqhnzr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026139209s
    Jan 17 07:50:54.623: INFO: Pod "execpod-affinityqhnzr": Phase="Running", Reason="", readiness=true. Elapsed: 4.022237106s
    Jan 17 07:50:54.623: INFO: Pod "execpod-affinityqhnzr" satisfied condition "running"
    Jan 17 07:50:55.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 17 07:50:55.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 17 07:50:55.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:55.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.122.129 80'
    Jan 17 07:50:56.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.122.129 80\nConnection to 10.254.122.129 80 port [tcp/http] succeeded!\n"
    Jan 17 07:50:56.170: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 07:50:56.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.122.129:80/ ; done'
    Jan 17 07:50:56.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n"
    Jan 17 07:50:56.614: INFO: stdout: "\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-bsr9l\naffinity-clusterip-transition-zs2q9\naffinity-clusterip-transition-zs2q9"
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-bsr9l
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:56.614: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-bsr9l
    Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.615: INFO: Received response from host: affinity-clusterip-transition-zs2q9
    Jan 17 07:50:56.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-7934 exec execpod-affinityqhnzr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.122.129:80/ ; done'
    Jan 17 07:50:57.118: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.122.129:80/\n"
    Jan 17 07:50:57.118: INFO: stdout: "\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r\naffinity-clusterip-transition-v4f4r"
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Received response from host: affinity-clusterip-transition-v4f4r
    Jan 17 07:50:57.118: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7934, will wait for the garbage collector to delete the pods 01/17/23 07:50:57.155
    Jan 17 07:50:57.224: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.666036ms
    Jan 17 07:50:57.328: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 104.11683ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 07:50:59.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7934" for this suite. 01/17/23 07:50:59.318
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:59.336
Jan 17 07:50:59.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename sysctl 01/17/23 07:50:59.338
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:59.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:59.39
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/17/23 07:50:59.402
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 07:50:59.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1794" for this suite. 01/17/23 07:50:59.426
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":282,"skipped":5109,"failed":0}
------------------------------
• [0.119 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:59.336
    Jan 17 07:50:59.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename sysctl 01/17/23 07:50:59.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:59.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:59.39
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/17/23 07:50:59.402
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 07:50:59.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1794" for this suite. 01/17/23 07:50:59.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:50:59.456
Jan 17 07:50:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 07:50:59.457
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:59.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:59.516
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/17/23 07:50:59.523
Jan 17 07:50:59.535: INFO: Waiting up to 5m0s for pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc" in namespace "pods-4250" to be "running and ready"
Jan 17 07:50:59.561: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.51874ms
Jan 17 07:50:59.561: INFO: The phase of Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:51:01.582: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.046690693s
Jan 17 07:51:01.582: INFO: The phase of Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc is Running (Ready = true)
Jan 17 07:51:01.582: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc" satisfied condition "running and ready"
Jan 17 07:51:01.595: INFO: Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc has hostIP: 10.0.0.22
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 07:51:01.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4250" for this suite. 01/17/23 07:51:01.613
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":283,"skipped":5123,"failed":0}
------------------------------
• [2.206 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:50:59.456
    Jan 17 07:50:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 07:50:59.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:50:59.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:50:59.516
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/17/23 07:50:59.523
    Jan 17 07:50:59.535: INFO: Waiting up to 5m0s for pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc" in namespace "pods-4250" to be "running and ready"
    Jan 17 07:50:59.561: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.51874ms
    Jan 17 07:50:59.561: INFO: The phase of Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:51:01.582: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc": Phase="Running", Reason="", readiness=true. Elapsed: 2.046690693s
    Jan 17 07:51:01.582: INFO: The phase of Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc is Running (Ready = true)
    Jan 17 07:51:01.582: INFO: Pod "pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc" satisfied condition "running and ready"
    Jan 17 07:51:01.595: INFO: Pod pod-hostip-6fce15a5-c36d-4bee-bbf9-e7d7594a72cc has hostIP: 10.0.0.22
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 07:51:01.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4250" for this suite. 01/17/23 07:51:01.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:51:01.662
Jan 17 07:51:01.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-probe 01/17/23 07:51:01.664
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:51:01.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:51:01.749
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 07:52:01.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3411" for this suite. 01/17/23 07:52:01.814
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":284,"skipped":5128,"failed":0}
------------------------------
• [SLOW TEST] [60.173 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:51:01.662
    Jan 17 07:51:01.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-probe 01/17/23 07:51:01.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:51:01.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:51:01.749
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 07:52:01.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3411" for this suite. 01/17/23 07:52:01.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:01.851
Jan 17 07:52:01.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption 01/17/23 07:52:01.852
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:01.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:01.895
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:01.903
Jan 17 07:52:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption-2 01/17/23 07:52:01.904
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:01.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:01.95
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/17/23 07:52:01.973
STEP: Waiting for the pdb to be processed 01/17/23 07:52:04.004
STEP: Waiting for the pdb to be processed 01/17/23 07:52:04.029
STEP: listing a collection of PDBs across all namespaces 01/17/23 07:52:04.039
STEP: listing a collection of PDBs in namespace disruption-5883 01/17/23 07:52:04.049
STEP: deleting a collection of PDBs 01/17/23 07:52:04.054
STEP: Waiting for the PDB collection to be deleted 01/17/23 07:52:04.085
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 17 07:52:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-572" for this suite. 01/17/23 07:52:04.103
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 07:52:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5883" for this suite. 01/17/23 07:52:04.137
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":285,"skipped":5242,"failed":0}
------------------------------
• [2.309 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:01.851
    Jan 17 07:52:01.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption 01/17/23 07:52:01.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:01.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:01.895
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:01.903
    Jan 17 07:52:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption-2 01/17/23 07:52:01.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:01.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:01.95
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/17/23 07:52:01.973
    STEP: Waiting for the pdb to be processed 01/17/23 07:52:04.004
    STEP: Waiting for the pdb to be processed 01/17/23 07:52:04.029
    STEP: listing a collection of PDBs across all namespaces 01/17/23 07:52:04.039
    STEP: listing a collection of PDBs in namespace disruption-5883 01/17/23 07:52:04.049
    STEP: deleting a collection of PDBs 01/17/23 07:52:04.054
    STEP: Waiting for the PDB collection to be deleted 01/17/23 07:52:04.085
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 17 07:52:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-572" for this suite. 01/17/23 07:52:04.103
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 07:52:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5883" for this suite. 01/17/23 07:52:04.137
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:04.16
Jan 17 07:52:04.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:52:04.161
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:04.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:04.207
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/17/23 07:52:04.215
Jan 17 07:52:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:52:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:52:33.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1879" for this suite. 01/17/23 07:52:33.496
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":286,"skipped":5243,"failed":0}
------------------------------
• [SLOW TEST] [29.364 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:04.16
    Jan 17 07:52:04.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:52:04.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:04.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:04.207
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/17/23 07:52:04.215
    Jan 17 07:52:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:52:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:52:33.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1879" for this suite. 01/17/23 07:52:33.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:33.538
Jan 17 07:52:33.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename proxy 01/17/23 07:52:33.539
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:33.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:33.616
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/17/23 07:52:33.648
STEP: creating replication controller proxy-service-pxtzf in namespace proxy-9283 01/17/23 07:52:33.649
I0117 07:52:33.695509      23 runners.go:193] Created replication controller with name: proxy-service-pxtzf, namespace: proxy-9283, replica count: 1
I0117 07:52:34.746450      23 runners.go:193] proxy-service-pxtzf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 07:52:35.747603      23 runners.go:193] proxy-service-pxtzf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 07:52:35.755: INFO: setup took 2.131812666s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/17/23 07:52:35.755
Jan 17 07:52:35.774: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 18.763421ms)
Jan 17 07:52:35.781: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 25.829245ms)
Jan 17 07:52:35.792: INFO: (0) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 36.245793ms)
Jan 17 07:52:35.796: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 39.815269ms)
Jan 17 07:52:35.804: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 48.144899ms)
Jan 17 07:52:35.807: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 51.669047ms)
Jan 17 07:52:35.807: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 51.630284ms)
Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 51.539385ms)
Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 51.667135ms)
Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 51.628978ms)
Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 52.432302ms)
Jan 17 07:52:35.812: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 55.668171ms)
Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 60.495598ms)
Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 60.729094ms)
Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 61.054498ms)
Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 60.877265ms)
Jan 17 07:52:35.827: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 9.797551ms)
Jan 17 07:52:35.834: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 17.179215ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.329686ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.426364ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 22.628731ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.888457ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 22.788581ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.943235ms)
Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 22.952225ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 26.522785ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 26.85642ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 26.591773ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 26.657111ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 26.693602ms)
Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 26.501296ms)
Jan 17 07:52:35.845: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.855477ms)
Jan 17 07:52:35.854: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 8.93637ms)
Jan 17 07:52:35.855: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 9.452838ms)
Jan 17 07:52:35.856: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 10.445569ms)
Jan 17 07:52:35.856: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 10.606323ms)
Jan 17 07:52:35.859: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 12.930242ms)
Jan 17 07:52:35.860: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.743712ms)
Jan 17 07:52:35.861: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 15.091783ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 17.225412ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 17.330842ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 16.797928ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.831269ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 17.215844ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 17.331642ms)
Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 17.678861ms)
Jan 17 07:52:35.865: INFO: (2) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.104408ms)
Jan 17 07:52:35.865: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.459116ms)
Jan 17 07:52:35.873: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 7.672207ms)
Jan 17 07:52:35.874: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 8.891117ms)
Jan 17 07:52:35.879: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 13.724714ms)
Jan 17 07:52:35.879: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.35905ms)
Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.519814ms)
Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 16.76056ms)
Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.851081ms)
Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.600004ms)
Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.856876ms)
Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 17.449603ms)
Jan 17 07:52:35.885: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 18.928708ms)
Jan 17 07:52:35.885: INFO: (3) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 19.041775ms)
Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 21.591957ms)
Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.780632ms)
Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 21.888492ms)
Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 21.798798ms)
Jan 17 07:52:35.900: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 12.099705ms)
Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.210509ms)
Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.538151ms)
Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.65844ms)
Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.740568ms)
Jan 17 07:52:35.903: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 14.921646ms)
Jan 17 07:52:35.904: INFO: (4) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 15.73319ms)
Jan 17 07:52:35.904: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.804081ms)
Jan 17 07:52:35.905: INFO: (4) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 16.695852ms)
Jan 17 07:52:35.905: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.275407ms)
Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.992083ms)
Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.289435ms)
Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 17.829513ms)
Jan 17 07:52:35.908: INFO: (4) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.421119ms)
Jan 17 07:52:35.908: INFO: (4) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.313373ms)
Jan 17 07:52:35.909: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.869961ms)
Jan 17 07:52:35.919: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 8.833847ms)
Jan 17 07:52:35.919: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 10.039126ms)
Jan 17 07:52:35.920: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 11.435301ms)
Jan 17 07:52:35.921: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 10.820406ms)
Jan 17 07:52:35.921: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 11.262092ms)
Jan 17 07:52:35.923: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.703695ms)
Jan 17 07:52:35.923: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.459811ms)
Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.671301ms)
Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 16.623975ms)
Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.576232ms)
Jan 17 07:52:35.927: INFO: (5) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 17.380446ms)
Jan 17 07:52:35.927: INFO: (5) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 17.134742ms)
Jan 17 07:52:35.928: INFO: (5) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 19.006937ms)
Jan 17 07:52:35.928: INFO: (5) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.216178ms)
Jan 17 07:52:35.929: INFO: (5) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.708422ms)
Jan 17 07:52:35.930: INFO: (5) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.437009ms)
Jan 17 07:52:35.940: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 9.740446ms)
Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 12.506845ms)
Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 13.085728ms)
Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 13.120043ms)
Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 12.736088ms)
Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.191942ms)
Jan 17 07:52:35.945: INFO: (6) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 15.01585ms)
Jan 17 07:52:35.945: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.863437ms)
Jan 17 07:52:35.946: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 14.938274ms)
Jan 17 07:52:35.946: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.265739ms)
Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 15.859953ms)
Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 16.105647ms)
Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.266988ms)
Jan 17 07:52:35.948: INFO: (6) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.868624ms)
Jan 17 07:52:35.950: INFO: (6) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.830748ms)
Jan 17 07:52:35.953: INFO: (6) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 21.390037ms)
Jan 17 07:52:35.962: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 8.99498ms)
Jan 17 07:52:35.964: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 10.669979ms)
Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.357008ms)
Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.618763ms)
Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 16.663547ms)
Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 17.110104ms)
Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 18.025756ms)
Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 17.983864ms)
Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 18.245623ms)
Jan 17 07:52:35.972: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.261174ms)
Jan 17 07:52:35.972: INFO: (7) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 18.989736ms)
Jan 17 07:52:35.973: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.601643ms)
Jan 17 07:52:35.976: INFO: (7) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 22.643764ms)
Jan 17 07:52:35.977: INFO: (7) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 24.077032ms)
Jan 17 07:52:35.978: INFO: (7) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 24.428099ms)
Jan 17 07:52:35.979: INFO: (7) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 25.135098ms)
Jan 17 07:52:35.986: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 7.64782ms)
Jan 17 07:52:35.987: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 8.510807ms)
Jan 17 07:52:35.988: INFO: (8) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 9.606464ms)
Jan 17 07:52:35.989: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 10.382656ms)
Jan 17 07:52:35.992: INFO: (8) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 13.360694ms)
Jan 17 07:52:35.994: INFO: (8) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 14.796184ms)
Jan 17 07:52:35.997: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 17.458952ms)
Jan 17 07:52:35.998: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 18.32174ms)
Jan 17 07:52:36.000: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 19.522573ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.412979ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.621394ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 23.559991ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.774672ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 23.876601ms)
Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 24.434343ms)
Jan 17 07:52:36.004: INFO: (8) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 25.17782ms)
Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 23.106168ms)
Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.801431ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.78583ms)
Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 22.827069ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.906664ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.970942ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 23.0276ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.743043ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.024139ms)
Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 22.917007ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 27.129643ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 27.114131ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.300074ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 27.205252ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 27.790926ms)
Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 27.313771ms)
Jan 17 07:52:36.044: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 11.723844ms)
Jan 17 07:52:36.045: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.623474ms)
Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 13.517831ms)
Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 13.331221ms)
Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 13.610573ms)
Jan 17 07:52:36.048: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 15.739454ms)
Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.440502ms)
Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 19.57576ms)
Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.408952ms)
Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.308579ms)
Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 19.495785ms)
Jan 17 07:52:36.054: INFO: (10) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 20.996936ms)
Jan 17 07:52:36.054: INFO: (10) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.200581ms)
Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 22.288191ms)
Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 22.230233ms)
Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 22.288464ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 14.947141ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 15.588136ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.587065ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 15.934112ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.470446ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.998001ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 16.142569ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 14.86806ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.968853ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 15.773862ms)
Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 14.951401ms)
Jan 17 07:52:36.072: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 16.614499ms)
Jan 17 07:52:36.073: INFO: (11) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 16.940726ms)
Jan 17 07:52:36.073: INFO: (11) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 17.769173ms)
Jan 17 07:52:36.074: INFO: (11) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 18.787622ms)
Jan 17 07:52:36.074: INFO: (11) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.289074ms)
Jan 17 07:52:36.087: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 11.594173ms)
Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 12.362716ms)
Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 12.508065ms)
Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 12.820264ms)
Jan 17 07:52:36.089: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.584959ms)
Jan 17 07:52:36.089: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.518242ms)
Jan 17 07:52:36.097: INFO: (12) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 22.338831ms)
Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 23.763086ms)
Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.844884ms)
Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 23.983457ms)
Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.856488ms)
Jan 17 07:52:36.103: INFO: (12) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 28.046347ms)
Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 28.620664ms)
Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 29.05935ms)
Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 28.372623ms)
Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 28.952368ms)
Jan 17 07:52:36.115: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 10.305941ms)
Jan 17 07:52:36.123: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.067459ms)
Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.938932ms)
Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.535869ms)
Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.891364ms)
Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 19.492339ms)
Jan 17 07:52:36.125: INFO: (13) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 21.619507ms)
Jan 17 07:52:36.126: INFO: (13) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 21.85617ms)
Jan 17 07:52:36.127: INFO: (13) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 22.507774ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 27.061218ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.108944ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 27.098571ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 27.337992ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 27.304473ms)
Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.719857ms)
Jan 17 07:52:36.133: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 28.023506ms)
Jan 17 07:52:36.146: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.356437ms)
Jan 17 07:52:36.151: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 18.456744ms)
Jan 17 07:52:36.152: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 18.532482ms)
Jan 17 07:52:36.152: INFO: (14) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 18.939581ms)
Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.898728ms)
Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 19.68633ms)
Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 20.185785ms)
Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 20.896906ms)
Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 21.244893ms)
Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 21.251502ms)
Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.357335ms)
Jan 17 07:52:36.155: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.207797ms)
Jan 17 07:52:36.160: INFO: (14) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 26.989811ms)
Jan 17 07:52:36.160: INFO: (14) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 27.63299ms)
Jan 17 07:52:36.162: INFO: (14) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 29.319143ms)
Jan 17 07:52:36.162: INFO: (14) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 28.962369ms)
Jan 17 07:52:36.176: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.804275ms)
Jan 17 07:52:36.176: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.8255ms)
Jan 17 07:52:36.179: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 17.100735ms)
Jan 17 07:52:36.179: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 17.421245ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 23.360246ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 23.387887ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 23.437772ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 23.859241ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 23.744983ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 23.575836ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.609732ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 23.620804ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.725286ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 23.82229ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.870875ms)
Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.970054ms)
Jan 17 07:52:36.198: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 11.525124ms)
Jan 17 07:52:36.201: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.128837ms)
Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.921464ms)
Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.163501ms)
Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 12.867986ms)
Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 15.266472ms)
Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.05427ms)
Jan 17 07:52:36.206: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.375278ms)
Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 29.810139ms)
Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 28.813923ms)
Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 26.629002ms)
Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 29.345042ms)
Jan 17 07:52:36.217: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 27.023962ms)
Jan 17 07:52:36.218: INFO: (16) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 30.585633ms)
Jan 17 07:52:36.229: INFO: (16) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 41.224294ms)
Jan 17 07:52:36.229: INFO: (16) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 39.214466ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 26.226398ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.765367ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 26.501274ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.102576ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 27.281231ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 26.752582ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 26.525273ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.9219ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 26.675455ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.01749ms)
Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 26.499904ms)
Jan 17 07:52:36.257: INFO: (17) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 27.832443ms)
Jan 17 07:52:36.257: INFO: (17) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 27.389704ms)
Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 28.650166ms)
Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 27.782256ms)
Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 28.65697ms)
Jan 17 07:52:36.272: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 11.184932ms)
Jan 17 07:52:36.280: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.14138ms)
Jan 17 07:52:36.280: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.387128ms)
Jan 17 07:52:36.282: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 20.743159ms)
Jan 17 07:52:36.283: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 21.507038ms)
Jan 17 07:52:36.283: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.183281ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 26.419329ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 26.529186ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 26.707508ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 26.801483ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.622432ms)
Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 26.800852ms)
Jan 17 07:52:36.290: INFO: (18) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 28.763806ms)
Jan 17 07:52:36.294: INFO: (18) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 32.301019ms)
Jan 17 07:52:36.295: INFO: (18) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 34.263096ms)
Jan 17 07:52:36.295: INFO: (18) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 34.295473ms)
Jan 17 07:52:36.307: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 12.015973ms)
Jan 17 07:52:36.313: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.584824ms)
Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 17.696641ms)
Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 17.853682ms)
Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 18.390386ms)
Jan 17 07:52:36.317: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 21.161341ms)
Jan 17 07:52:36.317: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 21.091839ms)
Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 24.406217ms)
Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 23.983692ms)
Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.68583ms)
Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 24.167244ms)
Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 24.266049ms)
Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 24.605337ms)
Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 24.471622ms)
Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 24.445006ms)
Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 25.455296ms)
STEP: deleting ReplicationController proxy-service-pxtzf in namespace proxy-9283, will wait for the garbage collector to delete the pods 01/17/23 07:52:36.322
Jan 17 07:52:36.392: INFO: Deleting ReplicationController proxy-service-pxtzf took: 13.070401ms
Jan 17 07:52:36.493: INFO: Terminating ReplicationController proxy-service-pxtzf pods took: 100.431314ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 07:52:39.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9283" for this suite. 01/17/23 07:52:39.421
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":287,"skipped":5332,"failed":0}
------------------------------
• [SLOW TEST] [5.896 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:33.538
    Jan 17 07:52:33.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename proxy 01/17/23 07:52:33.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:33.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:33.616
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/17/23 07:52:33.648
    STEP: creating replication controller proxy-service-pxtzf in namespace proxy-9283 01/17/23 07:52:33.649
    I0117 07:52:33.695509      23 runners.go:193] Created replication controller with name: proxy-service-pxtzf, namespace: proxy-9283, replica count: 1
    I0117 07:52:34.746450      23 runners.go:193] proxy-service-pxtzf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 07:52:35.747603      23 runners.go:193] proxy-service-pxtzf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 07:52:35.755: INFO: setup took 2.131812666s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/17/23 07:52:35.755
    Jan 17 07:52:35.774: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 18.763421ms)
    Jan 17 07:52:35.781: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 25.829245ms)
    Jan 17 07:52:35.792: INFO: (0) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 36.245793ms)
    Jan 17 07:52:35.796: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 39.815269ms)
    Jan 17 07:52:35.804: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 48.144899ms)
    Jan 17 07:52:35.807: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 51.669047ms)
    Jan 17 07:52:35.807: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 51.630284ms)
    Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 51.539385ms)
    Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 51.667135ms)
    Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 51.628978ms)
    Jan 17 07:52:35.808: INFO: (0) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 52.432302ms)
    Jan 17 07:52:35.812: INFO: (0) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 55.668171ms)
    Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 60.495598ms)
    Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 60.729094ms)
    Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 61.054498ms)
    Jan 17 07:52:35.817: INFO: (0) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 60.877265ms)
    Jan 17 07:52:35.827: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 9.797551ms)
    Jan 17 07:52:35.834: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 17.179215ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.329686ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.426364ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 22.628731ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.888457ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 22.788581ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.943235ms)
    Jan 17 07:52:35.840: INFO: (1) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 22.952225ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 26.522785ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 26.85642ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 26.591773ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 26.657111ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 26.693602ms)
    Jan 17 07:52:35.844: INFO: (1) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 26.501296ms)
    Jan 17 07:52:35.845: INFO: (1) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.855477ms)
    Jan 17 07:52:35.854: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 8.93637ms)
    Jan 17 07:52:35.855: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 9.452838ms)
    Jan 17 07:52:35.856: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 10.445569ms)
    Jan 17 07:52:35.856: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 10.606323ms)
    Jan 17 07:52:35.859: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 12.930242ms)
    Jan 17 07:52:35.860: INFO: (2) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.743712ms)
    Jan 17 07:52:35.861: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 15.091783ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 17.225412ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 17.330842ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 16.797928ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.831269ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 17.215844ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 17.331642ms)
    Jan 17 07:52:35.863: INFO: (2) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 17.678861ms)
    Jan 17 07:52:35.865: INFO: (2) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.104408ms)
    Jan 17 07:52:35.865: INFO: (2) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.459116ms)
    Jan 17 07:52:35.873: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 7.672207ms)
    Jan 17 07:52:35.874: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 8.891117ms)
    Jan 17 07:52:35.879: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 13.724714ms)
    Jan 17 07:52:35.879: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.35905ms)
    Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.519814ms)
    Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 16.76056ms)
    Jan 17 07:52:35.882: INFO: (3) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.851081ms)
    Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.600004ms)
    Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.856876ms)
    Jan 17 07:52:35.883: INFO: (3) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 17.449603ms)
    Jan 17 07:52:35.885: INFO: (3) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 18.928708ms)
    Jan 17 07:52:35.885: INFO: (3) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 19.041775ms)
    Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 21.591957ms)
    Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.780632ms)
    Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 21.888492ms)
    Jan 17 07:52:35.888: INFO: (3) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 21.798798ms)
    Jan 17 07:52:35.900: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 12.099705ms)
    Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.210509ms)
    Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.538151ms)
    Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.65844ms)
    Jan 17 07:52:35.902: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.740568ms)
    Jan 17 07:52:35.903: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 14.921646ms)
    Jan 17 07:52:35.904: INFO: (4) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 15.73319ms)
    Jan 17 07:52:35.904: INFO: (4) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.804081ms)
    Jan 17 07:52:35.905: INFO: (4) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 16.695852ms)
    Jan 17 07:52:35.905: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.275407ms)
    Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.992083ms)
    Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.289435ms)
    Jan 17 07:52:35.907: INFO: (4) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 17.829513ms)
    Jan 17 07:52:35.908: INFO: (4) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.421119ms)
    Jan 17 07:52:35.908: INFO: (4) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.313373ms)
    Jan 17 07:52:35.909: INFO: (4) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.869961ms)
    Jan 17 07:52:35.919: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 8.833847ms)
    Jan 17 07:52:35.919: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 10.039126ms)
    Jan 17 07:52:35.920: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 11.435301ms)
    Jan 17 07:52:35.921: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 10.820406ms)
    Jan 17 07:52:35.921: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 11.262092ms)
    Jan 17 07:52:35.923: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.703695ms)
    Jan 17 07:52:35.923: INFO: (5) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.459811ms)
    Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.671301ms)
    Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 16.623975ms)
    Jan 17 07:52:35.926: INFO: (5) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.576232ms)
    Jan 17 07:52:35.927: INFO: (5) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 17.380446ms)
    Jan 17 07:52:35.927: INFO: (5) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 17.134742ms)
    Jan 17 07:52:35.928: INFO: (5) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 19.006937ms)
    Jan 17 07:52:35.928: INFO: (5) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 19.216178ms)
    Jan 17 07:52:35.929: INFO: (5) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.708422ms)
    Jan 17 07:52:35.930: INFO: (5) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.437009ms)
    Jan 17 07:52:35.940: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 9.740446ms)
    Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 12.506845ms)
    Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 13.085728ms)
    Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 13.120043ms)
    Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 12.736088ms)
    Jan 17 07:52:35.943: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.191942ms)
    Jan 17 07:52:35.945: INFO: (6) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 15.01585ms)
    Jan 17 07:52:35.945: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.863437ms)
    Jan 17 07:52:35.946: INFO: (6) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 14.938274ms)
    Jan 17 07:52:35.946: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.265739ms)
    Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 15.859953ms)
    Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 16.105647ms)
    Jan 17 07:52:35.947: INFO: (6) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.266988ms)
    Jan 17 07:52:35.948: INFO: (6) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 16.868624ms)
    Jan 17 07:52:35.950: INFO: (6) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.830748ms)
    Jan 17 07:52:35.953: INFO: (6) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 21.390037ms)
    Jan 17 07:52:35.962: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 8.99498ms)
    Jan 17 07:52:35.964: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 10.669979ms)
    Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.357008ms)
    Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.618763ms)
    Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 16.663547ms)
    Jan 17 07:52:35.970: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 17.110104ms)
    Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 18.025756ms)
    Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 17.983864ms)
    Jan 17 07:52:35.971: INFO: (7) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 18.245623ms)
    Jan 17 07:52:35.972: INFO: (7) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.261174ms)
    Jan 17 07:52:35.972: INFO: (7) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 18.989736ms)
    Jan 17 07:52:35.973: INFO: (7) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.601643ms)
    Jan 17 07:52:35.976: INFO: (7) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 22.643764ms)
    Jan 17 07:52:35.977: INFO: (7) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 24.077032ms)
    Jan 17 07:52:35.978: INFO: (7) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 24.428099ms)
    Jan 17 07:52:35.979: INFO: (7) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 25.135098ms)
    Jan 17 07:52:35.986: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 7.64782ms)
    Jan 17 07:52:35.987: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 8.510807ms)
    Jan 17 07:52:35.988: INFO: (8) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 9.606464ms)
    Jan 17 07:52:35.989: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 10.382656ms)
    Jan 17 07:52:35.992: INFO: (8) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 13.360694ms)
    Jan 17 07:52:35.994: INFO: (8) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 14.796184ms)
    Jan 17 07:52:35.997: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 17.458952ms)
    Jan 17 07:52:35.998: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 18.32174ms)
    Jan 17 07:52:36.000: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 19.522573ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.412979ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.621394ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 23.559991ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.774672ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 23.876601ms)
    Jan 17 07:52:36.003: INFO: (8) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 24.434343ms)
    Jan 17 07:52:36.004: INFO: (8) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 25.17782ms)
    Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 23.106168ms)
    Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.801431ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.78583ms)
    Jan 17 07:52:36.027: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 22.827069ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.906664ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.970942ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 23.0276ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 22.743043ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.024139ms)
    Jan 17 07:52:36.028: INFO: (9) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 22.917007ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 27.129643ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 27.114131ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.300074ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 27.205252ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 27.790926ms)
    Jan 17 07:52:36.032: INFO: (9) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 27.313771ms)
    Jan 17 07:52:36.044: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 11.723844ms)
    Jan 17 07:52:36.045: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 12.623474ms)
    Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 13.517831ms)
    Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 13.331221ms)
    Jan 17 07:52:36.046: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 13.610573ms)
    Jan 17 07:52:36.048: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 15.739454ms)
    Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.440502ms)
    Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 19.57576ms)
    Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.408952ms)
    Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.308579ms)
    Jan 17 07:52:36.052: INFO: (10) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 19.495785ms)
    Jan 17 07:52:36.054: INFO: (10) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 20.996936ms)
    Jan 17 07:52:36.054: INFO: (10) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.200581ms)
    Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 22.288191ms)
    Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 22.230233ms)
    Jan 17 07:52:36.055: INFO: (10) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 22.288464ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 14.947141ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 15.588136ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 15.587065ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 15.934112ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.470446ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.998001ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 16.142569ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 14.86806ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 14.968853ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 15.773862ms)
    Jan 17 07:52:36.071: INFO: (11) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 14.951401ms)
    Jan 17 07:52:36.072: INFO: (11) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 16.614499ms)
    Jan 17 07:52:36.073: INFO: (11) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 16.940726ms)
    Jan 17 07:52:36.073: INFO: (11) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 17.769173ms)
    Jan 17 07:52:36.074: INFO: (11) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 18.787622ms)
    Jan 17 07:52:36.074: INFO: (11) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 18.289074ms)
    Jan 17 07:52:36.087: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 11.594173ms)
    Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 12.362716ms)
    Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 12.508065ms)
    Jan 17 07:52:36.088: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 12.820264ms)
    Jan 17 07:52:36.089: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.584959ms)
    Jan 17 07:52:36.089: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.518242ms)
    Jan 17 07:52:36.097: INFO: (12) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 22.338831ms)
    Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 23.763086ms)
    Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.844884ms)
    Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 23.983457ms)
    Jan 17 07:52:36.099: INFO: (12) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.856488ms)
    Jan 17 07:52:36.103: INFO: (12) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 28.046347ms)
    Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 28.620664ms)
    Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 29.05935ms)
    Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 28.372623ms)
    Jan 17 07:52:36.104: INFO: (12) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 28.952368ms)
    Jan 17 07:52:36.115: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 10.305941ms)
    Jan 17 07:52:36.123: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.067459ms)
    Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 19.938932ms)
    Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.535869ms)
    Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 19.891364ms)
    Jan 17 07:52:36.124: INFO: (13) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 19.492339ms)
    Jan 17 07:52:36.125: INFO: (13) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 21.619507ms)
    Jan 17 07:52:36.126: INFO: (13) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 21.85617ms)
    Jan 17 07:52:36.127: INFO: (13) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 22.507774ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 27.061218ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.108944ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 27.098571ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 27.337992ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 27.304473ms)
    Jan 17 07:52:36.132: INFO: (13) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.719857ms)
    Jan 17 07:52:36.133: INFO: (13) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 28.023506ms)
    Jan 17 07:52:36.146: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.356437ms)
    Jan 17 07:52:36.151: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 18.456744ms)
    Jan 17 07:52:36.152: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 18.532482ms)
    Jan 17 07:52:36.152: INFO: (14) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 18.939581ms)
    Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.898728ms)
    Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 19.68633ms)
    Jan 17 07:52:36.153: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 20.185785ms)
    Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 20.896906ms)
    Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 21.244893ms)
    Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 21.251502ms)
    Jan 17 07:52:36.154: INFO: (14) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 21.357335ms)
    Jan 17 07:52:36.155: INFO: (14) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 22.207797ms)
    Jan 17 07:52:36.160: INFO: (14) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 26.989811ms)
    Jan 17 07:52:36.160: INFO: (14) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 27.63299ms)
    Jan 17 07:52:36.162: INFO: (14) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 29.319143ms)
    Jan 17 07:52:36.162: INFO: (14) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 28.962369ms)
    Jan 17 07:52:36.176: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 13.804275ms)
    Jan 17 07:52:36.176: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.8255ms)
    Jan 17 07:52:36.179: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 17.100735ms)
    Jan 17 07:52:36.179: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 17.421245ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 23.360246ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 23.387887ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 23.437772ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 23.859241ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 23.744983ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 23.575836ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.609732ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 23.620804ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.725286ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 23.82229ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 23.870875ms)
    Jan 17 07:52:36.186: INFO: (15) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 23.970054ms)
    Jan 17 07:52:36.198: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 11.525124ms)
    Jan 17 07:52:36.201: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 13.128837ms)
    Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 15.921464ms)
    Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 13.163501ms)
    Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 12.867986ms)
    Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 15.266472ms)
    Jan 17 07:52:36.203: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 16.05427ms)
    Jan 17 07:52:36.206: INFO: (16) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 16.375278ms)
    Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 29.810139ms)
    Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 28.813923ms)
    Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 26.629002ms)
    Jan 17 07:52:36.216: INFO: (16) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 29.345042ms)
    Jan 17 07:52:36.217: INFO: (16) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 27.023962ms)
    Jan 17 07:52:36.218: INFO: (16) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 30.585633ms)
    Jan 17 07:52:36.229: INFO: (16) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 41.224294ms)
    Jan 17 07:52:36.229: INFO: (16) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 39.214466ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 26.226398ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.765367ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 26.501274ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 27.102576ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 27.281231ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 26.752582ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 26.525273ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.9219ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 26.675455ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 27.01749ms)
    Jan 17 07:52:36.256: INFO: (17) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 26.499904ms)
    Jan 17 07:52:36.257: INFO: (17) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 27.832443ms)
    Jan 17 07:52:36.257: INFO: (17) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 27.389704ms)
    Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 28.650166ms)
    Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 27.782256ms)
    Jan 17 07:52:36.258: INFO: (17) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 28.65697ms)
    Jan 17 07:52:36.272: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 11.184932ms)
    Jan 17 07:52:36.280: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 19.14138ms)
    Jan 17 07:52:36.280: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 19.387128ms)
    Jan 17 07:52:36.282: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 20.743159ms)
    Jan 17 07:52:36.283: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 21.507038ms)
    Jan 17 07:52:36.283: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 22.183281ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 26.419329ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 26.529186ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 26.707508ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 26.801483ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 26.622432ms)
    Jan 17 07:52:36.288: INFO: (18) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 26.800852ms)
    Jan 17 07:52:36.290: INFO: (18) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 28.763806ms)
    Jan 17 07:52:36.294: INFO: (18) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 32.301019ms)
    Jan 17 07:52:36.295: INFO: (18) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 34.263096ms)
    Jan 17 07:52:36.295: INFO: (18) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 34.295473ms)
    Jan 17 07:52:36.307: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:460/proxy/: tls baz (200; 12.015973ms)
    Jan 17 07:52:36.313: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 16.584824ms)
    Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">... (200; 17.696641ms)
    Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:462/proxy/: tls qux (200; 17.853682ms)
    Jan 17 07:52:36.314: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:160/proxy/: foo (200; 18.390386ms)
    Jan 17 07:52:36.317: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 21.161341ms)
    Jan 17 07:52:36.317: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn/proxy/rewriteme">test</a> (200; 21.091839ms)
    Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/proxy-service-pxtzf-6mmbn:1080/proxy/rewriteme">test<... (200; 24.406217ms)
    Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname2/proxy/: bar (200; 23.983692ms)
    Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname2/proxy/: tls qux (200; 23.68583ms)
    Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname1/proxy/: foo (200; 24.167244ms)
    Jan 17 07:52:36.320: INFO: (19) /api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/: <a href="/api/v1/namespaces/proxy-9283/pods/https:proxy-service-pxtzf-6mmbn:443/proxy/tlsrewritem... (200; 24.266049ms)
    Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/pods/http:proxy-service-pxtzf-6mmbn:162/proxy/: bar (200; 24.605337ms)
    Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/proxy-service-pxtzf:portname1/proxy/: foo (200; 24.471622ms)
    Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/http:proxy-service-pxtzf:portname2/proxy/: bar (200; 24.445006ms)
    Jan 17 07:52:36.321: INFO: (19) /api/v1/namespaces/proxy-9283/services/https:proxy-service-pxtzf:tlsportname1/proxy/: tls baz (200; 25.455296ms)
    STEP: deleting ReplicationController proxy-service-pxtzf in namespace proxy-9283, will wait for the garbage collector to delete the pods 01/17/23 07:52:36.322
    Jan 17 07:52:36.392: INFO: Deleting ReplicationController proxy-service-pxtzf took: 13.070401ms
    Jan 17 07:52:36.493: INFO: Terminating ReplicationController proxy-service-pxtzf pods took: 100.431314ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 07:52:39.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9283" for this suite. 01/17/23 07:52:39.421
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:39.436
Jan 17 07:52:39.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:52:39.438
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:39.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:39.49
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:52:39.496
Jan 17 07:52:39.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d" in namespace "projected-992" to be "Succeeded or Failed"
Jan 17 07:52:39.530: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.387316ms
Jan 17 07:52:41.578: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062548885s
Jan 17 07:52:43.541: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025452366s
STEP: Saw pod success 01/17/23 07:52:43.541
Jan 17 07:52:43.542: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d" satisfied condition "Succeeded or Failed"
Jan 17 07:52:43.547: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d container client-container: <nil>
STEP: delete the pod 01/17/23 07:52:43.64
Jan 17 07:52:43.674: INFO: Waiting for pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d to disappear
Jan 17 07:52:43.685: INFO: Pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:52:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-992" for this suite. 01/17/23 07:52:43.695
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":288,"skipped":5336,"failed":0}
------------------------------
• [4.277 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:39.436
    Jan 17 07:52:39.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:52:39.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:39.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:39.49
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:52:39.496
    Jan 17 07:52:39.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d" in namespace "projected-992" to be "Succeeded or Failed"
    Jan 17 07:52:39.530: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.387316ms
    Jan 17 07:52:41.578: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062548885s
    Jan 17 07:52:43.541: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025452366s
    STEP: Saw pod success 01/17/23 07:52:43.541
    Jan 17 07:52:43.542: INFO: Pod "downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d" satisfied condition "Succeeded or Failed"
    Jan 17 07:52:43.547: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d container client-container: <nil>
    STEP: delete the pod 01/17/23 07:52:43.64
    Jan 17 07:52:43.674: INFO: Waiting for pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d to disappear
    Jan 17 07:52:43.685: INFO: Pod downwardapi-volume-afe31cb8-263e-40d1-8746-339e7b00dd2d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:52:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-992" for this suite. 01/17/23 07:52:43.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:43.714
Jan 17 07:52:43.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename events 01/17/23 07:52:43.716
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:43.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:43.754
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/17/23 07:52:43.763
STEP: listing all events in all namespaces 01/17/23 07:52:43.782
STEP: patching the test event 01/17/23 07:52:43.791
STEP: fetching the test event 01/17/23 07:52:43.807
STEP: updating the test event 01/17/23 07:52:43.818
STEP: getting the test event 01/17/23 07:52:43.841
STEP: deleting the test event 01/17/23 07:52:43.845
STEP: listing all events in all namespaces 01/17/23 07:52:43.865
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 17 07:52:43.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4248" for this suite. 01/17/23 07:52:43.876
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":289,"skipped":5343,"failed":0}
------------------------------
• [0.173 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:43.714
    Jan 17 07:52:43.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename events 01/17/23 07:52:43.716
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:43.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:43.754
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/17/23 07:52:43.763
    STEP: listing all events in all namespaces 01/17/23 07:52:43.782
    STEP: patching the test event 01/17/23 07:52:43.791
    STEP: fetching the test event 01/17/23 07:52:43.807
    STEP: updating the test event 01/17/23 07:52:43.818
    STEP: getting the test event 01/17/23 07:52:43.841
    STEP: deleting the test event 01/17/23 07:52:43.845
    STEP: listing all events in all namespaces 01/17/23 07:52:43.865
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 17 07:52:43.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4248" for this suite. 01/17/23 07:52:43.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:43.889
Jan 17 07:52:43.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:52:43.89
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:43.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:43.928
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-a3c5d572-cf8a-4811-9000-4cf9ca2d97ad 01/17/23 07:52:43.935
STEP: Creating a pod to test consume secrets 01/17/23 07:52:43.947
Jan 17 07:52:43.971: INFO: Waiting up to 5m0s for pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a" in namespace "secrets-1560" to be "Succeeded or Failed"
Jan 17 07:52:43.983: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.526727ms
Jan 17 07:52:45.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Running", Reason="", readiness=true. Elapsed: 2.017683076s
Jan 17 07:52:47.992: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Running", Reason="", readiness=false. Elapsed: 4.020555981s
Jan 17 07:52:49.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017729679s
STEP: Saw pod success 01/17/23 07:52:49.989
Jan 17 07:52:49.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a" satisfied condition "Succeeded or Failed"
Jan 17 07:52:49.994: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a container secret-env-test: <nil>
STEP: delete the pod 01/17/23 07:52:50.005
Jan 17 07:52:50.026: INFO: Waiting for pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a to disappear
Jan 17 07:52:50.032: INFO: Pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:52:50.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1560" for this suite. 01/17/23 07:52:50.049
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":290,"skipped":5382,"failed":0}
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:43.889
    Jan 17 07:52:43.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:52:43.89
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:43.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:43.928
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-a3c5d572-cf8a-4811-9000-4cf9ca2d97ad 01/17/23 07:52:43.935
    STEP: Creating a pod to test consume secrets 01/17/23 07:52:43.947
    Jan 17 07:52:43.971: INFO: Waiting up to 5m0s for pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a" in namespace "secrets-1560" to be "Succeeded or Failed"
    Jan 17 07:52:43.983: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.526727ms
    Jan 17 07:52:45.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Running", Reason="", readiness=true. Elapsed: 2.017683076s
    Jan 17 07:52:47.992: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Running", Reason="", readiness=false. Elapsed: 4.020555981s
    Jan 17 07:52:49.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017729679s
    STEP: Saw pod success 01/17/23 07:52:49.989
    Jan 17 07:52:49.989: INFO: Pod "pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a" satisfied condition "Succeeded or Failed"
    Jan 17 07:52:49.994: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a container secret-env-test: <nil>
    STEP: delete the pod 01/17/23 07:52:50.005
    Jan 17 07:52:50.026: INFO: Waiting for pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a to disappear
    Jan 17 07:52:50.032: INFO: Pod pod-secrets-cff3d077-d4bb-4ab3-a865-6a6471b0967a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:52:50.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1560" for this suite. 01/17/23 07:52:50.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:50.072
Jan 17 07:52:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:52:50.073
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:50.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:50.113
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/17/23 07:52:50.121
Jan 17 07:52:50.143: INFO: Waiting up to 5m0s for pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b" in namespace "projected-4685" to be "running and ready"
Jan 17 07:52:50.157: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.491303ms
Jan 17 07:52:50.157: INFO: The phase of Pod labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:52:52.181: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.038583503s
Jan 17 07:52:52.181: INFO: The phase of Pod labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b is Running (Ready = true)
Jan 17 07:52:52.181: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b" satisfied condition "running and ready"
Jan 17 07:52:52.756: INFO: Successfully updated pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:52:56.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4685" for this suite. 01/17/23 07:52:56.838
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":291,"skipped":5417,"failed":0}
------------------------------
• [SLOW TEST] [6.780 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:50.072
    Jan 17 07:52:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:52:50.073
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:50.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:50.113
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/17/23 07:52:50.121
    Jan 17 07:52:50.143: INFO: Waiting up to 5m0s for pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b" in namespace "projected-4685" to be "running and ready"
    Jan 17 07:52:50.157: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.491303ms
    Jan 17 07:52:50.157: INFO: The phase of Pod labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:52:52.181: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.038583503s
    Jan 17 07:52:52.181: INFO: The phase of Pod labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b is Running (Ready = true)
    Jan 17 07:52:52.181: INFO: Pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b" satisfied condition "running and ready"
    Jan 17 07:52:52.756: INFO: Successfully updated pod "labelsupdatec84169f0-20c3-4e67-ba31-9cac410dbf6b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:52:56.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4685" for this suite. 01/17/23 07:52:56.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:52:56.854
Jan 17 07:52:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename configmap 01/17/23 07:52:56.855
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:56.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:56.894
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-d11b0258-c93f-4244-981e-725c277612c7 01/17/23 07:52:56.904
STEP: Creating a pod to test consume configMaps 01/17/23 07:52:56.92
Jan 17 07:52:56.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4" in namespace "configmap-7713" to be "Succeeded or Failed"
Jan 17 07:52:56.962: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.321238ms
Jan 17 07:52:58.971: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030033162s
Jan 17 07:53:00.969: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028411437s
STEP: Saw pod success 01/17/23 07:53:00.969
Jan 17 07:53:00.970: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4" satisfied condition "Succeeded or Failed"
Jan 17 07:53:00.974: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:53:01.045
Jan 17 07:53:01.087: INFO: Waiting for pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 to disappear
Jan 17 07:53:01.100: INFO: Pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 07:53:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7713" for this suite. 01/17/23 07:53:01.108
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":292,"skipped":5430,"failed":0}
------------------------------
• [4.272 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:52:56.854
    Jan 17 07:52:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename configmap 01/17/23 07:52:56.855
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:52:56.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:52:56.894
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-d11b0258-c93f-4244-981e-725c277612c7 01/17/23 07:52:56.904
    STEP: Creating a pod to test consume configMaps 01/17/23 07:52:56.92
    Jan 17 07:52:56.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4" in namespace "configmap-7713" to be "Succeeded or Failed"
    Jan 17 07:52:56.962: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.321238ms
    Jan 17 07:52:58.971: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030033162s
    Jan 17 07:53:00.969: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028411437s
    STEP: Saw pod success 01/17/23 07:53:00.969
    Jan 17 07:53:00.970: INFO: Pod "pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4" satisfied condition "Succeeded or Failed"
    Jan 17 07:53:00.974: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:53:01.045
    Jan 17 07:53:01.087: INFO: Waiting for pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 to disappear
    Jan 17 07:53:01.100: INFO: Pod pod-configmaps-14b1ca06-19ed-4966-ac87-6e7367e80ee4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 07:53:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7713" for this suite. 01/17/23 07:53:01.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:01.126
Jan 17 07:53:01.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 07:53:01.128
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:01.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:01.171
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4082 01/17/23 07:53:01.179
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-4082 01/17/23 07:53:01.219
Jan 17 07:53:01.244: INFO: Found 0 stateful pods, waiting for 1
Jan 17 07:53:11.251: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/17/23 07:53:11.265
STEP: Getting /status 01/17/23 07:53:11.288
Jan 17 07:53:11.298: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/17/23 07:53:11.298
Jan 17 07:53:11.324: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/17/23 07:53:11.324
Jan 17 07:53:11.330: INFO: Observed &StatefulSet event: ADDED
Jan 17 07:53:11.330: INFO: Found Statefulset ss in namespace statefulset-4082 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 07:53:11.330: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/17/23 07:53:11.33
Jan 17 07:53:11.330: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 07:53:11.347: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/17/23 07:53:11.347
Jan 17 07:53:11.354: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 07:53:11.354: INFO: Deleting all statefulset in ns statefulset-4082
Jan 17 07:53:11.364: INFO: Scaling statefulset ss to 0
Jan 17 07:53:21.459: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 07:53:21.481: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 07:53:21.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4082" for this suite. 01/17/23 07:53:21.616
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":293,"skipped":5435,"failed":0}
------------------------------
• [SLOW TEST] [20.506 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:01.126
    Jan 17 07:53:01.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 07:53:01.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:01.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:01.171
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4082 01/17/23 07:53:01.179
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-4082 01/17/23 07:53:01.219
    Jan 17 07:53:01.244: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 07:53:11.251: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/17/23 07:53:11.265
    STEP: Getting /status 01/17/23 07:53:11.288
    Jan 17 07:53:11.298: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/17/23 07:53:11.298
    Jan 17 07:53:11.324: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/17/23 07:53:11.324
    Jan 17 07:53:11.330: INFO: Observed &StatefulSet event: ADDED
    Jan 17 07:53:11.330: INFO: Found Statefulset ss in namespace statefulset-4082 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 07:53:11.330: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/17/23 07:53:11.33
    Jan 17 07:53:11.330: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 07:53:11.347: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/17/23 07:53:11.347
    Jan 17 07:53:11.354: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 07:53:11.354: INFO: Deleting all statefulset in ns statefulset-4082
    Jan 17 07:53:11.364: INFO: Scaling statefulset ss to 0
    Jan 17 07:53:21.459: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 07:53:21.481: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 07:53:21.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4082" for this suite. 01/17/23 07:53:21.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:21.636
Jan 17 07:53:21.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 07:53:21.639
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:21.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:21.699
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/17/23 07:53:21.709
Jan 17 07:53:21.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7273 api-versions'
Jan 17 07:53:21.853: INFO: stderr: ""
Jan 17 07:53:21.853: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 07:53:21.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7273" for this suite. 01/17/23 07:53:21.871
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":294,"skipped":5450,"failed":0}
------------------------------
• [0.260 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:21.636
    Jan 17 07:53:21.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 07:53:21.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:21.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:21.699
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/17/23 07:53:21.709
    Jan 17 07:53:21.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-7273 api-versions'
    Jan 17 07:53:21.853: INFO: stderr: ""
    Jan 17 07:53:21.853: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 07:53:21.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7273" for this suite. 01/17/23 07:53:21.871
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:21.897
Jan 17 07:53:21.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename resourcequota 01/17/23 07:53:21.899
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:21.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:21.935
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/17/23 07:53:21.947
STEP: Creating a ResourceQuota 01/17/23 07:53:26.952
STEP: Ensuring resource quota status is calculated 01/17/23 07:53:26.966
STEP: Creating a ReplicaSet 01/17/23 07:53:28.981
STEP: Ensuring resource quota status captures replicaset creation 01/17/23 07:53:29.017
STEP: Deleting a ReplicaSet 01/17/23 07:53:31.025
STEP: Ensuring resource quota status released usage 01/17/23 07:53:31.041
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 07:53:33.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6217" for this suite. 01/17/23 07:53:33.054
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":295,"skipped":5451,"failed":0}
------------------------------
• [SLOW TEST] [11.172 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:21.897
    Jan 17 07:53:21.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename resourcequota 01/17/23 07:53:21.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:21.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:21.935
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/17/23 07:53:21.947
    STEP: Creating a ResourceQuota 01/17/23 07:53:26.952
    STEP: Ensuring resource quota status is calculated 01/17/23 07:53:26.966
    STEP: Creating a ReplicaSet 01/17/23 07:53:28.981
    STEP: Ensuring resource quota status captures replicaset creation 01/17/23 07:53:29.017
    STEP: Deleting a ReplicaSet 01/17/23 07:53:31.025
    STEP: Ensuring resource quota status released usage 01/17/23 07:53:31.041
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 07:53:33.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6217" for this suite. 01/17/23 07:53:33.054
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:33.069
Jan 17 07:53:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename namespaces 01/17/23 07:53:33.071
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:33.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:33.107
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/17/23 07:53:33.113
Jan 17 07:53:33.119: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/17/23 07:53:33.12
Jan 17 07:53:33.136: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/17/23 07:53:33.136
Jan 17 07:53:33.160: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 07:53:33.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9787" for this suite. 01/17/23 07:53:33.167
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":296,"skipped":5454,"failed":0}
------------------------------
• [0.115 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:33.069
    Jan 17 07:53:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename namespaces 01/17/23 07:53:33.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:33.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:33.107
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/17/23 07:53:33.113
    Jan 17 07:53:33.119: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/17/23 07:53:33.12
    Jan 17 07:53:33.136: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/17/23 07:53:33.136
    Jan 17 07:53:33.160: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 07:53:33.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9787" for this suite. 01/17/23 07:53:33.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:33.187
Jan 17 07:53:33.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 07:53:33.19
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:33.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:33.238
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:53:33.25
Jan 17 07:53:33.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2" in namespace "downward-api-4650" to be "Succeeded or Failed"
Jan 17 07:53:33.297: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.215904ms
Jan 17 07:53:35.305: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019147638s
Jan 17 07:53:37.310: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024840528s
STEP: Saw pod success 01/17/23 07:53:37.31
Jan 17 07:53:37.311: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2" satisfied condition "Succeeded or Failed"
Jan 17 07:53:37.319: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 container client-container: <nil>
STEP: delete the pod 01/17/23 07:53:37.333
Jan 17 07:53:37.387: INFO: Waiting for pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 to disappear
Jan 17 07:53:37.395: INFO: Pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 07:53:37.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4650" for this suite. 01/17/23 07:53:37.412
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":297,"skipped":5489,"failed":0}
------------------------------
• [4.239 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:33.187
    Jan 17 07:53:33.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 07:53:33.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:33.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:33.238
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:53:33.25
    Jan 17 07:53:33.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2" in namespace "downward-api-4650" to be "Succeeded or Failed"
    Jan 17 07:53:33.297: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.215904ms
    Jan 17 07:53:35.305: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019147638s
    Jan 17 07:53:37.310: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024840528s
    STEP: Saw pod success 01/17/23 07:53:37.31
    Jan 17 07:53:37.311: INFO: Pod "downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2" satisfied condition "Succeeded or Failed"
    Jan 17 07:53:37.319: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 container client-container: <nil>
    STEP: delete the pod 01/17/23 07:53:37.333
    Jan 17 07:53:37.387: INFO: Waiting for pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 to disappear
    Jan 17 07:53:37.395: INFO: Pod downwardapi-volume-a8e1227c-3d00-49cc-92a5-5221c34358a2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 07:53:37.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4650" for this suite. 01/17/23 07:53:37.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:37.428
Jan 17 07:53:37.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename containers 01/17/23 07:53:37.429
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:37.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:37.481
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/17/23 07:53:37.497
Jan 17 07:53:37.563: INFO: Waiting up to 5m0s for pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5" in namespace "containers-202" to be "Succeeded or Failed"
Jan 17 07:53:37.590: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.816945ms
Jan 17 07:53:39.597: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033710169s
Jan 17 07:53:41.600: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036797075s
STEP: Saw pod success 01/17/23 07:53:41.6
Jan 17 07:53:41.600: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5" satisfied condition "Succeeded or Failed"
Jan 17 07:53:41.607: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 07:53:41.622
Jan 17 07:53:41.686: INFO: Waiting for pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 to disappear
Jan 17 07:53:41.700: INFO: Pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 07:53:41.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-202" for this suite. 01/17/23 07:53:41.72
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":298,"skipped":5505,"failed":0}
------------------------------
• [4.319 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:37.428
    Jan 17 07:53:37.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename containers 01/17/23 07:53:37.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:37.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:37.481
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/17/23 07:53:37.497
    Jan 17 07:53:37.563: INFO: Waiting up to 5m0s for pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5" in namespace "containers-202" to be "Succeeded or Failed"
    Jan 17 07:53:37.590: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.816945ms
    Jan 17 07:53:39.597: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033710169s
    Jan 17 07:53:41.600: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036797075s
    STEP: Saw pod success 01/17/23 07:53:41.6
    Jan 17 07:53:41.600: INFO: Pod "client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5" satisfied condition "Succeeded or Failed"
    Jan 17 07:53:41.607: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 07:53:41.622
    Jan 17 07:53:41.686: INFO: Waiting for pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 to disappear
    Jan 17 07:53:41.700: INFO: Pod client-containers-7e69be2b-16d4-4a67-a736-aa7ee39cddd5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 07:53:41.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-202" for this suite. 01/17/23 07:53:41.72
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:53:41.747
Jan 17 07:53:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename deployment 01/17/23 07:53:41.749
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:41.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:41.806
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 17 07:53:41.878: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 07:53:41.878
Jan 17 07:53:41.879: INFO: Waiting up to 5m0s for pod "test-rollover-controller-575pd" in namespace "deployment-2759" to be "running"
Jan 17 07:53:41.894: INFO: Pod "test-rollover-controller-575pd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.133203ms
Jan 17 07:53:43.902: INFO: Pod "test-rollover-controller-575pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.022558247s
Jan 17 07:53:43.902: INFO: Pod "test-rollover-controller-575pd" satisfied condition "running"
Jan 17 07:53:43.902: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 17 07:53:45.912: INFO: Creating deployment "test-rollover-deployment"
Jan 17 07:53:45.938: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 17 07:53:47.954: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 17 07:53:47.963: INFO: Ensure that both replica sets have 1 created replica
Jan 17 07:53:47.971: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 17 07:53:47.987: INFO: Updating deployment test-rollover-deployment
Jan 17 07:53:47.987: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 17 07:53:50.019: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 17 07:53:50.030: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 17 07:53:50.040: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 07:53:50.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:53:52.056: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 07:53:52.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:53:54.052: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 07:53:54.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:53:56.052: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 07:53:56.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:53:58.050: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 07:53:58.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 07:54:00.054: INFO: 
Jan 17 07:54:00.054: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 07:54:00.073: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2759  3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 37254 2 2023-01-17 07:53:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:53:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:53:45 +0000 UTC,LastTransitionTime:2023-01-17 07:53:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-17 07:53:59 +0000 UTC,LastTransitionTime:2023-01-17 07:53:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 07:54:00.078: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2759  a52657a4-e125-4e34-8a94-dcada32f8ab9 37243 2 2023-01-17 07:53:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3d27 0xc0067c3d28}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:54:00.078: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 17 07:54:00.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2759  4661c6a5-b72c-48d9-b2ec-4a12eb0206d6 37253 2 2023-01-17 07:53:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3ad7 0xc0067c3ad8}] [] [{e2e.test Update apps/v1 2023-01-17 07:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0067c3b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:54:00.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2759  4b9dc7db-7fc4-45f6-8664-4cfee159c3b6 37194 2 2023-01-17 07:53:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3c07 0xc0067c3c08}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 07:54:00.084: INFO: Pod "test-rollover-deployment-6d45fd857b-gp9mm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-gp9mm test-rollover-deployment-6d45fd857b- deployment-2759  e92247a5-d8ee-4d7e-888e-5b73bcb68023 37214 0 2023-01-17 07:53:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:9e93eb77fd7033f904634ebf1491cd0020aeb9655538a3ed64440694c8d014db cni.projectcalico.org/podIP:10.100.168.78/32 cni.projectcalico.org/podIPs:10.100.168.78/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a52657a4-e125-4e34-8a94-dcada32f8ab9 0xc001e072f7 0xc001e072f8}] [] [{Go-http-client Update v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a52657a4-e125-4e34-8a94-dcada32f8ab9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:53:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6cqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6cqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.78,StartTime:2023-01-17 07:53:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:53:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://34c38842d387999464fd6d5cf85abefba0482d007ea514f3b382f7de5a0eab53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 07:54:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2759" for this suite. 01/17/23 07:54:00.092
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":299,"skipped":5505,"failed":0}
------------------------------
• [SLOW TEST] [18.358 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:53:41.747
    Jan 17 07:53:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename deployment 01/17/23 07:53:41.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:53:41.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:53:41.806
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 17 07:53:41.878: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 07:53:41.878
    Jan 17 07:53:41.879: INFO: Waiting up to 5m0s for pod "test-rollover-controller-575pd" in namespace "deployment-2759" to be "running"
    Jan 17 07:53:41.894: INFO: Pod "test-rollover-controller-575pd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.133203ms
    Jan 17 07:53:43.902: INFO: Pod "test-rollover-controller-575pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.022558247s
    Jan 17 07:53:43.902: INFO: Pod "test-rollover-controller-575pd" satisfied condition "running"
    Jan 17 07:53:43.902: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 17 07:53:45.912: INFO: Creating deployment "test-rollover-deployment"
    Jan 17 07:53:45.938: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 17 07:53:47.954: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 17 07:53:47.963: INFO: Ensure that both replica sets have 1 created replica
    Jan 17 07:53:47.971: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 17 07:53:47.987: INFO: Updating deployment test-rollover-deployment
    Jan 17 07:53:47.987: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 17 07:53:50.019: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 17 07:53:50.030: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 17 07:53:50.040: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 07:53:50.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:53:52.056: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 07:53:52.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:53:54.052: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 07:53:54.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:53:56.052: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 07:53:56.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:53:58.050: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 07:53:58.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 53, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 07:54:00.054: INFO: 
    Jan 17 07:54:00.054: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 07:54:00.073: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2759  3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 37254 2 2023-01-17 07:53:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 07:53:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 07:53:45 +0000 UTC,LastTransitionTime:2023-01-17 07:53:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-17 07:53:59 +0000 UTC,LastTransitionTime:2023-01-17 07:53:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 07:54:00.078: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2759  a52657a4-e125-4e34-8a94-dcada32f8ab9 37243 2 2023-01-17 07:53:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3d27 0xc0067c3d28}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:54:00.078: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 17 07:54:00.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2759  4661c6a5-b72c-48d9-b2ec-4a12eb0206d6 37253 2 2023-01-17 07:53:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3ad7 0xc0067c3ad8}] [] [{e2e.test Update apps/v1 2023-01-17 07:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0067c3b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:54:00.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2759  4b9dc7db-7fc4-45f6-8664-4cfee159c3b6 37194 2 2023-01-17 07:53:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4 0xc0067c3c07 0xc0067c3c08}] [] [{kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fcc1c43-2ac1-4c0b-8022-aca74ac3e6d4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c3cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 07:54:00.084: INFO: Pod "test-rollover-deployment-6d45fd857b-gp9mm" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-gp9mm test-rollover-deployment-6d45fd857b- deployment-2759  e92247a5-d8ee-4d7e-888e-5b73bcb68023 37214 0 2023-01-17 07:53:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:9e93eb77fd7033f904634ebf1491cd0020aeb9655538a3ed64440694c8d014db cni.projectcalico.org/podIP:10.100.168.78/32 cni.projectcalico.org/podIPs:10.100.168.78/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a52657a4-e125-4e34-8a94-dcada32f8ab9 0xc001e072f7 0xc001e072f8}] [] [{Go-http-client Update v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 07:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a52657a4-e125-4e34-8a94-dcada32f8ab9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 07:53:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.168.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6cqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6cqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster125-w73dz53kvqes-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 07:53:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.168.78,StartTime:2023-01-17 07:53:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 07:53:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://34c38842d387999464fd6d5cf85abefba0482d007ea514f3b382f7de5a0eab53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.168.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 07:54:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2759" for this suite. 01/17/23 07:54:00.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:00.106
Jan 17 07:54:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replication-controller 01/17/23 07:54:00.108
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:00.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:00.142
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e 01/17/23 07:54:00.151
Jan 17 07:54:00.171: INFO: Pod name my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Found 0 pods out of 1
Jan 17 07:54:05.180: INFO: Pod name my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Found 1 pods out of 1
Jan 17 07:54:05.180: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e" are running
Jan 17 07:54:05.180: INFO: Waiting up to 5m0s for pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" in namespace "replication-controller-8248" to be "running"
Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26": Phase="Running", Reason="", readiness=true. Elapsed: 13.61311ms
Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" satisfied condition "running"
Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:00 +0000 UTC Reason: Message:}])
Jan 17 07:54:05.194: INFO: Trying to dial the pod
Jan 17 07:54:10.214: INFO: Controller my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Got expected result from replica 1 [my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26]: "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 07:54:10.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8248" for this suite. 01/17/23 07:54:10.227
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":300,"skipped":5515,"failed":0}
------------------------------
• [SLOW TEST] [10.139 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:00.106
    Jan 17 07:54:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replication-controller 01/17/23 07:54:00.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:00.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:00.142
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e 01/17/23 07:54:00.151
    Jan 17 07:54:00.171: INFO: Pod name my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Found 0 pods out of 1
    Jan 17 07:54:05.180: INFO: Pod name my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Found 1 pods out of 1
    Jan 17 07:54:05.180: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e" are running
    Jan 17 07:54:05.180: INFO: Waiting up to 5m0s for pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" in namespace "replication-controller-8248" to be "running"
    Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26": Phase="Running", Reason="", readiness=true. Elapsed: 13.61311ms
    Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" satisfied condition "running"
    Jan 17 07:54:05.194: INFO: Pod "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 07:54:00 +0000 UTC Reason: Message:}])
    Jan 17 07:54:05.194: INFO: Trying to dial the pod
    Jan 17 07:54:10.214: INFO: Controller my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e: Got expected result from replica 1 [my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26]: "my-hostname-basic-70f4e9a5-cb75-4792-8c55-b355f0c88f8e-j4p26", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 07:54:10.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8248" for this suite. 01/17/23 07:54:10.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:10.246
Jan 17 07:54:10.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:54:10.247
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:10.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:10.299
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-76b196c6-f2ee-40e2-937c-c0e9b111a319 01/17/23 07:54:10.305
STEP: Creating a pod to test consume secrets 01/17/23 07:54:10.317
Jan 17 07:54:10.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5" in namespace "projected-8813" to be "Succeeded or Failed"
Jan 17 07:54:10.341: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.949883ms
Jan 17 07:54:12.353: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017151093s
Jan 17 07:54:14.351: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015527061s
STEP: Saw pod success 01/17/23 07:54:14.351
Jan 17 07:54:14.351: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5" satisfied condition "Succeeded or Failed"
Jan 17 07:54:14.357: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:54:14.37
Jan 17 07:54:14.401: INFO: Waiting for pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 to disappear
Jan 17 07:54:14.410: INFO: Pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 07:54:14.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8813" for this suite. 01/17/23 07:54:14.418
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":301,"skipped":5523,"failed":0}
------------------------------
• [4.189 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:10.246
    Jan 17 07:54:10.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:54:10.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:10.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:10.299
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-76b196c6-f2ee-40e2-937c-c0e9b111a319 01/17/23 07:54:10.305
    STEP: Creating a pod to test consume secrets 01/17/23 07:54:10.317
    Jan 17 07:54:10.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5" in namespace "projected-8813" to be "Succeeded or Failed"
    Jan 17 07:54:10.341: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.949883ms
    Jan 17 07:54:12.353: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017151093s
    Jan 17 07:54:14.351: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015527061s
    STEP: Saw pod success 01/17/23 07:54:14.351
    Jan 17 07:54:14.351: INFO: Pod "pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5" satisfied condition "Succeeded or Failed"
    Jan 17 07:54:14.357: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:54:14.37
    Jan 17 07:54:14.401: INFO: Waiting for pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 to disappear
    Jan 17 07:54:14.410: INFO: Pod pod-projected-secrets-555cab01-2757-412b-95a0-8e900e8128f5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 07:54:14.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8813" for this suite. 01/17/23 07:54:14.418
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:14.436
Jan 17 07:54:14.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 07:54:14.438
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:14.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:14.493
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-73a05064-752e-4703-82c5-0c6673e30679 01/17/23 07:54:14.502
STEP: Creating a pod to test consume secrets 01/17/23 07:54:14.52
Jan 17 07:54:14.536: INFO: Waiting up to 5m0s for pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6" in namespace "secrets-7464" to be "Succeeded or Failed"
Jan 17 07:54:14.554: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.22359ms
Jan 17 07:54:16.560: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023281358s
Jan 17 07:54:18.562: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.025148574s
Jan 17 07:54:20.562: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026087828s
STEP: Saw pod success 01/17/23 07:54:20.563
Jan 17 07:54:20.563: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6" satisfied condition "Succeeded or Failed"
Jan 17 07:54:20.568: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:54:20.58
Jan 17 07:54:20.625: INFO: Waiting for pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 to disappear
Jan 17 07:54:20.630: INFO: Pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 07:54:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7464" for this suite. 01/17/23 07:54:20.64
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":302,"skipped":5527,"failed":0}
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:14.436
    Jan 17 07:54:14.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 07:54:14.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:14.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:14.493
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-73a05064-752e-4703-82c5-0c6673e30679 01/17/23 07:54:14.502
    STEP: Creating a pod to test consume secrets 01/17/23 07:54:14.52
    Jan 17 07:54:14.536: INFO: Waiting up to 5m0s for pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6" in namespace "secrets-7464" to be "Succeeded or Failed"
    Jan 17 07:54:14.554: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.22359ms
    Jan 17 07:54:16.560: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023281358s
    Jan 17 07:54:18.562: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.025148574s
    Jan 17 07:54:20.562: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026087828s
    STEP: Saw pod success 01/17/23 07:54:20.563
    Jan 17 07:54:20.563: INFO: Pod "pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6" satisfied condition "Succeeded or Failed"
    Jan 17 07:54:20.568: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:54:20.58
    Jan 17 07:54:20.625: INFO: Waiting for pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 to disappear
    Jan 17 07:54:20.630: INFO: Pod pod-secrets-094f17fd-02b4-4074-ab04-a9d007c1f8b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 07:54:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7464" for this suite. 01/17/23 07:54:20.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:20.66
Jan 17 07:54:20.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 07:54:20.662
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:20.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:20.724
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 07:54:20.783
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:54:21.576
STEP: Deploying the webhook pod 01/17/23 07:54:21.605
STEP: Wait for the deployment to be ready 01/17/23 07:54:21.638
Jan 17 07:54:21.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 17 07:54:23.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/17/23 07:54:25.74
STEP: Verifying the service has paired with the endpoint 01/17/23 07:54:25.765
Jan 17 07:54:26.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 07:54:26.772
STEP: create a pod 01/17/23 07:54:26.805
Jan 17 07:54:26.826: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3806" to be "running"
Jan 17 07:54:26.835: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353237ms
Jan 17 07:54:28.841: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01499127s
Jan 17 07:54:28.841: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/17/23 07:54:28.841
Jan 17 07:54:28.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=webhook-3806 attach --namespace=webhook-3806 to-be-attached-pod -i -c=container1'
Jan 17 07:54:29.077: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:54:29.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3806" for this suite. 01/17/23 07:54:29.1
STEP: Destroying namespace "webhook-3806-markers" for this suite. 01/17/23 07:54:29.118
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":303,"skipped":5534,"failed":0}
------------------------------
• [SLOW TEST] [8.637 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:20.66
    Jan 17 07:54:20.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 07:54:20.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:20.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:20.724
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 07:54:20.783
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 07:54:21.576
    STEP: Deploying the webhook pod 01/17/23 07:54:21.605
    STEP: Wait for the deployment to be ready 01/17/23 07:54:21.638
    Jan 17 07:54:21.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 17 07:54:23.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 7, 54, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/17/23 07:54:25.74
    STEP: Verifying the service has paired with the endpoint 01/17/23 07:54:25.765
    Jan 17 07:54:26.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 07:54:26.772
    STEP: create a pod 01/17/23 07:54:26.805
    Jan 17 07:54:26.826: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3806" to be "running"
    Jan 17 07:54:26.835: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353237ms
    Jan 17 07:54:28.841: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01499127s
    Jan 17 07:54:28.841: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/17/23 07:54:28.841
    Jan 17 07:54:28.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=webhook-3806 attach --namespace=webhook-3806 to-be-attached-pod -i -c=container1'
    Jan 17 07:54:29.077: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:54:29.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3806" for this suite. 01/17/23 07:54:29.1
    STEP: Destroying namespace "webhook-3806-markers" for this suite. 01/17/23 07:54:29.118
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:29.303
Jan 17 07:54:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 07:54:29.304
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:29.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:29.405
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/17/23 07:54:29.419
Jan 17 07:54:29.447: INFO: Waiting up to 5m0s for pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4" in namespace "var-expansion-4212" to be "Succeeded or Failed"
Jan 17 07:54:29.464: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.230468ms
Jan 17 07:54:31.472: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02545178s
Jan 17 07:54:33.471: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02440054s
STEP: Saw pod success 01/17/23 07:54:33.471
Jan 17 07:54:33.471: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4" satisfied condition "Succeeded or Failed"
Jan 17 07:54:33.482: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 container dapi-container: <nil>
STEP: delete the pod 01/17/23 07:54:33.505
Jan 17 07:54:33.538: INFO: Waiting for pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 to disappear
Jan 17 07:54:33.565: INFO: Pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 07:54:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4212" for this suite. 01/17/23 07:54:33.573
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":304,"skipped":5560,"failed":0}
------------------------------
• [4.286 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:29.303
    Jan 17 07:54:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 07:54:29.304
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:29.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:29.405
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/17/23 07:54:29.419
    Jan 17 07:54:29.447: INFO: Waiting up to 5m0s for pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4" in namespace "var-expansion-4212" to be "Succeeded or Failed"
    Jan 17 07:54:29.464: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.230468ms
    Jan 17 07:54:31.472: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02545178s
    Jan 17 07:54:33.471: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02440054s
    STEP: Saw pod success 01/17/23 07:54:33.471
    Jan 17 07:54:33.471: INFO: Pod "var-expansion-7e974755-2c48-4349-93c7-a597321f38d4" satisfied condition "Succeeded or Failed"
    Jan 17 07:54:33.482: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 07:54:33.505
    Jan 17 07:54:33.538: INFO: Waiting for pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 to disappear
    Jan 17 07:54:33.565: INFO: Pod var-expansion-7e974755-2c48-4349-93c7-a597321f38d4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 07:54:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4212" for this suite. 01/17/23 07:54:33.573
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:33.589
Jan 17 07:54:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 07:54:33.591
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:33.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:33.639
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/17/23 07:54:33.648
Jan 17 07:54:33.668: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6299  461a3657-0cba-4747-b731-f8d1085f1216 37572 0 2023-01-17 07:54:33 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-17 07:54:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tbzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tbzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 07:54:33.668: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6299" to be "running and ready"
Jan 17 07:54:33.675: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.360081ms
Jan 17 07:54:33.676: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:54:35.682: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013619381s
Jan 17 07:54:35.682: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:54:37.683: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.01472531s
Jan 17 07:54:37.683: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 17 07:54:37.683: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/17/23 07:54:37.683
Jan 17 07:54:37.683: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6299 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:54:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:54:37.684: INFO: ExecWithOptions: Clientset creation
Jan 17 07:54:37.684: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6299/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/17/23 07:54:37.875
Jan 17 07:54:37.876: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6299 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:54:37.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:54:37.876: INFO: ExecWithOptions: Clientset creation
Jan 17 07:54:37.876: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6299/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 07:54:38.076: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 07:54:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6299" for this suite. 01/17/23 07:54:38.126
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":305,"skipped":5560,"failed":0}
------------------------------
• [4.555 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:33.589
    Jan 17 07:54:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 07:54:33.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:33.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:33.639
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/17/23 07:54:33.648
    Jan 17 07:54:33.668: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6299  461a3657-0cba-4747-b731-f8d1085f1216 37572 0 2023-01-17 07:54:33 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-17 07:54:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tbzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tbzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 07:54:33.668: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6299" to be "running and ready"
    Jan 17 07:54:33.675: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.360081ms
    Jan 17 07:54:33.676: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:54:35.682: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013619381s
    Jan 17 07:54:35.682: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:54:37.683: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.01472531s
    Jan 17 07:54:37.683: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 17 07:54:37.683: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/17/23 07:54:37.683
    Jan 17 07:54:37.683: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6299 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:54:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:54:37.684: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:54:37.684: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6299/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/17/23 07:54:37.875
    Jan 17 07:54:37.876: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6299 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:54:37.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:54:37.876: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:54:37.876: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-6299/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 07:54:38.076: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 07:54:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6299" for this suite. 01/17/23 07:54:38.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:38.15
Jan 17 07:54:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename security-context 01/17/23 07:54:38.153
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:38.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:38.193
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 07:54:38.202
Jan 17 07:54:38.223: INFO: Waiting up to 5m0s for pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16" in namespace "security-context-3771" to be "Succeeded or Failed"
Jan 17 07:54:38.248: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Pending", Reason="", readiness=false. Elapsed: 24.82974ms
Jan 17 07:54:40.253: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030128745s
Jan 17 07:54:42.253: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030356212s
STEP: Saw pod success 01/17/23 07:54:42.253
Jan 17 07:54:42.254: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16" satisfied condition "Succeeded or Failed"
Jan 17 07:54:42.258: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 container test-container: <nil>
STEP: delete the pod 01/17/23 07:54:42.268
Jan 17 07:54:42.302: INFO: Waiting for pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 to disappear
Jan 17 07:54:42.308: INFO: Pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 07:54:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3771" for this suite. 01/17/23 07:54:42.315
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":306,"skipped":5625,"failed":0}
------------------------------
• [4.179 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:38.15
    Jan 17 07:54:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename security-context 01/17/23 07:54:38.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:38.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:38.193
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 07:54:38.202
    Jan 17 07:54:38.223: INFO: Waiting up to 5m0s for pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16" in namespace "security-context-3771" to be "Succeeded or Failed"
    Jan 17 07:54:38.248: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Pending", Reason="", readiness=false. Elapsed: 24.82974ms
    Jan 17 07:54:40.253: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030128745s
    Jan 17 07:54:42.253: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030356212s
    STEP: Saw pod success 01/17/23 07:54:42.253
    Jan 17 07:54:42.254: INFO: Pod "security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16" satisfied condition "Succeeded or Failed"
    Jan 17 07:54:42.258: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:54:42.268
    Jan 17 07:54:42.302: INFO: Waiting for pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 to disappear
    Jan 17 07:54:42.308: INFO: Pod security-context-75d8a764-3bd5-48cd-a38c-ffa241ae0a16 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 07:54:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3771" for this suite. 01/17/23 07:54:42.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:42.33
Jan 17 07:54:42.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:54:42.331
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:42.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:42.368
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-d03fcafc-6149-4d0d-83f2-a16c262a0024 01/17/23 07:54:42.376
STEP: Creating a pod to test consume secrets 01/17/23 07:54:42.389
Jan 17 07:54:42.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc" in namespace "projected-4102" to be "Succeeded or Failed"
Jan 17 07:54:42.417: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337909ms
Jan 17 07:54:44.425: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012931992s
Jan 17 07:54:46.424: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01219004s
STEP: Saw pod success 01/17/23 07:54:46.424
Jan 17 07:54:46.424: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc" satisfied condition "Succeeded or Failed"
Jan 17 07:54:46.433: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 07:54:46.45
Jan 17 07:54:46.483: INFO: Waiting for pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc to disappear
Jan 17 07:54:46.490: INFO: Pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 07:54:46.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4102" for this suite. 01/17/23 07:54:46.5
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":307,"skipped":5632,"failed":0}
------------------------------
• [4.187 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:42.33
    Jan 17 07:54:42.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:54:42.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:42.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:42.368
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-d03fcafc-6149-4d0d-83f2-a16c262a0024 01/17/23 07:54:42.376
    STEP: Creating a pod to test consume secrets 01/17/23 07:54:42.389
    Jan 17 07:54:42.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc" in namespace "projected-4102" to be "Succeeded or Failed"
    Jan 17 07:54:42.417: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.337909ms
    Jan 17 07:54:44.425: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012931992s
    Jan 17 07:54:46.424: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01219004s
    STEP: Saw pod success 01/17/23 07:54:46.424
    Jan 17 07:54:46.424: INFO: Pod "pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc" satisfied condition "Succeeded or Failed"
    Jan 17 07:54:46.433: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 07:54:46.45
    Jan 17 07:54:46.483: INFO: Waiting for pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc to disappear
    Jan 17 07:54:46.490: INFO: Pod pod-projected-secrets-bc6af033-2503-4084-9a61-3962a41782cc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 07:54:46.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4102" for this suite. 01/17/23 07:54:46.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:54:46.519
Jan 17 07:54:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename var-expansion 01/17/23 07:54:46.52
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:46.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:46.584
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/17/23 07:54:46.594
STEP: waiting for pod running 01/17/23 07:54:46.615
Jan 17 07:54:46.615: INFO: Waiting up to 2m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604" to be "running"
Jan 17 07:54:46.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.130044ms
Jan 17 07:54:48.637: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021994255s
Jan 17 07:54:48.637: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" satisfied condition "running"
STEP: creating a file in subpath 01/17/23 07:54:48.637
Jan 17 07:54:48.642: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3604 PodName:var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:54:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:54:48.643: INFO: ExecWithOptions: Clientset creation
Jan 17 07:54:48.643: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3604/pods/var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/17/23 07:54:48.849
Jan 17 07:54:48.854: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3604 PodName:var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 07:54:48.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 07:54:48.855: INFO: ExecWithOptions: Clientset creation
Jan 17 07:54:48.855: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3604/pods/var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/17/23 07:54:49.098
Jan 17 07:54:49.625: INFO: Successfully updated pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2"
STEP: waiting for annotated pod running 01/17/23 07:54:49.625
Jan 17 07:54:49.625: INFO: Waiting up to 2m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604" to be "running"
Jan 17 07:54:49.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Running", Reason="", readiness=true. Elapsed: 5.831456ms
Jan 17 07:54:49.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 07:54:49.631
Jan 17 07:54:49.631: INFO: Deleting pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604"
Jan 17 07:54:49.649: INFO: Wait up to 5m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 07:55:23.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3604" for this suite. 01/17/23 07:55:23.673
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":308,"skipped":5644,"failed":0}
------------------------------
• [SLOW TEST] [37.167 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:54:46.519
    Jan 17 07:54:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename var-expansion 01/17/23 07:54:46.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:54:46.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:54:46.584
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/17/23 07:54:46.594
    STEP: waiting for pod running 01/17/23 07:54:46.615
    Jan 17 07:54:46.615: INFO: Waiting up to 2m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604" to be "running"
    Jan 17 07:54:46.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.130044ms
    Jan 17 07:54:48.637: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021994255s
    Jan 17 07:54:48.637: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" satisfied condition "running"
    STEP: creating a file in subpath 01/17/23 07:54:48.637
    Jan 17 07:54:48.642: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3604 PodName:var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:54:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:54:48.643: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:54:48.643: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3604/pods/var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/17/23 07:54:48.849
    Jan 17 07:54:48.854: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3604 PodName:var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 07:54:48.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 07:54:48.855: INFO: ExecWithOptions: Clientset creation
    Jan 17 07:54:48.855: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3604/pods/var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/17/23 07:54:49.098
    Jan 17 07:54:49.625: INFO: Successfully updated pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2"
    STEP: waiting for annotated pod running 01/17/23 07:54:49.625
    Jan 17 07:54:49.625: INFO: Waiting up to 2m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604" to be "running"
    Jan 17 07:54:49.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2": Phase="Running", Reason="", readiness=true. Elapsed: 5.831456ms
    Jan 17 07:54:49.631: INFO: Pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 07:54:49.631
    Jan 17 07:54:49.631: INFO: Deleting pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" in namespace "var-expansion-3604"
    Jan 17 07:54:49.649: INFO: Wait up to 5m0s for pod "var-expansion-6b1267c3-4d44-43c8-a0e9-2dd07a8c66a2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 07:55:23.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3604" for this suite. 01/17/23 07:55:23.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:55:23.687
Jan 17 07:55:23.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 07:55:23.689
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:23.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:23.736
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/17/23 07:55:23.756
STEP: create the rc2 01/17/23 07:55:23.769
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/17/23 07:55:28.846
STEP: delete the rc simpletest-rc-to-be-deleted 01/17/23 07:55:31.966
STEP: wait for the rc to be deleted 01/17/23 07:55:32.009
Jan 17 07:55:37.102: INFO: 69 pods remaining
Jan 17 07:55:37.102: INFO: 69 pods has nil DeletionTimestamp
Jan 17 07:55:37.102: INFO: 
STEP: Gathering metrics 01/17/23 07:55:42.045
W0117 07:55:42.072471      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 07:55:42.072: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 07:55:42.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-25hcn" in namespace "gc-5190"
Jan 17 07:55:42.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nxg8" in namespace "gc-5190"
Jan 17 07:55:42.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pkjs" in namespace "gc-5190"
Jan 17 07:55:42.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zcjl" in namespace "gc-5190"
Jan 17 07:55:42.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ppk8" in namespace "gc-5190"
Jan 17 07:55:42.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s5cz" in namespace "gc-5190"
Jan 17 07:55:42.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bj7s" in namespace "gc-5190"
Jan 17 07:55:42.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dfbz" in namespace "gc-5190"
Jan 17 07:55:42.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mgp8" in namespace "gc-5190"
Jan 17 07:55:42.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c4x4" in namespace "gc-5190"
Jan 17 07:55:42.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dsqr" in namespace "gc-5190"
Jan 17 07:55:42.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lmg7" in namespace "gc-5190"
Jan 17 07:55:42.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vn6q" in namespace "gc-5190"
Jan 17 07:55:42.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-756d2" in namespace "gc-5190"
Jan 17 07:55:42.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-78jkw" in namespace "gc-5190"
Jan 17 07:55:42.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wmpl" in namespace "gc-5190"
Jan 17 07:55:42.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bdl8" in namespace "gc-5190"
Jan 17 07:55:42.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rjmq" in namespace "gc-5190"
Jan 17 07:55:42.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-92xdt" in namespace "gc-5190"
Jan 17 07:55:42.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cq68" in namespace "gc-5190"
Jan 17 07:55:42.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjnt" in namespace "gc-5190"
Jan 17 07:55:43.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r84x" in namespace "gc-5190"
Jan 17 07:55:43.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9j6" in namespace "gc-5190"
Jan 17 07:55:43.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v2ln" in namespace "gc-5190"
Jan 17 07:55:43.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbmws" in namespace "gc-5190"
Jan 17 07:55:43.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh2j6" in namespace "gc-5190"
Jan 17 07:55:43.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-bph6z" in namespace "gc-5190"
Jan 17 07:55:43.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccsrc" in namespace "gc-5190"
Jan 17 07:55:43.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch97f" in namespace "gc-5190"
Jan 17 07:55:43.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-crddv" in namespace "gc-5190"
Jan 17 07:55:43.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-crw76" in namespace "gc-5190"
Jan 17 07:55:43.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-db78m" in namespace "gc-5190"
Jan 17 07:55:43.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2wcx" in namespace "gc-5190"
Jan 17 07:55:43.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-f962t" in namespace "gc-5190"
Jan 17 07:55:44.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg96q" in namespace "gc-5190"
Jan 17 07:55:44.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgppw" in namespace "gc-5190"
Jan 17 07:55:44.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnhb8" in namespace "gc-5190"
Jan 17 07:55:44.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqxgp" in namespace "gc-5190"
Jan 17 07:55:44.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvdz9" in namespace "gc-5190"
Jan 17 07:55:44.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghd6j" in namespace "gc-5190"
Jan 17 07:55:44.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj6kx" in namespace "gc-5190"
Jan 17 07:55:44.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjrzs" in namespace "gc-5190"
Jan 17 07:55:44.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs97w" in namespace "gc-5190"
Jan 17 07:55:44.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjw5" in namespace "gc-5190"
Jan 17 07:55:44.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzgqq" in namespace "gc-5190"
Jan 17 07:55:44.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-h49v2" in namespace "gc-5190"
Jan 17 07:55:44.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-hckcv" in namespace "gc-5190"
Jan 17 07:55:44.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgfs8" in namespace "gc-5190"
Jan 17 07:55:45.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2dqv" in namespace "gc-5190"
Jan 17 07:55:45.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk9zf" in namespace "gc-5190"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 07:55:45.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5190" for this suite. 01/17/23 07:55:45.142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":309,"skipped":5656,"failed":0}
------------------------------
• [SLOW TEST] [21.471 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:55:23.687
    Jan 17 07:55:23.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 07:55:23.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:23.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:23.736
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/17/23 07:55:23.756
    STEP: create the rc2 01/17/23 07:55:23.769
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/17/23 07:55:28.846
    STEP: delete the rc simpletest-rc-to-be-deleted 01/17/23 07:55:31.966
    STEP: wait for the rc to be deleted 01/17/23 07:55:32.009
    Jan 17 07:55:37.102: INFO: 69 pods remaining
    Jan 17 07:55:37.102: INFO: 69 pods has nil DeletionTimestamp
    Jan 17 07:55:37.102: INFO: 
    STEP: Gathering metrics 01/17/23 07:55:42.045
    W0117 07:55:42.072471      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 07:55:42.072: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 17 07:55:42.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-25hcn" in namespace "gc-5190"
    Jan 17 07:55:42.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nxg8" in namespace "gc-5190"
    Jan 17 07:55:42.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pkjs" in namespace "gc-5190"
    Jan 17 07:55:42.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zcjl" in namespace "gc-5190"
    Jan 17 07:55:42.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ppk8" in namespace "gc-5190"
    Jan 17 07:55:42.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s5cz" in namespace "gc-5190"
    Jan 17 07:55:42.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bj7s" in namespace "gc-5190"
    Jan 17 07:55:42.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dfbz" in namespace "gc-5190"
    Jan 17 07:55:42.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mgp8" in namespace "gc-5190"
    Jan 17 07:55:42.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c4x4" in namespace "gc-5190"
    Jan 17 07:55:42.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dsqr" in namespace "gc-5190"
    Jan 17 07:55:42.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lmg7" in namespace "gc-5190"
    Jan 17 07:55:42.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vn6q" in namespace "gc-5190"
    Jan 17 07:55:42.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-756d2" in namespace "gc-5190"
    Jan 17 07:55:42.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-78jkw" in namespace "gc-5190"
    Jan 17 07:55:42.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wmpl" in namespace "gc-5190"
    Jan 17 07:55:42.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bdl8" in namespace "gc-5190"
    Jan 17 07:55:42.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rjmq" in namespace "gc-5190"
    Jan 17 07:55:42.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-92xdt" in namespace "gc-5190"
    Jan 17 07:55:42.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cq68" in namespace "gc-5190"
    Jan 17 07:55:42.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kjnt" in namespace "gc-5190"
    Jan 17 07:55:43.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r84x" in namespace "gc-5190"
    Jan 17 07:55:43.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9j6" in namespace "gc-5190"
    Jan 17 07:55:43.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v2ln" in namespace "gc-5190"
    Jan 17 07:55:43.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbmws" in namespace "gc-5190"
    Jan 17 07:55:43.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh2j6" in namespace "gc-5190"
    Jan 17 07:55:43.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-bph6z" in namespace "gc-5190"
    Jan 17 07:55:43.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccsrc" in namespace "gc-5190"
    Jan 17 07:55:43.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch97f" in namespace "gc-5190"
    Jan 17 07:55:43.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-crddv" in namespace "gc-5190"
    Jan 17 07:55:43.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-crw76" in namespace "gc-5190"
    Jan 17 07:55:43.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-db78m" in namespace "gc-5190"
    Jan 17 07:55:43.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2wcx" in namespace "gc-5190"
    Jan 17 07:55:43.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-f962t" in namespace "gc-5190"
    Jan 17 07:55:44.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg96q" in namespace "gc-5190"
    Jan 17 07:55:44.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgppw" in namespace "gc-5190"
    Jan 17 07:55:44.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnhb8" in namespace "gc-5190"
    Jan 17 07:55:44.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqxgp" in namespace "gc-5190"
    Jan 17 07:55:44.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvdz9" in namespace "gc-5190"
    Jan 17 07:55:44.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghd6j" in namespace "gc-5190"
    Jan 17 07:55:44.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj6kx" in namespace "gc-5190"
    Jan 17 07:55:44.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjrzs" in namespace "gc-5190"
    Jan 17 07:55:44.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs97w" in namespace "gc-5190"
    Jan 17 07:55:44.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjw5" in namespace "gc-5190"
    Jan 17 07:55:44.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzgqq" in namespace "gc-5190"
    Jan 17 07:55:44.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-h49v2" in namespace "gc-5190"
    Jan 17 07:55:44.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-hckcv" in namespace "gc-5190"
    Jan 17 07:55:44.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgfs8" in namespace "gc-5190"
    Jan 17 07:55:45.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2dqv" in namespace "gc-5190"
    Jan 17 07:55:45.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk9zf" in namespace "gc-5190"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 07:55:45.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5190" for this suite. 01/17/23 07:55:45.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:55:45.183
Jan 17 07:55:45.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 07:55:45.185
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:45.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:45.255
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/17/23 07:55:45.27
Jan 17 07:55:45.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e" in namespace "projected-3902" to be "Succeeded or Failed"
Jan 17 07:55:45.335: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.993264ms
Jan 17 07:55:47.347: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031685641s
Jan 17 07:55:49.342: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026487164s
Jan 17 07:55:51.343: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026746425s
STEP: Saw pod success 01/17/23 07:55:51.343
Jan 17 07:55:51.343: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e" satisfied condition "Succeeded or Failed"
Jan 17 07:55:51.352: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e container client-container: <nil>
STEP: delete the pod 01/17/23 07:55:51.381
Jan 17 07:55:51.436: INFO: Waiting for pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e to disappear
Jan 17 07:55:51.448: INFO: Pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 07:55:51.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3902" for this suite. 01/17/23 07:55:51.467
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":310,"skipped":5685,"failed":0}
------------------------------
• [SLOW TEST] [6.350 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:55:45.183
    Jan 17 07:55:45.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 07:55:45.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:45.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:45.255
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/17/23 07:55:45.27
    Jan 17 07:55:45.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e" in namespace "projected-3902" to be "Succeeded or Failed"
    Jan 17 07:55:45.335: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.993264ms
    Jan 17 07:55:47.347: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031685641s
    Jan 17 07:55:49.342: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026487164s
    Jan 17 07:55:51.343: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026746425s
    STEP: Saw pod success 01/17/23 07:55:51.343
    Jan 17 07:55:51.343: INFO: Pod "downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e" satisfied condition "Succeeded or Failed"
    Jan 17 07:55:51.352: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e container client-container: <nil>
    STEP: delete the pod 01/17/23 07:55:51.381
    Jan 17 07:55:51.436: INFO: Waiting for pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e to disappear
    Jan 17 07:55:51.448: INFO: Pod downwardapi-volume-71a662ea-c543-42fa-a851-cbcd7f5af67e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 07:55:51.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3902" for this suite. 01/17/23 07:55:51.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:55:51.534
Jan 17 07:55:51.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:55:51.536
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:51.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:51.619
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:55:51.672
Jan 17 07:55:51.718: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4810" to be "running and ready"
Jan 17 07:55:51.746: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 27.944707ms
Jan 17 07:55:51.746: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:55:53.758: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039743703s
Jan 17 07:55:53.758: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 07:55:53.758: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/17/23 07:55:53.764
Jan 17 07:55:53.802: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4810" to be "running and ready"
Jan 17 07:55:53.811: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.007914ms
Jan 17 07:55:53.812: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 07:55:55.818: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015620529s
Jan 17 07:55:55.818: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 17 07:55:55.818: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/17/23 07:55:55.828
STEP: delete the pod with lifecycle hook 01/17/23 07:55:55.839
Jan 17 07:55:55.858: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 07:55:55.864: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 07:55:57.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 07:55:57.882: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 07:55:59.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 07:55:59.872: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 07:55:59.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4810" for this suite. 01/17/23 07:55:59.883
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":311,"skipped":5699,"failed":0}
------------------------------
• [SLOW TEST] [8.364 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:55:51.534
    Jan 17 07:55:51.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 07:55:51.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:51.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:51.619
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 07:55:51.672
    Jan 17 07:55:51.718: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4810" to be "running and ready"
    Jan 17 07:55:51.746: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 27.944707ms
    Jan 17 07:55:51.746: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:55:53.758: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039743703s
    Jan 17 07:55:53.758: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 07:55:53.758: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/17/23 07:55:53.764
    Jan 17 07:55:53.802: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4810" to be "running and ready"
    Jan 17 07:55:53.811: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.007914ms
    Jan 17 07:55:53.812: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 07:55:55.818: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015620529s
    Jan 17 07:55:55.818: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 17 07:55:55.818: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/17/23 07:55:55.828
    STEP: delete the pod with lifecycle hook 01/17/23 07:55:55.839
    Jan 17 07:55:55.858: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 07:55:55.864: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 17 07:55:57.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 07:55:57.882: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 17 07:55:59.865: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 07:55:59.872: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 07:55:59.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4810" for this suite. 01/17/23 07:55:59.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:55:59.899
Jan 17 07:55:59.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 07:55:59.901
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:59.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:59.946
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 07:55:59.955
Jan 17 07:55:59.974: INFO: Waiting up to 5m0s for pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9" in namespace "emptydir-3056" to be "Succeeded or Failed"
Jan 17 07:55:59.980: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.483098ms
Jan 17 07:56:01.986: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012034943s
Jan 17 07:56:03.987: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Running", Reason="", readiness=false. Elapsed: 4.013095957s
Jan 17 07:56:05.988: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013849015s
STEP: Saw pod success 01/17/23 07:56:05.988
Jan 17 07:56:05.988: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9" satisfied condition "Succeeded or Failed"
Jan 17 07:56:05.992: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 container test-container: <nil>
STEP: delete the pod 01/17/23 07:56:06.001
Jan 17 07:56:06.032: INFO: Waiting for pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 to disappear
Jan 17 07:56:06.040: INFO: Pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 07:56:06.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3056" for this suite. 01/17/23 07:56:06.048
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":312,"skipped":5722,"failed":0}
------------------------------
• [SLOW TEST] [6.168 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:55:59.899
    Jan 17 07:55:59.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 07:55:59.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:55:59.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:55:59.946
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 07:55:59.955
    Jan 17 07:55:59.974: INFO: Waiting up to 5m0s for pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9" in namespace "emptydir-3056" to be "Succeeded or Failed"
    Jan 17 07:55:59.980: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.483098ms
    Jan 17 07:56:01.986: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012034943s
    Jan 17 07:56:03.987: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Running", Reason="", readiness=false. Elapsed: 4.013095957s
    Jan 17 07:56:05.988: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013849015s
    STEP: Saw pod success 01/17/23 07:56:05.988
    Jan 17 07:56:05.988: INFO: Pod "pod-66cae13e-0656-478e-af1e-5464f9cb4cb9" satisfied condition "Succeeded or Failed"
    Jan 17 07:56:05.992: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 container test-container: <nil>
    STEP: delete the pod 01/17/23 07:56:06.001
    Jan 17 07:56:06.032: INFO: Waiting for pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 to disappear
    Jan 17 07:56:06.040: INFO: Pod pod-66cae13e-0656-478e-af1e-5464f9cb4cb9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 07:56:06.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3056" for this suite. 01/17/23 07:56:06.048
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:56:06.068
Jan 17 07:56:06.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:56:06.071
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:56:06.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:56:06.111
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/17/23 07:56:06.12
Jan 17 07:56:06.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: mark a version not serverd 01/17/23 07:56:20.864
STEP: check the unserved version gets removed 01/17/23 07:56:20.917
STEP: check the other version is not changed 01/17/23 07:56:27.092
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 07:56:38.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-35" for this suite. 01/17/23 07:56:38.731
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":313,"skipped":5722,"failed":0}
------------------------------
• [SLOW TEST] [32.671 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:56:06.068
    Jan 17 07:56:06.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 07:56:06.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:56:06.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:56:06.111
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/17/23 07:56:06.12
    Jan 17 07:56:06.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: mark a version not serverd 01/17/23 07:56:20.864
    STEP: check the unserved version gets removed 01/17/23 07:56:20.917
    STEP: check the other version is not changed 01/17/23 07:56:27.092
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 07:56:38.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-35" for this suite. 01/17/23 07:56:38.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 07:56:38.742
Jan 17 07:56:38.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename cronjob 01/17/23 07:56:38.744
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:56:38.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:56:38.779
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/17/23 07:56:38.788
STEP: Ensuring a job is scheduled 01/17/23 07:56:38.801
STEP: Ensuring exactly one is scheduled 01/17/23 07:57:00.82
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 07:57:00.831
STEP: Ensuring no more jobs are scheduled 01/17/23 07:57:00.838
STEP: Removing cronjob 01/17/23 08:02:00.848
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 08:02:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1818" for this suite. 01/17/23 08:02:00.882
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":314,"skipped":5740,"failed":0}
------------------------------
• [SLOW TEST] [322.157 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 07:56:38.742
    Jan 17 07:56:38.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename cronjob 01/17/23 07:56:38.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 07:56:38.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 07:56:38.779
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/17/23 07:56:38.788
    STEP: Ensuring a job is scheduled 01/17/23 07:56:38.801
    STEP: Ensuring exactly one is scheduled 01/17/23 07:57:00.82
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 07:57:00.831
    STEP: Ensuring no more jobs are scheduled 01/17/23 07:57:00.838
    STEP: Removing cronjob 01/17/23 08:02:00.848
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 08:02:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1818" for this suite. 01/17/23 08:02:00.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:02:00.901
Jan 17 08:02:00.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replication-controller 01/17/23 08:02:00.903
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:02:00.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:02:00.955
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/17/23 08:02:00.975
STEP: waiting for RC to be added 01/17/23 08:02:00.986
STEP: waiting for available Replicas 01/17/23 08:02:00.986
STEP: patching ReplicationController 01/17/23 08:02:02.734
STEP: waiting for RC to be modified 01/17/23 08:02:02.752
STEP: patching ReplicationController status 01/17/23 08:02:02.752
STEP: waiting for RC to be modified 01/17/23 08:02:02.787
STEP: waiting for available Replicas 01/17/23 08:02:02.788
STEP: fetching ReplicationController status 01/17/23 08:02:02.788
STEP: patching ReplicationController scale 01/17/23 08:02:02.8
STEP: waiting for RC to be modified 01/17/23 08:02:02.82
STEP: waiting for ReplicationController's scale to be the max amount 01/17/23 08:02:02.82
STEP: fetching ReplicationController; ensuring that it's patched 01/17/23 08:02:04.777
STEP: updating ReplicationController status 01/17/23 08:02:04.782
STEP: waiting for RC to be modified 01/17/23 08:02:04.794
STEP: listing all ReplicationControllers 01/17/23 08:02:04.794
STEP: checking that ReplicationController has expected values 01/17/23 08:02:04.808
STEP: deleting ReplicationControllers by collection 01/17/23 08:02:04.808
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/17/23 08:02:04.838
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 08:02:04.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7572" for this suite. 01/17/23 08:02:04.918
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":315,"skipped":5759,"failed":0}
------------------------------
• [4.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:02:00.901
    Jan 17 08:02:00.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replication-controller 01/17/23 08:02:00.903
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:02:00.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:02:00.955
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/17/23 08:02:00.975
    STEP: waiting for RC to be added 01/17/23 08:02:00.986
    STEP: waiting for available Replicas 01/17/23 08:02:00.986
    STEP: patching ReplicationController 01/17/23 08:02:02.734
    STEP: waiting for RC to be modified 01/17/23 08:02:02.752
    STEP: patching ReplicationController status 01/17/23 08:02:02.752
    STEP: waiting for RC to be modified 01/17/23 08:02:02.787
    STEP: waiting for available Replicas 01/17/23 08:02:02.788
    STEP: fetching ReplicationController status 01/17/23 08:02:02.788
    STEP: patching ReplicationController scale 01/17/23 08:02:02.8
    STEP: waiting for RC to be modified 01/17/23 08:02:02.82
    STEP: waiting for ReplicationController's scale to be the max amount 01/17/23 08:02:02.82
    STEP: fetching ReplicationController; ensuring that it's patched 01/17/23 08:02:04.777
    STEP: updating ReplicationController status 01/17/23 08:02:04.782
    STEP: waiting for RC to be modified 01/17/23 08:02:04.794
    STEP: listing all ReplicationControllers 01/17/23 08:02:04.794
    STEP: checking that ReplicationController has expected values 01/17/23 08:02:04.808
    STEP: deleting ReplicationControllers by collection 01/17/23 08:02:04.808
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/17/23 08:02:04.838
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 08:02:04.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7572" for this suite. 01/17/23 08:02:04.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:02:04.937
Jan 17 08:02:04.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 08:02:04.938
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:02:04.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:02:04.969
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4313 01/17/23 08:02:05.015
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/17/23 08:02:05.025
Jan 17 08:02:05.055: INFO: Found 0 stateful pods, waiting for 3
Jan 17 08:02:15.060: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 08:02:15.060: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 08:02:15.060: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 08:02:15.072
Jan 17 08:02:15.095: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/17/23 08:02:15.095
STEP: Not applying an update when the partition is greater than the number of replicas 01/17/23 08:02:25.122
STEP: Performing a canary update 01/17/23 08:02:25.122
Jan 17 08:02:25.150: INFO: Updating stateful set ss2
Jan 17 08:02:25.165: INFO: Waiting for Pod statefulset-4313/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/17/23 08:02:35.175
Jan 17 08:02:35.292: INFO: Found 2 stateful pods, waiting for 3
Jan 17 08:02:45.300: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 08:02:45.300: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 08:02:45.300: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/17/23 08:02:45.307
Jan 17 08:02:45.330: INFO: Updating stateful set ss2
Jan 17 08:02:45.343: INFO: Waiting for Pod statefulset-4313/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 08:02:55.385: INFO: Updating stateful set ss2
Jan 17 08:02:55.403: INFO: Waiting for StatefulSet statefulset-4313/ss2 to complete update
Jan 17 08:02:55.403: INFO: Waiting for Pod statefulset-4313/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 08:03:05.424: INFO: Deleting all statefulset in ns statefulset-4313
Jan 17 08:03:05.430: INFO: Scaling statefulset ss2 to 0
Jan 17 08:03:15.463: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 08:03:15.468: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 08:03:15.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4313" for this suite. 01/17/23 08:03:15.519
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":316,"skipped":5783,"failed":0}
------------------------------
• [SLOW TEST] [70.603 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:02:04.937
    Jan 17 08:02:04.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 08:02:04.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:02:04.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:02:04.969
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4313 01/17/23 08:02:05.015
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/17/23 08:02:05.025
    Jan 17 08:02:05.055: INFO: Found 0 stateful pods, waiting for 3
    Jan 17 08:02:15.060: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 08:02:15.060: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 08:02:15.060: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 08:02:15.072
    Jan 17 08:02:15.095: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/17/23 08:02:15.095
    STEP: Not applying an update when the partition is greater than the number of replicas 01/17/23 08:02:25.122
    STEP: Performing a canary update 01/17/23 08:02:25.122
    Jan 17 08:02:25.150: INFO: Updating stateful set ss2
    Jan 17 08:02:25.165: INFO: Waiting for Pod statefulset-4313/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/17/23 08:02:35.175
    Jan 17 08:02:35.292: INFO: Found 2 stateful pods, waiting for 3
    Jan 17 08:02:45.300: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 08:02:45.300: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 08:02:45.300: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/17/23 08:02:45.307
    Jan 17 08:02:45.330: INFO: Updating stateful set ss2
    Jan 17 08:02:45.343: INFO: Waiting for Pod statefulset-4313/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 08:02:55.385: INFO: Updating stateful set ss2
    Jan 17 08:02:55.403: INFO: Waiting for StatefulSet statefulset-4313/ss2 to complete update
    Jan 17 08:02:55.403: INFO: Waiting for Pod statefulset-4313/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 08:03:05.424: INFO: Deleting all statefulset in ns statefulset-4313
    Jan 17 08:03:05.430: INFO: Scaling statefulset ss2 to 0
    Jan 17 08:03:15.463: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 08:03:15.468: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 08:03:15.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4313" for this suite. 01/17/23 08:03:15.519
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:03:15.541
Jan 17 08:03:15.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 08:03:15.542
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:03:15.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:03:15.585
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-b90dcffe-f765-4b38-8687-c1f36798cea4 01/17/23 08:03:15.591
STEP: Creating secret with name secret-projected-all-test-volume-367d0684-c576-4d68-8b12-2efa60a1d752 01/17/23 08:03:15.6
STEP: Creating a pod to test Check all projections for projected volume plugin 01/17/23 08:03:15.609
Jan 17 08:03:15.630: INFO: Waiting up to 5m0s for pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0" in namespace "projected-4033" to be "Succeeded or Failed"
Jan 17 08:03:15.645: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.893644ms
Jan 17 08:03:17.650: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020129795s
Jan 17 08:03:19.661: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03121325s
STEP: Saw pod success 01/17/23 08:03:19.661
Jan 17 08:03:19.661: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0" satisfied condition "Succeeded or Failed"
Jan 17 08:03:19.665: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 container projected-all-volume-test: <nil>
STEP: delete the pod 01/17/23 08:03:19.752
Jan 17 08:03:19.776: INFO: Waiting for pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 to disappear
Jan 17 08:03:19.784: INFO: Pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 17 08:03:19.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4033" for this suite. 01/17/23 08:03:19.79
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":317,"skipped":5783,"failed":0}
------------------------------
• [4.268 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:03:15.541
    Jan 17 08:03:15.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 08:03:15.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:03:15.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:03:15.585
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-b90dcffe-f765-4b38-8687-c1f36798cea4 01/17/23 08:03:15.591
    STEP: Creating secret with name secret-projected-all-test-volume-367d0684-c576-4d68-8b12-2efa60a1d752 01/17/23 08:03:15.6
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/17/23 08:03:15.609
    Jan 17 08:03:15.630: INFO: Waiting up to 5m0s for pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0" in namespace "projected-4033" to be "Succeeded or Failed"
    Jan 17 08:03:15.645: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.893644ms
    Jan 17 08:03:17.650: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020129795s
    Jan 17 08:03:19.661: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03121325s
    STEP: Saw pod success 01/17/23 08:03:19.661
    Jan 17 08:03:19.661: INFO: Pod "projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0" satisfied condition "Succeeded or Failed"
    Jan 17 08:03:19.665: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/17/23 08:03:19.752
    Jan 17 08:03:19.776: INFO: Waiting for pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 to disappear
    Jan 17 08:03:19.784: INFO: Pod projected-volume-eb1d4b5e-a47e-4e49-bb69-abaa7a2382b0 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 17 08:03:19.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4033" for this suite. 01/17/23 08:03:19.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:03:19.81
Jan 17 08:03:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 08:03:19.811
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:03:19.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:03:19.863
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-2a973ee8-5ce4-4f22-a315-dde7f19f7abe 01/17/23 08:03:19.875
STEP: Creating secret with name s-test-opt-upd-522c4768-147e-4a59-aa73-31763edc1e7d 01/17/23 08:03:19.884
STEP: Creating the pod 01/17/23 08:03:19.893
Jan 17 08:03:19.908: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67" in namespace "projected-6920" to be "running and ready"
Jan 17 08:03:19.916: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Pending", Reason="", readiness=false. Elapsed: 7.927144ms
Jan 17 08:03:19.916: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 08:03:21.922: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013777396s
Jan 17 08:03:21.922: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 08:03:23.921: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Running", Reason="", readiness=true. Elapsed: 4.013521562s
Jan 17 08:03:23.922: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Running (Ready = true)
Jan 17 08:03:23.922: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2a973ee8-5ce4-4f22-a315-dde7f19f7abe 01/17/23 08:03:23.945
STEP: Updating secret s-test-opt-upd-522c4768-147e-4a59-aa73-31763edc1e7d 01/17/23 08:03:23.955
STEP: Creating secret with name s-test-opt-create-be15eb24-5b2e-4bb6-804b-772d17e7f199 01/17/23 08:03:23.967
STEP: waiting to observe update in volume 01/17/23 08:03:23.978
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 08:04:26.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6920" for this suite. 01/17/23 08:04:26.364
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":318,"skipped":5808,"failed":0}
------------------------------
• [SLOW TEST] [66.565 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:03:19.81
    Jan 17 08:03:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 08:03:19.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:03:19.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:03:19.863
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-2a973ee8-5ce4-4f22-a315-dde7f19f7abe 01/17/23 08:03:19.875
    STEP: Creating secret with name s-test-opt-upd-522c4768-147e-4a59-aa73-31763edc1e7d 01/17/23 08:03:19.884
    STEP: Creating the pod 01/17/23 08:03:19.893
    Jan 17 08:03:19.908: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67" in namespace "projected-6920" to be "running and ready"
    Jan 17 08:03:19.916: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Pending", Reason="", readiness=false. Elapsed: 7.927144ms
    Jan 17 08:03:19.916: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 08:03:21.922: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013777396s
    Jan 17 08:03:21.922: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 08:03:23.921: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67": Phase="Running", Reason="", readiness=true. Elapsed: 4.013521562s
    Jan 17 08:03:23.922: INFO: The phase of Pod pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67 is Running (Ready = true)
    Jan 17 08:03:23.922: INFO: Pod "pod-projected-secrets-fa764d7a-a386-4fc1-8466-f7900638ae67" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2a973ee8-5ce4-4f22-a315-dde7f19f7abe 01/17/23 08:03:23.945
    STEP: Updating secret s-test-opt-upd-522c4768-147e-4a59-aa73-31763edc1e7d 01/17/23 08:03:23.955
    STEP: Creating secret with name s-test-opt-create-be15eb24-5b2e-4bb6-804b-772d17e7f199 01/17/23 08:03:23.967
    STEP: waiting to observe update in volume 01/17/23 08:03:23.978
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 08:04:26.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6920" for this suite. 01/17/23 08:04:26.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:26.375
Jan 17 08:04:26.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubelet-test 01/17/23 08:04:26.377
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:26.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:26.422
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 08:04:30.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-868" for this suite. 01/17/23 08:04:30.462
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":319,"skipped":5820,"failed":0}
------------------------------
• [4.098 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:26.375
    Jan 17 08:04:26.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 08:04:26.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:26.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:26.422
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 08:04:30.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-868" for this suite. 01/17/23 08:04:30.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:30.475
Jan 17 08:04:30.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 08:04:30.477
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:30.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:30.532
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/17/23 08:04:30.541
STEP: delete the rc 01/17/23 08:04:35.564
STEP: wait for all pods to be garbage collected 01/17/23 08:04:35.575
STEP: Gathering metrics 01/17/23 08:04:40.59
W0117 08:04:40.608720      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 08:04:40.608: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 08:04:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6464" for this suite. 01/17/23 08:04:40.62
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":320,"skipped":5842,"failed":0}
------------------------------
• [SLOW TEST] [10.155 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:30.475
    Jan 17 08:04:30.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 08:04:30.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:30.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:30.532
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/17/23 08:04:30.541
    STEP: delete the rc 01/17/23 08:04:35.564
    STEP: wait for all pods to be garbage collected 01/17/23 08:04:35.575
    STEP: Gathering metrics 01/17/23 08:04:40.59
    W0117 08:04:40.608720      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 08:04:40.608: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 08:04:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6464" for this suite. 01/17/23 08:04:40.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:40.648
Jan 17 08:04:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename runtimeclass 01/17/23 08:04:40.649
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:40.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:40.684
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 17 08:04:40.707: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5767 to be scheduled
Jan 17 08:04:40.717: INFO: 1 pods are not scheduled: [runtimeclass-5767/test-runtimeclass-runtimeclass-5767-preconfigured-handler-f24qp(45711723-654c-4c54-9351-b0caa32f0aef)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 08:04:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5767" for this suite. 01/17/23 08:04:42.762
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":321,"skipped":6011,"failed":0}
------------------------------
• [2.133 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:40.648
    Jan 17 08:04:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 08:04:40.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:40.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:40.684
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 17 08:04:40.707: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5767 to be scheduled
    Jan 17 08:04:40.717: INFO: 1 pods are not scheduled: [runtimeclass-5767/test-runtimeclass-runtimeclass-5767-preconfigured-handler-f24qp(45711723-654c-4c54-9351-b0caa32f0aef)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 08:04:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5767" for this suite. 01/17/23 08:04:42.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:42.781
Jan 17 08:04:42.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename podtemplate 01/17/23 08:04:42.783
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:42.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:42.815
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 08:04:42.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4858" for this suite. 01/17/23 08:04:42.877
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":322,"skipped":6018,"failed":0}
------------------------------
• [0.109 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:42.781
    Jan 17 08:04:42.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename podtemplate 01/17/23 08:04:42.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:42.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:42.815
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 08:04:42.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4858" for this suite. 01/17/23 08:04:42.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:42.896
Jan 17 08:04:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 08:04:42.898
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:42.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:42.941
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/17/23 08:04:42.952
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 08:04:42.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7113" for this suite. 01/17/23 08:04:42.964
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":323,"skipped":6037,"failed":0}
------------------------------
• [0.077 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:42.896
    Jan 17 08:04:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 08:04:42.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:42.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:42.941
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/17/23 08:04:42.952
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 08:04:42.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7113" for this suite. 01/17/23 08:04:42.964
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:42.977
Jan 17 08:04:42.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename replicaset 01/17/23 08:04:42.979
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:43.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:43.032
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/17/23 08:04:43.038
Jan 17 08:04:43.052: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 08:04:48.057: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 08:04:48.057
STEP: getting scale subresource 01/17/23 08:04:48.058
STEP: updating a scale subresource 01/17/23 08:04:48.068
STEP: verifying the replicaset Spec.Replicas was modified 01/17/23 08:04:48.085
STEP: Patch a scale subresource 01/17/23 08:04:48.096
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 08:04:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6457" for this suite. 01/17/23 08:04:48.175
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":324,"skipped":6064,"failed":0}
------------------------------
• [SLOW TEST] [5.243 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:42.977
    Jan 17 08:04:42.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename replicaset 01/17/23 08:04:42.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:43.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:43.032
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/17/23 08:04:43.038
    Jan 17 08:04:43.052: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 08:04:48.057: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 08:04:48.057
    STEP: getting scale subresource 01/17/23 08:04:48.058
    STEP: updating a scale subresource 01/17/23 08:04:48.068
    STEP: verifying the replicaset Spec.Replicas was modified 01/17/23 08:04:48.085
    STEP: Patch a scale subresource 01/17/23 08:04:48.096
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 08:04:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6457" for this suite. 01/17/23 08:04:48.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:48.221
Jan 17 08:04:48.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 08:04:48.223
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:48.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:48.291
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:04:48.303
Jan 17 08:04:48.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3962 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 17 08:04:48.541: INFO: stderr: ""
Jan 17 08:04:48.541: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 08:04:48.541
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 17 08:04:48.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3962 delete pods e2e-test-httpd-pod'
Jan 17 08:04:51.587: INFO: stderr: ""
Jan 17 08:04:51.587: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 08:04:51.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3962" for this suite. 01/17/23 08:04:51.6
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":325,"skipped":6069,"failed":0}
------------------------------
• [3.397 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:48.221
    Jan 17 08:04:48.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 08:04:48.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:48.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:48.291
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:04:48.303
    Jan 17 08:04:48.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3962 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 17 08:04:48.541: INFO: stderr: ""
    Jan 17 08:04:48.541: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 08:04:48.541
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 17 08:04:48.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3962 delete pods e2e-test-httpd-pod'
    Jan 17 08:04:51.587: INFO: stderr: ""
    Jan 17 08:04:51.587: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 08:04:51.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3962" for this suite. 01/17/23 08:04:51.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:04:51.623
Jan 17 08:04:51.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename statefulset 01/17/23 08:04:51.624
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:51.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:51.66
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5646 01/17/23 08:04:51.666
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 17 08:04:51.736: INFO: Found 0 stateful pods, waiting for 1
Jan 17 08:05:01.745: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/17/23 08:05:01.755
W0117 08:05:01.775813      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 08:05:01.816: INFO: Found 1 stateful pods, waiting for 2
Jan 17 08:05:11.823: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 08:05:11.823: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/17/23 08:05:11.831
STEP: Delete all of the StatefulSets 01/17/23 08:05:11.839
STEP: Verify that StatefulSets have been deleted 01/17/23 08:05:11.85
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 08:05:11.857: INFO: Deleting all statefulset in ns statefulset-5646
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 08:05:11.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5646" for this suite. 01/17/23 08:05:11.911
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":326,"skipped":6133,"failed":0}
------------------------------
• [SLOW TEST] [20.305 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:04:51.623
    Jan 17 08:04:51.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename statefulset 01/17/23 08:04:51.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:04:51.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:04:51.66
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5646 01/17/23 08:04:51.666
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 17 08:04:51.736: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 08:05:01.745: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/17/23 08:05:01.755
    W0117 08:05:01.775813      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 08:05:01.816: INFO: Found 1 stateful pods, waiting for 2
    Jan 17 08:05:11.823: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 08:05:11.823: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/17/23 08:05:11.831
    STEP: Delete all of the StatefulSets 01/17/23 08:05:11.839
    STEP: Verify that StatefulSets have been deleted 01/17/23 08:05:11.85
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 08:05:11.857: INFO: Deleting all statefulset in ns statefulset-5646
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 08:05:11.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5646" for this suite. 01/17/23 08:05:11.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:05:11.929
Jan 17 08:05:11.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pods 01/17/23 08:05:11.932
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:11.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:11.973
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/17/23 08:05:11.991
STEP: watching for Pod to be ready 01/17/23 08:05:12.006
Jan 17 08:05:12.011: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 17 08:05:12.023: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
Jan 17 08:05:12.078: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
Jan 17 08:05:12.810: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
Jan 17 08:05:13.674: INFO: Found Pod pod-test in namespace pods-4593 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/17/23 08:05:13.68
STEP: getting the Pod and ensuring that it's patched 01/17/23 08:05:13.701
STEP: replacing the Pod's status Ready condition to False 01/17/23 08:05:13.705
STEP: check the Pod again to ensure its Ready conditions are False 01/17/23 08:05:13.731
STEP: deleting the Pod via a Collection with a LabelSelector 01/17/23 08:05:13.731
STEP: watching for the Pod to be deleted 01/17/23 08:05:13.747
Jan 17 08:05:13.751: INFO: observed event type MODIFIED
Jan 17 08:05:15.697: INFO: observed event type MODIFIED
Jan 17 08:05:16.014: INFO: observed event type MODIFIED
Jan 17 08:05:16.691: INFO: observed event type MODIFIED
Jan 17 08:05:16.704: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 08:05:16.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4593" for this suite. 01/17/23 08:05:16.724
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":327,"skipped":6145,"failed":0}
------------------------------
• [4.814 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:05:11.929
    Jan 17 08:05:11.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pods 01/17/23 08:05:11.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:11.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:11.973
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/17/23 08:05:11.991
    STEP: watching for Pod to be ready 01/17/23 08:05:12.006
    Jan 17 08:05:12.011: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 17 08:05:12.023: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
    Jan 17 08:05:12.078: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
    Jan 17 08:05:12.810: INFO: observed Pod pod-test in namespace pods-4593 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
    Jan 17 08:05:13.674: INFO: Found Pod pod-test in namespace pods-4593 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 08:05:12 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/17/23 08:05:13.68
    STEP: getting the Pod and ensuring that it's patched 01/17/23 08:05:13.701
    STEP: replacing the Pod's status Ready condition to False 01/17/23 08:05:13.705
    STEP: check the Pod again to ensure its Ready conditions are False 01/17/23 08:05:13.731
    STEP: deleting the Pod via a Collection with a LabelSelector 01/17/23 08:05:13.731
    STEP: watching for the Pod to be deleted 01/17/23 08:05:13.747
    Jan 17 08:05:13.751: INFO: observed event type MODIFIED
    Jan 17 08:05:15.697: INFO: observed event type MODIFIED
    Jan 17 08:05:16.014: INFO: observed event type MODIFIED
    Jan 17 08:05:16.691: INFO: observed event type MODIFIED
    Jan 17 08:05:16.704: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 08:05:16.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4593" for this suite. 01/17/23 08:05:16.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:05:16.746
Jan 17 08:05:16.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 08:05:16.748
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:16.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:16.783
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/17/23 08:05:16.79
Jan 17 08:05:16.827: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce" in namespace "emptydir-1302" to be "running"
Jan 17 08:05:16.837: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391665ms
Jan 17 08:05:18.848: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce": Phase="Running", Reason="", readiness=false. Elapsed: 2.020795793s
Jan 17 08:05:18.848: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/17/23 08:05:18.848
Jan 17 08:05:18.849: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1302 PodName:pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 08:05:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 08:05:18.850: INFO: ExecWithOptions: Clientset creation
Jan 17 08:05:18.850: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-1302/pods/pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 17 08:05:19.029: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 08:05:19.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1302" for this suite. 01/17/23 08:05:19.034
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":328,"skipped":6167,"failed":0}
------------------------------
• [2.305 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:05:16.746
    Jan 17 08:05:16.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 08:05:16.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:16.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:16.783
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/17/23 08:05:16.79
    Jan 17 08:05:16.827: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce" in namespace "emptydir-1302" to be "running"
    Jan 17 08:05:16.837: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391665ms
    Jan 17 08:05:18.848: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce": Phase="Running", Reason="", readiness=false. Elapsed: 2.020795793s
    Jan 17 08:05:18.848: INFO: Pod "pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/17/23 08:05:18.848
    Jan 17 08:05:18.849: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1302 PodName:pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 08:05:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 08:05:18.850: INFO: ExecWithOptions: Clientset creation
    Jan 17 08:05:18.850: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-1302/pods/pod-sharedvolume-57c14157-94f8-4635-8361-aec7195a92ce/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 17 08:05:19.029: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 08:05:19.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1302" for this suite. 01/17/23 08:05:19.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:05:19.053
Jan 17 08:05:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 08:05:19.059
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:19.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:19.137
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/17/23 08:05:19.153
STEP: watching for the Service to be added 01/17/23 08:05:19.172
Jan 17 08:05:19.177: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 17 08:05:19.177: INFO: Service test-service-sqk4r created
STEP: Getting /status 01/17/23 08:05:19.177
Jan 17 08:05:19.191: INFO: Service test-service-sqk4r has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/17/23 08:05:19.191
STEP: watching for the Service to be patched 01/17/23 08:05:19.207
Jan 17 08:05:19.214: INFO: observed Service test-service-sqk4r in namespace services-1526 with annotations: map[] & LoadBalancer: {[]}
Jan 17 08:05:19.214: INFO: Found Service test-service-sqk4r in namespace services-1526 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 17 08:05:19.214: INFO: Service test-service-sqk4r has service status patched
STEP: updating the ServiceStatus 01/17/23 08:05:19.214
Jan 17 08:05:19.245: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/17/23 08:05:19.245
Jan 17 08:05:19.252: INFO: Observed Service test-service-sqk4r in namespace services-1526 with annotations: map[] & Conditions: {[]}
Jan 17 08:05:19.252: INFO: Observed event: &Service{ObjectMeta:{test-service-sqk4r  services-1526  13095835-8884-440d-b0a8-43cbc746539c 42620 0 2023-01-17 08:05:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-17 08:05:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-17 08:05:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.82.128,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.82.128],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 17 08:05:19.253: INFO: Found Service test-service-sqk4r in namespace services-1526 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 08:05:19.253: INFO: Service test-service-sqk4r has service status updated
STEP: patching the service 01/17/23 08:05:19.253
STEP: watching for the Service to be patched 01/17/23 08:05:19.276
Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
Jan 17 08:05:19.280: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service:patched test-service-static:true]
Jan 17 08:05:19.280: INFO: Service test-service-sqk4r patched
STEP: deleting the service 01/17/23 08:05:19.28
STEP: watching for the Service to be deleted 01/17/23 08:05:19.317
Jan 17 08:05:19.323: INFO: Observed event: ADDED
Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
Jan 17 08:05:19.323: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 17 08:05:19.323: INFO: Service test-service-sqk4r deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 08:05:19.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1526" for this suite. 01/17/23 08:05:19.328
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":329,"skipped":6186,"failed":0}
------------------------------
• [0.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:05:19.053
    Jan 17 08:05:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 08:05:19.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:19.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:19.137
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/17/23 08:05:19.153
    STEP: watching for the Service to be added 01/17/23 08:05:19.172
    Jan 17 08:05:19.177: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 17 08:05:19.177: INFO: Service test-service-sqk4r created
    STEP: Getting /status 01/17/23 08:05:19.177
    Jan 17 08:05:19.191: INFO: Service test-service-sqk4r has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/17/23 08:05:19.191
    STEP: watching for the Service to be patched 01/17/23 08:05:19.207
    Jan 17 08:05:19.214: INFO: observed Service test-service-sqk4r in namespace services-1526 with annotations: map[] & LoadBalancer: {[]}
    Jan 17 08:05:19.214: INFO: Found Service test-service-sqk4r in namespace services-1526 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 17 08:05:19.214: INFO: Service test-service-sqk4r has service status patched
    STEP: updating the ServiceStatus 01/17/23 08:05:19.214
    Jan 17 08:05:19.245: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/17/23 08:05:19.245
    Jan 17 08:05:19.252: INFO: Observed Service test-service-sqk4r in namespace services-1526 with annotations: map[] & Conditions: {[]}
    Jan 17 08:05:19.252: INFO: Observed event: &Service{ObjectMeta:{test-service-sqk4r  services-1526  13095835-8884-440d-b0a8-43cbc746539c 42620 0 2023-01-17 08:05:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-17 08:05:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-17 08:05:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.82.128,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.82.128],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 17 08:05:19.253: INFO: Found Service test-service-sqk4r in namespace services-1526 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 08:05:19.253: INFO: Service test-service-sqk4r has service status updated
    STEP: patching the service 01/17/23 08:05:19.253
    STEP: watching for the Service to be patched 01/17/23 08:05:19.276
    Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
    Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
    Jan 17 08:05:19.280: INFO: observed Service test-service-sqk4r in namespace services-1526 with labels: map[test-service-static:true]
    Jan 17 08:05:19.280: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service:patched test-service-static:true]
    Jan 17 08:05:19.280: INFO: Service test-service-sqk4r patched
    STEP: deleting the service 01/17/23 08:05:19.28
    STEP: watching for the Service to be deleted 01/17/23 08:05:19.317
    Jan 17 08:05:19.323: INFO: Observed event: ADDED
    Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
    Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
    Jan 17 08:05:19.323: INFO: Observed event: MODIFIED
    Jan 17 08:05:19.323: INFO: Found Service test-service-sqk4r in namespace services-1526 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 17 08:05:19.323: INFO: Service test-service-sqk4r deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 08:05:19.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1526" for this suite. 01/17/23 08:05:19.328
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:05:19.347
Jan 17 08:05:19.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 08:05:19.349
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:19.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:19.398
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/17/23 08:05:19.412
STEP: delete the rc 01/17/23 08:05:24.443
STEP: wait for the rc to be deleted 01/17/23 08:05:24.467
Jan 17 08:05:25.735: INFO: 80 pods remaining
Jan 17 08:05:25.735: INFO: 80 pods has nil DeletionTimestamp
Jan 17 08:05:25.735: INFO: 
Jan 17 08:05:26.510: INFO: 71 pods remaining
Jan 17 08:05:26.510: INFO: 70 pods has nil DeletionTimestamp
Jan 17 08:05:26.510: INFO: 
Jan 17 08:05:27.540: INFO: 58 pods remaining
Jan 17 08:05:27.540: INFO: 58 pods has nil DeletionTimestamp
Jan 17 08:05:27.540: INFO: 
Jan 17 08:05:28.488: INFO: 43 pods remaining
Jan 17 08:05:28.488: INFO: 43 pods has nil DeletionTimestamp
Jan 17 08:05:28.488: INFO: 
Jan 17 08:05:29.489: INFO: 30 pods remaining
Jan 17 08:05:29.489: INFO: 30 pods has nil DeletionTimestamp
Jan 17 08:05:29.489: INFO: 
Jan 17 08:05:30.486: INFO: 18 pods remaining
Jan 17 08:05:30.486: INFO: 18 pods has nil DeletionTimestamp
Jan 17 08:05:30.486: INFO: 
STEP: Gathering metrics 01/17/23 08:05:31.512
W0117 08:05:31.567492      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 08:05:31.567: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 08:05:31.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1893" for this suite. 01/17/23 08:05:31.588
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":330,"skipped":6198,"failed":0}
------------------------------
• [SLOW TEST] [12.261 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:05:19.347
    Jan 17 08:05:19.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 08:05:19.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:19.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:19.398
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/17/23 08:05:19.412
    STEP: delete the rc 01/17/23 08:05:24.443
    STEP: wait for the rc to be deleted 01/17/23 08:05:24.467
    Jan 17 08:05:25.735: INFO: 80 pods remaining
    Jan 17 08:05:25.735: INFO: 80 pods has nil DeletionTimestamp
    Jan 17 08:05:25.735: INFO: 
    Jan 17 08:05:26.510: INFO: 71 pods remaining
    Jan 17 08:05:26.510: INFO: 70 pods has nil DeletionTimestamp
    Jan 17 08:05:26.510: INFO: 
    Jan 17 08:05:27.540: INFO: 58 pods remaining
    Jan 17 08:05:27.540: INFO: 58 pods has nil DeletionTimestamp
    Jan 17 08:05:27.540: INFO: 
    Jan 17 08:05:28.488: INFO: 43 pods remaining
    Jan 17 08:05:28.488: INFO: 43 pods has nil DeletionTimestamp
    Jan 17 08:05:28.488: INFO: 
    Jan 17 08:05:29.489: INFO: 30 pods remaining
    Jan 17 08:05:29.489: INFO: 30 pods has nil DeletionTimestamp
    Jan 17 08:05:29.489: INFO: 
    Jan 17 08:05:30.486: INFO: 18 pods remaining
    Jan 17 08:05:30.486: INFO: 18 pods has nil DeletionTimestamp
    Jan 17 08:05:30.486: INFO: 
    STEP: Gathering metrics 01/17/23 08:05:31.512
    W0117 08:05:31.567492      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 08:05:31.567: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 08:05:31.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1893" for this suite. 01/17/23 08:05:31.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:05:31.612
Jan 17 08:05:31.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename endpointslice 01/17/23 08:05:31.614
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:31.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:31.66
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/17/23 08:05:41.827
STEP: referencing matching pods with named port 01/17/23 08:05:46.839
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/17/23 08:05:51.852
STEP: recreating EndpointSlices after they've been deleted 01/17/23 08:05:56.863
Jan 17 08:05:56.902: INFO: EndpointSlice for Service endpointslice-4751/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 08:06:06.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4751" for this suite. 01/17/23 08:06:06.921
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":331,"skipped":6233,"failed":0}
------------------------------
• [SLOW TEST] [35.318 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:05:31.612
    Jan 17 08:05:31.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename endpointslice 01/17/23 08:05:31.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:05:31.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:05:31.66
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/17/23 08:05:41.827
    STEP: referencing matching pods with named port 01/17/23 08:05:46.839
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/17/23 08:05:51.852
    STEP: recreating EndpointSlices after they've been deleted 01/17/23 08:05:56.863
    Jan 17 08:05:56.902: INFO: EndpointSlice for Service endpointslice-4751/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 08:06:06.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4751" for this suite. 01/17/23 08:06:06.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:06.933
Jan 17 08:06:06.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 08:06:06.935
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:06.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:06.984
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/17/23 08:06:06.999
STEP: Patching the Job 01/17/23 08:06:07.011
STEP: Watching for Job to be patched 01/17/23 08:06:07.063
Jan 17 08:06:07.071: INFO: Event ADDED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 17 08:06:07.071: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 17 08:06:07.071: INFO: Event MODIFIED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/17/23 08:06:07.071
STEP: Watching for Job to be updated 01/17/23 08:06:07.102
Jan 17 08:06:07.115: INFO: Event MODIFIED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:07.115: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/17/23 08:06:07.115
Jan 17 08:06:07.152: INFO: Job: e2e-bj7qx as labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx]
STEP: Waiting for job to complete 01/17/23 08:06:07.152
STEP: Delete a job collection with a labelselector 01/17/23 08:06:17.157
STEP: Watching for Job to be deleted 01/17/23 08:06:17.173
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.200: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 08:06:17.200: INFO: Event DELETED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/17/23 08:06:17.2
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 08:06:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5489" for this suite. 01/17/23 08:06:17.224
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":332,"skipped":6240,"failed":0}
------------------------------
• [SLOW TEST] [10.326 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:06.933
    Jan 17 08:06:06.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 08:06:06.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:06.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:06.984
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/17/23 08:06:06.999
    STEP: Patching the Job 01/17/23 08:06:07.011
    STEP: Watching for Job to be patched 01/17/23 08:06:07.063
    Jan 17 08:06:07.071: INFO: Event ADDED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 17 08:06:07.071: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 17 08:06:07.071: INFO: Event MODIFIED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/17/23 08:06:07.071
    STEP: Watching for Job to be updated 01/17/23 08:06:07.102
    Jan 17 08:06:07.115: INFO: Event MODIFIED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:07.115: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/17/23 08:06:07.115
    Jan 17 08:06:07.152: INFO: Job: e2e-bj7qx as labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx]
    STEP: Waiting for job to complete 01/17/23 08:06:07.152
    STEP: Delete a job collection with a labelselector 01/17/23 08:06:17.157
    STEP: Watching for Job to be deleted 01/17/23 08:06:17.173
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.199: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.200: INFO: Event MODIFIED observed for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 08:06:17.200: INFO: Event DELETED found for Job e2e-bj7qx in namespace job-5489 with labels: map[e2e-bj7qx:patched e2e-job-label:e2e-bj7qx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/17/23 08:06:17.2
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 08:06:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5489" for this suite. 01/17/23 08:06:17.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:17.26
Jan 17 08:06:17.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename downward-api 01/17/23 08:06:17.262
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:17.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:17.331
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/17/23 08:06:17.343
Jan 17 08:06:17.365: INFO: Waiting up to 5m0s for pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54" in namespace "downward-api-8122" to be "Succeeded or Failed"
Jan 17 08:06:17.377: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Pending", Reason="", readiness=false. Elapsed: 12.495833ms
Jan 17 08:06:19.388: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022842916s
Jan 17 08:06:21.388: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02317027s
STEP: Saw pod success 01/17/23 08:06:21.388
Jan 17 08:06:21.389: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54" satisfied condition "Succeeded or Failed"
Jan 17 08:06:21.398: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 container dapi-container: <nil>
STEP: delete the pod 01/17/23 08:06:21.479
Jan 17 08:06:21.498: INFO: Waiting for pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 to disappear
Jan 17 08:06:21.503: INFO: Pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 08:06:21.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8122" for this suite. 01/17/23 08:06:21.511
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":333,"skipped":6251,"failed":0}
------------------------------
• [4.267 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:17.26
    Jan 17 08:06:17.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename downward-api 01/17/23 08:06:17.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:17.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:17.331
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/17/23 08:06:17.343
    Jan 17 08:06:17.365: INFO: Waiting up to 5m0s for pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54" in namespace "downward-api-8122" to be "Succeeded or Failed"
    Jan 17 08:06:17.377: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Pending", Reason="", readiness=false. Elapsed: 12.495833ms
    Jan 17 08:06:19.388: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022842916s
    Jan 17 08:06:21.388: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02317027s
    STEP: Saw pod success 01/17/23 08:06:21.388
    Jan 17 08:06:21.389: INFO: Pod "downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54" satisfied condition "Succeeded or Failed"
    Jan 17 08:06:21.398: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 08:06:21.479
    Jan 17 08:06:21.498: INFO: Waiting for pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 to disappear
    Jan 17 08:06:21.503: INFO: Pod downward-api-d21e3d02-72bf-46cd-bc12-fabe0543fa54 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 08:06:21.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8122" for this suite. 01/17/23 08:06:21.511
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:21.527
Jan 17 08:06:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 08:06:21.534
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:21.571
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 08:06:21.581
Jan 17 08:06:21.603: INFO: Waiting up to 5m0s for pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c" in namespace "emptydir-9001" to be "Succeeded or Failed"
Jan 17 08:06:21.612: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.495327ms
Jan 17 08:06:23.618: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014751415s
Jan 17 08:06:25.620: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016904201s
Jan 17 08:06:27.619: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016303165s
STEP: Saw pod success 01/17/23 08:06:27.619
Jan 17 08:06:27.619: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c" satisfied condition "Succeeded or Failed"
Jan 17 08:06:27.623: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c container test-container: <nil>
STEP: delete the pod 01/17/23 08:06:27.631
Jan 17 08:06:27.659: INFO: Waiting for pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c to disappear
Jan 17 08:06:27.665: INFO: Pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 08:06:27.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9001" for this suite. 01/17/23 08:06:27.67
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6251,"failed":0}
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:21.527
    Jan 17 08:06:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 08:06:21.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:21.571
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 08:06:21.581
    Jan 17 08:06:21.603: INFO: Waiting up to 5m0s for pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c" in namespace "emptydir-9001" to be "Succeeded or Failed"
    Jan 17 08:06:21.612: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.495327ms
    Jan 17 08:06:23.618: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014751415s
    Jan 17 08:06:25.620: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016904201s
    Jan 17 08:06:27.619: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016303165s
    STEP: Saw pod success 01/17/23 08:06:27.619
    Jan 17 08:06:27.619: INFO: Pod "pod-5958b7cd-79b0-4d36-9142-dcb5b506241c" satisfied condition "Succeeded or Failed"
    Jan 17 08:06:27.623: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c container test-container: <nil>
    STEP: delete the pod 01/17/23 08:06:27.631
    Jan 17 08:06:27.659: INFO: Waiting for pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c to disappear
    Jan 17 08:06:27.665: INFO: Pod pod-5958b7cd-79b0-4d36-9142-dcb5b506241c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 08:06:27.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9001" for this suite. 01/17/23 08:06:27.67
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:27.68
Jan 17 08:06:27.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename init-container 01/17/23 08:06:27.681
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:27.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:27.713
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/17/23 08:06:27.719
Jan 17 08:06:27.720: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 08:06:31.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-385" for this suite. 01/17/23 08:06:31.743
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":335,"skipped":6251,"failed":0}
------------------------------
• [4.076 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:27.68
    Jan 17 08:06:27.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename init-container 01/17/23 08:06:27.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:27.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:27.713
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/17/23 08:06:27.719
    Jan 17 08:06:27.720: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 08:06:31.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-385" for this suite. 01/17/23 08:06:31.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:31.757
Jan 17 08:06:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename svcaccounts 01/17/23 08:06:31.759
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:31.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:31.797
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/17/23 08:06:31.806
Jan 17 08:06:31.823: INFO: Waiting up to 5m0s for pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2" in namespace "svcaccounts-3940" to be "Succeeded or Failed"
Jan 17 08:06:31.831: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.573623ms
Jan 17 08:06:33.837: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013879565s
Jan 17 08:06:35.836: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01314166s
STEP: Saw pod success 01/17/23 08:06:35.836
Jan 17 08:06:35.837: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2" satisfied condition "Succeeded or Failed"
Jan 17 08:06:35.843: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 08:06:35.901
Jan 17 08:06:35.924: INFO: Waiting for pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 to disappear
Jan 17 08:06:35.930: INFO: Pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 08:06:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3940" for this suite. 01/17/23 08:06:35.935
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":336,"skipped":6261,"failed":0}
------------------------------
• [4.187 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:31.757
    Jan 17 08:06:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 08:06:31.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:31.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:31.797
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/17/23 08:06:31.806
    Jan 17 08:06:31.823: INFO: Waiting up to 5m0s for pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2" in namespace "svcaccounts-3940" to be "Succeeded or Failed"
    Jan 17 08:06:31.831: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.573623ms
    Jan 17 08:06:33.837: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013879565s
    Jan 17 08:06:35.836: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01314166s
    STEP: Saw pod success 01/17/23 08:06:35.836
    Jan 17 08:06:35.837: INFO: Pod "test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2" satisfied condition "Succeeded or Failed"
    Jan 17 08:06:35.843: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-2 pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 08:06:35.901
    Jan 17 08:06:35.924: INFO: Waiting for pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 to disappear
    Jan 17 08:06:35.930: INFO: Pod test-pod-daa5767d-978b-4c1b-aef0-b255fc9d4cd2 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 08:06:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3940" for this suite. 01/17/23 08:06:35.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:35.949
Jan 17 08:06:35.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename disruption 01/17/23 08:06:35.951
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:35.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:35.978
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/17/23 08:06:35.992
STEP: Waiting for all pods to be running 01/17/23 08:06:38.064
Jan 17 08:06:38.099: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 08:06:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7356" for this suite. 01/17/23 08:06:40.122
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":337,"skipped":6283,"failed":0}
------------------------------
• [4.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:35.949
    Jan 17 08:06:35.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename disruption 01/17/23 08:06:35.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:35.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:35.978
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/17/23 08:06:35.992
    STEP: Waiting for all pods to be running 01/17/23 08:06:38.064
    Jan 17 08:06:38.099: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 08:06:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7356" for this suite. 01/17/23 08:06:40.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:06:40.136
Jan 17 08:06:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename subpath 01/17/23 08:06:40.138
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:40.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:40.175
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 08:06:40.184
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-h6v6 01/17/23 08:06:40.207
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 08:06:40.207
Jan 17 08:06:40.233: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h6v6" in namespace "subpath-9694" to be "Succeeded or Failed"
Jan 17 08:06:40.243: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.921469ms
Jan 17 08:06:42.247: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014082441s
Jan 17 08:06:44.254: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 4.021066702s
Jan 17 08:06:46.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 6.016035117s
Jan 17 08:06:48.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 8.016419456s
Jan 17 08:06:50.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 10.015841507s
Jan 17 08:06:52.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 12.016384876s
Jan 17 08:06:54.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 14.015851622s
Jan 17 08:06:56.248: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 16.014794687s
Jan 17 08:06:58.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 18.015918115s
Jan 17 08:07:00.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 20.016560364s
Jan 17 08:07:02.252: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 22.019104764s
Jan 17 08:07:04.248: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=false. Elapsed: 24.015134734s
Jan 17 08:07:06.254: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.021395106s
STEP: Saw pod success 01/17/23 08:07:06.254
Jan 17 08:07:06.255: INFO: Pod "pod-subpath-test-configmap-h6v6" satisfied condition "Succeeded or Failed"
Jan 17 08:07:06.260: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-configmap-h6v6 container test-container-subpath-configmap-h6v6: <nil>
STEP: delete the pod 01/17/23 08:07:06.269
Jan 17 08:07:06.301: INFO: Waiting for pod pod-subpath-test-configmap-h6v6 to disappear
Jan 17 08:07:06.308: INFO: Pod pod-subpath-test-configmap-h6v6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h6v6 01/17/23 08:07:06.308
Jan 17 08:07:06.309: INFO: Deleting pod "pod-subpath-test-configmap-h6v6" in namespace "subpath-9694"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 08:07:06.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9694" for this suite. 01/17/23 08:07:06.319
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":338,"skipped":6300,"failed":0}
------------------------------
• [SLOW TEST] [26.202 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:06:40.136
    Jan 17 08:06:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename subpath 01/17/23 08:06:40.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:06:40.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:06:40.175
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 08:06:40.184
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-h6v6 01/17/23 08:06:40.207
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 08:06:40.207
    Jan 17 08:06:40.233: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h6v6" in namespace "subpath-9694" to be "Succeeded or Failed"
    Jan 17 08:06:40.243: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.921469ms
    Jan 17 08:06:42.247: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014082441s
    Jan 17 08:06:44.254: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 4.021066702s
    Jan 17 08:06:46.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 6.016035117s
    Jan 17 08:06:48.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 8.016419456s
    Jan 17 08:06:50.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 10.015841507s
    Jan 17 08:06:52.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 12.016384876s
    Jan 17 08:06:54.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 14.015851622s
    Jan 17 08:06:56.248: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 16.014794687s
    Jan 17 08:06:58.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 18.015918115s
    Jan 17 08:07:00.249: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 20.016560364s
    Jan 17 08:07:02.252: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=true. Elapsed: 22.019104764s
    Jan 17 08:07:04.248: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Running", Reason="", readiness=false. Elapsed: 24.015134734s
    Jan 17 08:07:06.254: INFO: Pod "pod-subpath-test-configmap-h6v6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.021395106s
    STEP: Saw pod success 01/17/23 08:07:06.254
    Jan 17 08:07:06.255: INFO: Pod "pod-subpath-test-configmap-h6v6" satisfied condition "Succeeded or Failed"
    Jan 17 08:07:06.260: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-configmap-h6v6 container test-container-subpath-configmap-h6v6: <nil>
    STEP: delete the pod 01/17/23 08:07:06.269
    Jan 17 08:07:06.301: INFO: Waiting for pod pod-subpath-test-configmap-h6v6 to disappear
    Jan 17 08:07:06.308: INFO: Pod pod-subpath-test-configmap-h6v6 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-h6v6 01/17/23 08:07:06.308
    Jan 17 08:07:06.309: INFO: Deleting pod "pod-subpath-test-configmap-h6v6" in namespace "subpath-9694"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 08:07:06.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9694" for this suite. 01/17/23 08:07:06.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:06.352
Jan 17 08:07:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename job 01/17/23 08:07:06.353
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:06.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:06.39
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/17/23 08:07:06.396
STEP: Ensuring active pods == parallelism 01/17/23 08:07:06.408
STEP: delete a job 01/17/23 08:07:08.416
STEP: deleting Job.batch foo in namespace job-2635, will wait for the garbage collector to delete the pods 01/17/23 08:07:08.416
Jan 17 08:07:08.489: INFO: Deleting Job.batch foo took: 17.893108ms
Jan 17 08:07:08.590: INFO: Terminating Job.batch foo pods took: 100.538694ms
STEP: Ensuring job was deleted 01/17/23 08:07:41.091
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 08:07:41.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2635" for this suite. 01/17/23 08:07:41.106
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":339,"skipped":6341,"failed":0}
------------------------------
• [SLOW TEST] [34.768 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:06.352
    Jan 17 08:07:06.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename job 01/17/23 08:07:06.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:06.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:06.39
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/17/23 08:07:06.396
    STEP: Ensuring active pods == parallelism 01/17/23 08:07:06.408
    STEP: delete a job 01/17/23 08:07:08.416
    STEP: deleting Job.batch foo in namespace job-2635, will wait for the garbage collector to delete the pods 01/17/23 08:07:08.416
    Jan 17 08:07:08.489: INFO: Deleting Job.batch foo took: 17.893108ms
    Jan 17 08:07:08.590: INFO: Terminating Job.batch foo pods took: 100.538694ms
    STEP: Ensuring job was deleted 01/17/23 08:07:41.091
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 08:07:41.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2635" for this suite. 01/17/23 08:07:41.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:41.121
Jan 17 08:07:41.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 08:07:41.123
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:41.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:41.161
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-3d5b749b-63f8-45e6-ae6d-35b620cf95b2 01/17/23 08:07:41.18
STEP: Creating a pod to test consume configMaps 01/17/23 08:07:41.189
Jan 17 08:07:41.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391" in namespace "projected-3788" to be "Succeeded or Failed"
Jan 17 08:07:41.218: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029364ms
Jan 17 08:07:43.226: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013958125s
Jan 17 08:07:45.224: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012612385s
STEP: Saw pod success 01/17/23 08:07:45.224
Jan 17 08:07:45.225: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391" satisfied condition "Succeeded or Failed"
Jan 17 08:07:45.228: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/17/23 08:07:45.236
Jan 17 08:07:45.263: INFO: Waiting for pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 to disappear
Jan 17 08:07:45.274: INFO: Pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 08:07:45.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3788" for this suite. 01/17/23 08:07:45.28
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":340,"skipped":6346,"failed":0}
------------------------------
• [4.170 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:41.121
    Jan 17 08:07:41.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 08:07:41.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:41.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:41.161
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-3d5b749b-63f8-45e6-ae6d-35b620cf95b2 01/17/23 08:07:41.18
    STEP: Creating a pod to test consume configMaps 01/17/23 08:07:41.189
    Jan 17 08:07:41.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391" in namespace "projected-3788" to be "Succeeded or Failed"
    Jan 17 08:07:41.218: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029364ms
    Jan 17 08:07:43.226: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013958125s
    Jan 17 08:07:45.224: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012612385s
    STEP: Saw pod success 01/17/23 08:07:45.224
    Jan 17 08:07:45.225: INFO: Pod "pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391" satisfied condition "Succeeded or Failed"
    Jan 17 08:07:45.228: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/17/23 08:07:45.236
    Jan 17 08:07:45.263: INFO: Waiting for pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 to disappear
    Jan 17 08:07:45.274: INFO: Pod pod-projected-configmaps-63af87e8-c34e-48c7-be33-9eb76df85391 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 08:07:45.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3788" for this suite. 01/17/23 08:07:45.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:45.292
Jan 17 08:07:45.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename gc 01/17/23 08:07:45.293
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:45.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:45.326
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/17/23 08:07:45.333
STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 08:07:45.349
STEP: delete the deployment 01/17/23 08:07:45.508
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/17/23 08:07:45.525
STEP: Gathering metrics 01/17/23 08:07:46.07
W0117 08:07:46.088474      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 08:07:46.088: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 08:07:46.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5248" for this suite. 01/17/23 08:07:46.094
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":341,"skipped":6351,"failed":0}
------------------------------
• [0.815 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:45.292
    Jan 17 08:07:45.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename gc 01/17/23 08:07:45.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:45.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:45.326
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/17/23 08:07:45.333
    STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 08:07:45.349
    STEP: delete the deployment 01/17/23 08:07:45.508
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/17/23 08:07:45.525
    STEP: Gathering metrics 01/17/23 08:07:46.07
    W0117 08:07:46.088474      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 08:07:46.088: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 08:07:46.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5248" for this suite. 01/17/23 08:07:46.094
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:46.108
Jan 17 08:07:46.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 08:07:46.111
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:46.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:46.16
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-1c2f7268-c886-4af5-8163-96ffc3998627 01/17/23 08:07:46.169
STEP: Creating a pod to test consume secrets 01/17/23 08:07:46.188
Jan 17 08:07:46.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf" in namespace "projected-8053" to be "Succeeded or Failed"
Jan 17 08:07:46.238: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 25.869048ms
Jan 17 08:07:48.244: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032443106s
Jan 17 08:07:50.246: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034156864s
STEP: Saw pod success 01/17/23 08:07:50.246
Jan 17 08:07:50.246: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf" satisfied condition "Succeeded or Failed"
Jan 17 08:07:50.252: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 08:07:50.262
Jan 17 08:07:50.292: INFO: Waiting for pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf to disappear
Jan 17 08:07:50.299: INFO: Pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 08:07:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8053" for this suite. 01/17/23 08:07:50.32
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":342,"skipped":6351,"failed":0}
------------------------------
• [4.235 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:46.108
    Jan 17 08:07:46.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 08:07:46.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:46.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:46.16
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-1c2f7268-c886-4af5-8163-96ffc3998627 01/17/23 08:07:46.169
    STEP: Creating a pod to test consume secrets 01/17/23 08:07:46.188
    Jan 17 08:07:46.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf" in namespace "projected-8053" to be "Succeeded or Failed"
    Jan 17 08:07:46.238: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 25.869048ms
    Jan 17 08:07:48.244: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032443106s
    Jan 17 08:07:50.246: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034156864s
    STEP: Saw pod success 01/17/23 08:07:50.246
    Jan 17 08:07:50.246: INFO: Pod "pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf" satisfied condition "Succeeded or Failed"
    Jan 17 08:07:50.252: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 08:07:50.262
    Jan 17 08:07:50.292: INFO: Waiting for pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf to disappear
    Jan 17 08:07:50.299: INFO: Pod pod-projected-secrets-004da43d-1ee9-4070-a826-22d3b2d6e2bf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 08:07:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8053" for this suite. 01/17/23 08:07:50.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:50.347
Jan 17 08:07:50.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubelet-test 01/17/23 08:07:50.348
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:50.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:50.393
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 08:07:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9492" for this suite. 01/17/23 08:07:50.471
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":343,"skipped":6364,"failed":0}
------------------------------
• [0.139 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:50.347
    Jan 17 08:07:50.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 08:07:50.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:50.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:50.393
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 08:07:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9492" for this suite. 01/17/23 08:07:50.471
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:50.486
Jan 17 08:07:50.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 08:07:50.487
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:50.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:50.523
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:07:50.533
Jan 17 08:07:50.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 08:07:50.773: INFO: stderr: ""
Jan 17 08:07:50.773: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/17/23 08:07:50.773
Jan 17 08:07:50.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 17 08:07:52.178: INFO: stderr: ""
Jan 17 08:07:52.179: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:07:52.179
Jan 17 08:07:52.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 delete pods e2e-test-httpd-pod'
Jan 17 08:07:55.170: INFO: stderr: ""
Jan 17 08:07:55.170: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 08:07:55.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1831" for this suite. 01/17/23 08:07:55.178
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":344,"skipped":6365,"failed":0}
------------------------------
• [4.703 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:50.486
    Jan 17 08:07:50.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 08:07:50.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:50.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:50.523
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:07:50.533
    Jan 17 08:07:50.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 17 08:07:50.773: INFO: stderr: ""
    Jan 17 08:07:50.773: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/17/23 08:07:50.773
    Jan 17 08:07:50.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 17 08:07:52.178: INFO: stderr: ""
    Jan 17 08:07:52.179: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 08:07:52.179
    Jan 17 08:07:52.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-1831 delete pods e2e-test-httpd-pod'
    Jan 17 08:07:55.170: INFO: stderr: ""
    Jan 17 08:07:55.170: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 08:07:55.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1831" for this suite. 01/17/23 08:07:55.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:55.19
Jan 17 08:07:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 08:07:55.191
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:55.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:55.223
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 08:07:55.256
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:07:56.005
STEP: Deploying the webhook pod 01/17/23 08:07:56.021
STEP: Wait for the deployment to be ready 01/17/23 08:07:56.047
Jan 17 08:07:56.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 08:07:58.098
STEP: Verifying the service has paired with the endpoint 01/17/23 08:07:58.122
Jan 17 08:07:59.123: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/17/23 08:07:59.143
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.171
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/17/23 08:07:59.184
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.202
STEP: Patching a validating webhook configuration's rules to include the create operation 01/17/23 08:07:59.224
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.236
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:07:59.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7122" for this suite. 01/17/23 08:07:59.259
STEP: Destroying namespace "webhook-7122-markers" for this suite. 01/17/23 08:07:59.272
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":345,"skipped":6399,"failed":0}
------------------------------
• [4.203 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:55.19
    Jan 17 08:07:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 08:07:55.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:55.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:55.223
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 08:07:55.256
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:07:56.005
    STEP: Deploying the webhook pod 01/17/23 08:07:56.021
    STEP: Wait for the deployment to be ready 01/17/23 08:07:56.047
    Jan 17 08:07:56.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 08:07:58.098
    STEP: Verifying the service has paired with the endpoint 01/17/23 08:07:58.122
    Jan 17 08:07:59.123: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/17/23 08:07:59.143
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.171
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/17/23 08:07:59.184
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.202
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/17/23 08:07:59.224
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:07:59.236
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:07:59.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7122" for this suite. 01/17/23 08:07:59.259
    STEP: Destroying namespace "webhook-7122-markers" for this suite. 01/17/23 08:07:59.272
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:07:59.395
Jan 17 08:07:59.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename projected 01/17/23 08:07:59.396
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:59.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:59.493
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-c86d68fb-885b-462d-9d30-c3d257e9a25a 01/17/23 08:07:59.499
STEP: Creating a pod to test consume configMaps 01/17/23 08:07:59.508
Jan 17 08:07:59.520: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875" in namespace "projected-3487" to be "Succeeded or Failed"
Jan 17 08:07:59.532: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Pending", Reason="", readiness=false. Elapsed: 11.759614ms
Jan 17 08:08:01.538: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018543378s
Jan 17 08:08:03.538: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018518269s
STEP: Saw pod success 01/17/23 08:08:03.539
Jan 17 08:08:03.539: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875" satisfied condition "Succeeded or Failed"
Jan 17 08:08:03.543: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 08:08:03.553
Jan 17 08:08:03.578: INFO: Waiting for pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 to disappear
Jan 17 08:08:03.584: INFO: Pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 08:08:03.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3487" for this suite. 01/17/23 08:08:03.59
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":346,"skipped":6404,"failed":0}
------------------------------
• [4.205 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:07:59.395
    Jan 17 08:07:59.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename projected 01/17/23 08:07:59.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:07:59.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:07:59.493
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-c86d68fb-885b-462d-9d30-c3d257e9a25a 01/17/23 08:07:59.499
    STEP: Creating a pod to test consume configMaps 01/17/23 08:07:59.508
    Jan 17 08:07:59.520: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875" in namespace "projected-3487" to be "Succeeded or Failed"
    Jan 17 08:07:59.532: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Pending", Reason="", readiness=false. Elapsed: 11.759614ms
    Jan 17 08:08:01.538: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018543378s
    Jan 17 08:08:03.538: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018518269s
    STEP: Saw pod success 01/17/23 08:08:03.539
    Jan 17 08:08:03.539: INFO: Pod "pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875" satisfied condition "Succeeded or Failed"
    Jan 17 08:08:03.543: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 08:08:03.553
    Jan 17 08:08:03.578: INFO: Waiting for pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 to disappear
    Jan 17 08:08:03.584: INFO: Pod pod-projected-configmaps-8efa310f-2248-4802-aedd-659a45f39875 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 08:08:03.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3487" for this suite. 01/17/23 08:08:03.59
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:03.6
Jan 17 08:08:03.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:08:03.602
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:03.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:03.64
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 17 08:08:03.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 08:08:10.294
Jan 17 08:08:10.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 create -f -'
Jan 17 08:08:11.400: INFO: stderr: ""
Jan 17 08:08:11.400: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 08:08:11.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 delete e2e-test-crd-publish-openapi-7582-crds test-cr'
Jan 17 08:08:11.591: INFO: stderr: ""
Jan 17 08:08:11.591: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 17 08:08:11.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 apply -f -'
Jan 17 08:08:12.727: INFO: stderr: ""
Jan 17 08:08:12.727: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 08:08:12.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 delete e2e-test-crd-publish-openapi-7582-crds test-cr'
Jan 17 08:08:12.884: INFO: stderr: ""
Jan 17 08:08:12.884: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/17/23 08:08:12.884
Jan 17 08:08:12.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 explain e2e-test-crd-publish-openapi-7582-crds'
Jan 17 08:08:14.189: INFO: stderr: ""
Jan 17 08:08:14.189: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7582-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:08:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6049" for this suite. 01/17/23 08:08:21.518
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":347,"skipped":6405,"failed":0}
------------------------------
• [SLOW TEST] [17.957 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:03.6
    Jan 17 08:08:03.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:08:03.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:03.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:03.64
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 17 08:08:03.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 08:08:10.294
    Jan 17 08:08:10.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 create -f -'
    Jan 17 08:08:11.400: INFO: stderr: ""
    Jan 17 08:08:11.400: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 17 08:08:11.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 delete e2e-test-crd-publish-openapi-7582-crds test-cr'
    Jan 17 08:08:11.591: INFO: stderr: ""
    Jan 17 08:08:11.591: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 17 08:08:11.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 apply -f -'
    Jan 17 08:08:12.727: INFO: stderr: ""
    Jan 17 08:08:12.727: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 17 08:08:12.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 --namespace=crd-publish-openapi-6049 delete e2e-test-crd-publish-openapi-7582-crds test-cr'
    Jan 17 08:08:12.884: INFO: stderr: ""
    Jan 17 08:08:12.884: INFO: stdout: "e2e-test-crd-publish-openapi-7582-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/17/23 08:08:12.884
    Jan 17 08:08:12.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-6049 explain e2e-test-crd-publish-openapi-7582-crds'
    Jan 17 08:08:14.189: INFO: stderr: ""
    Jan 17 08:08:14.189: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7582-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:08:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6049" for this suite. 01/17/23 08:08:21.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:21.562
Jan 17 08:08:21.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename kubectl 01/17/23 08:08:21.564
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:21.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:21.62
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/17/23 08:08:21.666
Jan 17 08:08:21.667: INFO: namespace kubectl-3363
Jan 17 08:08:21.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 create -f -'
Jan 17 08:08:22.852: INFO: stderr: ""
Jan 17 08:08:22.852: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 08:08:22.852
Jan 17 08:08:23.860: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 08:08:23.860: INFO: Found 0 / 1
Jan 17 08:08:24.859: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 08:08:24.859: INFO: Found 1 / 1
Jan 17 08:08:24.859: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 08:08:24.863: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 08:08:24.863: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 08:08:24.863: INFO: wait on agnhost-primary startup in kubectl-3363 
Jan 17 08:08:24.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 logs agnhost-primary-c9djw agnhost-primary'
Jan 17 08:08:24.999: INFO: stderr: ""
Jan 17 08:08:24.999: INFO: stdout: "Paused\n"
STEP: exposing RC 01/17/23 08:08:24.999
Jan 17 08:08:25.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 17 08:08:25.176: INFO: stderr: ""
Jan 17 08:08:25.176: INFO: stdout: "service/rm2 exposed\n"
Jan 17 08:08:25.191: INFO: Service rm2 in namespace kubectl-3363 found.
STEP: exposing service 01/17/23 08:08:27.204
Jan 17 08:08:27.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 17 08:08:27.387: INFO: stderr: ""
Jan 17 08:08:27.387: INFO: stdout: "service/rm3 exposed\n"
Jan 17 08:08:27.397: INFO: Service rm3 in namespace kubectl-3363 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 08:08:29.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3363" for this suite. 01/17/23 08:08:29.413
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":348,"skipped":6462,"failed":0}
------------------------------
• [SLOW TEST] [7.863 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:21.562
    Jan 17 08:08:21.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename kubectl 01/17/23 08:08:21.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:21.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:21.62
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/17/23 08:08:21.666
    Jan 17 08:08:21.667: INFO: namespace kubectl-3363
    Jan 17 08:08:21.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 create -f -'
    Jan 17 08:08:22.852: INFO: stderr: ""
    Jan 17 08:08:22.852: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 08:08:22.852
    Jan 17 08:08:23.860: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 08:08:23.860: INFO: Found 0 / 1
    Jan 17 08:08:24.859: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 08:08:24.859: INFO: Found 1 / 1
    Jan 17 08:08:24.859: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 17 08:08:24.863: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 08:08:24.863: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 08:08:24.863: INFO: wait on agnhost-primary startup in kubectl-3363 
    Jan 17 08:08:24.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 logs agnhost-primary-c9djw agnhost-primary'
    Jan 17 08:08:24.999: INFO: stderr: ""
    Jan 17 08:08:24.999: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/17/23 08:08:24.999
    Jan 17 08:08:25.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 17 08:08:25.176: INFO: stderr: ""
    Jan 17 08:08:25.176: INFO: stdout: "service/rm2 exposed\n"
    Jan 17 08:08:25.191: INFO: Service rm2 in namespace kubectl-3363 found.
    STEP: exposing service 01/17/23 08:08:27.204
    Jan 17 08:08:27.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=kubectl-3363 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 17 08:08:27.387: INFO: stderr: ""
    Jan 17 08:08:27.387: INFO: stdout: "service/rm3 exposed\n"
    Jan 17 08:08:27.397: INFO: Service rm3 in namespace kubectl-3363 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 08:08:29.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3363" for this suite. 01/17/23 08:08:29.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:29.427
Jan 17 08:08:29.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename endpointslicemirroring 01/17/23 08:08:29.428
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:29.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:29.473
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/17/23 08:08:29.505
STEP: mirroring an update to a custom Endpoint 01/17/23 08:08:29.536
Jan 17 08:08:29.560: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/17/23 08:08:31.567
Jan 17 08:08:31.585: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 17 08:08:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1215" for this suite. 01/17/23 08:08:33.598
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":349,"skipped":6502,"failed":0}
------------------------------
• [4.183 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:29.427
    Jan 17 08:08:29.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename endpointslicemirroring 01/17/23 08:08:29.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:29.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:29.473
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/17/23 08:08:29.505
    STEP: mirroring an update to a custom Endpoint 01/17/23 08:08:29.536
    Jan 17 08:08:29.560: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/17/23 08:08:31.567
    Jan 17 08:08:31.585: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 17 08:08:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1215" for this suite. 01/17/23 08:08:33.598
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:33.611
Jan 17 08:08:33.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename services 01/17/23 08:08:33.612
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:33.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:33.696
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1477 01/17/23 08:08:33.703
STEP: changing the ExternalName service to type=NodePort 01/17/23 08:08:33.714
STEP: creating replication controller externalname-service in namespace services-1477 01/17/23 08:08:33.769
I0117 08:08:33.794244      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1477, replica count: 2
I0117 08:08:36.845310      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 08:08:36.845: INFO: Creating new exec pod
Jan 17 08:08:36.867: INFO: Waiting up to 5m0s for pod "execpodq6jl9" in namespace "services-1477" to be "running"
Jan 17 08:08:36.872: INFO: Pod "execpodq6jl9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699084ms
Jan 17 08:08:38.882: INFO: Pod "execpodq6jl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015281738s
Jan 17 08:08:38.882: INFO: Pod "execpodq6jl9" satisfied condition "running"
Jan 17 08:08:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 08:08:40.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 08:08:40.229: INFO: stdout: ""
Jan 17 08:08:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 08:08:41.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 08:08:41.548: INFO: stdout: "externalname-service-pz2ng"
Jan 17 08:08:41.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.232 80'
Jan 17 08:08:41.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.232 80\nConnection to 10.254.1.232 80 port [tcp/http] succeeded!\n"
Jan 17 08:08:41.891: INFO: stdout: ""
Jan 17 08:08:42.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.232 80'
Jan 17 08:08:43.182: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.232 80\nConnection to 10.254.1.232 80 port [tcp/http] succeeded!\n"
Jan 17 08:08:43.182: INFO: stdout: "externalname-service-pz2ng"
Jan 17 08:08:43.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30464'
Jan 17 08:08:43.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30464\nConnection to 10.0.0.22 30464 port [tcp/*] succeeded!\n"
Jan 17 08:08:43.512: INFO: stdout: "externalname-service-pz2ng"
Jan 17 08:08:43.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 30464'
Jan 17 08:08:43.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 30464\nConnection to 10.0.0.21 30464 port [tcp/*] succeeded!\n"
Jan 17 08:08:43.838: INFO: stdout: "externalname-service-pz2ng"
Jan 17 08:08:43.838: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 08:08:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1477" for this suite. 01/17/23 08:08:43.895
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":350,"skipped":6506,"failed":0}
------------------------------
• [SLOW TEST] [10.303 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:33.611
    Jan 17 08:08:33.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename services 01/17/23 08:08:33.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:33.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:33.696
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1477 01/17/23 08:08:33.703
    STEP: changing the ExternalName service to type=NodePort 01/17/23 08:08:33.714
    STEP: creating replication controller externalname-service in namespace services-1477 01/17/23 08:08:33.769
    I0117 08:08:33.794244      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1477, replica count: 2
    I0117 08:08:36.845310      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 08:08:36.845: INFO: Creating new exec pod
    Jan 17 08:08:36.867: INFO: Waiting up to 5m0s for pod "execpodq6jl9" in namespace "services-1477" to be "running"
    Jan 17 08:08:36.872: INFO: Pod "execpodq6jl9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699084ms
    Jan 17 08:08:38.882: INFO: Pod "execpodq6jl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015281738s
    Jan 17 08:08:38.882: INFO: Pod "execpodq6jl9" satisfied condition "running"
    Jan 17 08:08:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 17 08:08:40.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 17 08:08:40.229: INFO: stdout: ""
    Jan 17 08:08:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 17 08:08:41.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 17 08:08:41.548: INFO: stdout: "externalname-service-pz2ng"
    Jan 17 08:08:41.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.232 80'
    Jan 17 08:08:41.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.232 80\nConnection to 10.254.1.232 80 port [tcp/http] succeeded!\n"
    Jan 17 08:08:41.891: INFO: stdout: ""
    Jan 17 08:08:42.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.1.232 80'
    Jan 17 08:08:43.182: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.1.232 80\nConnection to 10.254.1.232 80 port [tcp/http] succeeded!\n"
    Jan 17 08:08:43.182: INFO: stdout: "externalname-service-pz2ng"
    Jan 17 08:08:43.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.22 30464'
    Jan 17 08:08:43.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.22 30464\nConnection to 10.0.0.22 30464 port [tcp/*] succeeded!\n"
    Jan 17 08:08:43.512: INFO: stdout: "externalname-service-pz2ng"
    Jan 17 08:08:43.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=services-1477 exec execpodq6jl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 30464'
    Jan 17 08:08:43.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 30464\nConnection to 10.0.0.21 30464 port [tcp/*] succeeded!\n"
    Jan 17 08:08:43.838: INFO: stdout: "externalname-service-pz2ng"
    Jan 17 08:08:43.838: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 08:08:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1477" for this suite. 01/17/23 08:08:43.895
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:43.914
Jan 17 08:08:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 08:08:43.916
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:43.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:43.967
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 08:08:44.014
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:08:44.546
STEP: Deploying the webhook pod 01/17/23 08:08:44.562
STEP: Wait for the deployment to be ready 01/17/23 08:08:44.587
Jan 17 08:08:44.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 08:08:46.618
STEP: Verifying the service has paired with the endpoint 01/17/23 08:08:46.651
Jan 17 08:08:47.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 08:08:47.658
STEP: create a pod that should be denied by the webhook 01/17/23 08:08:47.691
STEP: create a pod that causes the webhook to hang 01/17/23 08:08:47.729
STEP: create a configmap that should be denied by the webhook 01/17/23 08:08:57.749
STEP: create a configmap that should be admitted by the webhook 01/17/23 08:08:57.847
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 08:08:57.867
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 08:08:57.883
STEP: create a namespace that bypass the webhook 01/17/23 08:08:57.893
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/17/23 08:08:57.908
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:08:57.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9926" for this suite. 01/17/23 08:08:57.968
STEP: Destroying namespace "webhook-9926-markers" for this suite. 01/17/23 08:08:57.983
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":351,"skipped":6515,"failed":0}
------------------------------
• [SLOW TEST] [14.197 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:43.914
    Jan 17 08:08:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 08:08:43.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:43.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:43.967
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 08:08:44.014
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:08:44.546
    STEP: Deploying the webhook pod 01/17/23 08:08:44.562
    STEP: Wait for the deployment to be ready 01/17/23 08:08:44.587
    Jan 17 08:08:44.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 08:08:46.618
    STEP: Verifying the service has paired with the endpoint 01/17/23 08:08:46.651
    Jan 17 08:08:47.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 08:08:47.658
    STEP: create a pod that should be denied by the webhook 01/17/23 08:08:47.691
    STEP: create a pod that causes the webhook to hang 01/17/23 08:08:47.729
    STEP: create a configmap that should be denied by the webhook 01/17/23 08:08:57.749
    STEP: create a configmap that should be admitted by the webhook 01/17/23 08:08:57.847
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 08:08:57.867
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 08:08:57.883
    STEP: create a namespace that bypass the webhook 01/17/23 08:08:57.893
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/17/23 08:08:57.908
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:08:57.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9926" for this suite. 01/17/23 08:08:57.968
    STEP: Destroying namespace "webhook-9926-markers" for this suite. 01/17/23 08:08:57.983
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:08:58.114
Jan 17 08:08:58.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename dns 01/17/23 08:08:58.115
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:58.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:58.18
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4198.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4198.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/17/23 08:08:58.194
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4198.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4198.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/17/23 08:08:58.194
STEP: creating a pod to probe /etc/hosts 01/17/23 08:08:58.194
STEP: submitting the pod to kubernetes 01/17/23 08:08:58.194
Jan 17 08:08:58.218: INFO: Waiting up to 15m0s for pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e" in namespace "dns-4198" to be "running"
Jan 17 08:08:58.226: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.590987ms
Jan 17 08:09:00.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01532177s
Jan 17 08:09:02.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015373167s
Jan 17 08:09:02.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e" satisfied condition "running"
STEP: retrieving the pod 01/17/23 08:09:02.233
STEP: looking for the results for each expected name from probers 01/17/23 08:09:02.239
Jan 17 08:09:02.264: INFO: DNS probes using dns-4198/dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e succeeded

STEP: deleting the pod 01/17/23 08:09:02.264
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 08:09:02.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4198" for this suite. 01/17/23 08:09:02.303
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":352,"skipped":6562,"failed":0}
------------------------------
• [4.209 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:08:58.114
    Jan 17 08:08:58.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename dns 01/17/23 08:08:58.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:08:58.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:08:58.18
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4198.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4198.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/17/23 08:08:58.194
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4198.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4198.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/17/23 08:08:58.194
    STEP: creating a pod to probe /etc/hosts 01/17/23 08:08:58.194
    STEP: submitting the pod to kubernetes 01/17/23 08:08:58.194
    Jan 17 08:08:58.218: INFO: Waiting up to 15m0s for pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e" in namespace "dns-4198" to be "running"
    Jan 17 08:08:58.226: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.590987ms
    Jan 17 08:09:00.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01532177s
    Jan 17 08:09:02.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015373167s
    Jan 17 08:09:02.233: INFO: Pod "dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 08:09:02.233
    STEP: looking for the results for each expected name from probers 01/17/23 08:09:02.239
    Jan 17 08:09:02.264: INFO: DNS probes using dns-4198/dns-test-30ea3724-f7f3-4c17-8f7a-e2eea8ba019e succeeded

    STEP: deleting the pod 01/17/23 08:09:02.264
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 08:09:02.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4198" for this suite. 01/17/23 08:09:02.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:09:02.324
Jan 17 08:09:02.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename pod-network-test 01/17/23 08:09:02.325
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:02.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:02.366
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8370 01/17/23 08:09:02.372
STEP: creating a selector 01/17/23 08:09:02.372
STEP: Creating the service pods in kubernetes 01/17/23 08:09:02.372
Jan 17 08:09:02.372: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 08:09:02.445: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8370" to be "running and ready"
Jan 17 08:09:02.458: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.70058ms
Jan 17 08:09:02.458: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 08:09:04.468: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023740726s
Jan 17 08:09:04.469: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:06.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019435503s
Jan 17 08:09:06.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:08.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020092146s
Jan 17 08:09:08.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:10.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019695342s
Jan 17 08:09:10.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:12.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019606469s
Jan 17 08:09:12.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:14.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018867134s
Jan 17 08:09:14.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:16.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019696931s
Jan 17 08:09:16.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:18.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019984564s
Jan 17 08:09:18.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:20.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020092501s
Jan 17 08:09:20.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:22.463: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018632293s
Jan 17 08:09:22.463: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 08:09:24.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02081467s
Jan 17 08:09:24.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 08:09:24.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 08:09:24.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8370" to be "running and ready"
Jan 17 08:09:24.474: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.742664ms
Jan 17 08:09:24.474: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 08:09:24.474: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 08:09:24.477: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8370" to be "running and ready"
Jan 17 08:09:24.482: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.635405ms
Jan 17 08:09:24.482: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 08:09:24.482: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 08:09:24.487
Jan 17 08:09:24.511: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8370" to be "running"
Jan 17 08:09:24.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135855ms
Jan 17 08:09:26.523: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533698s
Jan 17 08:09:26.523: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 08:09:26.528: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8370" to be "running"
Jan 17 08:09:26.534: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.663154ms
Jan 17 08:09:26.534: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 17 08:09:26.541: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 08:09:26.541: INFO: Going to poll 10.100.206.54 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 08:09:26.545: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.206.54 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 08:09:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 08:09:26.546: INFO: ExecWithOptions: Clientset creation
Jan 17 08:09:26.546: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.206.54+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 08:09:27.689: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 08:09:27.689: INFO: Going to poll 10.100.135.63 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 08:09:27.695: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.135.63 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 08:09:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 08:09:27.696: INFO: ExecWithOptions: Clientset creation
Jan 17 08:09:27.696: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.135.63+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 08:09:28.845: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 08:09:28.845: INFO: Going to poll 10.100.168.92 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 08:09:28.850: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.168.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 08:09:28.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
Jan 17 08:09:28.851: INFO: ExecWithOptions: Clientset creation
Jan 17 08:09:28.851: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.168.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 08:09:29.976: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 08:09:29.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8370" for this suite. 01/17/23 08:09:29.986
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":353,"skipped":6594,"failed":0}
------------------------------
• [SLOW TEST] [27.676 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:09:02.324
    Jan 17 08:09:02.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 08:09:02.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:02.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:02.366
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8370 01/17/23 08:09:02.372
    STEP: creating a selector 01/17/23 08:09:02.372
    STEP: Creating the service pods in kubernetes 01/17/23 08:09:02.372
    Jan 17 08:09:02.372: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 08:09:02.445: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8370" to be "running and ready"
    Jan 17 08:09:02.458: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.70058ms
    Jan 17 08:09:02.458: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 08:09:04.468: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023740726s
    Jan 17 08:09:04.469: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:06.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019435503s
    Jan 17 08:09:06.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:08.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020092146s
    Jan 17 08:09:08.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:10.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019695342s
    Jan 17 08:09:10.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:12.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019606469s
    Jan 17 08:09:12.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:14.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018867134s
    Jan 17 08:09:14.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:16.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019696931s
    Jan 17 08:09:16.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:18.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019984564s
    Jan 17 08:09:18.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:20.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020092501s
    Jan 17 08:09:20.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:22.463: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018632293s
    Jan 17 08:09:22.463: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 08:09:24.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02081467s
    Jan 17 08:09:24.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 08:09:24.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 08:09:24.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8370" to be "running and ready"
    Jan 17 08:09:24.474: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.742664ms
    Jan 17 08:09:24.474: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 08:09:24.474: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 08:09:24.477: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8370" to be "running and ready"
    Jan 17 08:09:24.482: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.635405ms
    Jan 17 08:09:24.482: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 08:09:24.482: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 08:09:24.487
    Jan 17 08:09:24.511: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8370" to be "running"
    Jan 17 08:09:24.517: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135855ms
    Jan 17 08:09:26.523: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533698s
    Jan 17 08:09:26.523: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 08:09:26.528: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8370" to be "running"
    Jan 17 08:09:26.534: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.663154ms
    Jan 17 08:09:26.534: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 17 08:09:26.541: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 08:09:26.541: INFO: Going to poll 10.100.206.54 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 08:09:26.545: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.206.54 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 08:09:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 08:09:26.546: INFO: ExecWithOptions: Clientset creation
    Jan 17 08:09:26.546: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.206.54+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 08:09:27.689: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 17 08:09:27.689: INFO: Going to poll 10.100.135.63 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 08:09:27.695: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.135.63 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 08:09:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 08:09:27.696: INFO: ExecWithOptions: Clientset creation
    Jan 17 08:09:27.696: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.135.63+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 08:09:28.845: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 17 08:09:28.845: INFO: Going to poll 10.100.168.92 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 08:09:28.850: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.168.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8370 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 08:09:28.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    Jan 17 08:09:28.851: INFO: ExecWithOptions: Clientset creation
    Jan 17 08:09:28.851: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-8370/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.168.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 08:09:29.976: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 08:09:29.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8370" for this suite. 01/17/23 08:09:29.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:09:30.003
Jan 17 08:09:30.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:09:30.005
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:30.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:30.041
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 17 08:09:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 08:09:37.221
Jan 17 08:09:37.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 create -f -'
Jan 17 08:09:38.825: INFO: stderr: ""
Jan 17 08:09:38.825: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 08:09:38.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 delete e2e-test-crd-publish-openapi-7501-crds test-cr'
Jan 17 08:09:38.980: INFO: stderr: ""
Jan 17 08:09:38.980: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 17 08:09:38.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 apply -f -'
Jan 17 08:09:39.496: INFO: stderr: ""
Jan 17 08:09:39.496: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 08:09:39.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 delete e2e-test-crd-publish-openapi-7501-crds test-cr'
Jan 17 08:09:39.671: INFO: stderr: ""
Jan 17 08:09:39.674: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/17/23 08:09:39.674
Jan 17 08:09:39.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 explain e2e-test-crd-publish-openapi-7501-crds'
Jan 17 08:09:40.781: INFO: stderr: ""
Jan 17 08:09:40.781: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7501-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:09:47.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-265" for this suite. 01/17/23 08:09:47.392
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":354,"skipped":6610,"failed":0}
------------------------------
• [SLOW TEST] [17.417 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:09:30.003
    Jan 17 08:09:30.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:09:30.005
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:30.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:30.041
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 17 08:09:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 08:09:37.221
    Jan 17 08:09:37.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 create -f -'
    Jan 17 08:09:38.825: INFO: stderr: ""
    Jan 17 08:09:38.825: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 17 08:09:38.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 delete e2e-test-crd-publish-openapi-7501-crds test-cr'
    Jan 17 08:09:38.980: INFO: stderr: ""
    Jan 17 08:09:38.980: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 17 08:09:38.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 apply -f -'
    Jan 17 08:09:39.496: INFO: stderr: ""
    Jan 17 08:09:39.496: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 17 08:09:39.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 --namespace=crd-publish-openapi-265 delete e2e-test-crd-publish-openapi-7501-crds test-cr'
    Jan 17 08:09:39.671: INFO: stderr: ""
    Jan 17 08:09:39.674: INFO: stdout: "e2e-test-crd-publish-openapi-7501-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/17/23 08:09:39.674
    Jan 17 08:09:39.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3465506043 --namespace=crd-publish-openapi-265 explain e2e-test-crd-publish-openapi-7501-crds'
    Jan 17 08:09:40.781: INFO: stderr: ""
    Jan 17 08:09:40.781: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7501-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:09:47.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-265" for this suite. 01/17/23 08:09:47.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:09:47.424
Jan 17 08:09:47.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename subpath 01/17/23 08:09:47.425
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:47.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:47.47
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 08:09:47.476
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-9l4w 01/17/23 08:09:47.499
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 08:09:47.499
Jan 17 08:09:47.524: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9l4w" in namespace "subpath-3898" to be "Succeeded or Failed"
Jan 17 08:09:47.530: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595115ms
Jan 17 08:09:49.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.012884741s
Jan 17 08:09:51.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013666043s
Jan 17 08:09:53.560: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 6.035624803s
Jan 17 08:09:55.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 8.013881994s
Jan 17 08:09:57.552: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 10.028604866s
Jan 17 08:09:59.536: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 12.012494476s
Jan 17 08:10:01.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 14.013269864s
Jan 17 08:10:03.536: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 16.012410117s
Jan 17 08:10:05.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 18.012844069s
Jan 17 08:10:07.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 20.014239359s
Jan 17 08:10:09.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=false. Elapsed: 22.013989913s
Jan 17 08:10:11.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013796116s
STEP: Saw pod success 01/17/23 08:10:11.538
Jan 17 08:10:11.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w" satisfied condition "Succeeded or Failed"
Jan 17 08:10:11.545: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-downwardapi-9l4w container test-container-subpath-downwardapi-9l4w: <nil>
STEP: delete the pod 01/17/23 08:10:11.614
Jan 17 08:10:11.643: INFO: Waiting for pod pod-subpath-test-downwardapi-9l4w to disappear
Jan 17 08:10:11.653: INFO: Pod pod-subpath-test-downwardapi-9l4w no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9l4w 01/17/23 08:10:11.653
Jan 17 08:10:11.653: INFO: Deleting pod "pod-subpath-test-downwardapi-9l4w" in namespace "subpath-3898"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 08:10:11.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3898" for this suite. 01/17/23 08:10:11.669
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":355,"skipped":6630,"failed":0}
------------------------------
• [SLOW TEST] [24.264 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:09:47.424
    Jan 17 08:09:47.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename subpath 01/17/23 08:09:47.425
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:09:47.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:09:47.47
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 08:09:47.476
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-9l4w 01/17/23 08:09:47.499
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 08:09:47.499
    Jan 17 08:09:47.524: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9l4w" in namespace "subpath-3898" to be "Succeeded or Failed"
    Jan 17 08:09:47.530: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595115ms
    Jan 17 08:09:49.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.012884741s
    Jan 17 08:09:51.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013666043s
    Jan 17 08:09:53.560: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 6.035624803s
    Jan 17 08:09:55.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 8.013881994s
    Jan 17 08:09:57.552: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 10.028604866s
    Jan 17 08:09:59.536: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 12.012494476s
    Jan 17 08:10:01.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 14.013269864s
    Jan 17 08:10:03.536: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 16.012410117s
    Jan 17 08:10:05.537: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 18.012844069s
    Jan 17 08:10:07.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=true. Elapsed: 20.014239359s
    Jan 17 08:10:09.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Running", Reason="", readiness=false. Elapsed: 22.013989913s
    Jan 17 08:10:11.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013796116s
    STEP: Saw pod success 01/17/23 08:10:11.538
    Jan 17 08:10:11.538: INFO: Pod "pod-subpath-test-downwardapi-9l4w" satisfied condition "Succeeded or Failed"
    Jan 17 08:10:11.545: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-subpath-test-downwardapi-9l4w container test-container-subpath-downwardapi-9l4w: <nil>
    STEP: delete the pod 01/17/23 08:10:11.614
    Jan 17 08:10:11.643: INFO: Waiting for pod pod-subpath-test-downwardapi-9l4w to disappear
    Jan 17 08:10:11.653: INFO: Pod pod-subpath-test-downwardapi-9l4w no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-9l4w 01/17/23 08:10:11.653
    Jan 17 08:10:11.653: INFO: Deleting pod "pod-subpath-test-downwardapi-9l4w" in namespace "subpath-3898"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 08:10:11.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3898" for this suite. 01/17/23 08:10:11.669
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:11.689
Jan 17 08:10:11.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 08:10:11.691
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:11.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:11.742
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 08:10:11.754
Jan 17 08:10:11.771: INFO: Waiting up to 5m0s for pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe" in namespace "emptydir-47" to be "Succeeded or Failed"
Jan 17 08:10:11.792: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.811703ms
Jan 17 08:10:13.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.027378355s
Jan 17 08:10:15.800: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.028733373s
Jan 17 08:10:17.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027575285s
STEP: Saw pod success 01/17/23 08:10:17.799
Jan 17 08:10:17.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe" satisfied condition "Succeeded or Failed"
Jan 17 08:10:17.804: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe container test-container: <nil>
STEP: delete the pod 01/17/23 08:10:17.817
Jan 17 08:10:17.860: INFO: Waiting for pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe to disappear
Jan 17 08:10:17.867: INFO: Pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 08:10:17.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-47" for this suite. 01/17/23 08:10:17.894
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6630,"failed":0}
------------------------------
• [SLOW TEST] [6.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:11.689
    Jan 17 08:10:11.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 08:10:11.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:11.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:11.742
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 08:10:11.754
    Jan 17 08:10:11.771: INFO: Waiting up to 5m0s for pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe" in namespace "emptydir-47" to be "Succeeded or Failed"
    Jan 17 08:10:11.792: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 20.811703ms
    Jan 17 08:10:13.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.027378355s
    Jan 17 08:10:15.800: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.028733373s
    Jan 17 08:10:17.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027575285s
    STEP: Saw pod success 01/17/23 08:10:17.799
    Jan 17 08:10:17.799: INFO: Pod "pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe" satisfied condition "Succeeded or Failed"
    Jan 17 08:10:17.804: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe container test-container: <nil>
    STEP: delete the pod 01/17/23 08:10:17.817
    Jan 17 08:10:17.860: INFO: Waiting for pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe to disappear
    Jan 17 08:10:17.867: INFO: Pod pod-b56683c9-b8ca-4e8d-a3c5-6646c90ea8fe no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 08:10:17.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-47" for this suite. 01/17/23 08:10:17.894
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:17.909
Jan 17 08:10:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename events 01/17/23 08:10:17.911
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:17.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:18.001
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/17/23 08:10:18.011
STEP: listing events in all namespaces 01/17/23 08:10:18.026
STEP: listing events in test namespace 01/17/23 08:10:18.039
STEP: listing events with field selection filtering on source 01/17/23 08:10:18.045
STEP: listing events with field selection filtering on reportingController 01/17/23 08:10:18.053
STEP: getting the test event 01/17/23 08:10:18.059
STEP: patching the test event 01/17/23 08:10:18.066
STEP: getting the test event 01/17/23 08:10:18.08
STEP: updating the test event 01/17/23 08:10:18.09
STEP: getting the test event 01/17/23 08:10:18.107
STEP: deleting the test event 01/17/23 08:10:18.113
STEP: listing events in all namespaces 01/17/23 08:10:18.144
STEP: listing events in test namespace 01/17/23 08:10:18.162
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 17 08:10:18.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4995" for this suite. 01/17/23 08:10:18.173
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":357,"skipped":6632,"failed":0}
------------------------------
• [0.278 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:17.909
    Jan 17 08:10:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename events 01/17/23 08:10:17.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:17.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:18.001
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/17/23 08:10:18.011
    STEP: listing events in all namespaces 01/17/23 08:10:18.026
    STEP: listing events in test namespace 01/17/23 08:10:18.039
    STEP: listing events with field selection filtering on source 01/17/23 08:10:18.045
    STEP: listing events with field selection filtering on reportingController 01/17/23 08:10:18.053
    STEP: getting the test event 01/17/23 08:10:18.059
    STEP: patching the test event 01/17/23 08:10:18.066
    STEP: getting the test event 01/17/23 08:10:18.08
    STEP: updating the test event 01/17/23 08:10:18.09
    STEP: getting the test event 01/17/23 08:10:18.107
    STEP: deleting the test event 01/17/23 08:10:18.113
    STEP: listing events in all namespaces 01/17/23 08:10:18.144
    STEP: listing events in test namespace 01/17/23 08:10:18.162
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 17 08:10:18.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4995" for this suite. 01/17/23 08:10:18.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:18.188
Jan 17 08:10:18.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 08:10:18.19
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:18.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:18.231
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 08:10:18.253
Jan 17 08:10:18.279: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3165" to be "running and ready"
Jan 17 08:10:18.287: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.400437ms
Jan 17 08:10:18.287: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 08:10:20.295: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015642627s
Jan 17 08:10:20.295: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 08:10:20.295: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/17/23 08:10:20.3
Jan 17 08:10:20.316: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3165" to be "running and ready"
Jan 17 08:10:20.338: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 22.815394ms
Jan 17 08:10:20.339: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 08:10:22.344: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.028743018s
Jan 17 08:10:22.344: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 17 08:10:22.344: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/17/23 08:10:22.349
Jan 17 08:10:22.364: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 08:10:22.371: INFO: Pod pod-with-prestop-http-hook still exists
Jan 17 08:10:24.372: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 08:10:24.379: INFO: Pod pod-with-prestop-http-hook still exists
Jan 17 08:10:26.371: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 08:10:26.395: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/17/23 08:10:26.396
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 08:10:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3165" for this suite. 01/17/23 08:10:26.485
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":358,"skipped":6640,"failed":0}
------------------------------
• [SLOW TEST] [8.313 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:18.188
    Jan 17 08:10:18.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 08:10:18.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:18.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:18.231
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 08:10:18.253
    Jan 17 08:10:18.279: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3165" to be "running and ready"
    Jan 17 08:10:18.287: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.400437ms
    Jan 17 08:10:18.287: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 08:10:20.295: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015642627s
    Jan 17 08:10:20.295: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 08:10:20.295: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/17/23 08:10:20.3
    Jan 17 08:10:20.316: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3165" to be "running and ready"
    Jan 17 08:10:20.338: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 22.815394ms
    Jan 17 08:10:20.339: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 08:10:22.344: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.028743018s
    Jan 17 08:10:22.344: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 17 08:10:22.344: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/17/23 08:10:22.349
    Jan 17 08:10:22.364: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 08:10:22.371: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 17 08:10:24.372: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 08:10:24.379: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 17 08:10:26.371: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 08:10:26.395: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/17/23 08:10:26.396
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 08:10:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3165" for this suite. 01/17/23 08:10:26.485
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:26.502
Jan 17 08:10:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename emptydir 01/17/23 08:10:26.505
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:26.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:26.538
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 08:10:26.546
Jan 17 08:10:26.566: INFO: Waiting up to 5m0s for pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2" in namespace "emptydir-1770" to be "Succeeded or Failed"
Jan 17 08:10:26.579: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.072729ms
Jan 17 08:10:28.588: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021732751s
Jan 17 08:10:30.595: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028919054s
STEP: Saw pod success 01/17/23 08:10:30.595
Jan 17 08:10:30.595: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2" satisfied condition "Succeeded or Failed"
Jan 17 08:10:30.603: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 container test-container: <nil>
STEP: delete the pod 01/17/23 08:10:30.616
Jan 17 08:10:30.674: INFO: Waiting for pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 to disappear
Jan 17 08:10:30.682: INFO: Pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 08:10:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1770" for this suite. 01/17/23 08:10:30.691
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":359,"skipped":6641,"failed":0}
------------------------------
• [4.201 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:26.502
    Jan 17 08:10:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename emptydir 01/17/23 08:10:26.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:26.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:26.538
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 08:10:26.546
    Jan 17 08:10:26.566: INFO: Waiting up to 5m0s for pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2" in namespace "emptydir-1770" to be "Succeeded or Failed"
    Jan 17 08:10:26.579: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.072729ms
    Jan 17 08:10:28.588: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021732751s
    Jan 17 08:10:30.595: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028919054s
    STEP: Saw pod success 01/17/23 08:10:30.595
    Jan 17 08:10:30.595: INFO: Pod "pod-0a3df434-d0b7-439d-b1c1-063a652e84e2" satisfied condition "Succeeded or Failed"
    Jan 17 08:10:30.603: INFO: Trying to get logs from node cluster125-w73dz53kvqes-node-1 pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 container test-container: <nil>
    STEP: delete the pod 01/17/23 08:10:30.616
    Jan 17 08:10:30.674: INFO: Waiting for pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 to disappear
    Jan 17 08:10:30.682: INFO: Pod pod-0a3df434-d0b7-439d-b1c1-063a652e84e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 08:10:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1770" for this suite. 01/17/23 08:10:30.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:30.706
Jan 17 08:10:30.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename secrets 01/17/23 08:10:30.708
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:30.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:30.76
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/17/23 08:10:30.77
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/17/23 08:10:30.788
STEP: patching the secret 01/17/23 08:10:30.819
STEP: deleting the secret using a LabelSelector 01/17/23 08:10:30.841
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/17/23 08:10:30.856
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 08:10:30.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5623" for this suite. 01/17/23 08:10:30.871
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":360,"skipped":6671,"failed":0}
------------------------------
• [0.195 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:30.706
    Jan 17 08:10:30.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename secrets 01/17/23 08:10:30.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:30.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:30.76
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/17/23 08:10:30.77
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/17/23 08:10:30.788
    STEP: patching the secret 01/17/23 08:10:30.819
    STEP: deleting the secret using a LabelSelector 01/17/23 08:10:30.841
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/17/23 08:10:30.856
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 08:10:30.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5623" for this suite. 01/17/23 08:10:30.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:30.906
Jan 17 08:10:30.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename webhook 01/17/23 08:10:30.907
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:30.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:30.954
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 08:10:31.039
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:10:31.764
STEP: Deploying the webhook pod 01/17/23 08:10:31.798
STEP: Wait for the deployment to be ready 01/17/23 08:10:31.838
Jan 17 08:10:31.859: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 08:10:33.882
STEP: Verifying the service has paired with the endpoint 01/17/23 08:10:33.906
Jan 17 08:10:34.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/17/23 08:10:35.123
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:10:35.178
STEP: Deleting the collection of validation webhooks 01/17/23 08:10:35.212
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:10:35.296
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:10:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5904" for this suite. 01/17/23 08:10:35.337
STEP: Destroying namespace "webhook-5904-markers" for this suite. 01/17/23 08:10:35.351
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":361,"skipped":6703,"failed":0}
------------------------------
• [4.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:30.906
    Jan 17 08:10:30.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename webhook 01/17/23 08:10:30.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:30.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:30.954
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 08:10:31.039
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 08:10:31.764
    STEP: Deploying the webhook pod 01/17/23 08:10:31.798
    STEP: Wait for the deployment to be ready 01/17/23 08:10:31.838
    Jan 17 08:10:31.859: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 08:10:33.882
    STEP: Verifying the service has paired with the endpoint 01/17/23 08:10:33.906
    Jan 17 08:10:34.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/17/23 08:10:35.123
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:10:35.178
    STEP: Deleting the collection of validation webhooks 01/17/23 08:10:35.212
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 08:10:35.296
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:10:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5904" for this suite. 01/17/23 08:10:35.337
    STEP: Destroying namespace "webhook-5904-markers" for this suite. 01/17/23 08:10:35.351
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 08:10:35.484
Jan 17 08:10:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:10:35.486
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:35.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:35.535
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/17/23 08:10:35.544
Jan 17 08:10:35.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
STEP: rename a version 01/17/23 08:10:50.123
STEP: check the new version name is served 01/17/23 08:10:50.163
STEP: check the old version name is removed 01/17/23 08:10:56.268
STEP: check the other version is not changed 01/17/23 08:10:58.782
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 08:11:10.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6000" for this suite. 01/17/23 08:11:10.634
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":362,"skipped":6703,"failed":0}
------------------------------
• [SLOW TEST] [35.165 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 08:10:35.484
    Jan 17 08:10:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 08:10:35.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 08:10:35.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 08:10:35.535
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/17/23 08:10:35.544
    Jan 17 08:10:35.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3465506043
    STEP: rename a version 01/17/23 08:10:50.123
    STEP: check the new version name is served 01/17/23 08:10:50.163
    STEP: check the old version name is removed 01/17/23 08:10:56.268
    STEP: check the other version is not changed 01/17/23 08:10:58.782
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 08:11:10.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6000" for this suite. 01/17/23 08:11:10.634
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Jan 17 08:11:10.651: INFO: Running AfterSuite actions on all nodes
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 17 08:11:10.651: INFO: Running AfterSuite actions on node 1
Jan 17 08:11:10.651: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 17 08:11:10.651: INFO: Running AfterSuite actions on all nodes
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 17 08:11:10.651: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 17 08:11:10.651: INFO: Running AfterSuite actions on node 1
    Jan 17 08:11:10.651: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.087 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6269.309 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h44m29.833366766s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

