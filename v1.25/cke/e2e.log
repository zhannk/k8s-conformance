I0329 07:44:48.372826      22 e2e.go:116] Starting e2e run "5439d215-a232-4dfe-9691-4ba97c11aa52" on Ginkgo node 1
Mar 29 07:44:48.381: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1680075888 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar 29 07:44:48.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 07:44:48.436: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 29 07:44:48.444: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 29 07:44:48.454: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 29 07:44:48.454: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar 29 07:44:48.454: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 29 07:44:48.456: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 29 07:44:48.456: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-dns' (0 seconds elapsed)
Mar 29 07:44:48.456: INFO: e2e test version: v1.25.6
Mar 29 07:44:48.457: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar 29 07:44:48.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 07:44:48.458: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.023 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 29 07:44:48.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 07:44:48.436: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar 29 07:44:48.444: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 29 07:44:48.454: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 29 07:44:48.454: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Mar 29 07:44:48.454: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 29 07:44:48.456: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Mar 29 07:44:48.456: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-dns' (0 seconds elapsed)
    Mar 29 07:44:48.456: INFO: e2e test version: v1.25.6
    Mar 29 07:44:48.457: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 29 07:44:48.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 07:44:48.458: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:44:48.475
Mar 29 07:44:48.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 07:44:48.475
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:44:48.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:44:48.483
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/29/23 07:44:48.484
STEP: Ensuring ResourceQuota status is calculated 03/29/23 07:44:48.486
STEP: Creating a ResourceQuota with not best effort scope 03/29/23 07:44:50.489
STEP: Ensuring ResourceQuota status is calculated 03/29/23 07:44:50.491
STEP: Creating a best-effort pod 03/29/23 07:44:52.494
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/29/23 07:44:52.5
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/29/23 07:44:54.503
STEP: Deleting the pod 03/29/23 07:44:56.506
STEP: Ensuring resource quota status released the pod usage 03/29/23 07:44:56.512
STEP: Creating a not best-effort pod 03/29/23 07:44:58.515
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/29/23 07:44:58.52
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/29/23 07:45:00.523
STEP: Deleting the pod 03/29/23 07:45:02.525
STEP: Ensuring resource quota status released the pod usage 03/29/23 07:45:02.533
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 07:45:04.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3676" for this suite. 03/29/23 07:45:04.538
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":1,"skipped":3,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:44:48.475
    Mar 29 07:44:48.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 07:44:48.475
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:44:48.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:44:48.483
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/29/23 07:44:48.484
    STEP: Ensuring ResourceQuota status is calculated 03/29/23 07:44:48.486
    STEP: Creating a ResourceQuota with not best effort scope 03/29/23 07:44:50.489
    STEP: Ensuring ResourceQuota status is calculated 03/29/23 07:44:50.491
    STEP: Creating a best-effort pod 03/29/23 07:44:52.494
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/29/23 07:44:52.5
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/29/23 07:44:54.503
    STEP: Deleting the pod 03/29/23 07:44:56.506
    STEP: Ensuring resource quota status released the pod usage 03/29/23 07:44:56.512
    STEP: Creating a not best-effort pod 03/29/23 07:44:58.515
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/29/23 07:44:58.52
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/29/23 07:45:00.523
    STEP: Deleting the pod 03/29/23 07:45:02.525
    STEP: Ensuring resource quota status released the pod usage 03/29/23 07:45:02.533
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 07:45:04.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3676" for this suite. 03/29/23 07:45:04.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:04.542
Mar 29 07:45:04.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-webhook 03/29/23 07:45:04.542
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:04.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:04.55
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/29/23 07:45:04.551
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/29/23 07:45:04.793
STEP: Deploying the custom resource conversion webhook pod 03/29/23 07:45:04.799
STEP: Wait for the deployment to be ready 03/29/23 07:45:04.804
Mar 29 07:45:04.808: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 29 07:45:06.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/29/23 07:45:08.816
STEP: Verifying the service has paired with the endpoint 03/29/23 07:45:08.82
Mar 29 07:45:09.820: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 29 07:45:09.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Creating a v1 custom resource 03/29/23 07:45:12.372
STEP: Create a v2 custom resource 03/29/23 07:45:12.381
STEP: List CRs in v1 03/29/23 07:45:12.415
STEP: List CRs in v2 03/29/23 07:45:12.417
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:45:12.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6212" for this suite. 03/29/23 07:45:12.927
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":2,"skipped":42,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.698 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:04.542
    Mar 29 07:45:04.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-webhook 03/29/23 07:45:04.542
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:04.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:04.55
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/29/23 07:45:04.551
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/29/23 07:45:04.793
    STEP: Deploying the custom resource conversion webhook pod 03/29/23 07:45:04.799
    STEP: Wait for the deployment to be ready 03/29/23 07:45:04.804
    Mar 29 07:45:04.808: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Mar 29 07:45:06.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 7, 45, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/29/23 07:45:08.816
    STEP: Verifying the service has paired with the endpoint 03/29/23 07:45:08.82
    Mar 29 07:45:09.820: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 29 07:45:09.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Creating a v1 custom resource 03/29/23 07:45:12.372
    STEP: Create a v2 custom resource 03/29/23 07:45:12.381
    STEP: List CRs in v1 03/29/23 07:45:12.415
    STEP: List CRs in v2 03/29/23 07:45:12.417
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:45:12.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6212" for this suite. 03/29/23 07:45:12.927
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:13.24
Mar 29 07:45:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 07:45:13.24
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:13.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:13.39
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar 29 07:45:13.475: INFO: Waiting up to 2m0s for pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" in namespace "var-expansion-8859" to be "container 0 failed with reason CreateContainerConfigError"
Mar 29 07:45:13.476: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478821ms
Mar 29 07:45:15.479: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004028907s
Mar 29 07:45:17.480: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004981535s
Mar 29 07:45:17.480: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 29 07:45:17.480: INFO: Deleting pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" in namespace "var-expansion-8859"
Mar 29 07:45:17.484: INFO: Wait up to 5m0s for pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 07:45:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8859" for this suite. 03/29/23 07:45:19.49
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":3,"skipped":45,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.253 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:13.24
    Mar 29 07:45:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 07:45:13.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:13.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:13.39
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar 29 07:45:13.475: INFO: Waiting up to 2m0s for pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" in namespace "var-expansion-8859" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 29 07:45:13.476: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478821ms
    Mar 29 07:45:15.479: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004028907s
    Mar 29 07:45:17.480: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004981535s
    Mar 29 07:45:17.480: INFO: Pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 29 07:45:17.480: INFO: Deleting pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" in namespace "var-expansion-8859"
    Mar 29 07:45:17.484: INFO: Wait up to 5m0s for pod "var-expansion-8c225da3-d1a6-403e-aecd-14fb929dd525" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 07:45:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8859" for this suite. 03/29/23 07:45:19.49
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:19.493
Mar 29 07:45:19.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename server-version 03/29/23 07:45:19.494
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:19.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:19.503
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/29/23 07:45:19.504
STEP: Confirm major version 03/29/23 07:45:19.505
Mar 29 07:45:19.505: INFO: Major version: 1
STEP: Confirm minor version 03/29/23 07:45:19.505
Mar 29 07:45:19.505: INFO: cleanMinorVersion: 25
Mar 29 07:45:19.505: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar 29 07:45:19.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3100" for this suite. 03/29/23 07:45:19.507
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":4,"skipped":45,"failed":0}
------------------------------
â€¢ [0.016 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:19.493
    Mar 29 07:45:19.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename server-version 03/29/23 07:45:19.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:19.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:19.503
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/29/23 07:45:19.504
    STEP: Confirm major version 03/29/23 07:45:19.505
    Mar 29 07:45:19.505: INFO: Major version: 1
    STEP: Confirm minor version 03/29/23 07:45:19.505
    Mar 29 07:45:19.505: INFO: cleanMinorVersion: 25
    Mar 29 07:45:19.505: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar 29 07:45:19.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-3100" for this suite. 03/29/23 07:45:19.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:19.51
Mar 29 07:45:19.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:45:19.51
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:19.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:19.518
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/29/23 07:45:19.519
Mar 29 07:45:19.522: INFO: Waiting up to 5m0s for pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695" in namespace "emptydir-2447" to be "Succeeded or Failed"
Mar 29 07:45:19.524: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23827ms
Mar 29 07:45:21.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00334355s
Mar 29 07:45:23.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003259484s
STEP: Saw pod success 03/29/23 07:45:23.526
Mar 29 07:45:23.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695" satisfied condition "Succeeded or Failed"
Mar 29 07:45:23.527: INFO: Trying to get logs from node 10.146.0.115 pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 container test-container: <nil>
STEP: delete the pod 03/29/23 07:45:23.535
Mar 29 07:45:23.541: INFO: Waiting for pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 to disappear
Mar 29 07:45:23.542: INFO: Pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:45:23.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2447" for this suite. 03/29/23 07:45:23.544
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":64,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:19.51
    Mar 29 07:45:19.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:45:19.51
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:19.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:19.518
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/29/23 07:45:19.519
    Mar 29 07:45:19.522: INFO: Waiting up to 5m0s for pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695" in namespace "emptydir-2447" to be "Succeeded or Failed"
    Mar 29 07:45:19.524: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23827ms
    Mar 29 07:45:21.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00334355s
    Mar 29 07:45:23.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003259484s
    STEP: Saw pod success 03/29/23 07:45:23.526
    Mar 29 07:45:23.526: INFO: Pod "pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695" satisfied condition "Succeeded or Failed"
    Mar 29 07:45:23.527: INFO: Trying to get logs from node 10.146.0.115 pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 container test-container: <nil>
    STEP: delete the pod 03/29/23 07:45:23.535
    Mar 29 07:45:23.541: INFO: Waiting for pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 to disappear
    Mar 29 07:45:23.542: INFO: Pod pod-f3bfa3ea-dbe0-4e40-9c43-1f5a261e5695 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:45:23.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2447" for this suite. 03/29/23 07:45:23.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:23.548
Mar 29 07:45:23.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-webhook 03/29/23 07:45:23.549
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:23.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:23.556
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/29/23 07:45:23.558
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/29/23 07:45:23.858
STEP: Deploying the custom resource conversion webhook pod 03/29/23 07:45:23.861
STEP: Wait for the deployment to be ready 03/29/23 07:45:23.867
Mar 29 07:45:23.870: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 07:45:25.876
STEP: Verifying the service has paired with the endpoint 03/29/23 07:45:25.881
Mar 29 07:45:26.881: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 29 07:45:26.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Creating a v1 custom resource 03/29/23 07:45:29.433
STEP: v2 custom resource should be converted 03/29/23 07:45:29.436
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:45:29.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6185" for this suite. 03/29/23 07:45:29.947
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":6,"skipped":93,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.418 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:23.548
    Mar 29 07:45:23.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-webhook 03/29/23 07:45:23.549
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:23.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:23.556
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/29/23 07:45:23.558
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/29/23 07:45:23.858
    STEP: Deploying the custom resource conversion webhook pod 03/29/23 07:45:23.861
    STEP: Wait for the deployment to be ready 03/29/23 07:45:23.867
    Mar 29 07:45:23.870: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 07:45:25.876
    STEP: Verifying the service has paired with the endpoint 03/29/23 07:45:25.881
    Mar 29 07:45:26.881: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 29 07:45:26.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Creating a v1 custom resource 03/29/23 07:45:29.433
    STEP: v2 custom resource should be converted 03/29/23 07:45:29.436
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:45:29.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6185" for this suite. 03/29/23 07:45:29.947
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:29.968
Mar 29 07:45:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 07:45:29.969
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:29.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:29.981
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7182 03/29/23 07:45:29.984
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-7182 03/29/23 07:45:29.993
Mar 29 07:45:30.000: INFO: Found 0 stateful pods, waiting for 1
Mar 29 07:45:40.002: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/29/23 07:45:40.005
STEP: Getting /status 03/29/23 07:45:40.012
Mar 29 07:45:40.015: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/29/23 07:45:40.015
Mar 29 07:45:40.018: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/29/23 07:45:40.018
Mar 29 07:45:40.019: INFO: Observed &StatefulSet event: ADDED
Mar 29 07:45:40.019: INFO: Found Statefulset ss in namespace statefulset-7182 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 29 07:45:40.019: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/29/23 07:45:40.019
Mar 29 07:45:40.019: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 29 07:45:40.023: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/29/23 07:45:40.023
Mar 29 07:45:40.024: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 07:45:40.024: INFO: Deleting all statefulset in ns statefulset-7182
Mar 29 07:45:40.025: INFO: Scaling statefulset ss to 0
Mar 29 07:45:50.033: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 07:45:50.035: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 07:45:50.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7182" for this suite. 03/29/23 07:45:50.043
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":7,"skipped":140,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.077 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:29.968
    Mar 29 07:45:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 07:45:29.969
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:29.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:29.981
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7182 03/29/23 07:45:29.984
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-7182 03/29/23 07:45:29.993
    Mar 29 07:45:30.000: INFO: Found 0 stateful pods, waiting for 1
    Mar 29 07:45:40.002: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/29/23 07:45:40.005
    STEP: Getting /status 03/29/23 07:45:40.012
    Mar 29 07:45:40.015: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/29/23 07:45:40.015
    Mar 29 07:45:40.018: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/29/23 07:45:40.018
    Mar 29 07:45:40.019: INFO: Observed &StatefulSet event: ADDED
    Mar 29 07:45:40.019: INFO: Found Statefulset ss in namespace statefulset-7182 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 29 07:45:40.019: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/29/23 07:45:40.019
    Mar 29 07:45:40.019: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 29 07:45:40.023: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/29/23 07:45:40.023
    Mar 29 07:45:40.024: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 07:45:40.024: INFO: Deleting all statefulset in ns statefulset-7182
    Mar 29 07:45:40.025: INFO: Scaling statefulset ss to 0
    Mar 29 07:45:50.033: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 07:45:50.035: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 07:45:50.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7182" for this suite. 03/29/23 07:45:50.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:50.047
Mar 29 07:45:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename runtimeclass 03/29/23 07:45:50.047
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:50.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:50.055
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 29 07:45:50.062: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1927 to be scheduled
Mar 29 07:45:50.063: INFO: 1 pods are not scheduled: [runtimeclass-1927/test-runtimeclass-runtimeclass-1927-preconfigured-handler-nw6gk(d9b85622-b87b-4451-80a7-8c80d00fbf26)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 29 07:45:52.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1927" for this suite. 03/29/23 07:45:52.07
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":8,"skipped":146,"failed":0}
------------------------------
â€¢ [2.026 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:50.047
    Mar 29 07:45:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename runtimeclass 03/29/23 07:45:50.047
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:50.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:50.055
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 29 07:45:50.062: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1927 to be scheduled
    Mar 29 07:45:50.063: INFO: 1 pods are not scheduled: [runtimeclass-1927/test-runtimeclass-runtimeclass-1927-preconfigured-handler-nw6gk(d9b85622-b87b-4451-80a7-8c80d00fbf26)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 29 07:45:52.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1927" for this suite. 03/29/23 07:45:52.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:52.073
Mar 29 07:45:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 07:45:52.073
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:52.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:52.082
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/29/23 07:45:52.084
Mar 29 07:45:52.087: INFO: Waiting up to 5m0s for pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b" in namespace "downward-api-4690" to be "Succeeded or Failed"
Mar 29 07:45:52.088: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.078099ms
Mar 29 07:45:54.091: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003711327s
Mar 29 07:45:56.091: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004559974s
STEP: Saw pod success 03/29/23 07:45:56.091
Mar 29 07:45:56.092: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b" satisfied condition "Succeeded or Failed"
Mar 29 07:45:56.093: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b container dapi-container: <nil>
STEP: delete the pod 03/29/23 07:45:56.096
Mar 29 07:45:56.102: INFO: Waiting for pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b to disappear
Mar 29 07:45:56.104: INFO: Pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 29 07:45:56.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4690" for this suite. 03/29/23 07:45:56.106
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":9,"skipped":151,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:52.073
    Mar 29 07:45:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 07:45:52.073
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:52.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:52.082
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/29/23 07:45:52.084
    Mar 29 07:45:52.087: INFO: Waiting up to 5m0s for pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b" in namespace "downward-api-4690" to be "Succeeded or Failed"
    Mar 29 07:45:52.088: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.078099ms
    Mar 29 07:45:54.091: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003711327s
    Mar 29 07:45:56.091: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004559974s
    STEP: Saw pod success 03/29/23 07:45:56.091
    Mar 29 07:45:56.092: INFO: Pod "downward-api-734b916a-005d-4240-9314-01e23aa75b1b" satisfied condition "Succeeded or Failed"
    Mar 29 07:45:56.093: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b container dapi-container: <nil>
    STEP: delete the pod 03/29/23 07:45:56.096
    Mar 29 07:45:56.102: INFO: Waiting for pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b to disappear
    Mar 29 07:45:56.104: INFO: Pod downward-api-734b916a-005d-4240-9314-01e23aa75b1b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 29 07:45:56.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4690" for this suite. 03/29/23 07:45:56.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:45:56.109
Mar 29 07:45:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename subpath 03/29/23 07:45:56.109
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:56.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:56.116
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/29/23 07:45:56.117
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-k2nr 03/29/23 07:45:56.122
STEP: Creating a pod to test atomic-volume-subpath 03/29/23 07:45:56.122
Mar 29 07:45:56.126: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-k2nr" in namespace "subpath-5092" to be "Succeeded or Failed"
Mar 29 07:45:56.127: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.191683ms
Mar 29 07:45:58.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003395064s
Mar 29 07:46:00.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 4.003651103s
Mar 29 07:46:02.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 6.003944234s
Mar 29 07:46:04.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 8.003556488s
Mar 29 07:46:06.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 10.003584559s
Mar 29 07:46:08.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 12.004432258s
Mar 29 07:46:10.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 14.004711726s
Mar 29 07:46:12.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 16.004442048s
Mar 29 07:46:14.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 18.003120364s
Mar 29 07:46:16.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 20.004584092s
Mar 29 07:46:18.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=false. Elapsed: 22.004268289s
Mar 29 07:46:20.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004382716s
STEP: Saw pod success 03/29/23 07:46:20.13
Mar 29 07:46:20.130: INFO: Pod "pod-subpath-test-projected-k2nr" satisfied condition "Succeeded or Failed"
Mar 29 07:46:20.132: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-projected-k2nr container test-container-subpath-projected-k2nr: <nil>
STEP: delete the pod 03/29/23 07:46:20.135
Mar 29 07:46:20.141: INFO: Waiting for pod pod-subpath-test-projected-k2nr to disappear
Mar 29 07:46:20.142: INFO: Pod pod-subpath-test-projected-k2nr no longer exists
STEP: Deleting pod pod-subpath-test-projected-k2nr 03/29/23 07:46:20.142
Mar 29 07:46:20.142: INFO: Deleting pod "pod-subpath-test-projected-k2nr" in namespace "subpath-5092"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 29 07:46:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5092" for this suite. 03/29/23 07:46:20.145
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":10,"skipped":180,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.038 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:45:56.109
    Mar 29 07:45:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename subpath 03/29/23 07:45:56.109
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:45:56.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:45:56.116
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/29/23 07:45:56.117
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-k2nr 03/29/23 07:45:56.122
    STEP: Creating a pod to test atomic-volume-subpath 03/29/23 07:45:56.122
    Mar 29 07:45:56.126: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-k2nr" in namespace "subpath-5092" to be "Succeeded or Failed"
    Mar 29 07:45:56.127: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.191683ms
    Mar 29 07:45:58.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003395064s
    Mar 29 07:46:00.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 4.003651103s
    Mar 29 07:46:02.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 6.003944234s
    Mar 29 07:46:04.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 8.003556488s
    Mar 29 07:46:06.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 10.003584559s
    Mar 29 07:46:08.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 12.004432258s
    Mar 29 07:46:10.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 14.004711726s
    Mar 29 07:46:12.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 16.004442048s
    Mar 29 07:46:14.129: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 18.003120364s
    Mar 29 07:46:16.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=true. Elapsed: 20.004584092s
    Mar 29 07:46:18.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Running", Reason="", readiness=false. Elapsed: 22.004268289s
    Mar 29 07:46:20.130: INFO: Pod "pod-subpath-test-projected-k2nr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004382716s
    STEP: Saw pod success 03/29/23 07:46:20.13
    Mar 29 07:46:20.130: INFO: Pod "pod-subpath-test-projected-k2nr" satisfied condition "Succeeded or Failed"
    Mar 29 07:46:20.132: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-projected-k2nr container test-container-subpath-projected-k2nr: <nil>
    STEP: delete the pod 03/29/23 07:46:20.135
    Mar 29 07:46:20.141: INFO: Waiting for pod pod-subpath-test-projected-k2nr to disappear
    Mar 29 07:46:20.142: INFO: Pod pod-subpath-test-projected-k2nr no longer exists
    STEP: Deleting pod pod-subpath-test-projected-k2nr 03/29/23 07:46:20.142
    Mar 29 07:46:20.142: INFO: Deleting pod "pod-subpath-test-projected-k2nr" in namespace "subpath-5092"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 29 07:46:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5092" for this suite. 03/29/23 07:46:20.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:46:20.147
Mar 29 07:46:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 07:46:20.148
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:20.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:20.156
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/29/23 07:46:20.157
Mar 29 07:46:20.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: mark a version not serverd 03/29/23 07:46:26.439
STEP: check the unserved version gets removed 03/29/23 07:46:26.45
STEP: check the other version is not changed 03/29/23 07:46:28.764
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:46:32.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-346" for this suite. 03/29/23 07:46:32.851
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":11,"skipped":201,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.706 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:46:20.147
    Mar 29 07:46:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 07:46:20.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:20.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:20.156
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/29/23 07:46:20.157
    Mar 29 07:46:20.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: mark a version not serverd 03/29/23 07:46:26.439
    STEP: check the unserved version gets removed 03/29/23 07:46:26.45
    STEP: check the other version is not changed 03/29/23 07:46:28.764
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:46:32.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-346" for this suite. 03/29/23 07:46:32.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:46:32.855
Mar 29 07:46:32.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 07:46:32.855
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:32.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:32.864
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 07:46:32.865
Mar 29 07:46:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9491 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar 29 07:46:32.918: INFO: stderr: ""
Mar 29 07:46:32.918: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/29/23 07:46:32.918
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar 29 07:46:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9491 delete pods e2e-test-httpd-pod'
Mar 29 07:46:34.955: INFO: stderr: ""
Mar 29 07:46:34.955: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 07:46:34.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9491" for this suite. 03/29/23 07:46:34.957
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":12,"skipped":221,"failed":0}
------------------------------
â€¢ [2.104 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:46:32.855
    Mar 29 07:46:32.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 07:46:32.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:32.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:32.864
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 07:46:32.865
    Mar 29 07:46:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9491 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar 29 07:46:32.918: INFO: stderr: ""
    Mar 29 07:46:32.918: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/29/23 07:46:32.918
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar 29 07:46:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9491 delete pods e2e-test-httpd-pod'
    Mar 29 07:46:34.955: INFO: stderr: ""
    Mar 29 07:46:34.955: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 07:46:34.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9491" for this suite. 03/29/23 07:46:34.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:46:34.961
Mar 29 07:46:34.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename endpointslicemirroring 03/29/23 07:46:34.962
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:34.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:34.969
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/29/23 07:46:34.975
Mar 29 07:46:34.978: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/29/23 07:46:36.981
Mar 29 07:46:36.985: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/29/23 07:46:38.987
Mar 29 07:46:38.992: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar 29 07:46:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3043" for this suite. 03/29/23 07:46:40.997
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":13,"skipped":286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.039 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:46:34.961
    Mar 29 07:46:34.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename endpointslicemirroring 03/29/23 07:46:34.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:34.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:34.969
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/29/23 07:46:34.975
    Mar 29 07:46:34.978: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/29/23 07:46:36.981
    Mar 29 07:46:36.985: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/29/23 07:46:38.987
    Mar 29 07:46:38.992: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar 29 07:46:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3043" for this suite. 03/29/23 07:46:40.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:46:41.002
Mar 29 07:46:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 07:46:41.003
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:41.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:41.011
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d in namespace container-probe-997 03/29/23 07:46:41.012
Mar 29 07:46:41.015: INFO: Waiting up to 5m0s for pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d" in namespace "container-probe-997" to be "not pending"
Mar 29 07:46:41.016: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.165644ms
Mar 29 07:46:43.018: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003188311s
Mar 29 07:46:45.019: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Running", Reason="", readiness=true. Elapsed: 4.003778743s
Mar 29 07:46:45.019: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d" satisfied condition "not pending"
Mar 29 07:46:45.019: INFO: Started pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d in namespace container-probe-997
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 07:46:45.019
Mar 29 07:46:45.020: INFO: Initial restart count of pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d is 0
STEP: deleting the pod 03/29/23 07:50:45.335
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 07:50:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-997" for this suite. 03/29/23 07:50:45.347
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":14,"skipped":367,"failed":0}
------------------------------
â€¢ [SLOW TEST] [244.347 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:46:41.002
    Mar 29 07:46:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 07:46:41.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:46:41.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:46:41.011
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d in namespace container-probe-997 03/29/23 07:46:41.012
    Mar 29 07:46:41.015: INFO: Waiting up to 5m0s for pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d" in namespace "container-probe-997" to be "not pending"
    Mar 29 07:46:41.016: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.165644ms
    Mar 29 07:46:43.018: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003188311s
    Mar 29 07:46:45.019: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d": Phase="Running", Reason="", readiness=true. Elapsed: 4.003778743s
    Mar 29 07:46:45.019: INFO: Pod "test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d" satisfied condition "not pending"
    Mar 29 07:46:45.019: INFO: Started pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d in namespace container-probe-997
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 07:46:45.019
    Mar 29 07:46:45.020: INFO: Initial restart count of pod test-webserver-89f7d296-ef7b-4421-8431-f98641ea504d is 0
    STEP: deleting the pod 03/29/23 07:50:45.335
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 07:50:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-997" for this suite. 03/29/23 07:50:45.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:50:45.351
Mar 29 07:50:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 07:50:45.351
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:50:45.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:50:45.359
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8591 03/29/23 07:50:45.361
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/29/23 07:50:45.363
Mar 29 07:50:45.367: INFO: Found 0 stateful pods, waiting for 3
Mar 29 07:50:55.370: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 07:50:55.370: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 07:50:55.370: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/29/23 07:50:55.374
Mar 29 07:50:55.389: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/29/23 07:50:55.389
STEP: Not applying an update when the partition is greater than the number of replicas 03/29/23 07:51:05.397
STEP: Performing a canary update 03/29/23 07:51:05.397
Mar 29 07:51:05.411: INFO: Updating stateful set ss2
Mar 29 07:51:05.413: INFO: Waiting for Pod statefulset-8591/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/29/23 07:51:15.417
Mar 29 07:51:15.439: INFO: Found 2 stateful pods, waiting for 3
Mar 29 07:51:25.443: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 07:51:25.443: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 07:51:25.443: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/29/23 07:51:25.446
Mar 29 07:51:25.461: INFO: Updating stateful set ss2
Mar 29 07:51:25.463: INFO: Waiting for Pod statefulset-8591/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 29 07:51:35.485: INFO: Updating stateful set ss2
Mar 29 07:51:35.488: INFO: Waiting for StatefulSet statefulset-8591/ss2 to complete update
Mar 29 07:51:35.488: INFO: Waiting for Pod statefulset-8591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 07:51:45.493: INFO: Deleting all statefulset in ns statefulset-8591
Mar 29 07:51:45.494: INFO: Scaling statefulset ss2 to 0
Mar 29 07:51:55.502: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 07:51:55.504: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 07:51:55.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8591" for this suite. 03/29/23 07:51:55.512
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":15,"skipped":392,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:50:45.351
    Mar 29 07:50:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 07:50:45.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:50:45.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:50:45.359
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8591 03/29/23 07:50:45.361
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/29/23 07:50:45.363
    Mar 29 07:50:45.367: INFO: Found 0 stateful pods, waiting for 3
    Mar 29 07:50:55.370: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 07:50:55.370: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 07:50:55.370: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/29/23 07:50:55.374
    Mar 29 07:50:55.389: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/29/23 07:50:55.389
    STEP: Not applying an update when the partition is greater than the number of replicas 03/29/23 07:51:05.397
    STEP: Performing a canary update 03/29/23 07:51:05.397
    Mar 29 07:51:05.411: INFO: Updating stateful set ss2
    Mar 29 07:51:05.413: INFO: Waiting for Pod statefulset-8591/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/29/23 07:51:15.417
    Mar 29 07:51:15.439: INFO: Found 2 stateful pods, waiting for 3
    Mar 29 07:51:25.443: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 07:51:25.443: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 07:51:25.443: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/29/23 07:51:25.446
    Mar 29 07:51:25.461: INFO: Updating stateful set ss2
    Mar 29 07:51:25.463: INFO: Waiting for Pod statefulset-8591/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 29 07:51:35.485: INFO: Updating stateful set ss2
    Mar 29 07:51:35.488: INFO: Waiting for StatefulSet statefulset-8591/ss2 to complete update
    Mar 29 07:51:35.488: INFO: Waiting for Pod statefulset-8591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 07:51:45.493: INFO: Deleting all statefulset in ns statefulset-8591
    Mar 29 07:51:45.494: INFO: Scaling statefulset ss2 to 0
    Mar 29 07:51:55.502: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 07:51:55.504: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 07:51:55.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8591" for this suite. 03/29/23 07:51:55.512
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:51:55.515
Mar 29 07:51:55.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 07:51:55.516
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:51:55.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:51:55.523
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/29/23 07:51:55.524
STEP: Ensuring active pods == parallelism 03/29/23 07:51:55.528
STEP: delete a job 03/29/23 07:51:57.53
STEP: deleting Job.batch foo in namespace job-9440, will wait for the garbage collector to delete the pods 03/29/23 07:51:57.53
Mar 29 07:51:57.585: INFO: Deleting Job.batch foo took: 2.958048ms
Mar 29 07:51:57.685: INFO: Terminating Job.batch foo pods took: 100.097903ms
STEP: Ensuring job was deleted 03/29/23 07:52:30.586
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 07:52:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9440" for this suite. 03/29/23 07:52:30.591
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":16,"skipped":396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.080 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:51:55.515
    Mar 29 07:51:55.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 07:51:55.516
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:51:55.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:51:55.523
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/29/23 07:51:55.524
    STEP: Ensuring active pods == parallelism 03/29/23 07:51:55.528
    STEP: delete a job 03/29/23 07:51:57.53
    STEP: deleting Job.batch foo in namespace job-9440, will wait for the garbage collector to delete the pods 03/29/23 07:51:57.53
    Mar 29 07:51:57.585: INFO: Deleting Job.batch foo took: 2.958048ms
    Mar 29 07:51:57.685: INFO: Terminating Job.batch foo pods took: 100.097903ms
    STEP: Ensuring job was deleted 03/29/23 07:52:30.586
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 07:52:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9440" for this suite. 03/29/23 07:52:30.591
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:30.595
Mar 29 07:52:30.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 07:52:30.596
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:30.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:30.603
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/29/23 07:52:30.606
Mar 29 07:52:30.610: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2186" to be "running and ready"
Mar 29 07:52:30.611: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.220935ms
Mar 29 07:52:30.611: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:52:32.614: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003892181s
Mar 29 07:52:32.614: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:52:34.613: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.003375208s
Mar 29 07:52:34.613: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 29 07:52:34.613: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/29/23 07:52:34.615
Mar 29 07:52:34.618: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2186" to be "running and ready"
Mar 29 07:52:34.619: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.312776ms
Mar 29 07:52:34.619: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:52:36.621: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812428s
Mar 29 07:52:36.621: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 29 07:52:36.621: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/29/23 07:52:36.623
STEP: delete the pod with lifecycle hook 03/29/23 07:52:36.631
Mar 29 07:52:36.634: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 07:52:36.637: INFO: Pod pod-with-poststart-http-hook still exists
Mar 29 07:52:38.638: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 07:52:38.640: INFO: Pod pod-with-poststart-http-hook still exists
Mar 29 07:52:40.638: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 29 07:52:40.640: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 29 07:52:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2186" for this suite. 03/29/23 07:52:40.642
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":17,"skipped":397,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.051 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:30.595
    Mar 29 07:52:30.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 07:52:30.596
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:30.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:30.603
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/29/23 07:52:30.606
    Mar 29 07:52:30.610: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2186" to be "running and ready"
    Mar 29 07:52:30.611: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.220935ms
    Mar 29 07:52:30.611: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:52:32.614: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003892181s
    Mar 29 07:52:32.614: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:52:34.613: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.003375208s
    Mar 29 07:52:34.613: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 29 07:52:34.613: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/29/23 07:52:34.615
    Mar 29 07:52:34.618: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2186" to be "running and ready"
    Mar 29 07:52:34.619: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.312776ms
    Mar 29 07:52:34.619: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:52:36.621: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812428s
    Mar 29 07:52:36.621: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 29 07:52:36.621: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/29/23 07:52:36.623
    STEP: delete the pod with lifecycle hook 03/29/23 07:52:36.631
    Mar 29 07:52:36.634: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 29 07:52:36.637: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 29 07:52:38.638: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 29 07:52:38.640: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 29 07:52:40.638: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 29 07:52:40.640: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 29 07:52:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2186" for this suite. 03/29/23 07:52:40.642
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:40.646
Mar 29 07:52:40.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:52:40.647
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:40.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:40.654
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/29/23 07:52:40.655
Mar 29 07:52:40.659: INFO: Waiting up to 5m0s for pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f" in namespace "emptydir-2832" to be "Succeeded or Failed"
Mar 29 07:52:40.660: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.071729ms
Mar 29 07:52:42.662: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003498646s
Mar 29 07:52:44.663: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003628055s
STEP: Saw pod success 03/29/23 07:52:44.663
Mar 29 07:52:44.663: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f" satisfied condition "Succeeded or Failed"
Mar 29 07:52:44.664: INFO: Trying to get logs from node 10.146.0.116 pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f container test-container: <nil>
STEP: delete the pod 03/29/23 07:52:44.672
Mar 29 07:52:44.678: INFO: Waiting for pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f to disappear
Mar 29 07:52:44.680: INFO: Pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:52:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2832" for this suite. 03/29/23 07:52:44.681
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":399,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:40.646
    Mar 29 07:52:40.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:52:40.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:40.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:40.654
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/29/23 07:52:40.655
    Mar 29 07:52:40.659: INFO: Waiting up to 5m0s for pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f" in namespace "emptydir-2832" to be "Succeeded or Failed"
    Mar 29 07:52:40.660: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.071729ms
    Mar 29 07:52:42.662: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003498646s
    Mar 29 07:52:44.663: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003628055s
    STEP: Saw pod success 03/29/23 07:52:44.663
    Mar 29 07:52:44.663: INFO: Pod "pod-dc304b89-1914-4bb6-afe0-176ab3a1836f" satisfied condition "Succeeded or Failed"
    Mar 29 07:52:44.664: INFO: Trying to get logs from node 10.146.0.116 pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f container test-container: <nil>
    STEP: delete the pod 03/29/23 07:52:44.672
    Mar 29 07:52:44.678: INFO: Waiting for pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f to disappear
    Mar 29 07:52:44.680: INFO: Pod pod-dc304b89-1914-4bb6-afe0-176ab3a1836f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:52:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2832" for this suite. 03/29/23 07:52:44.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:44.684
Mar 29 07:52:44.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 07:52:44.685
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:44.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:44.691
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 07:52:44.698
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 07:52:45.101
STEP: Deploying the webhook pod 03/29/23 07:52:45.105
STEP: Wait for the deployment to be ready 03/29/23 07:52:45.112
Mar 29 07:52:45.116: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 07:52:47.121
STEP: Verifying the service has paired with the endpoint 03/29/23 07:52:47.126
Mar 29 07:52:48.126: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/29/23 07:52:48.128
STEP: create a configmap that should be updated by the webhook 03/29/23 07:52:48.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:52:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8858" for this suite. 03/29/23 07:52:48.148
STEP: Destroying namespace "webhook-8858-markers" for this suite. 03/29/23 07:52:48.15
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":19,"skipped":405,"failed":0}
------------------------------
â€¢ [3.485 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:44.684
    Mar 29 07:52:44.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 07:52:44.685
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:44.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:44.691
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 07:52:44.698
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 07:52:45.101
    STEP: Deploying the webhook pod 03/29/23 07:52:45.105
    STEP: Wait for the deployment to be ready 03/29/23 07:52:45.112
    Mar 29 07:52:45.116: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 07:52:47.121
    STEP: Verifying the service has paired with the endpoint 03/29/23 07:52:47.126
    Mar 29 07:52:48.126: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/29/23 07:52:48.128
    STEP: create a configmap that should be updated by the webhook 03/29/23 07:52:48.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:52:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8858" for this suite. 03/29/23 07:52:48.148
    STEP: Destroying namespace "webhook-8858-markers" for this suite. 03/29/23 07:52:48.15
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:48.169
Mar 29 07:52:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:52:48.17
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:48.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:48.181
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/29/23 07:52:48.183
Mar 29 07:52:48.188: INFO: Waiting up to 5m0s for pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6" in namespace "emptydir-148" to be "Succeeded or Failed"
Mar 29 07:52:48.190: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865877ms
Mar 29 07:52:50.192: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004119431s
Mar 29 07:52:52.193: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005151125s
STEP: Saw pod success 03/29/23 07:52:52.193
Mar 29 07:52:52.193: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6" satisfied condition "Succeeded or Failed"
Mar 29 07:52:52.195: INFO: Trying to get logs from node 10.146.0.117 pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 container test-container: <nil>
STEP: delete the pod 03/29/23 07:52:52.198
Mar 29 07:52:52.206: INFO: Waiting for pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 to disappear
Mar 29 07:52:52.207: INFO: Pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:52:52.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-148" for this suite. 03/29/23 07:52:52.208
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":20,"skipped":416,"failed":0}
------------------------------
â€¢ [4.042 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:48.169
    Mar 29 07:52:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:52:48.17
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:48.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:48.181
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/29/23 07:52:48.183
    Mar 29 07:52:48.188: INFO: Waiting up to 5m0s for pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6" in namespace "emptydir-148" to be "Succeeded or Failed"
    Mar 29 07:52:48.190: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865877ms
    Mar 29 07:52:50.192: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004119431s
    Mar 29 07:52:52.193: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005151125s
    STEP: Saw pod success 03/29/23 07:52:52.193
    Mar 29 07:52:52.193: INFO: Pod "pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6" satisfied condition "Succeeded or Failed"
    Mar 29 07:52:52.195: INFO: Trying to get logs from node 10.146.0.117 pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 container test-container: <nil>
    STEP: delete the pod 03/29/23 07:52:52.198
    Mar 29 07:52:52.206: INFO: Waiting for pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 to disappear
    Mar 29 07:52:52.207: INFO: Pod pod-b5c18d58-95e6-4f16-b83d-0b6bc9c93bc6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:52:52.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-148" for this suite. 03/29/23 07:52:52.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:52.212
Mar 29 07:52:52.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption 03/29/23 07:52:52.213
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:52.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:52.219
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/29/23 07:52:52.222
STEP: Updating PodDisruptionBudget status 03/29/23 07:52:54.225
STEP: Waiting for all pods to be running 03/29/23 07:52:54.229
Mar 29 07:52:54.231: INFO: running pods: 0 < 1
Mar 29 07:52:56.234: INFO: running pods: 0 < 1
STEP: locating a running pod 03/29/23 07:52:58.233
STEP: Waiting for the pdb to be processed 03/29/23 07:52:58.239
STEP: Patching PodDisruptionBudget status 03/29/23 07:52:58.243
STEP: Waiting for the pdb to be processed 03/29/23 07:52:58.246
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 29 07:52:58.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4705" for this suite. 03/29/23 07:52:58.249
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":21,"skipped":440,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.041 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:52.212
    Mar 29 07:52:52.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption 03/29/23 07:52:52.213
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:52.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:52.219
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/29/23 07:52:52.222
    STEP: Updating PodDisruptionBudget status 03/29/23 07:52:54.225
    STEP: Waiting for all pods to be running 03/29/23 07:52:54.229
    Mar 29 07:52:54.231: INFO: running pods: 0 < 1
    Mar 29 07:52:56.234: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/29/23 07:52:58.233
    STEP: Waiting for the pdb to be processed 03/29/23 07:52:58.239
    STEP: Patching PodDisruptionBudget status 03/29/23 07:52:58.243
    STEP: Waiting for the pdb to be processed 03/29/23 07:52:58.246
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 29 07:52:58.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4705" for this suite. 03/29/23 07:52:58.249
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:52:58.252
Mar 29 07:52:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 07:52:58.253
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:58.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:58.26
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/29/23 07:53:15.263
STEP: Creating a ResourceQuota 03/29/23 07:53:20.266
STEP: Ensuring resource quota status is calculated 03/29/23 07:53:20.269
STEP: Creating a ConfigMap 03/29/23 07:53:22.271
STEP: Ensuring resource quota status captures configMap creation 03/29/23 07:53:22.278
STEP: Deleting a ConfigMap 03/29/23 07:53:24.281
STEP: Ensuring resource quota status released usage 03/29/23 07:53:24.284
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 07:53:26.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3314" for this suite. 03/29/23 07:53:26.289
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":22,"skipped":442,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:52:58.252
    Mar 29 07:52:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 07:52:58.253
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:52:58.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:52:58.26
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/29/23 07:53:15.263
    STEP: Creating a ResourceQuota 03/29/23 07:53:20.266
    STEP: Ensuring resource quota status is calculated 03/29/23 07:53:20.269
    STEP: Creating a ConfigMap 03/29/23 07:53:22.271
    STEP: Ensuring resource quota status captures configMap creation 03/29/23 07:53:22.278
    STEP: Deleting a ConfigMap 03/29/23 07:53:24.281
    STEP: Ensuring resource quota status released usage 03/29/23 07:53:24.284
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 07:53:26.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3314" for this suite. 03/29/23 07:53:26.289
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:53:26.292
Mar 29 07:53:26.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:53:26.293
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:26.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:26.3
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/29/23 07:53:26.301
Mar 29 07:53:26.304: INFO: Waiting up to 5m0s for pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8" in namespace "emptydir-8459" to be "Succeeded or Failed"
Mar 29 07:53:26.306: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.251343ms
Mar 29 07:53:28.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004286846s
Mar 29 07:53:30.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004353778s
STEP: Saw pod success 03/29/23 07:53:30.309
Mar 29 07:53:30.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8" satisfied condition "Succeeded or Failed"
Mar 29 07:53:30.310: INFO: Trying to get logs from node 10.146.0.115 pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 container test-container: <nil>
STEP: delete the pod 03/29/23 07:53:30.318
Mar 29 07:53:30.324: INFO: Waiting for pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 to disappear
Mar 29 07:53:30.325: INFO: Pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:53:30.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8459" for this suite. 03/29/23 07:53:30.327
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":23,"skipped":444,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:53:26.292
    Mar 29 07:53:26.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:53:26.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:26.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:26.3
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/29/23 07:53:26.301
    Mar 29 07:53:26.304: INFO: Waiting up to 5m0s for pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8" in namespace "emptydir-8459" to be "Succeeded or Failed"
    Mar 29 07:53:26.306: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.251343ms
    Mar 29 07:53:28.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004286846s
    Mar 29 07:53:30.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004353778s
    STEP: Saw pod success 03/29/23 07:53:30.309
    Mar 29 07:53:30.309: INFO: Pod "pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8" satisfied condition "Succeeded or Failed"
    Mar 29 07:53:30.310: INFO: Trying to get logs from node 10.146.0.115 pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 container test-container: <nil>
    STEP: delete the pod 03/29/23 07:53:30.318
    Mar 29 07:53:30.324: INFO: Waiting for pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 to disappear
    Mar 29 07:53:30.325: INFO: Pod pod-a7871193-a767-4ce2-9a8b-7cf31218d0b8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:53:30.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8459" for this suite. 03/29/23 07:53:30.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:53:30.33
Mar 29 07:53:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:53:30.331
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:30.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:30.338
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/29/23 07:53:30.339
Mar 29 07:53:30.342: INFO: Waiting up to 5m0s for pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef" in namespace "emptydir-4767" to be "Succeeded or Failed"
Mar 29 07:53:30.343: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200004ms
Mar 29 07:53:32.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004032085s
Mar 29 07:53:34.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003618908s
STEP: Saw pod success 03/29/23 07:53:34.346
Mar 29 07:53:34.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef" satisfied condition "Succeeded or Failed"
Mar 29 07:53:34.347: INFO: Trying to get logs from node 10.146.0.115 pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef container test-container: <nil>
STEP: delete the pod 03/29/23 07:53:34.35
Mar 29 07:53:34.356: INFO: Waiting for pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef to disappear
Mar 29 07:53:34.357: INFO: Pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:53:34.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4767" for this suite. 03/29/23 07:53:34.358
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":460,"failed":0}
------------------------------
â€¢ [4.030 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:53:30.33
    Mar 29 07:53:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:53:30.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:30.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:30.338
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/29/23 07:53:30.339
    Mar 29 07:53:30.342: INFO: Waiting up to 5m0s for pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef" in namespace "emptydir-4767" to be "Succeeded or Failed"
    Mar 29 07:53:30.343: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200004ms
    Mar 29 07:53:32.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004032085s
    Mar 29 07:53:34.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003618908s
    STEP: Saw pod success 03/29/23 07:53:34.346
    Mar 29 07:53:34.346: INFO: Pod "pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef" satisfied condition "Succeeded or Failed"
    Mar 29 07:53:34.347: INFO: Trying to get logs from node 10.146.0.115 pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef container test-container: <nil>
    STEP: delete the pod 03/29/23 07:53:34.35
    Mar 29 07:53:34.356: INFO: Waiting for pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef to disappear
    Mar 29 07:53:34.357: INFO: Pod pod-07b63a85-f6b9-4a47-b31d-d8f58e8edbef no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:53:34.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4767" for this suite. 03/29/23 07:53:34.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:53:34.361
Mar 29 07:53:34.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename subpath 03/29/23 07:53:34.362
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:34.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:34.369
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/29/23 07:53:34.37
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-8wjp 03/29/23 07:53:34.374
STEP: Creating a pod to test atomic-volume-subpath 03/29/23 07:53:34.374
Mar 29 07:53:34.377: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8wjp" in namespace "subpath-3774" to be "Succeeded or Failed"
Mar 29 07:53:34.378: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.155761ms
Mar 29 07:53:36.380: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.003331622s
Mar 29 07:53:38.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.003443832s
Mar 29 07:53:40.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 6.00347271s
Mar 29 07:53:42.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 8.004423974s
Mar 29 07:53:44.380: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 10.003270344s
Mar 29 07:53:46.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 12.004308771s
Mar 29 07:53:48.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 14.003490928s
Mar 29 07:53:50.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 16.004526639s
Mar 29 07:53:52.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00388098s
Mar 29 07:53:54.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 20.003620254s
Mar 29 07:53:56.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=false. Elapsed: 22.004687116s
Mar 29 07:53:58.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003490909s
STEP: Saw pod success 03/29/23 07:53:58.381
Mar 29 07:53:58.381: INFO: Pod "pod-subpath-test-secret-8wjp" satisfied condition "Succeeded or Failed"
Mar 29 07:53:58.382: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-secret-8wjp container test-container-subpath-secret-8wjp: <nil>
STEP: delete the pod 03/29/23 07:53:58.385
Mar 29 07:53:58.390: INFO: Waiting for pod pod-subpath-test-secret-8wjp to disappear
Mar 29 07:53:58.391: INFO: Pod pod-subpath-test-secret-8wjp no longer exists
STEP: Deleting pod pod-subpath-test-secret-8wjp 03/29/23 07:53:58.391
Mar 29 07:53:58.392: INFO: Deleting pod "pod-subpath-test-secret-8wjp" in namespace "subpath-3774"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 29 07:53:58.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3774" for this suite. 03/29/23 07:53:58.394
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":25,"skipped":471,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.035 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:53:34.361
    Mar 29 07:53:34.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename subpath 03/29/23 07:53:34.362
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:34.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:34.369
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/29/23 07:53:34.37
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-8wjp 03/29/23 07:53:34.374
    STEP: Creating a pod to test atomic-volume-subpath 03/29/23 07:53:34.374
    Mar 29 07:53:34.377: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8wjp" in namespace "subpath-3774" to be "Succeeded or Failed"
    Mar 29 07:53:34.378: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.155761ms
    Mar 29 07:53:36.380: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.003331622s
    Mar 29 07:53:38.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.003443832s
    Mar 29 07:53:40.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 6.00347271s
    Mar 29 07:53:42.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 8.004423974s
    Mar 29 07:53:44.380: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 10.003270344s
    Mar 29 07:53:46.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 12.004308771s
    Mar 29 07:53:48.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 14.003490928s
    Mar 29 07:53:50.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 16.004526639s
    Mar 29 07:53:52.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00388098s
    Mar 29 07:53:54.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=true. Elapsed: 20.003620254s
    Mar 29 07:53:56.382: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Running", Reason="", readiness=false. Elapsed: 22.004687116s
    Mar 29 07:53:58.381: INFO: Pod "pod-subpath-test-secret-8wjp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003490909s
    STEP: Saw pod success 03/29/23 07:53:58.381
    Mar 29 07:53:58.381: INFO: Pod "pod-subpath-test-secret-8wjp" satisfied condition "Succeeded or Failed"
    Mar 29 07:53:58.382: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-secret-8wjp container test-container-subpath-secret-8wjp: <nil>
    STEP: delete the pod 03/29/23 07:53:58.385
    Mar 29 07:53:58.390: INFO: Waiting for pod pod-subpath-test-secret-8wjp to disappear
    Mar 29 07:53:58.391: INFO: Pod pod-subpath-test-secret-8wjp no longer exists
    STEP: Deleting pod pod-subpath-test-secret-8wjp 03/29/23 07:53:58.391
    Mar 29 07:53:58.392: INFO: Deleting pod "pod-subpath-test-secret-8wjp" in namespace "subpath-3774"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 29 07:53:58.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3774" for this suite. 03/29/23 07:53:58.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:53:58.397
Mar 29 07:53:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replication-controller 03/29/23 07:53:58.398
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:58.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:58.406
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/29/23 07:53:58.408
Mar 29 07:53:58.410: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5832" to be "running and ready"
Mar 29 07:53:58.412: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.143631ms
Mar 29 07:53:58.412: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:54:00.415: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004222539s
Mar 29 07:54:00.415: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 29 07:54:00.415: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/29/23 07:54:00.416
STEP: Then the orphan pod is adopted 03/29/23 07:54:00.419
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 29 07:54:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5832" for this suite. 03/29/23 07:54:01.424
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":26,"skipped":487,"failed":0}
------------------------------
â€¢ [3.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:53:58.397
    Mar 29 07:53:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replication-controller 03/29/23 07:53:58.398
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:53:58.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:53:58.406
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/29/23 07:53:58.408
    Mar 29 07:53:58.410: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5832" to be "running and ready"
    Mar 29 07:53:58.412: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.143631ms
    Mar 29 07:53:58.412: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:54:00.415: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004222539s
    Mar 29 07:54:00.415: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 29 07:54:00.415: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/29/23 07:54:00.416
    STEP: Then the orphan pod is adopted 03/29/23 07:54:00.419
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 29 07:54:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5832" for this suite. 03/29/23 07:54:01.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:54:01.427
Mar 29 07:54:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 07:54:01.428
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:54:01.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:54:01.436
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/29/23 07:54:01.437
Mar 29 07:54:01.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 29 07:54:01.480: INFO: stderr: ""
Mar 29 07:54:01.480: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/29/23 07:54:01.48
Mar 29 07:54:01.480: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 29 07:54:01.480: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3018" to be "running and ready, or succeeded"
Mar 29 07:54:01.481: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231951ms
Mar 29 07:54:01.481: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.146.0.115' to be 'Running' but was 'Pending'
Mar 29 07:54:03.484: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.003693339s
Mar 29 07:54:03.484: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 29 07:54:03.484: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/29/23 07:54:03.484
Mar 29 07:54:03.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator'
Mar 29 07:54:03.530: INFO: stderr: ""
Mar 29 07:54:03.530: INFO: stdout: "I0329 07:54:02.032137       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/f79 453\nI0329 07:54:02.232255       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/89n 568\nI0329 07:54:02.432756       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w8b 378\nI0329 07:54:02.633056       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/jvd 404\nI0329 07:54:02.832301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flz 274\nI0329 07:54:03.032604       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xbtk 252\nI0329 07:54:03.232904       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5sv2 573\nI0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
STEP: limiting log lines 03/29/23 07:54:03.53
Mar 29 07:54:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --tail=1'
Mar 29 07:54:03.574: INFO: stderr: ""
Mar 29 07:54:03.574: INFO: stdout: "I0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
Mar 29 07:54:03.574: INFO: got output "I0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
STEP: limiting log bytes 03/29/23 07:54:03.574
Mar 29 07:54:03.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --limit-bytes=1'
Mar 29 07:54:03.617: INFO: stderr: ""
Mar 29 07:54:03.617: INFO: stdout: "I"
Mar 29 07:54:03.617: INFO: got output "I"
STEP: exposing timestamps 03/29/23 07:54:03.617
Mar 29 07:54:03.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 29 07:54:03.659: INFO: stderr: ""
Mar 29 07:54:03.659: INFO: stdout: "2023-03-29T07:54:03.632569689Z I0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\n"
Mar 29 07:54:03.659: INFO: got output "2023-03-29T07:54:03.632569689Z I0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\n"
STEP: restricting to a time range 03/29/23 07:54:03.659
Mar 29 07:54:06.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --since=1s'
Mar 29 07:54:06.204: INFO: stderr: ""
Mar 29 07:54:06.204: INFO: stdout: "I0329 07:54:05.232810       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8xz 207\nI0329 07:54:05.433110       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lmdd 219\nI0329 07:54:05.632350       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/r96 421\nI0329 07:54:05.832660       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/6khk 585\nI0329 07:54:06.032969       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/vd4n 363\n"
Mar 29 07:54:06.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --since=24h'
Mar 29 07:54:06.246: INFO: stderr: ""
Mar 29 07:54:06.246: INFO: stdout: "I0329 07:54:02.032137       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/f79 453\nI0329 07:54:02.232255       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/89n 568\nI0329 07:54:02.432756       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w8b 378\nI0329 07:54:02.633056       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/jvd 404\nI0329 07:54:02.832301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flz 274\nI0329 07:54:03.032604       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xbtk 252\nI0329 07:54:03.232904       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5sv2 573\nI0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\nI0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\nI0329 07:54:03.832812       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/nmrk 220\nI0329 07:54:04.033113       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/c8j 244\nI0329 07:54:04.232356       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/95c 539\nI0329 07:54:04.432652       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/c99g 556\nI0329 07:54:04.632961       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/jdzj 495\nI0329 07:54:04.832217       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/k5k 378\nI0329 07:54:05.032510       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/nr5w 481\nI0329 07:54:05.232810       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8xz 207\nI0329 07:54:05.433110       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lmdd 219\nI0329 07:54:05.632350       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/r96 421\nI0329 07:54:05.832660       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/6khk 585\nI0329 07:54:06.032969       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/vd4n 363\nI0329 07:54:06.232224       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v45 395\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar 29 07:54:06.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 delete pod logs-generator'
Mar 29 07:54:06.640: INFO: stderr: ""
Mar 29 07:54:06.640: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 07:54:06.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3018" for this suite. 03/29/23 07:54:06.642
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":27,"skipped":498,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.217 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:54:01.427
    Mar 29 07:54:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 07:54:01.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:54:01.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:54:01.436
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/29/23 07:54:01.437
    Mar 29 07:54:01.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 29 07:54:01.480: INFO: stderr: ""
    Mar 29 07:54:01.480: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/29/23 07:54:01.48
    Mar 29 07:54:01.480: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 29 07:54:01.480: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3018" to be "running and ready, or succeeded"
    Mar 29 07:54:01.481: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231951ms
    Mar 29 07:54:01.481: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.146.0.115' to be 'Running' but was 'Pending'
    Mar 29 07:54:03.484: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.003693339s
    Mar 29 07:54:03.484: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 29 07:54:03.484: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/29/23 07:54:03.484
    Mar 29 07:54:03.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator'
    Mar 29 07:54:03.530: INFO: stderr: ""
    Mar 29 07:54:03.530: INFO: stdout: "I0329 07:54:02.032137       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/f79 453\nI0329 07:54:02.232255       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/89n 568\nI0329 07:54:02.432756       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w8b 378\nI0329 07:54:02.633056       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/jvd 404\nI0329 07:54:02.832301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flz 274\nI0329 07:54:03.032604       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xbtk 252\nI0329 07:54:03.232904       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5sv2 573\nI0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
    STEP: limiting log lines 03/29/23 07:54:03.53
    Mar 29 07:54:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --tail=1'
    Mar 29 07:54:03.574: INFO: stderr: ""
    Mar 29 07:54:03.574: INFO: stdout: "I0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
    Mar 29 07:54:03.574: INFO: got output "I0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\n"
    STEP: limiting log bytes 03/29/23 07:54:03.574
    Mar 29 07:54:03.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --limit-bytes=1'
    Mar 29 07:54:03.617: INFO: stderr: ""
    Mar 29 07:54:03.617: INFO: stdout: "I"
    Mar 29 07:54:03.617: INFO: got output "I"
    STEP: exposing timestamps 03/29/23 07:54:03.617
    Mar 29 07:54:03.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 29 07:54:03.659: INFO: stderr: ""
    Mar 29 07:54:03.659: INFO: stdout: "2023-03-29T07:54:03.632569689Z I0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\n"
    Mar 29 07:54:03.659: INFO: got output "2023-03-29T07:54:03.632569689Z I0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\n"
    STEP: restricting to a time range 03/29/23 07:54:03.659
    Mar 29 07:54:06.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --since=1s'
    Mar 29 07:54:06.204: INFO: stderr: ""
    Mar 29 07:54:06.204: INFO: stdout: "I0329 07:54:05.232810       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8xz 207\nI0329 07:54:05.433110       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lmdd 219\nI0329 07:54:05.632350       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/r96 421\nI0329 07:54:05.832660       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/6khk 585\nI0329 07:54:06.032969       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/vd4n 363\n"
    Mar 29 07:54:06.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 logs logs-generator logs-generator --since=24h'
    Mar 29 07:54:06.246: INFO: stderr: ""
    Mar 29 07:54:06.246: INFO: stdout: "I0329 07:54:02.032137       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/f79 453\nI0329 07:54:02.232255       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/89n 568\nI0329 07:54:02.432756       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w8b 378\nI0329 07:54:02.633056       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/jvd 404\nI0329 07:54:02.832301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/flz 274\nI0329 07:54:03.032604       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/xbtk 252\nI0329 07:54:03.232904       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5sv2 573\nI0329 07:54:03.433224       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/9tpr 249\nI0329 07:54:03.632504       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4h4g 427\nI0329 07:54:03.832812       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/nmrk 220\nI0329 07:54:04.033113       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/c8j 244\nI0329 07:54:04.232356       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/95c 539\nI0329 07:54:04.432652       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/c99g 556\nI0329 07:54:04.632961       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/jdzj 495\nI0329 07:54:04.832217       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/k5k 378\nI0329 07:54:05.032510       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/nr5w 481\nI0329 07:54:05.232810       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/8xz 207\nI0329 07:54:05.433110       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lmdd 219\nI0329 07:54:05.632350       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/r96 421\nI0329 07:54:05.832660       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/6khk 585\nI0329 07:54:06.032969       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/vd4n 363\nI0329 07:54:06.232224       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v45 395\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar 29 07:54:06.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-3018 delete pod logs-generator'
    Mar 29 07:54:06.640: INFO: stderr: ""
    Mar 29 07:54:06.640: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 07:54:06.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3018" for this suite. 03/29/23 07:54:06.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:54:06.646
Mar 29 07:54:06.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename taint-single-pod 03/29/23 07:54:06.647
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:54:06.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:54:06.656
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar 29 07:54:06.658: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 07:55:06.672: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar 29 07:55:06.673: INFO: Starting informer...
STEP: Starting pod... 03/29/23 07:55:06.673
Mar 29 07:55:06.880: INFO: Pod is running on 10.146.0.116. Tainting Node
STEP: Trying to apply a taint on the Node 03/29/23 07:55:06.88
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 07:55:06.886
STEP: Waiting short time to make sure Pod is queued for deletion 03/29/23 07:55:06.888
Mar 29 07:55:06.888: INFO: Pod wasn't evicted. Proceeding
Mar 29 07:55:06.888: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 07:55:06.893
STEP: Waiting some time to make sure that toleration time passed. 03/29/23 07:55:06.895
Mar 29 07:56:21.898: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar 29 07:56:21.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9879" for this suite. 03/29/23 07:56:21.9
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":28,"skipped":523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.257 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:54:06.646
    Mar 29 07:54:06.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename taint-single-pod 03/29/23 07:54:06.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:54:06.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:54:06.656
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar 29 07:54:06.658: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 07:55:06.672: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar 29 07:55:06.673: INFO: Starting informer...
    STEP: Starting pod... 03/29/23 07:55:06.673
    Mar 29 07:55:06.880: INFO: Pod is running on 10.146.0.116. Tainting Node
    STEP: Trying to apply a taint on the Node 03/29/23 07:55:06.88
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 07:55:06.886
    STEP: Waiting short time to make sure Pod is queued for deletion 03/29/23 07:55:06.888
    Mar 29 07:55:06.888: INFO: Pod wasn't evicted. Proceeding
    Mar 29 07:55:06.888: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 07:55:06.893
    STEP: Waiting some time to make sure that toleration time passed. 03/29/23 07:55:06.895
    Mar 29 07:56:21.898: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 07:56:21.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-9879" for this suite. 03/29/23 07:56:21.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:21.904
Mar 29 07:56:21.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 07:56:21.905
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:21.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:21.912
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar 29 07:56:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 create -f -'
Mar 29 07:56:22.306: INFO: stderr: ""
Mar 29 07:56:22.306: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 29 07:56:22.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 create -f -'
Mar 29 07:56:22.438: INFO: stderr: ""
Mar 29 07:56:22.438: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/29/23 07:56:22.438
Mar 29 07:56:23.440: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 07:56:23.440: INFO: Found 0 / 1
Mar 29 07:56:24.441: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 07:56:24.441: INFO: Found 1 / 1
Mar 29 07:56:24.441: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 29 07:56:24.442: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 07:56:24.442: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 07:56:24.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe pod agnhost-primary-mt664'
Mar 29 07:56:24.488: INFO: stderr: ""
Mar 29 07:56:24.488: INFO: stdout: "Name:             agnhost-primary-mt664\nNamespace:        kubectl-7188\nPriority:         0\nService Account:  default\nNode:             10.146.0.115/10.146.0.115\nStart Time:       Wed, 29 Mar 2023 07:56:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0184ea581c41dc5cdd2892cec909019d622b830cdd7d6a30924c9955f9eb518d\n                  cni.projectcalico.org/podIP: 192.168.30.20/32\n                  cni.projectcalico.org/podIPs: 192.168.30.20/32\nStatus:           Running\nIP:               192.168.30.20\nIPs:\n  IP:           192.168.30.20\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1a3efdf7b0e06216e1bd0cdd0ecf8125ac6c83664ebfaff8c5869561fed2c552\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 29 Mar 2023 07:56:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktxsm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ktxsm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7188/agnhost-primary-mt664 to 10.146.0.115\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar 29 07:56:24.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe rc agnhost-primary'
Mar 29 07:56:24.532: INFO: stderr: ""
Mar 29 07:56:24.532: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7188\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-mt664\n"
Mar 29 07:56:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe service agnhost-primary'
Mar 29 07:56:24.579: INFO: stderr: ""
Mar 29 07:56:24.579: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7188\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.17.116\nIPs:               10.100.17.116\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.30.20:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 29 07:56:24.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe node 10.146.0.115'
Mar 29 07:56:24.637: INFO: stderr: ""
Mar 29 07:56:24.637: INFO: stdout: "Name:               10.146.0.115\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cke.cybozu.com/master=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.146.0.115\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.146.0.115/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.30.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 Mar 2023 07:43:13 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.146.0.115\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 29 Mar 2023 07:56:20 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 29 Mar 2023 07:44:05 +0000   Wed, 29 Mar 2023 07:44:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:44:04 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.146.0.115\n  Hostname:    10.146.0.115\nCapacity:\n  cpu:                8\n  ephemeral-storage:  17885708Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32876480Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  16483468466\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32774080Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 aa7ecf95cebb0a809b15dfe8d17731d4\n  System UUID:                aa7ecf95-cebb-0a80-9b15-dfe8d17731d4\n  Boot ID:                    b8483039-18ca-47a4-9034-3dcf6730342c\n  Kernel Version:             5.15.92-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3374.2.5 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-8mtw5                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         12m\n  kube-system                 cluster-dns-77bdb56774-p98xx                               100m (1%)     0 (0%)      70Mi (0%)        0 (0%)         12m\n  kube-system                 node-dns-jqb4n                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\n  kubectl-7188                agnhost-primary-mt664                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                350m (4%)  0 (0%)\n  memory             70Mi (0%)  0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type     Reason                   Age                    From             Message\n  ----     ------                   ----                   ----             -------\n  Normal   Starting                 13m                    kube-proxy       \n  Normal   Starting                 13m                    kubelet          Starting kubelet.\n  Normal   NodeAllocatableEnforced  13m                    kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           12m                    node-controller  Node 10.146.0.115 event: Registered Node 10.146.0.115 in Controller\n  Normal   NodeReady                12m                    kubelet          Node 10.146.0.115 status is now: NodeReady\n  Warning  InvalidDiskCapacity      3m13s (x3 over 13m)    kubelet          invalid capacity 0 on image filesystem\n  Warning  ImageGCFailed            3m13s (x2 over 8m13s)  kubelet          invalid capacity 0 on image filesystem\n"
Mar 29 07:56:24.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe namespace kubectl-7188'
Mar 29 07:56:24.680: INFO: stderr: ""
Mar 29 07:56:24.680: INFO: stdout: "Name:         kubectl-7188\nLabels:       e2e-framework=kubectl\n              e2e-run=5439d215-a232-4dfe-9691-4ba97c11aa52\n              kubernetes.io/metadata.name=kubectl-7188\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 07:56:24.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7188" for this suite. 03/29/23 07:56:24.682
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":29,"skipped":548,"failed":0}
------------------------------
â€¢ [2.781 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:21.904
    Mar 29 07:56:21.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 07:56:21.905
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:21.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:21.912
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar 29 07:56:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 create -f -'
    Mar 29 07:56:22.306: INFO: stderr: ""
    Mar 29 07:56:22.306: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 29 07:56:22.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 create -f -'
    Mar 29 07:56:22.438: INFO: stderr: ""
    Mar 29 07:56:22.438: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/29/23 07:56:22.438
    Mar 29 07:56:23.440: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 07:56:23.440: INFO: Found 0 / 1
    Mar 29 07:56:24.441: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 07:56:24.441: INFO: Found 1 / 1
    Mar 29 07:56:24.441: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 29 07:56:24.442: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 07:56:24.442: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 29 07:56:24.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe pod agnhost-primary-mt664'
    Mar 29 07:56:24.488: INFO: stderr: ""
    Mar 29 07:56:24.488: INFO: stdout: "Name:             agnhost-primary-mt664\nNamespace:        kubectl-7188\nPriority:         0\nService Account:  default\nNode:             10.146.0.115/10.146.0.115\nStart Time:       Wed, 29 Mar 2023 07:56:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0184ea581c41dc5cdd2892cec909019d622b830cdd7d6a30924c9955f9eb518d\n                  cni.projectcalico.org/podIP: 192.168.30.20/32\n                  cni.projectcalico.org/podIPs: 192.168.30.20/32\nStatus:           Running\nIP:               192.168.30.20\nIPs:\n  IP:           192.168.30.20\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1a3efdf7b0e06216e1bd0cdd0ecf8125ac6c83664ebfaff8c5869561fed2c552\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 29 Mar 2023 07:56:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ktxsm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ktxsm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7188/agnhost-primary-mt664 to 10.146.0.115\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Mar 29 07:56:24.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe rc agnhost-primary'
    Mar 29 07:56:24.532: INFO: stderr: ""
    Mar 29 07:56:24.532: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7188\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-mt664\n"
    Mar 29 07:56:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe service agnhost-primary'
    Mar 29 07:56:24.579: INFO: stderr: ""
    Mar 29 07:56:24.579: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7188\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.17.116\nIPs:               10.100.17.116\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.30.20:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 29 07:56:24.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe node 10.146.0.115'
    Mar 29 07:56:24.637: INFO: stderr: ""
    Mar 29 07:56:24.637: INFO: stdout: "Name:               10.146.0.115\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cke.cybozu.com/master=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.146.0.115\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.146.0.115/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.30.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 Mar 2023 07:43:13 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.146.0.115\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 29 Mar 2023 07:56:20 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 29 Mar 2023 07:44:05 +0000   Wed, 29 Mar 2023 07:44:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:43:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 29 Mar 2023 07:53:15 +0000   Wed, 29 Mar 2023 07:44:04 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.146.0.115\n  Hostname:    10.146.0.115\nCapacity:\n  cpu:                8\n  ephemeral-storage:  17885708Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32876480Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  16483468466\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32774080Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 aa7ecf95cebb0a809b15dfe8d17731d4\n  System UUID:                aa7ecf95-cebb-0a80-9b15-dfe8d17731d4\n  Boot ID:                    b8483039-18ca-47a4-9034-3dcf6730342c\n  Kernel Version:             5.15.92-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3374.2.5 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-8mtw5                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         12m\n  kube-system                 cluster-dns-77bdb56774-p98xx                               100m (1%)     0 (0%)      70Mi (0%)        0 (0%)         12m\n  kube-system                 node-dns-jqb4n                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\n  kubectl-7188                agnhost-primary-mt664                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                350m (4%)  0 (0%)\n  memory             70Mi (0%)  0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type     Reason                   Age                    From             Message\n  ----     ------                   ----                   ----             -------\n  Normal   Starting                 13m                    kube-proxy       \n  Normal   Starting                 13m                    kubelet          Starting kubelet.\n  Normal   NodeAllocatableEnforced  13m                    kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     13m (x5 over 13m)      kubelet          Node 10.146.0.115 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           12m                    node-controller  Node 10.146.0.115 event: Registered Node 10.146.0.115 in Controller\n  Normal   NodeReady                12m                    kubelet          Node 10.146.0.115 status is now: NodeReady\n  Warning  InvalidDiskCapacity      3m13s (x3 over 13m)    kubelet          invalid capacity 0 on image filesystem\n  Warning  ImageGCFailed            3m13s (x2 over 8m13s)  kubelet          invalid capacity 0 on image filesystem\n"
    Mar 29 07:56:24.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7188 describe namespace kubectl-7188'
    Mar 29 07:56:24.680: INFO: stderr: ""
    Mar 29 07:56:24.680: INFO: stdout: "Name:         kubectl-7188\nLabels:       e2e-framework=kubectl\n              e2e-run=5439d215-a232-4dfe-9691-4ba97c11aa52\n              kubernetes.io/metadata.name=kubectl-7188\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 07:56:24.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7188" for this suite. 03/29/23 07:56:24.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:24.686
Mar 29 07:56:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 07:56:24.686
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:24.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:24.693
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 07:56:24.699
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 07:56:24.881
STEP: Deploying the webhook pod 03/29/23 07:56:24.885
STEP: Wait for the deployment to be ready 03/29/23 07:56:24.89
Mar 29 07:56:24.894: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 07:56:26.9
STEP: Verifying the service has paired with the endpoint 03/29/23 07:56:26.906
Mar 29 07:56:27.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar 29 07:56:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6673-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 07:56:28.415
STEP: Creating a custom resource that should be mutated by the webhook 03/29/23 07:56:28.424
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:56:30.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9410" for this suite. 03/29/23 07:56:30.959
STEP: Destroying namespace "webhook-9410-markers" for this suite. 03/29/23 07:56:30.962
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":30,"skipped":584,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.293 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:24.686
    Mar 29 07:56:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 07:56:24.686
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:24.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:24.693
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 07:56:24.699
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 07:56:24.881
    STEP: Deploying the webhook pod 03/29/23 07:56:24.885
    STEP: Wait for the deployment to be ready 03/29/23 07:56:24.89
    Mar 29 07:56:24.894: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 07:56:26.9
    STEP: Verifying the service has paired with the endpoint 03/29/23 07:56:26.906
    Mar 29 07:56:27.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar 29 07:56:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6673-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 07:56:28.415
    STEP: Creating a custom resource that should be mutated by the webhook 03/29/23 07:56:28.424
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:56:30.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9410" for this suite. 03/29/23 07:56:30.959
    STEP: Destroying namespace "webhook-9410-markers" for this suite. 03/29/23 07:56:30.962
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:30.98
Mar 29 07:56:30.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 07:56:30.98
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:30.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:30.99
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/29/23 07:56:30.991
Mar 29 07:56:30.991: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 29 07:56:30.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:31.613: INFO: stderr: ""
Mar 29 07:56:31.613: INFO: stdout: "service/agnhost-replica created\n"
Mar 29 07:56:31.613: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 29 07:56:31.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:32.227: INFO: stderr: ""
Mar 29 07:56:32.227: INFO: stdout: "service/agnhost-primary created\n"
Mar 29 07:56:32.227: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 29 07:56:32.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:32.351: INFO: stderr: ""
Mar 29 07:56:32.351: INFO: stdout: "service/frontend created\n"
Mar 29 07:56:32.351: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 29 07:56:32.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:32.474: INFO: stderr: ""
Mar 29 07:56:32.474: INFO: stdout: "deployment.apps/frontend created\n"
Mar 29 07:56:32.474: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 29 07:56:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:32.599: INFO: stderr: ""
Mar 29 07:56:32.599: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 29 07:56:32.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 29 07:56:32.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
Mar 29 07:56:32.719: INFO: stderr: ""
Mar 29 07:56:32.719: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/29/23 07:56:32.719
Mar 29 07:56:32.719: INFO: Waiting for all frontend pods to be Running.
Mar 29 07:56:37.773: INFO: Waiting for frontend to serve content.
Mar 29 07:56:37.780: INFO: Trying to add a new entry to the guestbook.
Mar 29 07:56:37.785: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/29/23 07:56:37.789
Mar 29 07:56:37.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:37.837: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:37.837: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/29/23 07:56:37.837
Mar 29 07:56:37.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:37.888: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:37.889: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/29/23 07:56:37.889
Mar 29 07:56:37.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:37.935: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:37.935: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/29/23 07:56:37.935
Mar 29 07:56:37.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:37.981: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:37.981: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/29/23 07:56:37.982
Mar 29 07:56:37.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:38.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:38.028: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/29/23 07:56:38.028
Mar 29 07:56:38.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
Mar 29 07:56:38.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 07:56:38.075: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 07:56:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2471" for this suite. 03/29/23 07:56:38.077
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":31,"skipped":600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.099 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:30.98
    Mar 29 07:56:30.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 07:56:30.98
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:30.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:30.99
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/29/23 07:56:30.991
    Mar 29 07:56:30.991: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 29 07:56:30.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:31.613: INFO: stderr: ""
    Mar 29 07:56:31.613: INFO: stdout: "service/agnhost-replica created\n"
    Mar 29 07:56:31.613: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 29 07:56:31.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:32.227: INFO: stderr: ""
    Mar 29 07:56:32.227: INFO: stdout: "service/agnhost-primary created\n"
    Mar 29 07:56:32.227: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 29 07:56:32.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:32.351: INFO: stderr: ""
    Mar 29 07:56:32.351: INFO: stdout: "service/frontend created\n"
    Mar 29 07:56:32.351: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 29 07:56:32.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:32.474: INFO: stderr: ""
    Mar 29 07:56:32.474: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 29 07:56:32.474: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 29 07:56:32.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:32.599: INFO: stderr: ""
    Mar 29 07:56:32.599: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 29 07:56:32.599: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 29 07:56:32.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 create -f -'
    Mar 29 07:56:32.719: INFO: stderr: ""
    Mar 29 07:56:32.719: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/29/23 07:56:32.719
    Mar 29 07:56:32.719: INFO: Waiting for all frontend pods to be Running.
    Mar 29 07:56:37.773: INFO: Waiting for frontend to serve content.
    Mar 29 07:56:37.780: INFO: Trying to add a new entry to the guestbook.
    Mar 29 07:56:37.785: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/29/23 07:56:37.789
    Mar 29 07:56:37.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:37.837: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:37.837: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/29/23 07:56:37.837
    Mar 29 07:56:37.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:37.888: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:37.889: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/29/23 07:56:37.889
    Mar 29 07:56:37.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:37.935: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:37.935: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/29/23 07:56:37.935
    Mar 29 07:56:37.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:37.981: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:37.981: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/29/23 07:56:37.982
    Mar 29 07:56:37.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:38.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:38.028: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/29/23 07:56:38.028
    Mar 29 07:56:38.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
    Mar 29 07:56:38.075: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 07:56:38.075: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 07:56:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2471" for this suite. 03/29/23 07:56:38.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:38.079
Mar 29 07:56:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:56:38.08
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:38.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:38.095
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/29/23 07:56:38.097
Mar 29 07:56:38.102: INFO: Waiting up to 5m0s for pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41" in namespace "emptydir-9245" to be "Succeeded or Failed"
Mar 29 07:56:38.103: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552649ms
Mar 29 07:56:40.107: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004931903s
Mar 29 07:56:42.106: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003886414s
STEP: Saw pod success 03/29/23 07:56:42.106
Mar 29 07:56:42.106: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41" satisfied condition "Succeeded or Failed"
Mar 29 07:56:42.107: INFO: Trying to get logs from node 10.146.0.115 pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 container test-container: <nil>
STEP: delete the pod 03/29/23 07:56:42.114
Mar 29 07:56:42.120: INFO: Waiting for pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 to disappear
Mar 29 07:56:42.121: INFO: Pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:56:42.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9245" for this suite. 03/29/23 07:56:42.123
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":605,"failed":0}
------------------------------
â€¢ [4.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:38.079
    Mar 29 07:56:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:56:38.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:38.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:38.095
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/29/23 07:56:38.097
    Mar 29 07:56:38.102: INFO: Waiting up to 5m0s for pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41" in namespace "emptydir-9245" to be "Succeeded or Failed"
    Mar 29 07:56:38.103: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552649ms
    Mar 29 07:56:40.107: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004931903s
    Mar 29 07:56:42.106: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003886414s
    STEP: Saw pod success 03/29/23 07:56:42.106
    Mar 29 07:56:42.106: INFO: Pod "pod-7082b652-f75c-4e16-9352-8b4c3a48aa41" satisfied condition "Succeeded or Failed"
    Mar 29 07:56:42.107: INFO: Trying to get logs from node 10.146.0.115 pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 container test-container: <nil>
    STEP: delete the pod 03/29/23 07:56:42.114
    Mar 29 07:56:42.120: INFO: Waiting for pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 to disappear
    Mar 29 07:56:42.121: INFO: Pod pod-7082b652-f75c-4e16-9352-8b4c3a48aa41 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:56:42.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9245" for this suite. 03/29/23 07:56:42.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:42.127
Mar 29 07:56:42.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 07:56:42.127
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:42.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:42.134
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 29 07:56:42.139: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 29 07:56:47.144: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 07:56:47.144
Mar 29 07:56:47.144: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/29/23 07:56:47.148
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 07:56:47.152: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1170  d1bda599-368d-48c9-bfdb-9d8259f54698 3695 1 2023-03-29 07:56:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-29 07:56:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372bc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 29 07:56:47.153: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 07:56:47.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1170" for this suite. 03/29/23 07:56:47.16
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":33,"skipped":638,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.037 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:42.127
    Mar 29 07:56:42.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 07:56:42.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:42.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:42.134
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 29 07:56:42.139: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 29 07:56:47.144: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 07:56:47.144
    Mar 29 07:56:47.144: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/29/23 07:56:47.148
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 07:56:47.152: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1170  d1bda599-368d-48c9-bfdb-9d8259f54698 3695 1 2023-03-29 07:56:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-29 07:56:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00372bc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 29 07:56:47.153: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 07:56:47.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1170" for this suite. 03/29/23 07:56:47.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:47.164
Mar 29 07:56:47.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 07:56:47.165
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:47.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:47.175
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-49e65969-bfb2-4365-a91e-fadf8fea54eb 03/29/23 07:56:47.176
STEP: Creating a pod to test consume secrets 03/29/23 07:56:47.178
Mar 29 07:56:47.182: INFO: Waiting up to 5m0s for pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0" in namespace "secrets-956" to be "Succeeded or Failed"
Mar 29 07:56:47.184: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589001ms
Mar 29 07:56:49.186: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003717653s
Mar 29 07:56:51.187: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00479553s
STEP: Saw pod success 03/29/23 07:56:51.187
Mar 29 07:56:51.187: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0" satisfied condition "Succeeded or Failed"
Mar 29 07:56:51.188: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 07:56:51.195
Mar 29 07:56:51.201: INFO: Waiting for pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 to disappear
Mar 29 07:56:51.203: INFO: Pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 07:56:51.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-956" for this suite. 03/29/23 07:56:51.204
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":652,"failed":0}
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:47.164
    Mar 29 07:56:47.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 07:56:47.165
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:47.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:47.175
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-49e65969-bfb2-4365-a91e-fadf8fea54eb 03/29/23 07:56:47.176
    STEP: Creating a pod to test consume secrets 03/29/23 07:56:47.178
    Mar 29 07:56:47.182: INFO: Waiting up to 5m0s for pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0" in namespace "secrets-956" to be "Succeeded or Failed"
    Mar 29 07:56:47.184: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589001ms
    Mar 29 07:56:49.186: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003717653s
    Mar 29 07:56:51.187: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00479553s
    STEP: Saw pod success 03/29/23 07:56:51.187
    Mar 29 07:56:51.187: INFO: Pod "pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0" satisfied condition "Succeeded or Failed"
    Mar 29 07:56:51.188: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 07:56:51.195
    Mar 29 07:56:51.201: INFO: Waiting for pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 to disappear
    Mar 29 07:56:51.203: INFO: Pod pod-secrets-00e514ae-5b5a-4334-a11c-9088031778c0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 07:56:51.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-956" for this suite. 03/29/23 07:56:51.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:51.208
Mar 29 07:56:51.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 07:56:51.209
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:51.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:51.215
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-48b9fd40-e5fc-4bba-a823-39e9fa01f732 03/29/23 07:56:51.216
STEP: Creating a pod to test consume configMaps 03/29/23 07:56:51.218
Mar 29 07:56:51.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9" in namespace "configmap-4799" to be "Succeeded or Failed"
Mar 29 07:56:51.223: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.09945ms
Mar 29 07:56:53.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003373033s
Mar 29 07:56:55.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0034864s
STEP: Saw pod success 03/29/23 07:56:55.225
Mar 29 07:56:55.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9" satisfied condition "Succeeded or Failed"
Mar 29 07:56:55.227: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 container configmap-volume-test: <nil>
STEP: delete the pod 03/29/23 07:56:55.23
Mar 29 07:56:55.237: INFO: Waiting for pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 to disappear
Mar 29 07:56:55.239: INFO: Pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 07:56:55.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4799" for this suite. 03/29/23 07:56:55.241
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":35,"skipped":699,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:51.208
    Mar 29 07:56:51.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 07:56:51.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:51.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:51.215
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-48b9fd40-e5fc-4bba-a823-39e9fa01f732 03/29/23 07:56:51.216
    STEP: Creating a pod to test consume configMaps 03/29/23 07:56:51.218
    Mar 29 07:56:51.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9" in namespace "configmap-4799" to be "Succeeded or Failed"
    Mar 29 07:56:51.223: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.09945ms
    Mar 29 07:56:53.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003373033s
    Mar 29 07:56:55.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0034864s
    STEP: Saw pod success 03/29/23 07:56:55.225
    Mar 29 07:56:55.225: INFO: Pod "pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9" satisfied condition "Succeeded or Failed"
    Mar 29 07:56:55.227: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 container configmap-volume-test: <nil>
    STEP: delete the pod 03/29/23 07:56:55.23
    Mar 29 07:56:55.237: INFO: Waiting for pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 to disappear
    Mar 29 07:56:55.239: INFO: Pod pod-configmaps-b403f8f8-14f4-4e2e-beb4-2bd9570fc1d9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 07:56:55.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4799" for this suite. 03/29/23 07:56:55.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:55.245
Mar 29 07:56:55.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 07:56:55.246
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:55.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:55.254
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/29/23 07:56:55.255
Mar 29 07:56:55.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52" in namespace "downward-api-2621" to be "Succeeded or Failed"
Mar 29 07:56:55.260: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.14135ms
Mar 29 07:56:57.262: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003110398s
Mar 29 07:56:59.263: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003909256s
STEP: Saw pod success 03/29/23 07:56:59.263
Mar 29 07:56:59.263: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52" satisfied condition "Succeeded or Failed"
Mar 29 07:56:59.264: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 container client-container: <nil>
STEP: delete the pod 03/29/23 07:56:59.267
Mar 29 07:56:59.274: INFO: Waiting for pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 to disappear
Mar 29 07:56:59.276: INFO: Pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 07:56:59.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2621" for this suite. 03/29/23 07:56:59.278
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":36,"skipped":704,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:55.245
    Mar 29 07:56:55.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 07:56:55.246
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:55.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:55.254
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/29/23 07:56:55.255
    Mar 29 07:56:55.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52" in namespace "downward-api-2621" to be "Succeeded or Failed"
    Mar 29 07:56:55.260: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.14135ms
    Mar 29 07:56:57.262: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003110398s
    Mar 29 07:56:59.263: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003909256s
    STEP: Saw pod success 03/29/23 07:56:59.263
    Mar 29 07:56:59.263: INFO: Pod "downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52" satisfied condition "Succeeded or Failed"
    Mar 29 07:56:59.264: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 container client-container: <nil>
    STEP: delete the pod 03/29/23 07:56:59.267
    Mar 29 07:56:59.274: INFO: Waiting for pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 to disappear
    Mar 29 07:56:59.276: INFO: Pod downwardapi-volume-4572d901-225c-4593-bea5-d0964109db52 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 07:56:59.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2621" for this suite. 03/29/23 07:56:59.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:56:59.281
Mar 29 07:56:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-runtime 03/29/23 07:56:59.282
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:59.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:59.29
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/29/23 07:56:59.291
STEP: wait for the container to reach Failed 03/29/23 07:56:59.295
STEP: get the container status 03/29/23 07:57:02.302
STEP: the container should be terminated 03/29/23 07:57:02.303
STEP: the termination message should be set 03/29/23 07:57:02.303
Mar 29 07:57:02.303: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/29/23 07:57:02.303
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 29 07:57:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6345" for this suite. 03/29/23 07:57:02.313
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":37,"skipped":714,"failed":0}
------------------------------
â€¢ [3.034 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:56:59.281
    Mar 29 07:56:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-runtime 03/29/23 07:56:59.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:56:59.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:56:59.29
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/29/23 07:56:59.291
    STEP: wait for the container to reach Failed 03/29/23 07:56:59.295
    STEP: get the container status 03/29/23 07:57:02.302
    STEP: the container should be terminated 03/29/23 07:57:02.303
    STEP: the termination message should be set 03/29/23 07:57:02.303
    Mar 29 07:57:02.303: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/29/23 07:57:02.303
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 29 07:57:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6345" for this suite. 03/29/23 07:57:02.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:02.317
Mar 29 07:57:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 07:57:02.318
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:02.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:02.326
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar 29 07:57:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/29/23 07:57:04.159
Mar 29 07:57:04.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
Mar 29 07:57:04.594: INFO: stderr: ""
Mar 29 07:57:04.594: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 29 07:57:04.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 delete e2e-test-crd-publish-openapi-2457-crds test-foo'
Mar 29 07:57:04.636: INFO: stderr: ""
Mar 29 07:57:04.636: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 29 07:57:04.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
Mar 29 07:57:04.760: INFO: stderr: ""
Mar 29 07:57:04.760: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 29 07:57:04.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 delete e2e-test-crd-publish-openapi-2457-crds test-foo'
Mar 29 07:57:04.801: INFO: stderr: ""
Mar 29 07:57:04.801: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/29/23 07:57:04.801
Mar 29 07:57:04.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
Mar 29 07:57:04.922: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/29/23 07:57:04.922
Mar 29 07:57:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
Mar 29 07:57:05.043: INFO: rc: 1
Mar 29 07:57:05.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
Mar 29 07:57:05.167: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/29/23 07:57:05.167
Mar 29 07:57:05.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
Mar 29 07:57:05.300: INFO: rc: 1
Mar 29 07:57:05.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
Mar 29 07:57:05.422: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/29/23 07:57:05.422
Mar 29 07:57:05.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds'
Mar 29 07:57:05.541: INFO: stderr: ""
Mar 29 07:57:05.541: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/29/23 07:57:05.541
Mar 29 07:57:05.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.metadata'
Mar 29 07:57:05.667: INFO: stderr: ""
Mar 29 07:57:05.667: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 29 07:57:05.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec'
Mar 29 07:57:05.789: INFO: stderr: ""
Mar 29 07:57:05.789: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 29 07:57:05.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec.bars'
Mar 29 07:57:05.909: INFO: stderr: ""
Mar 29 07:57:05.909: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/29/23 07:57:05.909
Mar 29 07:57:05.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec.bars2'
Mar 29 07:57:06.028: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 07:57:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7369" for this suite. 03/29/23 07:57:07.833
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":38,"skipped":779,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.519 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:02.317
    Mar 29 07:57:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 07:57:02.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:02.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:02.326
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar 29 07:57:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/29/23 07:57:04.159
    Mar 29 07:57:04.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
    Mar 29 07:57:04.594: INFO: stderr: ""
    Mar 29 07:57:04.594: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 29 07:57:04.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 delete e2e-test-crd-publish-openapi-2457-crds test-foo'
    Mar 29 07:57:04.636: INFO: stderr: ""
    Mar 29 07:57:04.636: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 29 07:57:04.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
    Mar 29 07:57:04.760: INFO: stderr: ""
    Mar 29 07:57:04.760: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 29 07:57:04.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 delete e2e-test-crd-publish-openapi-2457-crds test-foo'
    Mar 29 07:57:04.801: INFO: stderr: ""
    Mar 29 07:57:04.801: INFO: stdout: "e2e-test-crd-publish-openapi-2457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/29/23 07:57:04.801
    Mar 29 07:57:04.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
    Mar 29 07:57:04.922: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/29/23 07:57:04.922
    Mar 29 07:57:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
    Mar 29 07:57:05.043: INFO: rc: 1
    Mar 29 07:57:05.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
    Mar 29 07:57:05.167: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/29/23 07:57:05.167
    Mar 29 07:57:05.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 create -f -'
    Mar 29 07:57:05.300: INFO: rc: 1
    Mar 29 07:57:05.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 --namespace=crd-publish-openapi-7369 apply -f -'
    Mar 29 07:57:05.422: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/29/23 07:57:05.422
    Mar 29 07:57:05.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds'
    Mar 29 07:57:05.541: INFO: stderr: ""
    Mar 29 07:57:05.541: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/29/23 07:57:05.541
    Mar 29 07:57:05.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.metadata'
    Mar 29 07:57:05.667: INFO: stderr: ""
    Mar 29 07:57:05.667: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 29 07:57:05.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec'
    Mar 29 07:57:05.789: INFO: stderr: ""
    Mar 29 07:57:05.789: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 29 07:57:05.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec.bars'
    Mar 29 07:57:05.909: INFO: stderr: ""
    Mar 29 07:57:05.909: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/29/23 07:57:05.909
    Mar 29 07:57:05.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7369 explain e2e-test-crd-publish-openapi-2457-crds.spec.bars2'
    Mar 29 07:57:06.028: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 07:57:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7369" for this suite. 03/29/23 07:57:07.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:07.837
Mar 29 07:57:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replication-controller 03/29/23 07:57:07.837
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:07.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:07.845
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar 29 07:57:07.847: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/29/23 07:57:08.851
STEP: Checking rc "condition-test" has the desired failure condition set 03/29/23 07:57:08.855
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/29/23 07:57:09.858
Mar 29 07:57:09.862: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/29/23 07:57:09.862
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 29 07:57:10.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4121" for this suite. 03/29/23 07:57:10.867
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":39,"skipped":792,"failed":0}
------------------------------
â€¢ [3.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:07.837
    Mar 29 07:57:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replication-controller 03/29/23 07:57:07.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:07.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:07.845
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar 29 07:57:07.847: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/29/23 07:57:08.851
    STEP: Checking rc "condition-test" has the desired failure condition set 03/29/23 07:57:08.855
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/29/23 07:57:09.858
    Mar 29 07:57:09.862: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/29/23 07:57:09.862
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 29 07:57:10.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4121" for this suite. 03/29/23 07:57:10.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:10.871
Mar 29 07:57:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 07:57:10.871
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:10.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:10.879
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar 29 07:57:10.885: INFO: Waiting up to 5m0s for pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f" in namespace "pods-791" to be "running and ready"
Mar 29 07:57:10.886: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299532ms
Mar 29 07:57:10.886: INFO: The phase of Pod server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:57:12.889: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.003868513s
Mar 29 07:57:12.889: INFO: The phase of Pod server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f is Running (Ready = true)
Mar 29 07:57:12.889: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f" satisfied condition "running and ready"
Mar 29 07:57:12.898: INFO: Waiting up to 5m0s for pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839" in namespace "pods-791" to be "Succeeded or Failed"
Mar 29 07:57:12.900: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816445ms
Mar 29 07:57:14.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004780694s
Mar 29 07:57:16.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005295188s
STEP: Saw pod success 03/29/23 07:57:16.903
Mar 29 07:57:16.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839" satisfied condition "Succeeded or Failed"
Mar 29 07:57:16.905: INFO: Trying to get logs from node 10.146.0.116 pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 container env3cont: <nil>
STEP: delete the pod 03/29/23 07:57:16.907
Mar 29 07:57:16.913: INFO: Waiting for pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 to disappear
Mar 29 07:57:16.915: INFO: Pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 07:57:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-791" for this suite. 03/29/23 07:57:16.916
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":40,"skipped":805,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:10.871
    Mar 29 07:57:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 07:57:10.871
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:10.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:10.879
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar 29 07:57:10.885: INFO: Waiting up to 5m0s for pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f" in namespace "pods-791" to be "running and ready"
    Mar 29 07:57:10.886: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299532ms
    Mar 29 07:57:10.886: INFO: The phase of Pod server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:57:12.889: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.003868513s
    Mar 29 07:57:12.889: INFO: The phase of Pod server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f is Running (Ready = true)
    Mar 29 07:57:12.889: INFO: Pod "server-envvars-f68737d4-b107-4edb-9cfa-a8bfc0ce2a3f" satisfied condition "running and ready"
    Mar 29 07:57:12.898: INFO: Waiting up to 5m0s for pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839" in namespace "pods-791" to be "Succeeded or Failed"
    Mar 29 07:57:12.900: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816445ms
    Mar 29 07:57:14.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004780694s
    Mar 29 07:57:16.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005295188s
    STEP: Saw pod success 03/29/23 07:57:16.903
    Mar 29 07:57:16.903: INFO: Pod "client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839" satisfied condition "Succeeded or Failed"
    Mar 29 07:57:16.905: INFO: Trying to get logs from node 10.146.0.116 pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 container env3cont: <nil>
    STEP: delete the pod 03/29/23 07:57:16.907
    Mar 29 07:57:16.913: INFO: Waiting for pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 to disappear
    Mar 29 07:57:16.915: INFO: Pod client-envvars-3d9f4568-3a3b-438c-8961-b1b73143c839 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 07:57:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-791" for this suite. 03/29/23 07:57:16.916
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:16.919
Mar 29 07:57:16.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 07:57:16.92
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:16.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:16.929
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-47ec0457-ffdf-4c80-9713-808b69e80ea8 03/29/23 07:57:16.93
STEP: Creating a pod to test consume secrets 03/29/23 07:57:16.932
Mar 29 07:57:16.935: INFO: Waiting up to 5m0s for pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9" in namespace "secrets-690" to be "Succeeded or Failed"
Mar 29 07:57:16.936: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1805ms
Mar 29 07:57:18.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00359275s
Mar 29 07:57:20.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003682743s
STEP: Saw pod success 03/29/23 07:57:20.939
Mar 29 07:57:20.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9" satisfied condition "Succeeded or Failed"
Mar 29 07:57:20.940: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 container secret-env-test: <nil>
STEP: delete the pod 03/29/23 07:57:20.943
Mar 29 07:57:20.949: INFO: Waiting for pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 to disappear
Mar 29 07:57:20.951: INFO: Pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 29 07:57:20.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-690" for this suite. 03/29/23 07:57:20.953
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":41,"skipped":806,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:16.919
    Mar 29 07:57:16.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 07:57:16.92
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:16.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:16.929
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-47ec0457-ffdf-4c80-9713-808b69e80ea8 03/29/23 07:57:16.93
    STEP: Creating a pod to test consume secrets 03/29/23 07:57:16.932
    Mar 29 07:57:16.935: INFO: Waiting up to 5m0s for pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9" in namespace "secrets-690" to be "Succeeded or Failed"
    Mar 29 07:57:16.936: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1805ms
    Mar 29 07:57:18.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00359275s
    Mar 29 07:57:20.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003682743s
    STEP: Saw pod success 03/29/23 07:57:20.939
    Mar 29 07:57:20.939: INFO: Pod "pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9" satisfied condition "Succeeded or Failed"
    Mar 29 07:57:20.940: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 container secret-env-test: <nil>
    STEP: delete the pod 03/29/23 07:57:20.943
    Mar 29 07:57:20.949: INFO: Waiting for pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 to disappear
    Mar 29 07:57:20.951: INFO: Pod pod-secrets-57ab414e-d8b7-4a68-b4ee-3da828ad87e9 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 07:57:20.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-690" for this suite. 03/29/23 07:57:20.953
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:20.955
Mar 29 07:57:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 07:57:20.956
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:20.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:20.963
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar 29 07:57:20.965: INFO: Got root ca configmap in namespace "svcaccounts-7048"
Mar 29 07:57:20.967: INFO: Deleted root ca configmap in namespace "svcaccounts-7048"
STEP: waiting for a new root ca configmap created 03/29/23 07:57:21.468
Mar 29 07:57:21.469: INFO: Recreated root ca configmap in namespace "svcaccounts-7048"
Mar 29 07:57:21.472: INFO: Updated root ca configmap in namespace "svcaccounts-7048"
STEP: waiting for the root ca configmap reconciled 03/29/23 07:57:21.972
Mar 29 07:57:21.974: INFO: Reconciled root ca configmap in namespace "svcaccounts-7048"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 07:57:21.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7048" for this suite. 03/29/23 07:57:21.976
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":42,"skipped":806,"failed":0}
------------------------------
â€¢ [1.023 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:20.955
    Mar 29 07:57:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 07:57:20.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:20.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:20.963
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar 29 07:57:20.965: INFO: Got root ca configmap in namespace "svcaccounts-7048"
    Mar 29 07:57:20.967: INFO: Deleted root ca configmap in namespace "svcaccounts-7048"
    STEP: waiting for a new root ca configmap created 03/29/23 07:57:21.468
    Mar 29 07:57:21.469: INFO: Recreated root ca configmap in namespace "svcaccounts-7048"
    Mar 29 07:57:21.472: INFO: Updated root ca configmap in namespace "svcaccounts-7048"
    STEP: waiting for the root ca configmap reconciled 03/29/23 07:57:21.972
    Mar 29 07:57:21.974: INFO: Reconciled root ca configmap in namespace "svcaccounts-7048"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 07:57:21.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7048" for this suite. 03/29/23 07:57:21.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:21.979
Mar 29 07:57:21.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-runtime 03/29/23 07:57:21.98
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:21.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:21.99
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/29/23 07:57:21.992
STEP: wait for the container to reach Succeeded 03/29/23 07:57:22
STEP: get the container status 03/29/23 07:57:25.009
STEP: the container should be terminated 03/29/23 07:57:25.01
STEP: the termination message should be set 03/29/23 07:57:25.01
Mar 29 07:57:25.010: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/29/23 07:57:25.01
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 29 07:57:25.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4987" for this suite. 03/29/23 07:57:25.019
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":43,"skipped":814,"failed":0}
------------------------------
â€¢ [3.043 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:21.979
    Mar 29 07:57:21.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-runtime 03/29/23 07:57:21.98
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:21.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:21.99
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/29/23 07:57:21.992
    STEP: wait for the container to reach Succeeded 03/29/23 07:57:22
    STEP: get the container status 03/29/23 07:57:25.009
    STEP: the container should be terminated 03/29/23 07:57:25.01
    STEP: the termination message should be set 03/29/23 07:57:25.01
    Mar 29 07:57:25.010: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/29/23 07:57:25.01
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 29 07:57:25.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4987" for this suite. 03/29/23 07:57:25.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:25.022
Mar 29 07:57:25.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 07:57:25.022
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:25.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:25.03
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/29/23 07:57:25.031
Mar 29 07:57:25.034: INFO: Waiting up to 5m0s for pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4" in namespace "var-expansion-9723" to be "Succeeded or Failed"
Mar 29 07:57:25.036: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193047ms
Mar 29 07:57:27.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003274313s
Mar 29 07:57:29.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003390354s
STEP: Saw pod success 03/29/23 07:57:29.038
Mar 29 07:57:29.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4" satisfied condition "Succeeded or Failed"
Mar 29 07:57:29.039: INFO: Trying to get logs from node 10.146.0.116 pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 container dapi-container: <nil>
STEP: delete the pod 03/29/23 07:57:29.042
Mar 29 07:57:29.049: INFO: Waiting for pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 to disappear
Mar 29 07:57:29.050: INFO: Pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 07:57:29.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9723" for this suite. 03/29/23 07:57:29.052
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":44,"skipped":821,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:25.022
    Mar 29 07:57:25.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 07:57:25.022
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:25.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:25.03
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/29/23 07:57:25.031
    Mar 29 07:57:25.034: INFO: Waiting up to 5m0s for pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4" in namespace "var-expansion-9723" to be "Succeeded or Failed"
    Mar 29 07:57:25.036: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193047ms
    Mar 29 07:57:27.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003274313s
    Mar 29 07:57:29.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003390354s
    STEP: Saw pod success 03/29/23 07:57:29.038
    Mar 29 07:57:29.038: INFO: Pod "var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4" satisfied condition "Succeeded or Failed"
    Mar 29 07:57:29.039: INFO: Trying to get logs from node 10.146.0.116 pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 container dapi-container: <nil>
    STEP: delete the pod 03/29/23 07:57:29.042
    Mar 29 07:57:29.049: INFO: Waiting for pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 to disappear
    Mar 29 07:57:29.050: INFO: Pod var-expansion-5bf23126-3bca-4ad4-aceb-a7747ec65de4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 07:57:29.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9723" for this suite. 03/29/23 07:57:29.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:29.055
Mar 29 07:57:29.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 07:57:29.056
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:29.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:29.063
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/29/23 07:57:29.065
Mar 29 07:57:29.069: INFO: Waiting up to 5m0s for pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a" in namespace "emptydir-2035" to be "Succeeded or Failed"
Mar 29 07:57:29.071: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387604ms
Mar 29 07:57:31.073: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004189836s
Mar 29 07:57:33.074: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005161734s
STEP: Saw pod success 03/29/23 07:57:33.074
Mar 29 07:57:33.074: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a" satisfied condition "Succeeded or Failed"
Mar 29 07:57:33.076: INFO: Trying to get logs from node 10.146.0.116 pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a container test-container: <nil>
STEP: delete the pod 03/29/23 07:57:33.078
Mar 29 07:57:33.084: INFO: Waiting for pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a to disappear
Mar 29 07:57:33.085: INFO: Pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 07:57:33.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2035" for this suite. 03/29/23 07:57:33.087
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":844,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:29.055
    Mar 29 07:57:29.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 07:57:29.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:29.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:29.063
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/29/23 07:57:29.065
    Mar 29 07:57:29.069: INFO: Waiting up to 5m0s for pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a" in namespace "emptydir-2035" to be "Succeeded or Failed"
    Mar 29 07:57:29.071: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387604ms
    Mar 29 07:57:31.073: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004189836s
    Mar 29 07:57:33.074: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005161734s
    STEP: Saw pod success 03/29/23 07:57:33.074
    Mar 29 07:57:33.074: INFO: Pod "pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a" satisfied condition "Succeeded or Failed"
    Mar 29 07:57:33.076: INFO: Trying to get logs from node 10.146.0.116 pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a container test-container: <nil>
    STEP: delete the pod 03/29/23 07:57:33.078
    Mar 29 07:57:33.084: INFO: Waiting for pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a to disappear
    Mar 29 07:57:33.085: INFO: Pod pod-d73fee5e-35ea-4dce-9813-0fd77c03a16a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 07:57:33.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2035" for this suite. 03/29/23 07:57:33.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:33.09
Mar 29 07:57:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 07:57:33.091
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:33.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:33.097
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/29/23 07:57:33.098
STEP: submitting the pod to kubernetes 03/29/23 07:57:33.099
Mar 29 07:57:33.103: INFO: Waiting up to 5m0s for pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" in namespace "pods-8555" to be "running and ready"
Mar 29 07:57:33.104: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.209449ms
Mar 29 07:57:33.104: INFO: The phase of Pod pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:57:35.106: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003289601s
Mar 29 07:57:35.106: INFO: The phase of Pod pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2 is Running (Ready = true)
Mar 29 07:57:35.106: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/29/23 07:57:35.107
STEP: updating the pod 03/29/23 07:57:35.109
Mar 29 07:57:35.616: INFO: Successfully updated pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2"
Mar 29 07:57:35.616: INFO: Waiting up to 5m0s for pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" in namespace "pods-8555" to be "running"
Mar 29 07:57:35.617: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Running", Reason="", readiness=true. Elapsed: 1.351956ms
Mar 29 07:57:35.617: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/29/23 07:57:35.617
Mar 29 07:57:35.618: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 07:57:35.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8555" for this suite. 03/29/23 07:57:35.62
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":46,"skipped":853,"failed":0}
------------------------------
â€¢ [2.532 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:33.09
    Mar 29 07:57:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 07:57:33.091
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:33.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:33.097
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/29/23 07:57:33.098
    STEP: submitting the pod to kubernetes 03/29/23 07:57:33.099
    Mar 29 07:57:33.103: INFO: Waiting up to 5m0s for pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" in namespace "pods-8555" to be "running and ready"
    Mar 29 07:57:33.104: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.209449ms
    Mar 29 07:57:33.104: INFO: The phase of Pod pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:57:35.106: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003289601s
    Mar 29 07:57:35.106: INFO: The phase of Pod pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2 is Running (Ready = true)
    Mar 29 07:57:35.106: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/29/23 07:57:35.107
    STEP: updating the pod 03/29/23 07:57:35.109
    Mar 29 07:57:35.616: INFO: Successfully updated pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2"
    Mar 29 07:57:35.616: INFO: Waiting up to 5m0s for pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" in namespace "pods-8555" to be "running"
    Mar 29 07:57:35.617: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2": Phase="Running", Reason="", readiness=true. Elapsed: 1.351956ms
    Mar 29 07:57:35.617: INFO: Pod "pod-update-4d71c737-6776-4ecb-a7e9-8fe251a585c2" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/29/23 07:57:35.617
    Mar 29 07:57:35.618: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 07:57:35.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8555" for this suite. 03/29/23 07:57:35.62
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:35.623
Mar 29 07:57:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 07:57:35.623
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:35.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:35.633
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-5c4a53b5-5582-4298-9bc5-e60880bc8054 03/29/23 07:57:35.634
STEP: Creating a pod to test consume secrets 03/29/23 07:57:35.637
Mar 29 07:57:35.641: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d" in namespace "projected-440" to be "Succeeded or Failed"
Mar 29 07:57:35.642: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258848ms
Mar 29 07:57:37.645: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004531749s
Mar 29 07:57:39.644: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003387945s
STEP: Saw pod success 03/29/23 07:57:39.644
Mar 29 07:57:39.644: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d" satisfied condition "Succeeded or Failed"
Mar 29 07:57:39.645: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d container projected-secret-volume-test: <nil>
STEP: delete the pod 03/29/23 07:57:39.648
Mar 29 07:57:39.654: INFO: Waiting for pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d to disappear
Mar 29 07:57:39.656: INFO: Pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 07:57:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-440" for this suite. 03/29/23 07:57:39.657
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":47,"skipped":856,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:35.623
    Mar 29 07:57:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 07:57:35.623
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:35.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:35.633
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-5c4a53b5-5582-4298-9bc5-e60880bc8054 03/29/23 07:57:35.634
    STEP: Creating a pod to test consume secrets 03/29/23 07:57:35.637
    Mar 29 07:57:35.641: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d" in namespace "projected-440" to be "Succeeded or Failed"
    Mar 29 07:57:35.642: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258848ms
    Mar 29 07:57:37.645: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004531749s
    Mar 29 07:57:39.644: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003387945s
    STEP: Saw pod success 03/29/23 07:57:39.644
    Mar 29 07:57:39.644: INFO: Pod "pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d" satisfied condition "Succeeded or Failed"
    Mar 29 07:57:39.645: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 07:57:39.648
    Mar 29 07:57:39.654: INFO: Waiting for pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d to disappear
    Mar 29 07:57:39.656: INFO: Pod pod-projected-secrets-098d2f84-bc55-4e02-b719-b77439025f6d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 07:57:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-440" for this suite. 03/29/23 07:57:39.657
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:57:39.66
Mar 29 07:57:39.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pod-network-test 03/29/23 07:57:39.66
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:39.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:39.67
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-185 03/29/23 07:57:39.671
STEP: creating a selector 03/29/23 07:57:39.671
STEP: Creating the service pods in kubernetes 03/29/23 07:57:39.671
Mar 29 07:57:39.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 07:57:39.686: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-185" to be "running and ready"
Mar 29 07:57:39.687: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.208218ms
Mar 29 07:57:39.687: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:57:41.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004152329s
Mar 29 07:57:41.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:43.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003226454s
Mar 29 07:57:43.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:45.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00339001s
Mar 29 07:57:45.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:47.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004424649s
Mar 29 07:57:47.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:49.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.003714047s
Mar 29 07:57:49.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:51.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004571823s
Mar 29 07:57:51.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:53.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003291032s
Mar 29 07:57:53.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:55.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00430844s
Mar 29 07:57:55.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:57.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004221809s
Mar 29 07:57:57.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:57:59.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004264402s
Mar 29 07:57:59.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 07:58:01.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005119474s
Mar 29 07:58:01.691: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 29 07:58:01.691: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 29 07:58:01.692: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-185" to be "running and ready"
Mar 29 07:58:01.693: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.228614ms
Mar 29 07:58:01.694: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 29 07:58:01.694: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 29 07:58:01.695: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-185" to be "running and ready"
Mar 29 07:58:01.696: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.102385ms
Mar 29 07:58:01.696: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 29 07:58:01.696: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/29/23 07:58:01.697
Mar 29 07:58:01.699: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-185" to be "running"
Mar 29 07:58:01.701: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.104348ms
Mar 29 07:58:03.703: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003371112s
Mar 29 07:58:03.703: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 29 07:58:03.704: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 29 07:58:03.704: INFO: Breadth first check of 192.168.30.30 on host 10.146.0.115...
Mar 29 07:58:03.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.30.30&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 07:58:03.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 07:58:03.706: INFO: ExecWithOptions: Clientset creation
Mar 29 07:58:03.706: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.30.30%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 07:58:03.751: INFO: Waiting for responses: map[]
Mar 29 07:58:03.751: INFO: reached 192.168.30.30 after 0/1 tries
Mar 29 07:58:03.751: INFO: Breadth first check of 192.168.219.150 on host 10.146.0.116...
Mar 29 07:58:03.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.219.150&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 07:58:03.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 07:58:03.752: INFO: ExecWithOptions: Clientset creation
Mar 29 07:58:03.752: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.219.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 07:58:03.800: INFO: Waiting for responses: map[]
Mar 29 07:58:03.800: INFO: reached 192.168.219.150 after 0/1 tries
Mar 29 07:58:03.800: INFO: Breadth first check of 192.168.87.203 on host 10.146.0.117...
Mar 29 07:58:03.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.87.203&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 07:58:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 07:58:03.802: INFO: ExecWithOptions: Clientset creation
Mar 29 07:58:03.802: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.87.203%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 07:58:03.842: INFO: Waiting for responses: map[]
Mar 29 07:58:03.843: INFO: reached 192.168.87.203 after 0/1 tries
Mar 29 07:58:03.843: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 29 07:58:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-185" for this suite. 03/29/23 07:58:03.844
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":48,"skipped":860,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.187 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:57:39.66
    Mar 29 07:57:39.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pod-network-test 03/29/23 07:57:39.66
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:57:39.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:57:39.67
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-185 03/29/23 07:57:39.671
    STEP: creating a selector 03/29/23 07:57:39.671
    STEP: Creating the service pods in kubernetes 03/29/23 07:57:39.671
    Mar 29 07:57:39.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 29 07:57:39.686: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-185" to be "running and ready"
    Mar 29 07:57:39.687: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.208218ms
    Mar 29 07:57:39.687: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:57:41.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004152329s
    Mar 29 07:57:41.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:43.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.003226454s
    Mar 29 07:57:43.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:45.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00339001s
    Mar 29 07:57:45.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:47.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004424649s
    Mar 29 07:57:47.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:49.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.003714047s
    Mar 29 07:57:49.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:51.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004571823s
    Mar 29 07:57:51.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:53.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003291032s
    Mar 29 07:57:53.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:55.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00430844s
    Mar 29 07:57:55.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:57.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004221809s
    Mar 29 07:57:57.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:57:59.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004264402s
    Mar 29 07:57:59.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 07:58:01.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005119474s
    Mar 29 07:58:01.691: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 29 07:58:01.691: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 29 07:58:01.692: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-185" to be "running and ready"
    Mar 29 07:58:01.693: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.228614ms
    Mar 29 07:58:01.694: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 29 07:58:01.694: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 29 07:58:01.695: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-185" to be "running and ready"
    Mar 29 07:58:01.696: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.102385ms
    Mar 29 07:58:01.696: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 29 07:58:01.696: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/29/23 07:58:01.697
    Mar 29 07:58:01.699: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-185" to be "running"
    Mar 29 07:58:01.701: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.104348ms
    Mar 29 07:58:03.703: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003371112s
    Mar 29 07:58:03.703: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 29 07:58:03.704: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 29 07:58:03.704: INFO: Breadth first check of 192.168.30.30 on host 10.146.0.115...
    Mar 29 07:58:03.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.30.30&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 07:58:03.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 07:58:03.706: INFO: ExecWithOptions: Clientset creation
    Mar 29 07:58:03.706: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.30.30%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 07:58:03.751: INFO: Waiting for responses: map[]
    Mar 29 07:58:03.751: INFO: reached 192.168.30.30 after 0/1 tries
    Mar 29 07:58:03.751: INFO: Breadth first check of 192.168.219.150 on host 10.146.0.116...
    Mar 29 07:58:03.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.219.150&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 07:58:03.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 07:58:03.752: INFO: ExecWithOptions: Clientset creation
    Mar 29 07:58:03.752: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.219.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 07:58:03.800: INFO: Waiting for responses: map[]
    Mar 29 07:58:03.800: INFO: reached 192.168.219.150 after 0/1 tries
    Mar 29 07:58:03.800: INFO: Breadth first check of 192.168.87.203 on host 10.146.0.117...
    Mar 29 07:58:03.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.219.151:9080/dial?request=hostname&protocol=http&host=192.168.87.203&port=8083&tries=1'] Namespace:pod-network-test-185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 07:58:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 07:58:03.802: INFO: ExecWithOptions: Clientset creation
    Mar 29 07:58:03.802: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.219.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.87.203%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 07:58:03.842: INFO: Waiting for responses: map[]
    Mar 29 07:58:03.843: INFO: reached 192.168.87.203 after 0/1 tries
    Mar 29 07:58:03.843: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 29 07:58:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-185" for this suite. 03/29/23 07:58:03.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:58:03.848
Mar 29 07:58:03.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-pred 03/29/23 07:58:03.849
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:03.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:03.857
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 29 07:58:03.859: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 07:58:03.861: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 07:58:03.862: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.115 before test
Mar 29 07:58:03.865: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.865: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 07:58:03.865: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.865: INFO: 	Container coredns ready: true, restart count 0
Mar 29 07:58:03.865: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 07:58:03.865: INFO: 	Container exporter ready: true, restart count 0
Mar 29 07:58:03.865: INFO: 	Container reload ready: true, restart count 0
Mar 29 07:58:03.865: INFO: 	Container unbound ready: true, restart count 0
Mar 29 07:58:03.865: INFO: netserver-0 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.865: INFO: 	Container webserver ready: true, restart count 0
Mar 29 07:58:03.865: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 07:58:03.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 07:58:03.865: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 07:58:03.865: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.116 before test
Mar 29 07:58:03.868: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 07:58:03.868: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container coredns ready: true, restart count 0
Mar 29 07:58:03.868: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container exporter ready: true, restart count 0
Mar 29 07:58:03.868: INFO: 	Container reload ready: true, restart count 0
Mar 29 07:58:03.868: INFO: 	Container unbound ready: true, restart count 0
Mar 29 07:58:03.868: INFO: netserver-1 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container webserver ready: true, restart count 0
Mar 29 07:58:03.868: INFO: test-container-pod from pod-network-test-185 started at 2023-03-29 07:58:01 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container webserver ready: true, restart count 0
Mar 29 07:58:03.868: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 07:58:03.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 07:58:03.868: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 07:58:03.868: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.117 before test
Mar 29 07:58:03.871: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 07:58:03.871: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 07:58:03.871: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container exporter ready: true, restart count 0
Mar 29 07:58:03.871: INFO: 	Container reload ready: true, restart count 0
Mar 29 07:58:03.871: INFO: 	Container unbound ready: true, restart count 0
Mar 29 07:58:03.871: INFO: netserver-2 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container webserver ready: true, restart count 0
Mar 29 07:58:03.871: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 07:58:03.871: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container e2e ready: true, restart count 0
Mar 29 07:58:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 07:58:03.871: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 07:58:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 07:58:03.871: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.146.0.115 03/29/23 07:58:03.879
STEP: verifying the node has the label node 10.146.0.116 03/29/23 07:58:03.885
STEP: verifying the node has the label node 10.146.0.117 03/29/23 07:58:03.892
Mar 29 07:58:03.897: INFO: Pod calico-kube-controllers-6747f75cdc-ngkn7 requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod calico-node-8mtw5 requesting resource cpu=250m on Node 10.146.0.115
Mar 29 07:58:03.897: INFO: Pod calico-node-l2g4d requesting resource cpu=250m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod calico-node-nj5q6 requesting resource cpu=250m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod cluster-dns-77bdb56774-p98xx requesting resource cpu=100m on Node 10.146.0.115
Mar 29 07:58:03.897: INFO: Pod cluster-dns-77bdb56774-pt7gn requesting resource cpu=100m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod node-dns-hb64l requesting resource cpu=0m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod node-dns-jqb4n requesting resource cpu=0m on Node 10.146.0.115
Mar 29 07:58:03.897: INFO: Pod node-dns-nldsw requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod netserver-0 requesting resource cpu=0m on Node 10.146.0.115
Mar 29 07:58:03.897: INFO: Pod netserver-1 requesting resource cpu=0m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod netserver-2 requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod test-container-pod requesting resource cpu=0m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod sonobuoy-e2e-job-733d8bf70035488f requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh requesting resource cpu=0m on Node 10.146.0.117
Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 requesting resource cpu=0m on Node 10.146.0.116
Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm requesting resource cpu=0m on Node 10.146.0.115
STEP: Starting Pods to consume most of the cluster CPU. 03/29/23 07:58:03.897
Mar 29 07:58:03.897: INFO: Creating a pod which consumes cpu=5355m on Node 10.146.0.115
Mar 29 07:58:03.901: INFO: Creating a pod which consumes cpu=5355m on Node 10.146.0.116
Mar 29 07:58:03.904: INFO: Creating a pod which consumes cpu=5425m on Node 10.146.0.117
Mar 29 07:58:03.909: INFO: Waiting up to 5m0s for pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e" in namespace "sched-pred-4971" to be "running"
Mar 29 07:58:03.914: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663745ms
Mar 29 07:58:05.916: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007652619s
Mar 29 07:58:05.916: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e" satisfied condition "running"
Mar 29 07:58:05.916: INFO: Waiting up to 5m0s for pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c" in namespace "sched-pred-4971" to be "running"
Mar 29 07:58:05.918: INFO: Pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c": Phase="Running", Reason="", readiness=true. Elapsed: 1.360854ms
Mar 29 07:58:05.918: INFO: Pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c" satisfied condition "running"
Mar 29 07:58:05.918: INFO: Waiting up to 5m0s for pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab" in namespace "sched-pred-4971" to be "running"
Mar 29 07:58:05.919: INFO: Pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab": Phase="Running", Reason="", readiness=true. Elapsed: 1.240188ms
Mar 29 07:58:05.919: INFO: Pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/29/23 07:58:05.919
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7b3a18c9e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c to 10.146.0.116] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d16c9b91], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d2188ef9], Reason = [Created], Message = [Created container filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d5622014], Reason = [Started], Message = [Started container filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7b35fe4f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e to 10.146.0.115] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d0e2e13d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d18248ee], Reason = [Created], Message = [Created container filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d4ef95e9], Reason = [Started], Message = [Started container filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7b3dcae96], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab to 10.146.0.117] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d159c08d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d20dfd0a], Reason = [Created], Message = [Created container filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d5817d4c], Reason = [Started], Message = [Started container filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab] 03/29/23 07:58:05.921
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1750d4d82baad325], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/29/23 07:58:05.928
STEP: removing the label node off the node 10.146.0.115 03/29/23 07:58:06.926
STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.933
STEP: removing the label node off the node 10.146.0.116 03/29/23 07:58:06.934
STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.94
STEP: removing the label node off the node 10.146.0.117 03/29/23 07:58:06.942
STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.948
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 29 07:58:06.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4971" for this suite. 03/29/23 07:58:06.952
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":49,"skipped":893,"failed":0}
------------------------------
â€¢ [3.106 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:58:03.848
    Mar 29 07:58:03.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-pred 03/29/23 07:58:03.849
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:03.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:03.857
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 29 07:58:03.859: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 29 07:58:03.861: INFO: Waiting for terminating namespaces to be deleted...
    Mar 29 07:58:03.862: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.115 before test
    Mar 29 07:58:03.865: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.865: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.865: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 07:58:03.865: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: 	Container reload ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: netserver-0 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.865: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 07:58:03.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 07:58:03.865: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.116 before test
    Mar 29 07:58:03.868: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: 	Container reload ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: netserver-1 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: test-container-pod from pod-network-test-185 started at 2023-03-29 07:58:01 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 07:58:03.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 07:58:03.868: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.117 before test
    Mar 29 07:58:03.871: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: 	Container reload ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: netserver-2 from pod-network-test-185 started at 2023-03-29 07:57:39 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container e2e ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 07:58:03.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 07:58:03.871: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.146.0.115 03/29/23 07:58:03.879
    STEP: verifying the node has the label node 10.146.0.116 03/29/23 07:58:03.885
    STEP: verifying the node has the label node 10.146.0.117 03/29/23 07:58:03.892
    Mar 29 07:58:03.897: INFO: Pod calico-kube-controllers-6747f75cdc-ngkn7 requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod calico-node-8mtw5 requesting resource cpu=250m on Node 10.146.0.115
    Mar 29 07:58:03.897: INFO: Pod calico-node-l2g4d requesting resource cpu=250m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod calico-node-nj5q6 requesting resource cpu=250m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod cluster-dns-77bdb56774-p98xx requesting resource cpu=100m on Node 10.146.0.115
    Mar 29 07:58:03.897: INFO: Pod cluster-dns-77bdb56774-pt7gn requesting resource cpu=100m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod node-dns-hb64l requesting resource cpu=0m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod node-dns-jqb4n requesting resource cpu=0m on Node 10.146.0.115
    Mar 29 07:58:03.897: INFO: Pod node-dns-nldsw requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod netserver-0 requesting resource cpu=0m on Node 10.146.0.115
    Mar 29 07:58:03.897: INFO: Pod netserver-1 requesting resource cpu=0m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod netserver-2 requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod test-container-pod requesting resource cpu=0m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod sonobuoy-e2e-job-733d8bf70035488f requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh requesting resource cpu=0m on Node 10.146.0.117
    Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 requesting resource cpu=0m on Node 10.146.0.116
    Mar 29 07:58:03.897: INFO: Pod sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm requesting resource cpu=0m on Node 10.146.0.115
    STEP: Starting Pods to consume most of the cluster CPU. 03/29/23 07:58:03.897
    Mar 29 07:58:03.897: INFO: Creating a pod which consumes cpu=5355m on Node 10.146.0.115
    Mar 29 07:58:03.901: INFO: Creating a pod which consumes cpu=5355m on Node 10.146.0.116
    Mar 29 07:58:03.904: INFO: Creating a pod which consumes cpu=5425m on Node 10.146.0.117
    Mar 29 07:58:03.909: INFO: Waiting up to 5m0s for pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e" in namespace "sched-pred-4971" to be "running"
    Mar 29 07:58:03.914: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663745ms
    Mar 29 07:58:05.916: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007652619s
    Mar 29 07:58:05.916: INFO: Pod "filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e" satisfied condition "running"
    Mar 29 07:58:05.916: INFO: Waiting up to 5m0s for pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c" in namespace "sched-pred-4971" to be "running"
    Mar 29 07:58:05.918: INFO: Pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c": Phase="Running", Reason="", readiness=true. Elapsed: 1.360854ms
    Mar 29 07:58:05.918: INFO: Pod "filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c" satisfied condition "running"
    Mar 29 07:58:05.918: INFO: Waiting up to 5m0s for pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab" in namespace "sched-pred-4971" to be "running"
    Mar 29 07:58:05.919: INFO: Pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab": Phase="Running", Reason="", readiness=true. Elapsed: 1.240188ms
    Mar 29 07:58:05.919: INFO: Pod "filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/29/23 07:58:05.919
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7b3a18c9e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c to 10.146.0.116] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d16c9b91], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d2188ef9], Reason = [Created], Message = [Created container filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c.1750d4d7d5622014], Reason = [Started], Message = [Started container filler-pod-22d2ea20-4035-44bc-9dc6-78799e8f5a5c] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7b35fe4f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e to 10.146.0.115] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d0e2e13d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d18248ee], Reason = [Created], Message = [Created container filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e.1750d4d7d4ef95e9], Reason = [Started], Message = [Started container filler-pod-8efbd89a-1cf4-462b-8f0d-ed897ecdc06e] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7b3dcae96], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4971/filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab to 10.146.0.117] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d159c08d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d20dfd0a], Reason = [Created], Message = [Created container filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab.1750d4d7d5817d4c], Reason = [Started], Message = [Started container filler-pod-9d0b5d08-cbac-49ee-8ec5-1a18b8f75fab] 03/29/23 07:58:05.921
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1750d4d82baad325], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/29/23 07:58:05.928
    STEP: removing the label node off the node 10.146.0.115 03/29/23 07:58:06.926
    STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.933
    STEP: removing the label node off the node 10.146.0.116 03/29/23 07:58:06.934
    STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.94
    STEP: removing the label node off the node 10.146.0.117 03/29/23 07:58:06.942
    STEP: verifying the node doesn't have the label node 03/29/23 07:58:06.948
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 07:58:06.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4971" for this suite. 03/29/23 07:58:06.952
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:58:06.955
Mar 29 07:58:06.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 07:58:06.956
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:06.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:06.964
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/29/23 07:58:06.965
Mar 29 07:58:06.969: INFO: Waiting up to 5m0s for pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b" in namespace "downward-api-3265" to be "running and ready"
Mar 29 07:58:06.971: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311054ms
Mar 29 07:58:06.971: INFO: The phase of Pod labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:58:08.973: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003035576s
Mar 29 07:58:08.973: INFO: The phase of Pod labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b is Running (Ready = true)
Mar 29 07:58:08.973: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b" satisfied condition "running and ready"
Mar 29 07:58:09.484: INFO: Successfully updated pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 07:58:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3265" for this suite. 03/29/23 07:58:13.496
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":50,"skipped":899,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.544 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:58:06.955
    Mar 29 07:58:06.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 07:58:06.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:06.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:06.964
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/29/23 07:58:06.965
    Mar 29 07:58:06.969: INFO: Waiting up to 5m0s for pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b" in namespace "downward-api-3265" to be "running and ready"
    Mar 29 07:58:06.971: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311054ms
    Mar 29 07:58:06.971: INFO: The phase of Pod labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:58:08.973: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003035576s
    Mar 29 07:58:08.973: INFO: The phase of Pod labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b is Running (Ready = true)
    Mar 29 07:58:08.973: INFO: Pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b" satisfied condition "running and ready"
    Mar 29 07:58:09.484: INFO: Successfully updated pod "labelsupdate8ca44301-ed39-4d4c-9448-76ef3f08b43b"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 07:58:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3265" for this suite. 03/29/23 07:58:13.496
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:58:13.5
Mar 29 07:58:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 07:58:13.5
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:13.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:13.507
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/29/23 07:58:13.508
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/29/23 07:58:13.508
STEP: creating a pod to probe DNS 03/29/23 07:58:13.508
STEP: submitting the pod to kubernetes 03/29/23 07:58:13.508
Mar 29 07:58:13.512: INFO: Waiting up to 15m0s for pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f" in namespace "dns-7627" to be "running"
Mar 29 07:58:13.514: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.925589ms
Mar 29 07:58:15.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004419033s
Mar 29 07:58:17.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004384867s
Mar 29 07:58:19.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004397527s
Mar 29 07:58:21.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Running", Reason="", readiness=true. Elapsed: 8.005111123s
Mar 29 07:58:21.518: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f" satisfied condition "running"
STEP: retrieving the pod 03/29/23 07:58:21.518
STEP: looking for the results for each expected name from probers 03/29/23 07:58:21.519
Mar 29 07:58:21.526: INFO: DNS probes using dns-7627/dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f succeeded

STEP: deleting the pod 03/29/23 07:58:21.526
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 07:58:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7627" for this suite. 03/29/23 07:58:21.534
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":51,"skipped":900,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.037 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:58:13.5
    Mar 29 07:58:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 07:58:13.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:13.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:13.507
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/29/23 07:58:13.508
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/29/23 07:58:13.508
    STEP: creating a pod to probe DNS 03/29/23 07:58:13.508
    STEP: submitting the pod to kubernetes 03/29/23 07:58:13.508
    Mar 29 07:58:13.512: INFO: Waiting up to 15m0s for pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f" in namespace "dns-7627" to be "running"
    Mar 29 07:58:13.514: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.925589ms
    Mar 29 07:58:15.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004419033s
    Mar 29 07:58:17.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004384867s
    Mar 29 07:58:19.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004397527s
    Mar 29 07:58:21.517: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f": Phase="Running", Reason="", readiness=true. Elapsed: 8.005111123s
    Mar 29 07:58:21.518: INFO: Pod "dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 07:58:21.518
    STEP: looking for the results for each expected name from probers 03/29/23 07:58:21.519
    Mar 29 07:58:21.526: INFO: DNS probes using dns-7627/dns-test-a9660121-c8ec-4ce9-818e-e0d5847a342f succeeded

    STEP: deleting the pod 03/29/23 07:58:21.526
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 07:58:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7627" for this suite. 03/29/23 07:58:21.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 07:58:21.537
Mar 29 07:58:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 07:58:21.538
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:21.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:21.546
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-4263 03/29/23 07:58:21.547
Mar 29 07:58:21.551: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4263" to be "running and ready"
Mar 29 07:58:21.552: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198876ms
Mar 29 07:58:21.552: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 29 07:58:23.555: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004041273s
Mar 29 07:58:23.555: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 29 07:58:23.555: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 29 07:58:23.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 29 07:58:23.650: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 29 07:58:23.650: INFO: stdout: "ipvs"
Mar 29 07:58:23.650: INFO: proxyMode: ipvs
Mar 29 07:58:23.655: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 29 07:58:23.656: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4263 03/29/23 07:58:23.656
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4263 03/29/23 07:58:23.662
I0329 07:58:23.665618      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4263, replica count: 3
I0329 07:58:26.716385      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 07:58:26.719: INFO: Creating new exec pod
Mar 29 07:58:26.721: INFO: Waiting up to 5m0s for pod "execpod-affinitywfms5" in namespace "services-4263" to be "running"
Mar 29 07:58:26.723: INFO: Pod "execpod-affinitywfms5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296194ms
Mar 29 07:58:28.725: INFO: Pod "execpod-affinitywfms5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00330282s
Mar 29 07:58:28.725: INFO: Pod "execpod-affinitywfms5" satisfied condition "running"
Mar 29 07:58:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 29 07:58:29.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 29 07:58:29.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 07:58:29.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.80.194 80'
Mar 29 07:58:29.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.80.194 80\nConnection to 10.100.80.194 80 port [tcp/http] succeeded!\n"
Mar 29 07:58:29.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 07:58:29.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.80.194:80/ ; done'
Mar 29 07:58:30.038: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
Mar 29 07:58:30.038: INFO: stdout: "\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq"
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
Mar 29 07:58:30.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.80.194:80/'
Mar 29 07:58:30.126: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
Mar 29 07:58:30.126: INFO: stdout: "affinity-clusterip-timeout-6m8vq"
Mar 29 08:00:40.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.80.194:80/'
Mar 29 08:00:40.220: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
Mar 29 08:00:40.220: INFO: stdout: "affinity-clusterip-timeout-n8ptz"
Mar 29 08:00:40.220: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4263, will wait for the garbage collector to delete the pods 03/29/23 08:00:40.231
Mar 29 08:00:40.288: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.139788ms
Mar 29 08:00:40.388: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.846841ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:00:42.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4263" for this suite. 03/29/23 08:00:42.299
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":52,"skipped":923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [140.764 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 07:58:21.537
    Mar 29 07:58:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 07:58:21.538
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 07:58:21.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 07:58:21.546
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-4263 03/29/23 07:58:21.547
    Mar 29 07:58:21.551: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4263" to be "running and ready"
    Mar 29 07:58:21.552: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198876ms
    Mar 29 07:58:21.552: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 07:58:23.555: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.004041273s
    Mar 29 07:58:23.555: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 29 07:58:23.555: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 29 07:58:23.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 29 07:58:23.650: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 29 07:58:23.650: INFO: stdout: "ipvs"
    Mar 29 07:58:23.650: INFO: proxyMode: ipvs
    Mar 29 07:58:23.655: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 29 07:58:23.656: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-4263 03/29/23 07:58:23.656
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-4263 03/29/23 07:58:23.662
    I0329 07:58:23.665618      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4263, replica count: 3
    I0329 07:58:26.716385      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 07:58:26.719: INFO: Creating new exec pod
    Mar 29 07:58:26.721: INFO: Waiting up to 5m0s for pod "execpod-affinitywfms5" in namespace "services-4263" to be "running"
    Mar 29 07:58:26.723: INFO: Pod "execpod-affinitywfms5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296194ms
    Mar 29 07:58:28.725: INFO: Pod "execpod-affinitywfms5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00330282s
    Mar 29 07:58:28.725: INFO: Pod "execpod-affinitywfms5" satisfied condition "running"
    Mar 29 07:58:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar 29 07:58:29.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar 29 07:58:29.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 07:58:29.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.80.194 80'
    Mar 29 07:58:29.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.80.194 80\nConnection to 10.100.80.194 80 port [tcp/http] succeeded!\n"
    Mar 29 07:58:29.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 07:58:29.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.80.194:80/ ; done'
    Mar 29 07:58:30.038: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
    Mar 29 07:58:30.038: INFO: stdout: "\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq\naffinity-clusterip-timeout-6m8vq"
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.038: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.039: INFO: Received response from host: affinity-clusterip-timeout-6m8vq
    Mar 29 07:58:30.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.80.194:80/'
    Mar 29 07:58:30.126: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
    Mar 29 07:58:30.126: INFO: stdout: "affinity-clusterip-timeout-6m8vq"
    Mar 29 08:00:40.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4263 exec execpod-affinitywfms5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.80.194:80/'
    Mar 29 08:00:40.220: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.80.194:80/\n"
    Mar 29 08:00:40.220: INFO: stdout: "affinity-clusterip-timeout-n8ptz"
    Mar 29 08:00:40.220: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4263, will wait for the garbage collector to delete the pods 03/29/23 08:00:40.231
    Mar 29 08:00:40.288: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.139788ms
    Mar 29 08:00:40.388: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.846841ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:00:42.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4263" for this suite. 03/29/23 08:00:42.299
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:00:42.302
Mar 29 08:00:42.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:00:42.302
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:42.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:42.311
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/29/23 08:00:42.312
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/29/23 08:00:42.314
STEP: patching the secret 03/29/23 08:00:42.316
STEP: deleting the secret using a LabelSelector 03/29/23 08:00:42.32
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/29/23 08:00:42.323
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:00:42.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2803" for this suite. 03/29/23 08:00:42.326
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":53,"skipped":932,"failed":0}
------------------------------
â€¢ [0.027 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:00:42.302
    Mar 29 08:00:42.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:00:42.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:42.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:42.311
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/29/23 08:00:42.312
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/29/23 08:00:42.314
    STEP: patching the secret 03/29/23 08:00:42.316
    STEP: deleting the secret using a LabelSelector 03/29/23 08:00:42.32
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/29/23 08:00:42.323
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:00:42.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2803" for this suite. 03/29/23 08:00:42.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:00:42.329
Mar 29 08:00:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 08:00:42.329
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:42.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:42.337
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/29/23 08:00:42.34
Mar 29 08:00:42.344: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6302" to be "running and ready"
Mar 29 08:00:42.345: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.158284ms
Mar 29 08:00:42.345: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:00:44.348: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003988575s
Mar 29 08:00:44.348: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 29 08:00:44.348: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/29/23 08:00:44.35
Mar 29 08:00:44.353: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6302" to be "running and ready"
Mar 29 08:00:44.354: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259724ms
Mar 29 08:00:44.354: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:00:46.357: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004131731s
Mar 29 08:00:46.357: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 29 08:00:46.357: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/29/23 08:00:46.359
Mar 29 08:00:46.362: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 29 08:00:46.364: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 29 08:00:48.364: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 29 08:00:48.365: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/29/23 08:00:48.365
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 29 08:00:48.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6302" for this suite. 03/29/23 08:00:48.375
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":54,"skipped":945,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.048 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:00:42.329
    Mar 29 08:00:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 08:00:42.329
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:42.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:42.337
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/29/23 08:00:42.34
    Mar 29 08:00:42.344: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6302" to be "running and ready"
    Mar 29 08:00:42.345: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.158284ms
    Mar 29 08:00:42.345: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:00:44.348: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003988575s
    Mar 29 08:00:44.348: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 29 08:00:44.348: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/29/23 08:00:44.35
    Mar 29 08:00:44.353: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6302" to be "running and ready"
    Mar 29 08:00:44.354: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259724ms
    Mar 29 08:00:44.354: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:00:46.357: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004131731s
    Mar 29 08:00:46.357: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 29 08:00:46.357: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/29/23 08:00:46.359
    Mar 29 08:00:46.362: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 29 08:00:46.364: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 29 08:00:48.364: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 29 08:00:48.365: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/29/23 08:00:48.365
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 29 08:00:48.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6302" for this suite. 03/29/23 08:00:48.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:00:48.378
Mar 29 08:00:48.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:00:48.379
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:48.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:48.386
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/29/23 08:00:48.388
Mar 29 08:00:48.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:00:50.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:01:00.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5919" for this suite. 03/29/23 08:01:00.021
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":55,"skipped":971,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.646 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:00:48.378
    Mar 29 08:00:48.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:00:48.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:00:48.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:00:48.386
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/29/23 08:00:48.388
    Mar 29 08:00:48.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:00:50.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:01:00.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5919" for this suite. 03/29/23 08:01:00.021
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:00.024
Mar 29 08:01:00.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:01:00.025
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:00.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:00.033
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 29 08:01:00.034: INFO: Creating ReplicaSet my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8
Mar 29 08:01:00.038: INFO: Pod name my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Found 0 pods out of 1
Mar 29 08:01:05.040: INFO: Pod name my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Found 1 pods out of 1
Mar 29 08:01:05.040: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8" is running
Mar 29 08:01:05.041: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" in namespace "replicaset-5830" to be "running"
Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f": Phase="Running", Reason="", readiness=true. Elapsed: 1.408382ms
Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" satisfied condition "running"
Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:00 +0000 UTC Reason: Message:}])
Mar 29 08:01:05.042: INFO: Trying to dial the pod
Mar 29 08:01:10.049: INFO: Controller my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Got expected result from replica 1 [my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f]: "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:01:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5830" for this suite. 03/29/23 08:01:10.05
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":56,"skipped":974,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.029 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:00.024
    Mar 29 08:01:00.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:01:00.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:00.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:00.033
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 29 08:01:00.034: INFO: Creating ReplicaSet my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8
    Mar 29 08:01:00.038: INFO: Pod name my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Found 0 pods out of 1
    Mar 29 08:01:05.040: INFO: Pod name my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Found 1 pods out of 1
    Mar 29 08:01:05.040: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8" is running
    Mar 29 08:01:05.041: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" in namespace "replicaset-5830" to be "running"
    Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f": Phase="Running", Reason="", readiness=true. Elapsed: 1.408382ms
    Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" satisfied condition "running"
    Mar 29 08:01:05.042: INFO: Pod "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:01:00 +0000 UTC Reason: Message:}])
    Mar 29 08:01:05.042: INFO: Trying to dial the pod
    Mar 29 08:01:10.049: INFO: Controller my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8: Got expected result from replica 1 [my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f]: "my-hostname-basic-a32a7686-eaa5-4c3f-9c0d-3e4eb4d8b2d8-qfn6f", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:01:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5830" for this suite. 03/29/23 08:01:10.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:10.054
Mar 29 08:01:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:01:10.054
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:10.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:10.062
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:01:10.064
Mar 29 08:01:10.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b" in namespace "projected-7252" to be "Succeeded or Failed"
Mar 29 08:01:10.069: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202306ms
Mar 29 08:01:12.072: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526877s
Mar 29 08:01:14.071: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004439911s
STEP: Saw pod success 03/29/23 08:01:14.071
Mar 29 08:01:14.072: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b" satisfied condition "Succeeded or Failed"
Mar 29 08:01:14.073: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b container client-container: <nil>
STEP: delete the pod 03/29/23 08:01:14.075
Mar 29 08:01:14.082: INFO: Waiting for pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b to disappear
Mar 29 08:01:14.083: INFO: Pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:01:14.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7252" for this suite. 03/29/23 08:01:14.085
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":57,"skipped":983,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:10.054
    Mar 29 08:01:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:01:10.054
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:10.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:10.062
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:01:10.064
    Mar 29 08:01:10.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b" in namespace "projected-7252" to be "Succeeded or Failed"
    Mar 29 08:01:10.069: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202306ms
    Mar 29 08:01:12.072: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526877s
    Mar 29 08:01:14.071: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004439911s
    STEP: Saw pod success 03/29/23 08:01:14.071
    Mar 29 08:01:14.072: INFO: Pod "downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b" satisfied condition "Succeeded or Failed"
    Mar 29 08:01:14.073: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b container client-container: <nil>
    STEP: delete the pod 03/29/23 08:01:14.075
    Mar 29 08:01:14.082: INFO: Waiting for pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b to disappear
    Mar 29 08:01:14.083: INFO: Pod downwardapi-volume-5ee7f4f7-2547-4915-aa22-bb753b5b1b1b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:01:14.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7252" for this suite. 03/29/23 08:01:14.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:14.087
Mar 29 08:01:14.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:01:14.088
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:14.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:14.094
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/29/23 08:01:14.096
Mar 29 08:01:14.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 create -f -'
Mar 29 08:01:14.503: INFO: stderr: ""
Mar 29 08:01:14.503: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:01:14.503
Mar 29 08:01:14.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:01:14.546: INFO: stderr: ""
Mar 29 08:01:14.546: INFO: stdout: "update-demo-nautilus-9j7qg update-demo-nautilus-dpv8d "
Mar 29 08:01:14.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:01:14.586: INFO: stderr: ""
Mar 29 08:01:14.586: INFO: stdout: ""
Mar 29 08:01:14.586: INFO: update-demo-nautilus-9j7qg is created but not running
Mar 29 08:01:19.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:01:19.631: INFO: stderr: ""
Mar 29 08:01:19.631: INFO: stdout: "update-demo-nautilus-9j7qg update-demo-nautilus-dpv8d "
Mar 29 08:01:19.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:01:19.671: INFO: stderr: ""
Mar 29 08:01:19.671: INFO: stdout: "true"
Mar 29 08:01:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:01:19.714: INFO: stderr: ""
Mar 29 08:01:19.714: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:01:19.714: INFO: validating pod update-demo-nautilus-9j7qg
Mar 29 08:01:19.717: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:01:19.717: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:01:19.717: INFO: update-demo-nautilus-9j7qg is verified up and running
Mar 29 08:01:19.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-dpv8d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:01:19.756: INFO: stderr: ""
Mar 29 08:01:19.756: INFO: stdout: "true"
Mar 29 08:01:19.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-dpv8d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:01:19.795: INFO: stderr: ""
Mar 29 08:01:19.795: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:01:19.795: INFO: validating pod update-demo-nautilus-dpv8d
Mar 29 08:01:19.797: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:01:19.797: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:01:19.797: INFO: update-demo-nautilus-dpv8d is verified up and running
STEP: using delete to clean up resources 03/29/23 08:01:19.797
Mar 29 08:01:19.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 delete --grace-period=0 --force -f -'
Mar 29 08:01:19.840: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 08:01:19.840: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 29 08:01:19.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get rc,svc -l name=update-demo --no-headers'
Mar 29 08:01:19.882: INFO: stderr: "No resources found in kubectl-5096 namespace.\n"
Mar 29 08:01:19.882: INFO: stdout: ""
Mar 29 08:01:19.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 08:01:19.926: INFO: stderr: ""
Mar 29 08:01:19.926: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:01:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5096" for this suite. 03/29/23 08:01:19.928
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":58,"skipped":989,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.844 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:14.087
    Mar 29 08:01:14.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:01:14.088
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:14.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:14.094
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/29/23 08:01:14.096
    Mar 29 08:01:14.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 create -f -'
    Mar 29 08:01:14.503: INFO: stderr: ""
    Mar 29 08:01:14.503: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:01:14.503
    Mar 29 08:01:14.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:01:14.546: INFO: stderr: ""
    Mar 29 08:01:14.546: INFO: stdout: "update-demo-nautilus-9j7qg update-demo-nautilus-dpv8d "
    Mar 29 08:01:14.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:01:14.586: INFO: stderr: ""
    Mar 29 08:01:14.586: INFO: stdout: ""
    Mar 29 08:01:14.586: INFO: update-demo-nautilus-9j7qg is created but not running
    Mar 29 08:01:19.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:01:19.631: INFO: stderr: ""
    Mar 29 08:01:19.631: INFO: stdout: "update-demo-nautilus-9j7qg update-demo-nautilus-dpv8d "
    Mar 29 08:01:19.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:01:19.671: INFO: stderr: ""
    Mar 29 08:01:19.671: INFO: stdout: "true"
    Mar 29 08:01:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-9j7qg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:01:19.714: INFO: stderr: ""
    Mar 29 08:01:19.714: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:01:19.714: INFO: validating pod update-demo-nautilus-9j7qg
    Mar 29 08:01:19.717: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:01:19.717: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:01:19.717: INFO: update-demo-nautilus-9j7qg is verified up and running
    Mar 29 08:01:19.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-dpv8d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:01:19.756: INFO: stderr: ""
    Mar 29 08:01:19.756: INFO: stdout: "true"
    Mar 29 08:01:19.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods update-demo-nautilus-dpv8d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:01:19.795: INFO: stderr: ""
    Mar 29 08:01:19.795: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:01:19.795: INFO: validating pod update-demo-nautilus-dpv8d
    Mar 29 08:01:19.797: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:01:19.797: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:01:19.797: INFO: update-demo-nautilus-dpv8d is verified up and running
    STEP: using delete to clean up resources 03/29/23 08:01:19.797
    Mar 29 08:01:19.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 delete --grace-period=0 --force -f -'
    Mar 29 08:01:19.840: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 08:01:19.840: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 29 08:01:19.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get rc,svc -l name=update-demo --no-headers'
    Mar 29 08:01:19.882: INFO: stderr: "No resources found in kubectl-5096 namespace.\n"
    Mar 29 08:01:19.882: INFO: stdout: ""
    Mar 29 08:01:19.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5096 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 29 08:01:19.926: INFO: stderr: ""
    Mar 29 08:01:19.926: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:01:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5096" for this suite. 03/29/23 08:01:19.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:19.931
Mar 29 08:01:19.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:01:19.932
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:19.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:19.939
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 29 08:01:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:01:23.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8758" for this suite. 03/29/23 08:01:23.018
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":59,"skipped":997,"failed":0}
------------------------------
â€¢ [3.090 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:19.931
    Mar 29 08:01:19.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:01:19.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:19.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:19.939
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 29 08:01:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:01:23.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8758" for this suite. 03/29/23 08:01:23.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:23.023
Mar 29 08:01:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename namespaces 03/29/23 08:01:23.023
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:23.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:23.03
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/29/23 08:01:23.032
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:23.037
STEP: Creating a service in the namespace 03/29/23 08:01:23.039
STEP: Deleting the namespace 03/29/23 08:01:23.044
STEP: Waiting for the namespace to be removed. 03/29/23 08:01:23.049
STEP: Recreating the namespace 03/29/23 08:01:29.052
STEP: Verifying there is no service in the namespace 03/29/23 08:01:29.057
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:01:29.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2610" for this suite. 03/29/23 08:01:29.061
STEP: Destroying namespace "nsdeletetest-1982" for this suite. 03/29/23 08:01:29.063
Mar 29 08:01:29.064: INFO: Namespace nsdeletetest-1982 was already deleted
STEP: Destroying namespace "nsdeletetest-3171" for this suite. 03/29/23 08:01:29.064
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":60,"skipped":1021,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.043 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:23.023
    Mar 29 08:01:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename namespaces 03/29/23 08:01:23.023
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:23.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:23.03
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/29/23 08:01:23.032
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:23.037
    STEP: Creating a service in the namespace 03/29/23 08:01:23.039
    STEP: Deleting the namespace 03/29/23 08:01:23.044
    STEP: Waiting for the namespace to be removed. 03/29/23 08:01:23.049
    STEP: Recreating the namespace 03/29/23 08:01:29.052
    STEP: Verifying there is no service in the namespace 03/29/23 08:01:29.057
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:01:29.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2610" for this suite. 03/29/23 08:01:29.061
    STEP: Destroying namespace "nsdeletetest-1982" for this suite. 03/29/23 08:01:29.063
    Mar 29 08:01:29.064: INFO: Namespace nsdeletetest-1982 was already deleted
    STEP: Destroying namespace "nsdeletetest-3171" for this suite. 03/29/23 08:01:29.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:29.067
Mar 29 08:01:29.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename watch 03/29/23 08:01:29.067
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:29.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:29.075
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/29/23 08:01:29.077
STEP: creating a new configmap 03/29/23 08:01:29.077
STEP: modifying the configmap once 03/29/23 08:01:29.079
STEP: changing the label value of the configmap 03/29/23 08:01:29.082
STEP: Expecting to observe a delete notification for the watched object 03/29/23 08:01:29.085
Mar 29 08:01:29.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5322 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:01:29.085: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5323 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:01:29.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5324 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/29/23 08:01:29.085
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/29/23 08:01:29.088
STEP: changing the label value of the configmap back 03/29/23 08:01:39.089
STEP: modifying the configmap a third time 03/29/23 08:01:39.094
STEP: deleting the configmap 03/29/23 08:01:39.097
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/29/23 08:01:39.099
Mar 29 08:01:39.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5348 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:01:39.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5349 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:01:39.099: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5350 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 29 08:01:39.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7853" for this suite. 03/29/23 08:01:39.101
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":61,"skipped":1046,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.036 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:29.067
    Mar 29 08:01:29.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename watch 03/29/23 08:01:29.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:29.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:29.075
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/29/23 08:01:29.077
    STEP: creating a new configmap 03/29/23 08:01:29.077
    STEP: modifying the configmap once 03/29/23 08:01:29.079
    STEP: changing the label value of the configmap 03/29/23 08:01:29.082
    STEP: Expecting to observe a delete notification for the watched object 03/29/23 08:01:29.085
    Mar 29 08:01:29.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5322 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:01:29.085: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5323 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:01:29.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5324 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/29/23 08:01:29.085
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/29/23 08:01:29.088
    STEP: changing the label value of the configmap back 03/29/23 08:01:39.089
    STEP: modifying the configmap a third time 03/29/23 08:01:39.094
    STEP: deleting the configmap 03/29/23 08:01:39.097
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/29/23 08:01:39.099
    Mar 29 08:01:39.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5348 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:01:39.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5349 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:01:39.099: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7853  54acba42-21e2-4090-a45d-f87a004a2d4b 5350 0 2023-03-29 08:01:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-29 08:01:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 29 08:01:39.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7853" for this suite. 03/29/23 08:01:39.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:39.103
Mar 29 08:01:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename lease-test 03/29/23 08:01:39.104
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:39.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:39.111
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar 29 08:01:39.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5257" for this suite. 03/29/23 08:01:39.135
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":62,"skipped":1054,"failed":0}
------------------------------
â€¢ [0.033 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:39.103
    Mar 29 08:01:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename lease-test 03/29/23 08:01:39.104
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:39.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:39.111
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar 29 08:01:39.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-5257" for this suite. 03/29/23 08:01:39.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:01:39.137
Mar 29 08:01:39.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:01:39.138
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:39.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:39.145
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 29 08:01:39.151: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 08:02:39.161: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:02:39.162
Mar 29 08:02:39.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption-path 03/29/23 08:02:39.163
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:02:39.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:02:39.172
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/29/23 08:02:39.173
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:02:39.173
Mar 29 08:02:39.176: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8068" to be "running"
Mar 29 08:02:39.177: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231009ms
Mar 29 08:02:41.181: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00451829s
Mar 29 08:02:41.181: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:02:41.182
Mar 29 08:02:41.189: INFO: found a healthy node: 10.146.0.116
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar 29 08:02:53.222: INFO: pods created so far: [1 1 1]
Mar 29 08:02:53.222: INFO: length of pods created so far: 3
Mar 29 08:02:55.228: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar 29 08:03:02.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8068" for this suite. 03/29/23 08:03:02.231
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:03:02.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-295" for this suite. 03/29/23 08:03:02.249
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":63,"skipped":1059,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.130 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:01:39.137
    Mar 29 08:01:39.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:01:39.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:01:39.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:01:39.145
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 29 08:01:39.151: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 08:02:39.161: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:02:39.162
    Mar 29 08:02:39.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption-path 03/29/23 08:02:39.163
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:02:39.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:02:39.172
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/29/23 08:02:39.173
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:02:39.173
    Mar 29 08:02:39.176: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8068" to be "running"
    Mar 29 08:02:39.177: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231009ms
    Mar 29 08:02:41.181: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00451829s
    Mar 29 08:02:41.181: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:02:41.182
    Mar 29 08:02:41.189: INFO: found a healthy node: 10.146.0.116
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar 29 08:02:53.222: INFO: pods created so far: [1 1 1]
    Mar 29 08:02:53.222: INFO: length of pods created so far: 3
    Mar 29 08:02:55.228: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar 29 08:03:02.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8068" for this suite. 03/29/23 08:03:02.231
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:03:02.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-295" for this suite. 03/29/23 08:03:02.249
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:03:02.267
Mar 29 08:03:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:03:02.268
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:03:02.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:03:02.275
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6672 03/29/23 08:03:02.276
STEP: creating a selector 03/29/23 08:03:02.276
STEP: Creating the service pods in kubernetes 03/29/23 08:03:02.276
Mar 29 08:03:02.276: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 08:03:02.288: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6672" to be "running and ready"
Mar 29 08:03:02.290: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.001958ms
Mar 29 08:03:02.290: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:03:04.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004339177s
Mar 29 08:03:04.292: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:03:06.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005187524s
Mar 29 08:03:06.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:03:08.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00489285s
Mar 29 08:03:08.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:03:10.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004972371s
Mar 29 08:03:10.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:03:12.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004772454s
Mar 29 08:03:12.292: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:03:14.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004647643s
Mar 29 08:03:14.292: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 29 08:03:14.292: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 29 08:03:14.294: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6672" to be "running and ready"
Mar 29 08:03:14.295: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.20656ms
Mar 29 08:03:14.295: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 29 08:03:14.295: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 29 08:03:14.296: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6672" to be "running and ready"
Mar 29 08:03:14.297: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.135817ms
Mar 29 08:03:14.297: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 29 08:03:14.297: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/29/23 08:03:14.3
Mar 29 08:03:14.303: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6672" to be "running"
Mar 29 08:03:14.304: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.222071ms
Mar 29 08:03:16.307: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003906732s
Mar 29 08:03:16.307: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 29 08:03:16.308: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 29 08:03:16.308: INFO: Breadth first check of 192.168.30.38 on host 10.146.0.115...
Mar 29 08:03:16.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.30.38&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:03:16.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:03:16.310: INFO: ExecWithOptions: Clientset creation
Mar 29 08:03:16.310: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.30.38%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 08:03:16.353: INFO: Waiting for responses: map[]
Mar 29 08:03:16.353: INFO: reached 192.168.30.38 after 0/1 tries
Mar 29 08:03:16.353: INFO: Breadth first check of 192.168.219.163 on host 10.146.0.116...
Mar 29 08:03:16.355: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.219.163&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:03:16.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:03:16.355: INFO: ExecWithOptions: Clientset creation
Mar 29 08:03:16.355: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.219.163%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 08:03:16.399: INFO: Waiting for responses: map[]
Mar 29 08:03:16.399: INFO: reached 192.168.219.163 after 0/1 tries
Mar 29 08:03:16.399: INFO: Breadth first check of 192.168.87.206 on host 10.146.0.117...
Mar 29 08:03:16.400: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.87.206&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:03:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:03:16.401: INFO: ExecWithOptions: Clientset creation
Mar 29 08:03:16.401: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.87.206%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 29 08:03:16.444: INFO: Waiting for responses: map[]
Mar 29 08:03:16.444: INFO: reached 192.168.87.206 after 0/1 tries
Mar 29 08:03:16.444: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 29 08:03:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6672" for this suite. 03/29/23 08:03:16.446
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":64,"skipped":1068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.181 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:03:02.267
    Mar 29 08:03:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:03:02.268
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:03:02.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:03:02.275
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6672 03/29/23 08:03:02.276
    STEP: creating a selector 03/29/23 08:03:02.276
    STEP: Creating the service pods in kubernetes 03/29/23 08:03:02.276
    Mar 29 08:03:02.276: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 29 08:03:02.288: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6672" to be "running and ready"
    Mar 29 08:03:02.290: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.001958ms
    Mar 29 08:03:02.290: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:03:04.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004339177s
    Mar 29 08:03:04.292: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:03:06.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005187524s
    Mar 29 08:03:06.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:03:08.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00489285s
    Mar 29 08:03:08.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:03:10.293: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004972371s
    Mar 29 08:03:10.293: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:03:12.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004772454s
    Mar 29 08:03:12.292: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:03:14.292: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004647643s
    Mar 29 08:03:14.292: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 29 08:03:14.292: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 29 08:03:14.294: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6672" to be "running and ready"
    Mar 29 08:03:14.295: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.20656ms
    Mar 29 08:03:14.295: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 29 08:03:14.295: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 29 08:03:14.296: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6672" to be "running and ready"
    Mar 29 08:03:14.297: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.135817ms
    Mar 29 08:03:14.297: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 29 08:03:14.297: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/29/23 08:03:14.3
    Mar 29 08:03:14.303: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6672" to be "running"
    Mar 29 08:03:14.304: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.222071ms
    Mar 29 08:03:16.307: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003906732s
    Mar 29 08:03:16.307: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 29 08:03:16.308: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 29 08:03:16.308: INFO: Breadth first check of 192.168.30.38 on host 10.146.0.115...
    Mar 29 08:03:16.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.30.38&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:03:16.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:03:16.310: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:03:16.310: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.30.38%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 08:03:16.353: INFO: Waiting for responses: map[]
    Mar 29 08:03:16.353: INFO: reached 192.168.30.38 after 0/1 tries
    Mar 29 08:03:16.353: INFO: Breadth first check of 192.168.219.163 on host 10.146.0.116...
    Mar 29 08:03:16.355: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.219.163&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:03:16.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:03:16.355: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:03:16.355: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.219.163%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 08:03:16.399: INFO: Waiting for responses: map[]
    Mar 29 08:03:16.399: INFO: reached 192.168.219.163 after 0/1 tries
    Mar 29 08:03:16.399: INFO: Breadth first check of 192.168.87.206 on host 10.146.0.117...
    Mar 29 08:03:16.400: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.30.39:9080/dial?request=hostname&protocol=udp&host=192.168.87.206&port=8081&tries=1'] Namespace:pod-network-test-6672 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:03:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:03:16.401: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:03:16.401: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-6672/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.30.39%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.87.206%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 29 08:03:16.444: INFO: Waiting for responses: map[]
    Mar 29 08:03:16.444: INFO: reached 192.168.87.206 after 0/1 tries
    Mar 29 08:03:16.444: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 29 08:03:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6672" for this suite. 03/29/23 08:03:16.446
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:03:16.449
Mar 29 08:03:16.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-pred 03/29/23 08:03:16.449
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:03:16.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:03:16.459
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 29 08:03:16.460: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 08:03:16.463: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 08:03:16.464: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.115 before test
Mar 29 08:03:16.467: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:03:16.467: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:03:16.467: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:03:16.467: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:03:16.467: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:03:16.467: INFO: netserver-0 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container webserver ready: true, restart count 0
Mar 29 08:03:16.467: INFO: test-container-pod from pod-network-test-6672 started at 2023-03-29 08:03:14 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container webserver ready: true, restart count 0
Mar 29 08:03:16.467: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:03:16.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:03:16.467: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:03:16.467: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.116 before test
Mar 29 08:03:16.469: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.469: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:03:16.469: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.469: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:03:16.469: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:03:16.469: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:03:16.469: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:03:16.469: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:03:16.469: INFO: netserver-1 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.469: INFO: 	Container webserver ready: true, restart count 0
Mar 29 08:03:16.469: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:03:16.469: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:03:16.469: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:03:16.469: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.117 before test
Mar 29 08:03:16.472: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 08:03:16.472: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:03:16.472: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:03:16.472: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:03:16.472: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:03:16.472: INFO: netserver-2 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container webserver ready: true, restart count 0
Mar 29 08:03:16.472: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 08:03:16.472: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container e2e ready: true, restart count 0
Mar 29 08:03:16.472: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:03:16.472: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:03:16.472: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:03:16.472: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:03:16.472
Mar 29 08:03:16.475: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5678" to be "running"
Mar 29 08:03:16.476: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.260214ms
Mar 29 08:03:18.478: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003222685s
Mar 29 08:03:18.478: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:03:18.48
STEP: Trying to apply a random label on the found node. 03/29/23 08:03:18.486
STEP: verifying the node has the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd 95 03/29/23 08:03:18.491
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/29/23 08:03:18.493
Mar 29 08:03:18.495: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5678" to be "not pending"
Mar 29 08:03:18.496: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.18807ms
Mar 29 08:03:20.499: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.003791432s
Mar 29 08:03:20.499: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.146.0.116 on the node which pod4 resides and expect not scheduled 03/29/23 08:03:20.499
Mar 29 08:03:20.502: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5678" to be "not pending"
Mar 29 08:03:20.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321812ms
Mar 29 08:03:22.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004816298s
Mar 29 08:03:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00373793s
Mar 29 08:03:26.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003788188s
Mar 29 08:03:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003719057s
Mar 29 08:03:30.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004588285s
Mar 29 08:03:32.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004625328s
Mar 29 08:03:34.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003929382s
Mar 29 08:03:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00484833s
Mar 29 08:03:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00376431s
Mar 29 08:03:40.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004753254s
Mar 29 08:03:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006504024s
Mar 29 08:03:44.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003328196s
Mar 29 08:03:46.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.003572961s
Mar 29 08:03:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.003453145s
Mar 29 08:03:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004313065s
Mar 29 08:03:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005173356s
Mar 29 08:03:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004290379s
Mar 29 08:03:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005233928s
Mar 29 08:03:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.003206512s
Mar 29 08:04:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004985648s
Mar 29 08:04:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004834578s
Mar 29 08:04:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.003738463s
Mar 29 08:04:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004804588s
Mar 29 08:04:08.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003342152s
Mar 29 08:04:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005150084s
Mar 29 08:04:12.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004110623s
Mar 29 08:04:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003947769s
Mar 29 08:04:16.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004899455s
Mar 29 08:04:18.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003849152s
Mar 29 08:04:20.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004758178s
Mar 29 08:04:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004146048s
Mar 29 08:04:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004181566s
Mar 29 08:04:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005106318s
Mar 29 08:04:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004030743s
Mar 29 08:04:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004862115s
Mar 29 08:04:32.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0032183s
Mar 29 08:04:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003360176s
Mar 29 08:04:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005172713s
Mar 29 08:04:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004413264s
Mar 29 08:04:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004188674s
Mar 29 08:04:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005063802s
Mar 29 08:04:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004037825s
Mar 29 08:04:46.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004177734s
Mar 29 08:04:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00358261s
Mar 29 08:04:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003841308s
Mar 29 08:04:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004712392s
Mar 29 08:04:54.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003660184s
Mar 29 08:04:56.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.003606507s
Mar 29 08:04:58.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003708004s
Mar 29 08:05:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004796372s
Mar 29 08:05:02.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004482821s
Mar 29 08:05:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004017728s
Mar 29 08:05:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004878812s
Mar 29 08:05:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003878078s
Mar 29 08:05:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004688364s
Mar 29 08:05:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004809684s
Mar 29 08:05:14.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.003625481s
Mar 29 08:05:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004607451s
Mar 29 08:05:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.003623213s
Mar 29 08:05:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004442448s
Mar 29 08:05:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00419234s
Mar 29 08:05:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004161852s
Mar 29 08:05:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004948586s
Mar 29 08:05:28.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.003562149s
Mar 29 08:05:30.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.003826287s
Mar 29 08:05:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004689237s
Mar 29 08:05:34.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00401476s
Mar 29 08:05:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004865309s
Mar 29 08:05:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.003881753s
Mar 29 08:05:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00381201s
Mar 29 08:05:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.004817164s
Mar 29 08:05:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004053765s
Mar 29 08:05:46.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005035457s
Mar 29 08:05:48.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004047035s
Mar 29 08:05:50.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004752707s
Mar 29 08:05:52.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004165094s
Mar 29 08:05:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003975247s
Mar 29 08:05:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005204544s
Mar 29 08:05:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.003197223s
Mar 29 08:06:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005125351s
Mar 29 08:06:02.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.003225614s
Mar 29 08:06:04.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.003556489s
Mar 29 08:06:06.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004404269s
Mar 29 08:06:08.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00343867s
Mar 29 08:06:10.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004444308s
Mar 29 08:06:12.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004417707s
Mar 29 08:06:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00444932s
Mar 29 08:06:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004184476s
Mar 29 08:06:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.003129613s
Mar 29 08:06:20.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005038009s
Mar 29 08:06:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004218035s
Mar 29 08:06:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00413382s
Mar 29 08:06:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00506039s
Mar 29 08:06:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004347317s
Mar 29 08:06:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005138585s
Mar 29 08:06:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005010911s
Mar 29 08:06:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003552345s
Mar 29 08:06:36.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004415583s
Mar 29 08:06:38.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.003479138s
Mar 29 08:06:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004562414s
Mar 29 08:06:42.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004493073s
Mar 29 08:06:44.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.003456157s
Mar 29 08:06:46.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004396748s
Mar 29 08:06:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.003438571s
Mar 29 08:06:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00423023s
Mar 29 08:06:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005191211s
Mar 29 08:06:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004085181s
Mar 29 08:06:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004980123s
Mar 29 08:06:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.003272496s
Mar 29 08:07:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00505635s
Mar 29 08:07:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004864773s
Mar 29 08:07:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004357465s
Mar 29 08:07:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005137002s
Mar 29 08:07:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004193095s
Mar 29 08:07:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.004912262s
Mar 29 08:07:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004743042s
Mar 29 08:07:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.003813433s
Mar 29 08:07:16.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004823241s
Mar 29 08:07:18.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.003933553s
Mar 29 08:07:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004446549s
Mar 29 08:07:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004164592s
Mar 29 08:07:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004280499s
Mar 29 08:07:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005175865s
Mar 29 08:07:28.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.003127978s
Mar 29 08:07:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005099021s
Mar 29 08:07:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005068989s
Mar 29 08:07:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.003593924s
Mar 29 08:07:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004678504s
Mar 29 08:07:38.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.003388326s
Mar 29 08:07:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004264515s
Mar 29 08:07:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005106517s
Mar 29 08:07:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004055775s
Mar 29 08:07:46.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00515255s
Mar 29 08:07:48.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004195688s
Mar 29 08:07:50.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00496794s
Mar 29 08:07:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004851479s
Mar 29 08:07:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.003865345s
Mar 29 08:07:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004877883s
Mar 29 08:07:58.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004051451s
Mar 29 08:08:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004817136s
Mar 29 08:08:02.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004658599s
Mar 29 08:08:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.003694315s
Mar 29 08:08:06.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004629462s
Mar 29 08:08:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.003962246s
Mar 29 08:08:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004828044s
Mar 29 08:08:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004732027s
Mar 29 08:08:14.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003594962s
Mar 29 08:08:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004361115s
Mar 29 08:08:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.003448679s
Mar 29 08:08:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004522385s
Mar 29 08:08:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005986155s
STEP: removing the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd off the node 10.146.0.116 03/29/23 08:08:20.508
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd 03/29/23 08:08:20.514
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:08:20.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5678" for this suite. 03/29/23 08:08:20.517
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":65,"skipped":1072,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.071 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:03:16.449
    Mar 29 08:03:16.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-pred 03/29/23 08:03:16.449
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:03:16.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:03:16.459
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 29 08:03:16.460: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 29 08:03:16.463: INFO: Waiting for terminating namespaces to be deleted...
    Mar 29 08:03:16.464: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.115 before test
    Mar 29 08:03:16.467: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: netserver-0 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: test-container-pod from pod-network-test-6672 started at 2023-03-29 08:03:14 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:03:16.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:03:16.467: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.116 before test
    Mar 29 08:03:16.469: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.469: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.469: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:03:16.469: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: netserver-1 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.469: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:03:16.469: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:03:16.469: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.117 before test
    Mar 29 08:03:16.472: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: netserver-2 from pod-network-test-6672 started at 2023-03-29 08:03:02 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container webserver ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container e2e ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:03:16.472: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:03:16.472: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:03:16.472
    Mar 29 08:03:16.475: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5678" to be "running"
    Mar 29 08:03:16.476: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.260214ms
    Mar 29 08:03:18.478: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003222685s
    Mar 29 08:03:18.478: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:03:18.48
    STEP: Trying to apply a random label on the found node. 03/29/23 08:03:18.486
    STEP: verifying the node has the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd 95 03/29/23 08:03:18.491
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/29/23 08:03:18.493
    Mar 29 08:03:18.495: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5678" to be "not pending"
    Mar 29 08:03:18.496: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.18807ms
    Mar 29 08:03:20.499: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.003791432s
    Mar 29 08:03:20.499: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.146.0.116 on the node which pod4 resides and expect not scheduled 03/29/23 08:03:20.499
    Mar 29 08:03:20.502: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5678" to be "not pending"
    Mar 29 08:03:20.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321812ms
    Mar 29 08:03:22.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004816298s
    Mar 29 08:03:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00373793s
    Mar 29 08:03:26.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003788188s
    Mar 29 08:03:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003719057s
    Mar 29 08:03:30.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004588285s
    Mar 29 08:03:32.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004625328s
    Mar 29 08:03:34.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.003929382s
    Mar 29 08:03:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00484833s
    Mar 29 08:03:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00376431s
    Mar 29 08:03:40.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004753254s
    Mar 29 08:03:42.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006504024s
    Mar 29 08:03:44.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003328196s
    Mar 29 08:03:46.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.003572961s
    Mar 29 08:03:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.003453145s
    Mar 29 08:03:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004313065s
    Mar 29 08:03:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005173356s
    Mar 29 08:03:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004290379s
    Mar 29 08:03:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005233928s
    Mar 29 08:03:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.003206512s
    Mar 29 08:04:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004985648s
    Mar 29 08:04:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004834578s
    Mar 29 08:04:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.003738463s
    Mar 29 08:04:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004804588s
    Mar 29 08:04:08.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003342152s
    Mar 29 08:04:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005150084s
    Mar 29 08:04:12.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004110623s
    Mar 29 08:04:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003947769s
    Mar 29 08:04:16.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004899455s
    Mar 29 08:04:18.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003849152s
    Mar 29 08:04:20.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004758178s
    Mar 29 08:04:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004146048s
    Mar 29 08:04:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004181566s
    Mar 29 08:04:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005106318s
    Mar 29 08:04:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004030743s
    Mar 29 08:04:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004862115s
    Mar 29 08:04:32.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0032183s
    Mar 29 08:04:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003360176s
    Mar 29 08:04:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005172713s
    Mar 29 08:04:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004413264s
    Mar 29 08:04:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004188674s
    Mar 29 08:04:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005063802s
    Mar 29 08:04:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004037825s
    Mar 29 08:04:46.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004177734s
    Mar 29 08:04:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00358261s
    Mar 29 08:04:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003841308s
    Mar 29 08:04:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004712392s
    Mar 29 08:04:54.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003660184s
    Mar 29 08:04:56.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.003606507s
    Mar 29 08:04:58.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003708004s
    Mar 29 08:05:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004796372s
    Mar 29 08:05:02.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004482821s
    Mar 29 08:05:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004017728s
    Mar 29 08:05:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004878812s
    Mar 29 08:05:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003878078s
    Mar 29 08:05:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004688364s
    Mar 29 08:05:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004809684s
    Mar 29 08:05:14.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.003625481s
    Mar 29 08:05:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004607451s
    Mar 29 08:05:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.003623213s
    Mar 29 08:05:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004442448s
    Mar 29 08:05:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00419234s
    Mar 29 08:05:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004161852s
    Mar 29 08:05:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004948586s
    Mar 29 08:05:28.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.003562149s
    Mar 29 08:05:30.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.003826287s
    Mar 29 08:05:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004689237s
    Mar 29 08:05:34.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00401476s
    Mar 29 08:05:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004865309s
    Mar 29 08:05:38.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.003881753s
    Mar 29 08:05:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00381201s
    Mar 29 08:05:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.004817164s
    Mar 29 08:05:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004053765s
    Mar 29 08:05:46.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005035457s
    Mar 29 08:05:48.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004047035s
    Mar 29 08:05:50.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004752707s
    Mar 29 08:05:52.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004165094s
    Mar 29 08:05:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003975247s
    Mar 29 08:05:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005204544s
    Mar 29 08:05:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.003197223s
    Mar 29 08:06:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005125351s
    Mar 29 08:06:02.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.003225614s
    Mar 29 08:06:04.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.003556489s
    Mar 29 08:06:06.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004404269s
    Mar 29 08:06:08.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00343867s
    Mar 29 08:06:10.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004444308s
    Mar 29 08:06:12.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004417707s
    Mar 29 08:06:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00444932s
    Mar 29 08:06:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004184476s
    Mar 29 08:06:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.003129613s
    Mar 29 08:06:20.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005038009s
    Mar 29 08:06:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004218035s
    Mar 29 08:06:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00413382s
    Mar 29 08:06:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00506039s
    Mar 29 08:06:28.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004347317s
    Mar 29 08:06:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005138585s
    Mar 29 08:06:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005010911s
    Mar 29 08:06:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003552345s
    Mar 29 08:06:36.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004415583s
    Mar 29 08:06:38.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.003479138s
    Mar 29 08:06:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004562414s
    Mar 29 08:06:42.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004493073s
    Mar 29 08:06:44.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.003456157s
    Mar 29 08:06:46.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004396748s
    Mar 29 08:06:48.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.003438571s
    Mar 29 08:06:50.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00423023s
    Mar 29 08:06:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005191211s
    Mar 29 08:06:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004085181s
    Mar 29 08:06:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004980123s
    Mar 29 08:06:58.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.003272496s
    Mar 29 08:07:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00505635s
    Mar 29 08:07:02.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004864773s
    Mar 29 08:07:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004357465s
    Mar 29 08:07:06.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005137002s
    Mar 29 08:07:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004193095s
    Mar 29 08:07:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.004912262s
    Mar 29 08:07:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004743042s
    Mar 29 08:07:14.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.003813433s
    Mar 29 08:07:16.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004823241s
    Mar 29 08:07:18.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.003933553s
    Mar 29 08:07:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004446549s
    Mar 29 08:07:22.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004164592s
    Mar 29 08:07:24.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004280499s
    Mar 29 08:07:26.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005175865s
    Mar 29 08:07:28.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.003127978s
    Mar 29 08:07:30.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005099021s
    Mar 29 08:07:32.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005068989s
    Mar 29 08:07:34.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.003593924s
    Mar 29 08:07:36.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004678504s
    Mar 29 08:07:38.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.003388326s
    Mar 29 08:07:40.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004264515s
    Mar 29 08:07:42.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005106517s
    Mar 29 08:07:44.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004055775s
    Mar 29 08:07:46.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00515255s
    Mar 29 08:07:48.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004195688s
    Mar 29 08:07:50.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00496794s
    Mar 29 08:07:52.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004851479s
    Mar 29 08:07:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.003865345s
    Mar 29 08:07:56.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004877883s
    Mar 29 08:07:58.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004051451s
    Mar 29 08:08:00.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004817136s
    Mar 29 08:08:02.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004658599s
    Mar 29 08:08:04.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.003694315s
    Mar 29 08:08:06.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004629462s
    Mar 29 08:08:08.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.003962246s
    Mar 29 08:08:10.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004828044s
    Mar 29 08:08:12.507: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004732027s
    Mar 29 08:08:14.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003594962s
    Mar 29 08:08:16.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004361115s
    Mar 29 08:08:18.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.003448679s
    Mar 29 08:08:20.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004522385s
    Mar 29 08:08:20.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005986155s
    STEP: removing the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd off the node 10.146.0.116 03/29/23 08:08:20.508
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-dad5e22a-2f8f-471a-be78-6155f0ab05dd 03/29/23 08:08:20.514
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:08:20.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5678" for this suite. 03/29/23 08:08:20.517
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:08:20.521
Mar 29 08:08:20.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename hostport 03/29/23 08:08:20.521
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:08:20.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:08:20.529
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/29/23 08:08:20.531
Mar 29 08:08:20.534: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2368" to be "running and ready"
Mar 29 08:08:20.536: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302857ms
Mar 29 08:08:20.536: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:08:22.538: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003940822s
Mar 29 08:08:22.538: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 29 08:08:22.538: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.146.0.116 on the node which pod1 resides and expect scheduled 03/29/23 08:08:22.538
Mar 29 08:08:22.542: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2368" to be "running and ready"
Mar 29 08:08:22.544: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.527523ms
Mar 29 08:08:22.544: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:08:24.546: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003603297s
Mar 29 08:08:24.546: INFO: The phase of Pod pod2 is Running (Ready = false)
Mar 29 08:08:26.547: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004579109s
Mar 29 08:08:26.547: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 29 08:08:26.547: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.146.0.116 but use UDP protocol on the node which pod2 resides 03/29/23 08:08:26.547
Mar 29 08:08:26.550: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2368" to be "running and ready"
Mar 29 08:08:26.551: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25602ms
Mar 29 08:08:26.551: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:08:28.553: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003261917s
Mar 29 08:08:28.553: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 29 08:08:28.553: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 29 08:08:28.556: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2368" to be "running and ready"
Mar 29 08:08:28.557: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31441ms
Mar 29 08:08:28.557: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:08:30.559: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00344281s
Mar 29 08:08:30.559: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 29 08:08:30.559: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/29/23 08:08:30.561
Mar 29 08:08:30.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.146.0.116 http://127.0.0.1:54323/hostname] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:08:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:08:30.561: INFO: ExecWithOptions: Clientset creation
Mar 29 08:08:30.561: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.146.0.116+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.146.0.116, port: 54323 03/29/23 08:08:30.609
Mar 29 08:08:30.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.146.0.116:54323/hostname] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:08:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:08:30.610: INFO: ExecWithOptions: Clientset creation
Mar 29 08:08:30.610: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.146.0.116%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.146.0.116, port: 54323 UDP 03/29/23 08:08:30.654
Mar 29 08:08:30.654: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.146.0.116 54323] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:08:30.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:08:30.654: INFO: ExecWithOptions: Clientset creation
Mar 29 08:08:30.654: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.146.0.116+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar 29 08:08:35.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2368" for this suite. 03/29/23 08:08:35.702
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":66,"skipped":1098,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.185 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:08:20.521
    Mar 29 08:08:20.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename hostport 03/29/23 08:08:20.521
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:08:20.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:08:20.529
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/29/23 08:08:20.531
    Mar 29 08:08:20.534: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2368" to be "running and ready"
    Mar 29 08:08:20.536: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302857ms
    Mar 29 08:08:20.536: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:08:22.538: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003940822s
    Mar 29 08:08:22.538: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 29 08:08:22.538: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.146.0.116 on the node which pod1 resides and expect scheduled 03/29/23 08:08:22.538
    Mar 29 08:08:22.542: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2368" to be "running and ready"
    Mar 29 08:08:22.544: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.527523ms
    Mar 29 08:08:22.544: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:08:24.546: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.003603297s
    Mar 29 08:08:24.546: INFO: The phase of Pod pod2 is Running (Ready = false)
    Mar 29 08:08:26.547: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004579109s
    Mar 29 08:08:26.547: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 29 08:08:26.547: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.146.0.116 but use UDP protocol on the node which pod2 resides 03/29/23 08:08:26.547
    Mar 29 08:08:26.550: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2368" to be "running and ready"
    Mar 29 08:08:26.551: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25602ms
    Mar 29 08:08:26.551: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:08:28.553: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003261917s
    Mar 29 08:08:28.553: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 29 08:08:28.553: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 29 08:08:28.556: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2368" to be "running and ready"
    Mar 29 08:08:28.557: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31441ms
    Mar 29 08:08:28.557: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:08:30.559: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00344281s
    Mar 29 08:08:30.559: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 29 08:08:30.559: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/29/23 08:08:30.561
    Mar 29 08:08:30.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.146.0.116 http://127.0.0.1:54323/hostname] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:08:30.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:08:30.561: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:08:30.561: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.146.0.116+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.146.0.116, port: 54323 03/29/23 08:08:30.609
    Mar 29 08:08:30.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.146.0.116:54323/hostname] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:08:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:08:30.610: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:08:30.610: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.146.0.116%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.146.0.116, port: 54323 UDP 03/29/23 08:08:30.654
    Mar 29 08:08:30.654: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.146.0.116 54323] Namespace:hostport-2368 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:08:30.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:08:30.654: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:08:30.654: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/hostport-2368/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.146.0.116+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar 29 08:08:35.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-2368" for this suite. 03/29/23 08:08:35.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:08:35.706
Mar 29 08:08:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename taint-multiple-pods 03/29/23 08:08:35.706
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:08:35.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:08:35.713
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar 29 08:08:35.715: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 08:09:35.724: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar 29 08:09:35.726: INFO: Starting informer...
STEP: Starting pods... 03/29/23 08:09:35.726
Mar 29 08:09:35.935: INFO: Pod1 is running on 10.146.0.115. Tainting Node
Mar 29 08:09:36.140: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3710" to be "running"
Mar 29 08:09:36.141: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.410951ms
Mar 29 08:09:38.144: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004328017s
Mar 29 08:09:38.144: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 29 08:09:38.144: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3710" to be "running"
Mar 29 08:09:38.146: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.376858ms
Mar 29 08:09:38.146: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 29 08:09:38.146: INFO: Pod2 is running on 10.146.0.115. Tainting Node
STEP: Trying to apply a taint on the Node 03/29/23 08:09:38.146
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 08:09:38.152
STEP: Waiting for Pod1 and Pod2 to be deleted 03/29/23 08:09:38.154
Mar 29 08:09:44.039: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 29 08:10:04.070: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 08:10:04.076
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:10:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3710" for this suite. 03/29/23 08:10:04.079
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":67,"skipped":1101,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.375 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:08:35.706
    Mar 29 08:08:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename taint-multiple-pods 03/29/23 08:08:35.706
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:08:35.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:08:35.713
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar 29 08:08:35.715: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 08:09:35.724: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar 29 08:09:35.726: INFO: Starting informer...
    STEP: Starting pods... 03/29/23 08:09:35.726
    Mar 29 08:09:35.935: INFO: Pod1 is running on 10.146.0.115. Tainting Node
    Mar 29 08:09:36.140: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3710" to be "running"
    Mar 29 08:09:36.141: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.410951ms
    Mar 29 08:09:38.144: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004328017s
    Mar 29 08:09:38.144: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 29 08:09:38.144: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3710" to be "running"
    Mar 29 08:09:38.146: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.376858ms
    Mar 29 08:09:38.146: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 29 08:09:38.146: INFO: Pod2 is running on 10.146.0.115. Tainting Node
    STEP: Trying to apply a taint on the Node 03/29/23 08:09:38.146
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 08:09:38.152
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/29/23 08:09:38.154
    Mar 29 08:09:44.039: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 29 08:10:04.070: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/29/23 08:10:04.076
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:10:04.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-3710" for this suite. 03/29/23 08:10:04.079
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:10:04.081
Mar 29 08:10:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:10:04.082
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:04.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:04.089
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/29/23 08:10:04.094
Mar 29 08:10:04.094: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615" in namespace "kubelet-test-4255" to be "completed"
Mar 29 08:10:04.096: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32089ms
Mar 29 08:10:06.098: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003359771s
Mar 29 08:10:08.099: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00453378s
Mar 29 08:10:08.099: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 29 08:10:08.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4255" for this suite. 03/29/23 08:10:08.109
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":68,"skipped":1102,"failed":0}
------------------------------
â€¢ [4.030 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:10:04.081
    Mar 29 08:10:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:10:04.082
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:04.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:04.089
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/29/23 08:10:04.094
    Mar 29 08:10:04.094: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615" in namespace "kubelet-test-4255" to be "completed"
    Mar 29 08:10:04.096: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32089ms
    Mar 29 08:10:06.098: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003359771s
    Mar 29 08:10:08.099: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00453378s
    Mar 29 08:10:08.099: INFO: Pod "agnhost-host-aliases83050ea3-2f43-4daf-a139-d2cb052a0615" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 29 08:10:08.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4255" for this suite. 03/29/23 08:10:08.109
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:10:08.111
Mar 29 08:10:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:10:08.112
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:08.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:08.119
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar 29 08:10:08.132: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:10:08.134
Mar 29 08:10:08.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:10:08.137: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:10:09.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:10:09.141: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/29/23 08:10:09.146
STEP: Check that daemon pods images are updated. 03/29/23 08:10:09.152
Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-c2x4n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:10.157: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:10.157: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:11.158: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:11.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:12.158: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:12.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:12.158: INFO: Pod daemon-set-rt865 is not available
Mar 29 08:10:13.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:14.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 29 08:10:14.158: INFO: Pod daemon-set-rtvzk is not available
Mar 29 08:10:16.158: INFO: Pod daemon-set-9rf4q is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/29/23 08:10:16.16
Mar 29 08:10:16.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:10:16.163: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:10:17.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:10:17.167: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:10:17.173
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5485, will wait for the garbage collector to delete the pods 03/29/23 08:10:17.173
Mar 29 08:10:17.227: INFO: Deleting DaemonSet.extensions daemon-set took: 2.58961ms
Mar 29 08:10:17.328: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.503552ms
Mar 29 08:10:19.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:10:19.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 08:10:19.931: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6777"},"items":null}

Mar 29 08:10:19.932: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6777"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:10:19.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5485" for this suite. 03/29/23 08:10:19.939
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":69,"skipped":1102,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.830 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:10:08.111
    Mar 29 08:10:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:10:08.112
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:08.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:08.119
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar 29 08:10:08.132: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:10:08.134
    Mar 29 08:10:08.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:10:08.137: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:10:09.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:10:09.141: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/29/23 08:10:09.146
    STEP: Check that daemon pods images are updated. 03/29/23 08:10:09.152
    Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-c2x4n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:09.153: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:10.157: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:10.157: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:11.158: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:11.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:12.158: INFO: Wrong image for pod: daemon-set-c4f7n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:12.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:12.158: INFO: Pod daemon-set-rt865 is not available
    Mar 29 08:10:13.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:14.158: INFO: Wrong image for pod: daemon-set-hwmcp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 29 08:10:14.158: INFO: Pod daemon-set-rtvzk is not available
    Mar 29 08:10:16.158: INFO: Pod daemon-set-9rf4q is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/29/23 08:10:16.16
    Mar 29 08:10:16.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:10:16.163: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:10:17.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:10:17.167: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:10:17.173
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5485, will wait for the garbage collector to delete the pods 03/29/23 08:10:17.173
    Mar 29 08:10:17.227: INFO: Deleting DaemonSet.extensions daemon-set took: 2.58961ms
    Mar 29 08:10:17.328: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.503552ms
    Mar 29 08:10:19.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:10:19.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 08:10:19.931: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6777"},"items":null}

    Mar 29 08:10:19.932: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6777"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:10:19.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5485" for this suite. 03/29/23 08:10:19.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:10:19.942
Mar 29 08:10:19.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename cronjob 03/29/23 08:10:19.943
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:19.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:19.95
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/29/23 08:10:19.951
STEP: Ensuring no jobs are scheduled 03/29/23 08:10:19.953
STEP: Ensuring no job exists by listing jobs explicitly 03/29/23 08:15:19.957
STEP: Removing cronjob 03/29/23 08:15:19.958
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 29 08:15:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4950" for this suite. 03/29/23 08:15:19.963
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":70,"skipped":1129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.023 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:10:19.942
    Mar 29 08:10:19.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename cronjob 03/29/23 08:10:19.943
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:10:19.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:10:19.95
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/29/23 08:10:19.951
    STEP: Ensuring no jobs are scheduled 03/29/23 08:10:19.953
    STEP: Ensuring no job exists by listing jobs explicitly 03/29/23 08:15:19.957
    STEP: Removing cronjob 03/29/23 08:15:19.958
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 29 08:15:19.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4950" for this suite. 03/29/23 08:15:19.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:19.966
Mar 29 08:15:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:15:19.967
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:19.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:19.973
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/29/23 08:15:19.975
Mar 29 08:15:19.978: INFO: Waiting up to 5m0s for pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e" in namespace "svcaccounts-7594" to be "Succeeded or Failed"
Mar 29 08:15:19.979: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.233376ms
Mar 29 08:15:21.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004211716s
Mar 29 08:15:23.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004228581s
STEP: Saw pod success 03/29/23 08:15:23.982
Mar 29 08:15:23.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e" satisfied condition "Succeeded or Failed"
Mar 29 08:15:23.984: INFO: Trying to get logs from node 10.146.0.115 pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:15:23.991
Mar 29 08:15:23.997: INFO: Waiting for pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e to disappear
Mar 29 08:15:23.998: INFO: Pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 08:15:23.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7594" for this suite. 03/29/23 08:15:24
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":71,"skipped":1147,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:19.966
    Mar 29 08:15:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:15:19.967
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:19.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:19.973
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/29/23 08:15:19.975
    Mar 29 08:15:19.978: INFO: Waiting up to 5m0s for pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e" in namespace "svcaccounts-7594" to be "Succeeded or Failed"
    Mar 29 08:15:19.979: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.233376ms
    Mar 29 08:15:21.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004211716s
    Mar 29 08:15:23.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004228581s
    STEP: Saw pod success 03/29/23 08:15:23.982
    Mar 29 08:15:23.982: INFO: Pod "test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e" satisfied condition "Succeeded or Failed"
    Mar 29 08:15:23.984: INFO: Trying to get logs from node 10.146.0.115 pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:15:23.991
    Mar 29 08:15:23.997: INFO: Waiting for pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e to disappear
    Mar 29 08:15:23.998: INFO: Pod test-pod-ab8695d1-7e64-46f2-9db7-e99ed6f6873e no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 08:15:23.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7594" for this suite. 03/29/23 08:15:24
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:24.002
Mar 29 08:15:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:15:24.003
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:24.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:24.009
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/29/23 08:15:24.011
STEP: Creating a ResourceQuota 03/29/23 08:15:29.012
STEP: Ensuring resource quota status is calculated 03/29/23 08:15:29.016
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:15:31.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3220" for this suite. 03/29/23 08:15:31.021
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":72,"skipped":1147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.021 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:24.002
    Mar 29 08:15:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:15:24.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:24.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:24.009
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/29/23 08:15:24.011
    STEP: Creating a ResourceQuota 03/29/23 08:15:29.012
    STEP: Ensuring resource quota status is calculated 03/29/23 08:15:29.016
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:15:31.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3220" for this suite. 03/29/23 08:15:31.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:31.024
Mar 29 08:15:31.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:15:31.025
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:31.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:31.032
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:15:31.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9075" for this suite. 03/29/23 08:15:31.035
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":73,"skipped":1159,"failed":0}
------------------------------
â€¢ [0.014 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:31.024
    Mar 29 08:15:31.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:15:31.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:31.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:31.032
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:15:31.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9075" for this suite. 03/29/23 08:15:31.035
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:31.038
Mar 29 08:15:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:15:31.039
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:31.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:31.045
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/29/23 08:15:31.046
Mar 29 08:15:31.051: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 29 08:15:36.055: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 08:15:36.055
STEP: getting scale subresource 03/29/23 08:15:36.055
STEP: updating a scale subresource 03/29/23 08:15:36.057
STEP: verifying the replicaset Spec.Replicas was modified 03/29/23 08:15:36.061
STEP: Patch a scale subresource 03/29/23 08:15:36.062
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:15:36.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-463" for this suite. 03/29/23 08:15:36.071
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":74,"skipped":1160,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.036 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:31.038
    Mar 29 08:15:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:15:31.039
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:31.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:31.045
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/29/23 08:15:31.046
    Mar 29 08:15:31.051: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 29 08:15:36.055: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 08:15:36.055
    STEP: getting scale subresource 03/29/23 08:15:36.055
    STEP: updating a scale subresource 03/29/23 08:15:36.057
    STEP: verifying the replicaset Spec.Replicas was modified 03/29/23 08:15:36.061
    STEP: Patch a scale subresource 03/29/23 08:15:36.062
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:15:36.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-463" for this suite. 03/29/23 08:15:36.071
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:36.075
Mar 29 08:15:36.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:15:36.076
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:36.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:36.086
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-ec583297-a976-4325-9633-dbd9ef1ffded 03/29/23 08:15:36.088
STEP: Creating a pod to test consume secrets 03/29/23 08:15:36.09
Mar 29 08:15:36.094: INFO: Waiting up to 5m0s for pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96" in namespace "secrets-6212" to be "Succeeded or Failed"
Mar 29 08:15:36.095: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.192208ms
Mar 29 08:15:38.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003296702s
Mar 29 08:15:40.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00343646s
STEP: Saw pod success 03/29/23 08:15:40.098
Mar 29 08:15:40.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96" satisfied condition "Succeeded or Failed"
Mar 29 08:15:40.099: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:15:40.102
Mar 29 08:15:40.108: INFO: Waiting for pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 to disappear
Mar 29 08:15:40.109: INFO: Pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:15:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6212" for this suite. 03/29/23 08:15:40.111
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":75,"skipped":1161,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:36.075
    Mar 29 08:15:36.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:15:36.076
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:36.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:36.086
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-ec583297-a976-4325-9633-dbd9ef1ffded 03/29/23 08:15:36.088
    STEP: Creating a pod to test consume secrets 03/29/23 08:15:36.09
    Mar 29 08:15:36.094: INFO: Waiting up to 5m0s for pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96" in namespace "secrets-6212" to be "Succeeded or Failed"
    Mar 29 08:15:36.095: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.192208ms
    Mar 29 08:15:38.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003296702s
    Mar 29 08:15:40.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00343646s
    STEP: Saw pod success 03/29/23 08:15:40.098
    Mar 29 08:15:40.098: INFO: Pod "pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96" satisfied condition "Succeeded or Failed"
    Mar 29 08:15:40.099: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:15:40.102
    Mar 29 08:15:40.108: INFO: Waiting for pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 to disappear
    Mar 29 08:15:40.109: INFO: Pod pod-secrets-ba9b495a-ec67-46e7-9628-36033b64db96 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:15:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6212" for this suite. 03/29/23 08:15:40.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:40.115
Mar 29 08:15:40.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:15:40.116
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:40.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:40.123
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/29/23 08:15:40.124
Mar 29 08:15:40.127: INFO: Waiting up to 5m0s for pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68" in namespace "emptydir-1357" to be "Succeeded or Failed"
Mar 29 08:15:40.129: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054ms
Mar 29 08:15:42.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143104s
Mar 29 08:15:44.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00405496s
STEP: Saw pod success 03/29/23 08:15:44.131
Mar 29 08:15:44.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68" satisfied condition "Succeeded or Failed"
Mar 29 08:15:44.133: INFO: Trying to get logs from node 10.146.0.116 pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 container test-container: <nil>
STEP: delete the pod 03/29/23 08:15:44.14
Mar 29 08:15:44.146: INFO: Waiting for pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 to disappear
Mar 29 08:15:44.147: INFO: Pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:15:44.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1357" for this suite. 03/29/23 08:15:44.149
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1224,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:40.115
    Mar 29 08:15:40.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:15:40.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:40.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:40.123
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/29/23 08:15:40.124
    Mar 29 08:15:40.127: INFO: Waiting up to 5m0s for pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68" in namespace "emptydir-1357" to be "Succeeded or Failed"
    Mar 29 08:15:40.129: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054ms
    Mar 29 08:15:42.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143104s
    Mar 29 08:15:44.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00405496s
    STEP: Saw pod success 03/29/23 08:15:44.131
    Mar 29 08:15:44.131: INFO: Pod "pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68" satisfied condition "Succeeded or Failed"
    Mar 29 08:15:44.133: INFO: Trying to get logs from node 10.146.0.116 pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:15:44.14
    Mar 29 08:15:44.146: INFO: Waiting for pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 to disappear
    Mar 29 08:15:44.147: INFO: Pod pod-196dcc74-6e96-455f-8c3b-f7f4fbd8ff68 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:15:44.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1357" for this suite. 03/29/23 08:15:44.149
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:44.151
Mar 29 08:15:44.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:15:44.152
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:44.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:44.159
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 29 08:15:48.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1615" for this suite. 03/29/23 08:15:48.171
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":77,"skipped":1224,"failed":0}
------------------------------
â€¢ [4.022 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:44.151
    Mar 29 08:15:44.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:15:44.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:44.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:44.159
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 29 08:15:48.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1615" for this suite. 03/29/23 08:15:48.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:48.174
Mar 29 08:15:48.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:15:48.175
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:48.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:48.182
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/29/23 08:15:48.183
Mar 29 08:15:48.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5940 cluster-info'
Mar 29 08:15:48.228: INFO: stderr: ""
Mar 29 08:15:48.228: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:15:48.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5940" for this suite. 03/29/23 08:15:48.229
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":78,"skipped":1255,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:48.174
    Mar 29 08:15:48.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:15:48.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:48.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:48.182
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/29/23 08:15:48.183
    Mar 29 08:15:48.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5940 cluster-info'
    Mar 29 08:15:48.228: INFO: stderr: ""
    Mar 29 08:15:48.228: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:15:48.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5940" for this suite. 03/29/23 08:15:48.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:48.232
Mar 29 08:15:48.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption 03/29/23 08:15:48.233
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:48.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:48.241
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/29/23 08:15:48.244
STEP: Waiting for all pods to be running 03/29/23 08:15:50.259
Mar 29 08:15:50.261: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 29 08:15:52.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3864" for this suite. 03/29/23 08:15:52.267
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":79,"skipped":1267,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:48.232
    Mar 29 08:15:48.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption 03/29/23 08:15:48.233
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:48.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:48.241
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/29/23 08:15:48.244
    STEP: Waiting for all pods to be running 03/29/23 08:15:50.259
    Mar 29 08:15:50.261: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 29 08:15:52.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3864" for this suite. 03/29/23 08:15:52.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:52.27
Mar 29 08:15:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:15:52.271
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.279
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar 29 08:15:52.288: INFO: created pod pod-service-account-defaultsa
Mar 29 08:15:52.288: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 29 08:15:52.291: INFO: created pod pod-service-account-mountsa
Mar 29 08:15:52.291: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 29 08:15:52.294: INFO: created pod pod-service-account-nomountsa
Mar 29 08:15:52.294: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 29 08:15:52.297: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 29 08:15:52.297: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 29 08:15:52.300: INFO: created pod pod-service-account-mountsa-mountspec
Mar 29 08:15:52.300: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 29 08:15:52.303: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 29 08:15:52.303: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 29 08:15:52.307: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 29 08:15:52.307: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 29 08:15:52.310: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 29 08:15:52.310: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 29 08:15:52.314: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 29 08:15:52.314: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 08:15:52.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9502" for this suite. 03/29/23 08:15:52.318
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":80,"skipped":1275,"failed":0}
------------------------------
â€¢ [0.052 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:52.27
    Mar 29 08:15:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:15:52.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.279
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar 29 08:15:52.288: INFO: created pod pod-service-account-defaultsa
    Mar 29 08:15:52.288: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 29 08:15:52.291: INFO: created pod pod-service-account-mountsa
    Mar 29 08:15:52.291: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 29 08:15:52.294: INFO: created pod pod-service-account-nomountsa
    Mar 29 08:15:52.294: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 29 08:15:52.297: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 29 08:15:52.297: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 29 08:15:52.300: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 29 08:15:52.300: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 29 08:15:52.303: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 29 08:15:52.303: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 29 08:15:52.307: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 29 08:15:52.307: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 29 08:15:52.310: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 29 08:15:52.310: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 29 08:15:52.314: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 29 08:15:52.314: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 08:15:52.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9502" for this suite. 03/29/23 08:15:52.318
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:52.322
Mar 29 08:15:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename endpointslice 03/29/23 08:15:52.323
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.332
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/29/23 08:15:52.334
STEP: getting /apis/discovery.k8s.io 03/29/23 08:15:52.335
STEP: getting /apis/discovery.k8s.iov1 03/29/23 08:15:52.335
STEP: creating 03/29/23 08:15:52.336
STEP: getting 03/29/23 08:15:52.341
STEP: listing 03/29/23 08:15:52.344
STEP: watching 03/29/23 08:15:52.345
Mar 29 08:15:52.345: INFO: starting watch
STEP: cluster-wide listing 03/29/23 08:15:52.346
STEP: cluster-wide watching 03/29/23 08:15:52.347
Mar 29 08:15:52.347: INFO: starting watch
STEP: patching 03/29/23 08:15:52.348
STEP: updating 03/29/23 08:15:52.35
Mar 29 08:15:52.355: INFO: waiting for watch events with expected annotations
Mar 29 08:15:52.355: INFO: saw patched and updated annotations
STEP: deleting 03/29/23 08:15:52.355
STEP: deleting a collection 03/29/23 08:15:52.36
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 29 08:15:52.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8729" for this suite. 03/29/23 08:15:52.367
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":81,"skipped":1279,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:52.322
    Mar 29 08:15:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename endpointslice 03/29/23 08:15:52.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.332
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/29/23 08:15:52.334
    STEP: getting /apis/discovery.k8s.io 03/29/23 08:15:52.335
    STEP: getting /apis/discovery.k8s.iov1 03/29/23 08:15:52.335
    STEP: creating 03/29/23 08:15:52.336
    STEP: getting 03/29/23 08:15:52.341
    STEP: listing 03/29/23 08:15:52.344
    STEP: watching 03/29/23 08:15:52.345
    Mar 29 08:15:52.345: INFO: starting watch
    STEP: cluster-wide listing 03/29/23 08:15:52.346
    STEP: cluster-wide watching 03/29/23 08:15:52.347
    Mar 29 08:15:52.347: INFO: starting watch
    STEP: patching 03/29/23 08:15:52.348
    STEP: updating 03/29/23 08:15:52.35
    Mar 29 08:15:52.355: INFO: waiting for watch events with expected annotations
    Mar 29 08:15:52.355: INFO: saw patched and updated annotations
    STEP: deleting 03/29/23 08:15:52.355
    STEP: deleting a collection 03/29/23 08:15:52.36
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 29 08:15:52.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8729" for this suite. 03/29/23 08:15:52.367
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:52.371
Mar 29 08:15:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:15:52.372
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.382
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar 29 08:15:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: creating the pod 03/29/23 08:15:52.384
STEP: submitting the pod to kubernetes 03/29/23 08:15:52.384
Mar 29 08:15:52.388: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e" in namespace "pods-4233" to be "running and ready"
Mar 29 08:15:52.389: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.367566ms
Mar 29 08:15:52.389: INFO: The phase of Pod pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:15:54.391: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003640614s
Mar 29 08:15:54.391: INFO: The phase of Pod pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e is Running (Ready = true)
Mar 29 08:15:54.391: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 08:15:54.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4233" for this suite. 03/29/23 08:15:54.447
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":82,"skipped":1283,"failed":0}
------------------------------
â€¢ [2.079 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:52.371
    Mar 29 08:15:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:15:52.372
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:52.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:52.382
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar 29 08:15:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: creating the pod 03/29/23 08:15:52.384
    STEP: submitting the pod to kubernetes 03/29/23 08:15:52.384
    Mar 29 08:15:52.388: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e" in namespace "pods-4233" to be "running and ready"
    Mar 29 08:15:52.389: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.367566ms
    Mar 29 08:15:52.389: INFO: The phase of Pod pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:15:54.391: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003640614s
    Mar 29 08:15:54.391: INFO: The phase of Pod pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e is Running (Ready = true)
    Mar 29 08:15:54.391: INFO: Pod "pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 08:15:54.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4233" for this suite. 03/29/23 08:15:54.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:54.451
Mar 29 08:15:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:15:54.452
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:54.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:54.46
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:15:54.461
Mar 29 08:15:54.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98" in namespace "downward-api-4537" to be "Succeeded or Failed"
Mar 29 08:15:54.467: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193953ms
Mar 29 08:15:56.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003610194s
Mar 29 08:15:58.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003882402s
STEP: Saw pod success 03/29/23 08:15:58.469
Mar 29 08:15:58.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98" satisfied condition "Succeeded or Failed"
Mar 29 08:15:58.471: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 container client-container: <nil>
STEP: delete the pod 03/29/23 08:15:58.473
Mar 29 08:15:58.479: INFO: Waiting for pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 to disappear
Mar 29 08:15:58.481: INFO: Pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:15:58.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4537" for this suite. 03/29/23 08:15:58.482
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":83,"skipped":1305,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:54.451
    Mar 29 08:15:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:15:54.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:54.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:54.46
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:15:54.461
    Mar 29 08:15:54.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98" in namespace "downward-api-4537" to be "Succeeded or Failed"
    Mar 29 08:15:54.467: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193953ms
    Mar 29 08:15:56.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003610194s
    Mar 29 08:15:58.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003882402s
    STEP: Saw pod success 03/29/23 08:15:58.469
    Mar 29 08:15:58.469: INFO: Pod "downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98" satisfied condition "Succeeded or Failed"
    Mar 29 08:15:58.471: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:15:58.473
    Mar 29 08:15:58.479: INFO: Waiting for pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 to disappear
    Mar 29 08:15:58.481: INFO: Pod downwardapi-volume-5aa8c092-007c-46e5-a908-fcb63146bc98 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:15:58.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4537" for this suite. 03/29/23 08:15:58.482
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:15:58.485
Mar 29 08:15:58.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:15:58.486
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:58.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:58.493
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-7ebc677c-c3ed-4646-a2b2-605b3b64ad0a 03/29/23 08:15:58.494
STEP: Creating a pod to test consume secrets 03/29/23 08:15:58.496
Mar 29 08:15:58.500: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571" in namespace "projected-1637" to be "Succeeded or Failed"
Mar 29 08:15:58.501: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Pending", Reason="", readiness=false. Elapsed: 1.269969ms
Mar 29 08:16:00.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003659792s
Mar 29 08:16:02.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003417463s
STEP: Saw pod success 03/29/23 08:16:02.503
Mar 29 08:16:02.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571" satisfied condition "Succeeded or Failed"
Mar 29 08:16:02.505: INFO: Trying to get logs from node 10.146.0.117 pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:16:02.512
Mar 29 08:16:02.519: INFO: Waiting for pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 to disappear
Mar 29 08:16:02.520: INFO: Pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 08:16:02.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1637" for this suite. 03/29/23 08:16:02.522
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1308,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:15:58.485
    Mar 29 08:15:58.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:15:58.486
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:15:58.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:15:58.493
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-7ebc677c-c3ed-4646-a2b2-605b3b64ad0a 03/29/23 08:15:58.494
    STEP: Creating a pod to test consume secrets 03/29/23 08:15:58.496
    Mar 29 08:15:58.500: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571" in namespace "projected-1637" to be "Succeeded or Failed"
    Mar 29 08:15:58.501: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Pending", Reason="", readiness=false. Elapsed: 1.269969ms
    Mar 29 08:16:00.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003659792s
    Mar 29 08:16:02.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003417463s
    STEP: Saw pod success 03/29/23 08:16:02.503
    Mar 29 08:16:02.503: INFO: Pod "pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571" satisfied condition "Succeeded or Failed"
    Mar 29 08:16:02.505: INFO: Trying to get logs from node 10.146.0.117 pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:16:02.512
    Mar 29 08:16:02.519: INFO: Waiting for pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 to disappear
    Mar 29 08:16:02.520: INFO: Pod pod-projected-secrets-58657fac-bf91-4f1f-a946-558234b46571 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 08:16:02.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1637" for this suite. 03/29/23 08:16:02.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:02.526
Mar 29 08:16:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename ingress 03/29/23 08:16:02.527
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:02.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:02.534
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/29/23 08:16:02.535
STEP: getting /apis/networking.k8s.io 03/29/23 08:16:02.536
STEP: getting /apis/networking.k8s.iov1 03/29/23 08:16:02.536
STEP: creating 03/29/23 08:16:02.537
STEP: getting 03/29/23 08:16:02.544
STEP: listing 03/29/23 08:16:02.545
STEP: watching 03/29/23 08:16:02.546
Mar 29 08:16:02.546: INFO: starting watch
STEP: cluster-wide listing 03/29/23 08:16:02.546
STEP: cluster-wide watching 03/29/23 08:16:02.548
Mar 29 08:16:02.548: INFO: starting watch
STEP: patching 03/29/23 08:16:02.548
STEP: updating 03/29/23 08:16:02.551
Mar 29 08:16:02.554: INFO: waiting for watch events with expected annotations
Mar 29 08:16:02.554: INFO: saw patched and updated annotations
STEP: patching /status 03/29/23 08:16:02.554
STEP: updating /status 03/29/23 08:16:02.557
STEP: get /status 03/29/23 08:16:02.561
STEP: deleting 03/29/23 08:16:02.562
STEP: deleting a collection 03/29/23 08:16:02.566
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar 29 08:16:02.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2047" for this suite. 03/29/23 08:16:02.573
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":85,"skipped":1367,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:02.526
    Mar 29 08:16:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename ingress 03/29/23 08:16:02.527
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:02.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:02.534
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/29/23 08:16:02.535
    STEP: getting /apis/networking.k8s.io 03/29/23 08:16:02.536
    STEP: getting /apis/networking.k8s.iov1 03/29/23 08:16:02.536
    STEP: creating 03/29/23 08:16:02.537
    STEP: getting 03/29/23 08:16:02.544
    STEP: listing 03/29/23 08:16:02.545
    STEP: watching 03/29/23 08:16:02.546
    Mar 29 08:16:02.546: INFO: starting watch
    STEP: cluster-wide listing 03/29/23 08:16:02.546
    STEP: cluster-wide watching 03/29/23 08:16:02.548
    Mar 29 08:16:02.548: INFO: starting watch
    STEP: patching 03/29/23 08:16:02.548
    STEP: updating 03/29/23 08:16:02.551
    Mar 29 08:16:02.554: INFO: waiting for watch events with expected annotations
    Mar 29 08:16:02.554: INFO: saw patched and updated annotations
    STEP: patching /status 03/29/23 08:16:02.554
    STEP: updating /status 03/29/23 08:16:02.557
    STEP: get /status 03/29/23 08:16:02.561
    STEP: deleting 03/29/23 08:16:02.562
    STEP: deleting a collection 03/29/23 08:16:02.566
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar 29 08:16:02.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2047" for this suite. 03/29/23 08:16:02.573
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:02.576
Mar 29 08:16:02.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:16:02.576
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:02.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:02.584
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4461 03/29/23 08:16:02.585
STEP: creating service affinity-clusterip in namespace services-4461 03/29/23 08:16:02.585
STEP: creating replication controller affinity-clusterip in namespace services-4461 03/29/23 08:16:02.589
I0329 08:16:02.593331      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4461, replica count: 3
I0329 08:16:05.644930      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:16:05.647: INFO: Creating new exec pod
Mar 29 08:16:05.650: INFO: Waiting up to 5m0s for pod "execpod-affinityxbzp6" in namespace "services-4461" to be "running"
Mar 29 08:16:05.652: INFO: Pod "execpod-affinityxbzp6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660874ms
Mar 29 08:16:07.654: INFO: Pod "execpod-affinityxbzp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004125172s
Mar 29 08:16:07.654: INFO: Pod "execpod-affinityxbzp6" satisfied condition "running"
Mar 29 08:16:08.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 29 08:16:08.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 29 08:16:08.746: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:16:08.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.53.194 80'
Mar 29 08:16:08.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.53.194 80\nConnection to 10.100.53.194 80 port [tcp/http] succeeded!\n"
Mar 29 08:16:08.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:16:08.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.53.194:80/ ; done'
Mar 29 08:16:08.955: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n"
Mar 29 08:16:08.955: INFO: stdout: "\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt"
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
Mar 29 08:16:08.955: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4461, will wait for the garbage collector to delete the pods 03/29/23 08:16:08.962
Mar 29 08:16:09.017: INFO: Deleting ReplicationController affinity-clusterip took: 2.582521ms
Mar 29 08:16:09.117: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.584114ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:16:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4461" for this suite. 03/29/23 08:16:10.728
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":86,"skipped":1369,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.155 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:02.576
    Mar 29 08:16:02.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:16:02.576
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:02.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:02.584
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4461 03/29/23 08:16:02.585
    STEP: creating service affinity-clusterip in namespace services-4461 03/29/23 08:16:02.585
    STEP: creating replication controller affinity-clusterip in namespace services-4461 03/29/23 08:16:02.589
    I0329 08:16:02.593331      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4461, replica count: 3
    I0329 08:16:05.644930      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:16:05.647: INFO: Creating new exec pod
    Mar 29 08:16:05.650: INFO: Waiting up to 5m0s for pod "execpod-affinityxbzp6" in namespace "services-4461" to be "running"
    Mar 29 08:16:05.652: INFO: Pod "execpod-affinityxbzp6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660874ms
    Mar 29 08:16:07.654: INFO: Pod "execpod-affinityxbzp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004125172s
    Mar 29 08:16:07.654: INFO: Pod "execpod-affinityxbzp6" satisfied condition "running"
    Mar 29 08:16:08.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar 29 08:16:08.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 29 08:16:08.746: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:16:08.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.53.194 80'
    Mar 29 08:16:08.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.53.194 80\nConnection to 10.100.53.194 80 port [tcp/http] succeeded!\n"
    Mar 29 08:16:08.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:16:08.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4461 exec execpod-affinityxbzp6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.53.194:80/ ; done'
    Mar 29 08:16:08.955: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.53.194:80/\n"
    Mar 29 08:16:08.955: INFO: stdout: "\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt\naffinity-clusterip-q5nxt"
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Received response from host: affinity-clusterip-q5nxt
    Mar 29 08:16:08.955: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4461, will wait for the garbage collector to delete the pods 03/29/23 08:16:08.962
    Mar 29 08:16:09.017: INFO: Deleting ReplicationController affinity-clusterip took: 2.582521ms
    Mar 29 08:16:09.117: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.584114ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:16:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4461" for this suite. 03/29/23 08:16:10.728
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:10.731
Mar 29 08:16:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename endpointslice 03/29/23 08:16:10.732
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:10.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:10.74
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar 29 08:16:10.745: INFO: Endpoints addresses: [10.146.0.115] , ports: [6443]
Mar 29 08:16:10.745: INFO: EndpointSlices addresses: [10.146.0.115] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 29 08:16:10.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9135" for this suite. 03/29/23 08:16:10.746
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":87,"skipped":1375,"failed":0}
------------------------------
â€¢ [0.018 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:10.731
    Mar 29 08:16:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename endpointslice 03/29/23 08:16:10.732
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:10.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:10.74
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar 29 08:16:10.745: INFO: Endpoints addresses: [10.146.0.115] , ports: [6443]
    Mar 29 08:16:10.745: INFO: EndpointSlices addresses: [10.146.0.115] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 29 08:16:10.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9135" for this suite. 03/29/23 08:16:10.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:10.751
Mar 29 08:16:10.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-pred 03/29/23 08:16:10.751
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:10.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:10.759
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 29 08:16:10.760: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 08:16:10.763: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 08:16:10.764: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.115 before test
Mar 29 08:16:10.767: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:16:10.767: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:16:10.767: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:16:10.767: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:16:10.767: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:16:10.767: INFO: pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e from pods-4233 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container main ready: true, restart count 0
Mar 29 08:16:10.767: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:16:10.767: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:16:10.767: INFO: pod-service-account-mountsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.767: INFO: 	Container token-test ready: false, restart count 0
Mar 29 08:16:10.767: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.116 before test
Mar 29 08:16:10.770: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:16:10.770: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:16:10.770: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:16:10.770: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:16:10.770: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:16:10.770: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:16:10.770: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:16:10.770: INFO: pod-service-account-defaultsa from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container token-test ready: false, restart count 0
Mar 29 08:16:10.770: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.770: INFO: 	Container token-test ready: false, restart count 0
Mar 29 08:16:10.770: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.117 before test
Mar 29 08:16:10.773: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 08:16:10.773: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:16:10.773: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:16:10.773: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:16:10.773: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:16:10.773: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 08:16:10.773: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container e2e ready: true, restart count 0
Mar 29 08:16:10.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:16:10.773: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:16:10.773: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:16:10.773: INFO: pod-service-account-mountsa from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container token-test ready: false, restart count 0
Mar 29 08:16:10.773: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
Mar 29 08:16:10.773: INFO: 	Container token-test ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/29/23 08:16:10.773
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1750d5d4c21beb40], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/29/23 08:16:10.785
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:16:11.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8646" for this suite. 03/29/23 08:16:11.786
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":88,"skipped":1440,"failed":0}
------------------------------
â€¢ [1.038 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:10.751
    Mar 29 08:16:10.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-pred 03/29/23 08:16:10.751
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:10.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:10.759
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 29 08:16:10.760: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 29 08:16:10.763: INFO: Waiting for terminating namespaces to be deleted...
    Mar 29 08:16:10.764: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.115 before test
    Mar 29 08:16:10.767: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: pod-exec-websocket-e0fb8028-597c-4e8e-b99f-daeabe2c649e from pods-4233 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container main ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:16:10.767: INFO: pod-service-account-mountsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.767: INFO: 	Container token-test ready: false, restart count 0
    Mar 29 08:16:10.767: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.116 before test
    Mar 29 08:16:10.770: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:16:10.770: INFO: pod-service-account-defaultsa from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container token-test ready: false, restart count 0
    Mar 29 08:16:10.770: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.770: INFO: 	Container token-test ready: false, restart count 0
    Mar 29 08:16:10.770: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.117 before test
    Mar 29 08:16:10.773: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container e2e ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:16:10.773: INFO: pod-service-account-mountsa from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container token-test ready: false, restart count 0
    Mar 29 08:16:10.773: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-9502 started at 2023-03-29 08:15:52 +0000 UTC (1 container statuses recorded)
    Mar 29 08:16:10.773: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/29/23 08:16:10.773
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1750d5d4c21beb40], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/29/23 08:16:10.785
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:16:11.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8646" for this suite. 03/29/23 08:16:11.786
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:11.79
Mar 29 08:16:11.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context 03/29/23 08:16:11.791
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:11.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:11.797
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/29/23 08:16:11.798
Mar 29 08:16:11.801: INFO: Waiting up to 5m0s for pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8" in namespace "security-context-7838" to be "Succeeded or Failed"
Mar 29 08:16:11.803: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318055ms
Mar 29 08:16:13.805: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00377561s
Mar 29 08:16:15.806: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004235385s
STEP: Saw pod success 03/29/23 08:16:15.806
Mar 29 08:16:15.806: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8" satisfied condition "Succeeded or Failed"
Mar 29 08:16:15.807: INFO: Trying to get logs from node 10.146.0.116 pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 container test-container: <nil>
STEP: delete the pod 03/29/23 08:16:15.81
Mar 29 08:16:15.817: INFO: Waiting for pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 to disappear
Mar 29 08:16:15.818: INFO: Pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 08:16:15.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7838" for this suite. 03/29/23 08:16:15.82
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":89,"skipped":1483,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:11.79
    Mar 29 08:16:11.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context 03/29/23 08:16:11.791
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:11.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:11.797
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/29/23 08:16:11.798
    Mar 29 08:16:11.801: INFO: Waiting up to 5m0s for pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8" in namespace "security-context-7838" to be "Succeeded or Failed"
    Mar 29 08:16:11.803: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318055ms
    Mar 29 08:16:13.805: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00377561s
    Mar 29 08:16:15.806: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004235385s
    STEP: Saw pod success 03/29/23 08:16:15.806
    Mar 29 08:16:15.806: INFO: Pod "security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8" satisfied condition "Succeeded or Failed"
    Mar 29 08:16:15.807: INFO: Trying to get logs from node 10.146.0.116 pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:16:15.81
    Mar 29 08:16:15.817: INFO: Waiting for pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 to disappear
    Mar 29 08:16:15.818: INFO: Pod security-context-6c8dd4e4-d937-4023-bfac-e5a4f9803dd8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 08:16:15.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7838" for this suite. 03/29/23 08:16:15.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:15.823
Mar 29 08:16:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename namespaces 03/29/23 08:16:15.824
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:15.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:15.831
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/29/23 08:16:15.832
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:15.839
STEP: Creating a pod in the namespace 03/29/23 08:16:15.84
STEP: Waiting for the pod to have running status 03/29/23 08:16:15.845
Mar 29 08:16:15.845: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5415" to be "running"
Mar 29 08:16:15.846: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236649ms
Mar 29 08:16:17.849: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004045563s
Mar 29 08:16:17.849: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/29/23 08:16:17.849
STEP: Waiting for the namespace to be removed. 03/29/23 08:16:17.851
STEP: Recreating the namespace 03/29/23 08:16:28.853
STEP: Verifying there are no pods in the namespace 03/29/23 08:16:28.859
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:16:28.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7315" for this suite. 03/29/23 08:16:28.862
STEP: Destroying namespace "nsdeletetest-5415" for this suite. 03/29/23 08:16:28.864
Mar 29 08:16:28.866: INFO: Namespace nsdeletetest-5415 was already deleted
STEP: Destroying namespace "nsdeletetest-3109" for this suite. 03/29/23 08:16:28.866
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":90,"skipped":1525,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.045 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:15.823
    Mar 29 08:16:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename namespaces 03/29/23 08:16:15.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:15.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:15.831
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/29/23 08:16:15.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:15.839
    STEP: Creating a pod in the namespace 03/29/23 08:16:15.84
    STEP: Waiting for the pod to have running status 03/29/23 08:16:15.845
    Mar 29 08:16:15.845: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5415" to be "running"
    Mar 29 08:16:15.846: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236649ms
    Mar 29 08:16:17.849: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004045563s
    Mar 29 08:16:17.849: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/29/23 08:16:17.849
    STEP: Waiting for the namespace to be removed. 03/29/23 08:16:17.851
    STEP: Recreating the namespace 03/29/23 08:16:28.853
    STEP: Verifying there are no pods in the namespace 03/29/23 08:16:28.859
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:16:28.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7315" for this suite. 03/29/23 08:16:28.862
    STEP: Destroying namespace "nsdeletetest-5415" for this suite. 03/29/23 08:16:28.864
    Mar 29 08:16:28.866: INFO: Namespace nsdeletetest-5415 was already deleted
    STEP: Destroying namespace "nsdeletetest-3109" for this suite. 03/29/23 08:16:28.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:28.869
Mar 29 08:16:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:16:28.87
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:28.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:28.877
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 29 08:16:28.878: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 29 08:16:28.882: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 29 08:16:33.884: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 08:16:33.884
Mar 29 08:16:33.884: INFO: Creating deployment "test-rolling-update-deployment"
Mar 29 08:16:33.889: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 29 08:16:33.892: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 29 08:16:35.896: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 29 08:16:35.897: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:16:35.900: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9682  de816707-db47-493a-895b-7290e50f0640 8153 1 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac45f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 08:16:33 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-29 08:16:34 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 08:16:35.902: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9682  586895c2-b982-4844-ae47-f9198c14a678 8143 1 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment de816707-db47-493a-895b-7290e50f0640 0xc005ac4af7 0xc005ac4af8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de816707-db47-493a-895b-7290e50f0640\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac4bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:16:35.902: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 29 08:16:35.902: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9682  d816d5ec-0d99-4d9a-b829-1b1a13e5e9a1 8152 2 2023-03-29 08:16:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment de816707-db47-493a-895b-7290e50f0640 0xc005ac49d7 0xc005ac49d8}] [] [{e2e.test Update apps/v1 2023-03-29 08:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de816707-db47-493a-895b-7290e50f0640\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ac4a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:16:35.904: INFO: Pod "test-rolling-update-deployment-78f575d8ff-5mxvz" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-5mxvz test-rolling-update-deployment-78f575d8ff- deployment-9682  dbcef331-a43e-45ed-b617-12eb9a69775e 8142 0 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:f1781569868da4367985a52e7f35f83c796a9c9ee69cf09e28828858e2c7fd54 cni.projectcalico.org/podIP:192.168.30.55/32 cni.projectcalico.org/podIPs:192.168.30.55/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 586895c2-b982-4844-ae47-f9198c14a678 0xc005ac5077 0xc005ac5078}] [] [{kube-controller-manager Update v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586895c2-b982-4844-ae47-f9198c14a678\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72g8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72g8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.55,StartTime:2023-03-29 08:16:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:16:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://58eb5b3281959640bdb991b6cc387acddf01322c2002c57bc27466fa3044b4e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:16:35.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9682" for this suite. 03/29/23 08:16:35.905
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":91,"skipped":1548,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.038 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:28.869
    Mar 29 08:16:28.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:16:28.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:28.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:28.877
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 29 08:16:28.878: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 29 08:16:28.882: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 29 08:16:33.884: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 08:16:33.884
    Mar 29 08:16:33.884: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 29 08:16:33.889: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 29 08:16:33.892: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 29 08:16:35.896: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 29 08:16:35.897: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:16:35.900: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9682  de816707-db47-493a-895b-7290e50f0640 8153 1 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac45f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 08:16:33 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-29 08:16:34 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 29 08:16:35.902: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9682  586895c2-b982-4844-ae47-f9198c14a678 8143 1 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment de816707-db47-493a-895b-7290e50f0640 0xc005ac4af7 0xc005ac4af8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de816707-db47-493a-895b-7290e50f0640\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac4bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:16:35.902: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 29 08:16:35.902: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9682  d816d5ec-0d99-4d9a-b829-1b1a13e5e9a1 8152 2 2023-03-29 08:16:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment de816707-db47-493a-895b-7290e50f0640 0xc005ac49d7 0xc005ac49d8}] [] [{e2e.test Update apps/v1 2023-03-29 08:16:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de816707-db47-493a-895b-7290e50f0640\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ac4a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:16:35.904: INFO: Pod "test-rolling-update-deployment-78f575d8ff-5mxvz" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-5mxvz test-rolling-update-deployment-78f575d8ff- deployment-9682  dbcef331-a43e-45ed-b617-12eb9a69775e 8142 0 2023-03-29 08:16:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:f1781569868da4367985a52e7f35f83c796a9c9ee69cf09e28828858e2c7fd54 cni.projectcalico.org/podIP:192.168.30.55/32 cni.projectcalico.org/podIPs:192.168.30.55/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 586895c2-b982-4844-ae47-f9198c14a678 0xc005ac5077 0xc005ac5078}] [] [{kube-controller-manager Update v1 2023-03-29 08:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586895c2-b982-4844-ae47-f9198c14a678\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:16:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72g8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72g8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:16:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.55,StartTime:2023-03-29 08:16:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:16:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://58eb5b3281959640bdb991b6cc387acddf01322c2002c57bc27466fa3044b4e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:16:35.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9682" for this suite. 03/29/23 08:16:35.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:35.908
Mar 29 08:16:35.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename podtemplate 03/29/23 08:16:35.909
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.917
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/29/23 08:16:35.918
STEP: Replace a pod template 03/29/23 08:16:35.92
Mar 29 08:16:35.923: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 29 08:16:35.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6097" for this suite. 03/29/23 08:16:35.925
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":92,"skipped":1561,"failed":0}
------------------------------
â€¢ [0.018 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:35.908
    Mar 29 08:16:35.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename podtemplate 03/29/23 08:16:35.909
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.917
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/29/23 08:16:35.918
    STEP: Replace a pod template 03/29/23 08:16:35.92
    Mar 29 08:16:35.923: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 29 08:16:35.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6097" for this suite. 03/29/23 08:16:35.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:35.928
Mar 29 08:16:35.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:16:35.928
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.935
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:16:35.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4885" for this suite. 03/29/23 08:16:35.952
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":93,"skipped":1607,"failed":0}
------------------------------
â€¢ [0.026 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:35.928
    Mar 29 08:16:35.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:16:35.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.935
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:16:35.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4885" for this suite. 03/29/23 08:16:35.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:35.954
Mar 29 08:16:35.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:16:35.955
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.962
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/29/23 08:16:35.964
Mar 29 08:16:35.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-51 api-versions'
Mar 29 08:16:36.002: INFO: stderr: ""
Mar 29 08:16:36.002: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:16:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-51" for this suite. 03/29/23 08:16:36.004
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":94,"skipped":1622,"failed":0}
------------------------------
â€¢ [0.052 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:35.954
    Mar 29 08:16:35.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:16:35.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:35.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:35.962
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/29/23 08:16:35.964
    Mar 29 08:16:35.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-51 api-versions'
    Mar 29 08:16:36.002: INFO: stderr: ""
    Mar 29 08:16:36.002: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:16:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-51" for this suite. 03/29/23 08:16:36.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:16:36.007
Mar 29 08:16:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:16:36.008
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:36.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:36.015
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bdef6d88-668c-40db-8817-f415de3dcac5 03/29/23 08:16:36.017
STEP: Creating the pod 03/29/23 08:16:36.02
Mar 29 08:16:36.023: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc" in namespace "projected-6193" to be "running and ready"
Mar 29 08:16:36.024: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306347ms
Mar 29 08:16:36.024: INFO: The phase of Pod pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:16:38.027: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004122239s
Mar 29 08:16:38.027: INFO: The phase of Pod pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc is Running (Ready = true)
Mar 29 08:16:38.027: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-bdef6d88-668c-40db-8817-f415de3dcac5 03/29/23 08:16:38.031
STEP: waiting to observe update in volume 03/29/23 08:16:38.034
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:17:54.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6193" for this suite. 03/29/23 08:17:54.199
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":95,"skipped":1640,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.195 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:16:36.007
    Mar 29 08:16:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:16:36.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:16:36.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:16:36.015
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-bdef6d88-668c-40db-8817-f415de3dcac5 03/29/23 08:16:36.017
    STEP: Creating the pod 03/29/23 08:16:36.02
    Mar 29 08:16:36.023: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc" in namespace "projected-6193" to be "running and ready"
    Mar 29 08:16:36.024: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306347ms
    Mar 29 08:16:36.024: INFO: The phase of Pod pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:16:38.027: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004122239s
    Mar 29 08:16:38.027: INFO: The phase of Pod pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc is Running (Ready = true)
    Mar 29 08:16:38.027: INFO: Pod "pod-projected-configmaps-2b8282b2-f900-4083-921a-381250929ccc" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-bdef6d88-668c-40db-8817-f415de3dcac5 03/29/23 08:16:38.031
    STEP: waiting to observe update in volume 03/29/23 08:16:38.034
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:17:54.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6193" for this suite. 03/29/23 08:17:54.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:17:54.202
Mar 29 08:17:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:17:54.203
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:17:54.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:17:54.211
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-19f858d5-918f-433d-9cb1-15004aabac6c 03/29/23 08:17:54.212
STEP: Creating a pod to test consume secrets 03/29/23 08:17:54.214
Mar 29 08:17:54.218: INFO: Waiting up to 5m0s for pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46" in namespace "secrets-8052" to be "Succeeded or Failed"
Mar 29 08:17:54.219: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.174621ms
Mar 29 08:17:56.222: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004062383s
Mar 29 08:17:58.223: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005034165s
STEP: Saw pod success 03/29/23 08:17:58.223
Mar 29 08:17:58.223: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46" satisfied condition "Succeeded or Failed"
Mar 29 08:17:58.225: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:17:58.232
Mar 29 08:17:58.238: INFO: Waiting for pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 to disappear
Mar 29 08:17:58.240: INFO: Pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:17:58.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8052" for this suite. 03/29/23 08:17:58.242
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1651,"failed":0}
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:17:54.202
    Mar 29 08:17:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:17:54.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:17:54.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:17:54.211
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-19f858d5-918f-433d-9cb1-15004aabac6c 03/29/23 08:17:54.212
    STEP: Creating a pod to test consume secrets 03/29/23 08:17:54.214
    Mar 29 08:17:54.218: INFO: Waiting up to 5m0s for pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46" in namespace "secrets-8052" to be "Succeeded or Failed"
    Mar 29 08:17:54.219: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.174621ms
    Mar 29 08:17:56.222: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004062383s
    Mar 29 08:17:58.223: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005034165s
    STEP: Saw pod success 03/29/23 08:17:58.223
    Mar 29 08:17:58.223: INFO: Pod "pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46" satisfied condition "Succeeded or Failed"
    Mar 29 08:17:58.225: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:17:58.232
    Mar 29 08:17:58.238: INFO: Waiting for pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 to disappear
    Mar 29 08:17:58.240: INFO: Pod pod-secrets-0e6a01a6-d94d-4b32-b0b6-da1a59030f46 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:17:58.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8052" for this suite. 03/29/23 08:17:58.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:17:58.246
Mar 29 08:17:58.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:17:58.246
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:17:58.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:17:58.254
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 29 08:17:58.259: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a" in namespace "kubelet-test-3505" to be "running and ready"
Mar 29 08:17:58.261: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.186965ms
Mar 29 08:17:58.261: INFO: The phase of Pod busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:18:00.263: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a": Phase="Running", Reason="", readiness=true. Elapsed: 2.003963203s
Mar 29 08:18:00.263: INFO: The phase of Pod busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a is Running (Ready = true)
Mar 29 08:18:00.263: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 29 08:18:00.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3505" for this suite. 03/29/23 08:18:00.269
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1676,"failed":0}
------------------------------
â€¢ [2.026 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:17:58.246
    Mar 29 08:17:58.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:17:58.246
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:17:58.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:17:58.254
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 29 08:17:58.259: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a" in namespace "kubelet-test-3505" to be "running and ready"
    Mar 29 08:17:58.261: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.186965ms
    Mar 29 08:17:58.261: INFO: The phase of Pod busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:18:00.263: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a": Phase="Running", Reason="", readiness=true. Elapsed: 2.003963203s
    Mar 29 08:18:00.263: INFO: The phase of Pod busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a is Running (Ready = true)
    Mar 29 08:18:00.263: INFO: Pod "busybox-readonly-fs1a6e74a9-a88e-494a-ae5c-17d413a2177a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 29 08:18:00.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3505" for this suite. 03/29/23 08:18:00.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:18:00.275
Mar 29 08:18:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:18:00.276
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:18:00.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:18:00.283
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 29 08:18:00.289: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 08:19:00.303: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/29/23 08:19:00.304
Mar 29 08:19:00.315: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 29 08:19:00.317: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 29 08:19:00.325: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 29 08:19:00.328: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 29 08:19:00.336: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 29 08:19:00.339: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/29/23 08:19:00.339
Mar 29 08:19:00.339: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:00.340: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471193ms
Mar 29 08:19:02.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003678269s
Mar 29 08:19:04.343: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.004226645s
Mar 29 08:19:04.343: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 29 08:19:04.343: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:04.344: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.343899ms
Mar 29 08:19:04.344: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:19:04.344: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:04.345: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.17718ms
Mar 29 08:19:06.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004153974s
Mar 29 08:19:08.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004050444s
Mar 29 08:19:10.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.003229321s
Mar 29 08:19:10.348: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:19:10.348: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:10.349: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.320275ms
Mar 29 08:19:10.349: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:19:10.349: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:10.350: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.255527ms
Mar 29 08:19:10.350: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:19:10.350: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
Mar 29 08:19:10.351: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.11956ms
Mar 29 08:19:10.351: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/29/23 08:19:10.351
Mar 29 08:19:10.357: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 29 08:19:10.358: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253671ms
Mar 29 08:19:12.361: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328144s
Mar 29 08:19:14.361: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003959096s
Mar 29 08:19:14.361: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:19:14.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-988" for this suite. 03/29/23 08:19:14.378
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":98,"skipped":1773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.124 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:18:00.275
    Mar 29 08:18:00.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:18:00.276
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:18:00.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:18:00.283
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 29 08:18:00.289: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 08:19:00.303: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/29/23 08:19:00.304
    Mar 29 08:19:00.315: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 29 08:19:00.317: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 29 08:19:00.325: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 29 08:19:00.328: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 29 08:19:00.336: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 29 08:19:00.339: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/29/23 08:19:00.339
    Mar 29 08:19:00.339: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:00.340: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471193ms
    Mar 29 08:19:02.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003678269s
    Mar 29 08:19:04.343: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.004226645s
    Mar 29 08:19:04.343: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 29 08:19:04.343: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:04.344: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.343899ms
    Mar 29 08:19:04.344: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:19:04.344: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:04.345: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.17718ms
    Mar 29 08:19:06.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004153974s
    Mar 29 08:19:08.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004050444s
    Mar 29 08:19:10.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.003229321s
    Mar 29 08:19:10.348: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:19:10.348: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:10.349: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.320275ms
    Mar 29 08:19:10.349: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:19:10.349: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:10.350: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.255527ms
    Mar 29 08:19:10.350: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:19:10.350: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-988" to be "running"
    Mar 29 08:19:10.351: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.11956ms
    Mar 29 08:19:10.351: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/29/23 08:19:10.351
    Mar 29 08:19:10.357: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 29 08:19:10.358: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253671ms
    Mar 29 08:19:12.361: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328144s
    Mar 29 08:19:14.361: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003959096s
    Mar 29 08:19:14.361: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:19:14.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-988" for this suite. 03/29/23 08:19:14.378
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:14.4
Mar 29 08:19:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:19:14.4
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:14.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:14.407
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:19:14.408
Mar 29 08:19:14.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 29 08:19:14.456: INFO: stderr: ""
Mar 29 08:19:14.456: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/29/23 08:19:14.456
STEP: verifying the pod e2e-test-httpd-pod was created 03/29/23 08:19:19.508
Mar 29 08:19:19.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 get pod e2e-test-httpd-pod -o json'
Mar 29 08:19:19.554: INFO: stderr: ""
Mar 29 08:19:19.554: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"280b9adc0507839c9015638058d96364e93bfc3c2721c157ab017965ec3766fd\",\n            \"cni.projectcalico.org/podIP\": \"192.168.30.60/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.30.60/32\"\n        },\n        \"creationTimestamp\": \"2023-03-29T08:19:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9796\",\n        \"resourceVersion\": \"8684\",\n        \"uid\": \"ff7ab2b6-9e99-410b-bf86-8f05aeebd7ee\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zwgfh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.146.0.115\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zwgfh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6b189dba47f3cc72990482832991f6d0536d4b1ec93dc36da8155174b5a25d1f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-29T08:19:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.146.0.115\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.30.60\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.30.60\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-29T08:19:14Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/29/23 08:19:19.554
Mar 29 08:19:19.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 replace -f -'
Mar 29 08:19:20.178: INFO: stderr: ""
Mar 29 08:19:20.178: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/29/23 08:19:20.178
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar 29 08:19:20.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 delete pods e2e-test-httpd-pod'
Mar 29 08:19:21.933: INFO: stderr: ""
Mar 29 08:19:21.933: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:19:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9796" for this suite. 03/29/23 08:19:21.935
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":99,"skipped":1773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.538 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:14.4
    Mar 29 08:19:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:19:14.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:14.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:14.407
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:19:14.408
    Mar 29 08:19:14.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 29 08:19:14.456: INFO: stderr: ""
    Mar 29 08:19:14.456: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/29/23 08:19:14.456
    STEP: verifying the pod e2e-test-httpd-pod was created 03/29/23 08:19:19.508
    Mar 29 08:19:19.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 get pod e2e-test-httpd-pod -o json'
    Mar 29 08:19:19.554: INFO: stderr: ""
    Mar 29 08:19:19.554: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"280b9adc0507839c9015638058d96364e93bfc3c2721c157ab017965ec3766fd\",\n            \"cni.projectcalico.org/podIP\": \"192.168.30.60/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.30.60/32\"\n        },\n        \"creationTimestamp\": \"2023-03-29T08:19:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9796\",\n        \"resourceVersion\": \"8684\",\n        \"uid\": \"ff7ab2b6-9e99-410b-bf86-8f05aeebd7ee\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zwgfh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.146.0.115\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zwgfh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-29T08:19:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6b189dba47f3cc72990482832991f6d0536d4b1ec93dc36da8155174b5a25d1f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-29T08:19:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.146.0.115\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.30.60\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.30.60\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-29T08:19:14Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/29/23 08:19:19.554
    Mar 29 08:19:19.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 replace -f -'
    Mar 29 08:19:20.178: INFO: stderr: ""
    Mar 29 08:19:20.178: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/29/23 08:19:20.178
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar 29 08:19:20.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-9796 delete pods e2e-test-httpd-pod'
    Mar 29 08:19:21.933: INFO: stderr: ""
    Mar 29 08:19:21.933: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:19:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9796" for this suite. 03/29/23 08:19:21.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:21.939
Mar 29 08:19:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:19:21.939
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:21.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:21.947
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-572e1f5e-fe5f-44f6-92a3-ab01145bab0a 03/29/23 08:19:21.948
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:19:21.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1251" for this suite. 03/29/23 08:19:21.951
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":100,"skipped":1801,"failed":0}
------------------------------
â€¢ [0.015 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:21.939
    Mar 29 08:19:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:19:21.939
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:21.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:21.947
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-572e1f5e-fe5f-44f6-92a3-ab01145bab0a 03/29/23 08:19:21.948
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:19:21.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1251" for this suite. 03/29/23 08:19:21.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:21.955
Mar 29 08:19:21.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:19:21.955
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:21.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:21.962
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/29/23 08:19:21.964
Mar 29 08:19:21.967: INFO: Waiting up to 5m0s for pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d" in namespace "emptydir-2464" to be "Succeeded or Failed"
Mar 29 08:19:21.968: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225527ms
Mar 29 08:19:23.970: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002741696s
Mar 29 08:19:25.971: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004448359s
STEP: Saw pod success 03/29/23 08:19:25.971
Mar 29 08:19:25.971: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d" satisfied condition "Succeeded or Failed"
Mar 29 08:19:25.973: INFO: Trying to get logs from node 10.146.0.116 pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d container test-container: <nil>
STEP: delete the pod 03/29/23 08:19:25.975
Mar 29 08:19:25.982: INFO: Waiting for pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d to disappear
Mar 29 08:19:25.983: INFO: Pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:19:25.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2464" for this suite. 03/29/23 08:19:25.985
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1821,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:21.955
    Mar 29 08:19:21.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:19:21.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:21.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:21.962
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/29/23 08:19:21.964
    Mar 29 08:19:21.967: INFO: Waiting up to 5m0s for pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d" in namespace "emptydir-2464" to be "Succeeded or Failed"
    Mar 29 08:19:21.968: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225527ms
    Mar 29 08:19:23.970: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002741696s
    Mar 29 08:19:25.971: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004448359s
    STEP: Saw pod success 03/29/23 08:19:25.971
    Mar 29 08:19:25.971: INFO: Pod "pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d" satisfied condition "Succeeded or Failed"
    Mar 29 08:19:25.973: INFO: Trying to get logs from node 10.146.0.116 pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d container test-container: <nil>
    STEP: delete the pod 03/29/23 08:19:25.975
    Mar 29 08:19:25.982: INFO: Waiting for pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d to disappear
    Mar 29 08:19:25.983: INFO: Pod pod-053f6f8f-2133-4fdf-bcc3-0dd450d46c4d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:19:25.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2464" for this suite. 03/29/23 08:19:25.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:25.988
Mar 29 08:19:25.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename events 03/29/23 08:19:25.989
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:25.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:25.995
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/29/23 08:19:25.997
Mar 29 08:19:25.998: INFO: created test-event-1
Mar 29 08:19:26.000: INFO: created test-event-2
Mar 29 08:19:26.002: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/29/23 08:19:26.002
STEP: delete collection of events 03/29/23 08:19:26.003
Mar 29 08:19:26.003: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/29/23 08:19:26.01
Mar 29 08:19:26.010: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 29 08:19:26.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2348" for this suite. 03/29/23 08:19:26.014
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":102,"skipped":1870,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:25.988
    Mar 29 08:19:25.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename events 03/29/23 08:19:25.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:25.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:25.995
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/29/23 08:19:25.997
    Mar 29 08:19:25.998: INFO: created test-event-1
    Mar 29 08:19:26.000: INFO: created test-event-2
    Mar 29 08:19:26.002: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/29/23 08:19:26.002
    STEP: delete collection of events 03/29/23 08:19:26.003
    Mar 29 08:19:26.003: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/29/23 08:19:26.01
    Mar 29 08:19:26.010: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 29 08:19:26.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2348" for this suite. 03/29/23 08:19:26.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:26.016
Mar 29 08:19:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename podtemplate 03/29/23 08:19:26.017
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:26.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:26.023
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/29/23 08:19:26.024
Mar 29 08:19:26.027: INFO: created test-podtemplate-1
Mar 29 08:19:26.029: INFO: created test-podtemplate-2
Mar 29 08:19:26.031: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/29/23 08:19:26.031
STEP: delete collection of pod templates 03/29/23 08:19:26.032
Mar 29 08:19:26.032: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/29/23 08:19:26.037
Mar 29 08:19:26.038: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 29 08:19:26.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3475" for this suite. 03/29/23 08:19:26.04
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":103,"skipped":1876,"failed":0}
------------------------------
â€¢ [0.027 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:26.016
    Mar 29 08:19:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename podtemplate 03/29/23 08:19:26.017
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:26.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:26.023
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/29/23 08:19:26.024
    Mar 29 08:19:26.027: INFO: created test-podtemplate-1
    Mar 29 08:19:26.029: INFO: created test-podtemplate-2
    Mar 29 08:19:26.031: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/29/23 08:19:26.031
    STEP: delete collection of pod templates 03/29/23 08:19:26.032
    Mar 29 08:19:26.032: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/29/23 08:19:26.037
    Mar 29 08:19:26.038: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 29 08:19:26.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3475" for this suite. 03/29/23 08:19:26.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:26.043
Mar 29 08:19:26.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:19:26.044
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:26.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:26.051
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/29/23 08:19:26.052
Mar 29 08:19:26.055: INFO: Waiting up to 5m0s for pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87" in namespace "emptydir-2939" to be "Succeeded or Failed"
Mar 29 08:19:26.056: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.150817ms
Mar 29 08:19:28.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002669903s
Mar 29 08:19:30.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003268579s
STEP: Saw pod success 03/29/23 08:19:30.058
Mar 29 08:19:30.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87" satisfied condition "Succeeded or Failed"
Mar 29 08:19:30.060: INFO: Trying to get logs from node 10.146.0.115 pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 container test-container: <nil>
STEP: delete the pod 03/29/23 08:19:30.066
Mar 29 08:19:30.072: INFO: Waiting for pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 to disappear
Mar 29 08:19:30.075: INFO: Pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:19:30.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2939" for this suite. 03/29/23 08:19:30.077
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":104,"skipped":1887,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:26.043
    Mar 29 08:19:26.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:19:26.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:26.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:26.051
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/29/23 08:19:26.052
    Mar 29 08:19:26.055: INFO: Waiting up to 5m0s for pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87" in namespace "emptydir-2939" to be "Succeeded or Failed"
    Mar 29 08:19:26.056: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.150817ms
    Mar 29 08:19:28.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002669903s
    Mar 29 08:19:30.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003268579s
    STEP: Saw pod success 03/29/23 08:19:30.058
    Mar 29 08:19:30.058: INFO: Pod "pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87" satisfied condition "Succeeded or Failed"
    Mar 29 08:19:30.060: INFO: Trying to get logs from node 10.146.0.115 pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:19:30.066
    Mar 29 08:19:30.072: INFO: Waiting for pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 to disappear
    Mar 29 08:19:30.075: INFO: Pod pod-4b58892c-ab5a-40bb-9ab5-fbffbfdc8c87 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:19:30.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2939" for this suite. 03/29/23 08:19:30.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:30.081
Mar 29 08:19:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:19:30.081
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:30.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:30.088
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/29/23 08:19:30.089
STEP: getting /apis/node.k8s.io 03/29/23 08:19:30.09
STEP: getting /apis/node.k8s.io/v1 03/29/23 08:19:30.091
STEP: creating 03/29/23 08:19:30.091
STEP: watching 03/29/23 08:19:30.098
Mar 29 08:19:30.098: INFO: starting watch
STEP: getting 03/29/23 08:19:30.112
STEP: listing 03/29/23 08:19:30.114
STEP: patching 03/29/23 08:19:30.115
STEP: updating 03/29/23 08:19:30.119
Mar 29 08:19:30.121: INFO: waiting for watch events with expected annotations
STEP: deleting 03/29/23 08:19:30.122
STEP: deleting a collection 03/29/23 08:19:30.126
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 29 08:19:30.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1849" for this suite. 03/29/23 08:19:30.135
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":105,"skipped":1895,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:30.081
    Mar 29 08:19:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:19:30.081
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:30.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:30.088
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/29/23 08:19:30.089
    STEP: getting /apis/node.k8s.io 03/29/23 08:19:30.09
    STEP: getting /apis/node.k8s.io/v1 03/29/23 08:19:30.091
    STEP: creating 03/29/23 08:19:30.091
    STEP: watching 03/29/23 08:19:30.098
    Mar 29 08:19:30.098: INFO: starting watch
    STEP: getting 03/29/23 08:19:30.112
    STEP: listing 03/29/23 08:19:30.114
    STEP: patching 03/29/23 08:19:30.115
    STEP: updating 03/29/23 08:19:30.119
    Mar 29 08:19:30.121: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/29/23 08:19:30.122
    STEP: deleting a collection 03/29/23 08:19:30.126
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 29 08:19:30.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1849" for this suite. 03/29/23 08:19:30.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:30.139
Mar 29 08:19:30.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:19:30.139
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:30.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:30.146
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-d0ef1cd4-4d63-4308-aa18-8857fbc0a4c6 03/29/23 08:19:30.147
STEP: Creating a pod to test consume secrets 03/29/23 08:19:30.149
Mar 29 08:19:30.154: INFO: Waiting up to 5m0s for pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57" in namespace "secrets-7894" to be "Succeeded or Failed"
Mar 29 08:19:30.156: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451442ms
Mar 29 08:19:32.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004121155s
Mar 29 08:19:34.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003578932s
STEP: Saw pod success 03/29/23 08:19:34.158
Mar 29 08:19:34.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57" satisfied condition "Succeeded or Failed"
Mar 29 08:19:34.159: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:19:34.162
Mar 29 08:19:34.168: INFO: Waiting for pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 to disappear
Mar 29 08:19:34.169: INFO: Pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:19:34.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7894" for this suite. 03/29/23 08:19:34.171
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":106,"skipped":1908,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:30.139
    Mar 29 08:19:30.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:19:30.139
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:30.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:30.146
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-d0ef1cd4-4d63-4308-aa18-8857fbc0a4c6 03/29/23 08:19:30.147
    STEP: Creating a pod to test consume secrets 03/29/23 08:19:30.149
    Mar 29 08:19:30.154: INFO: Waiting up to 5m0s for pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57" in namespace "secrets-7894" to be "Succeeded or Failed"
    Mar 29 08:19:30.156: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451442ms
    Mar 29 08:19:32.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004121155s
    Mar 29 08:19:34.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003578932s
    STEP: Saw pod success 03/29/23 08:19:34.158
    Mar 29 08:19:34.158: INFO: Pod "pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57" satisfied condition "Succeeded or Failed"
    Mar 29 08:19:34.159: INFO: Trying to get logs from node 10.146.0.116 pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:19:34.162
    Mar 29 08:19:34.168: INFO: Waiting for pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 to disappear
    Mar 29 08:19:34.169: INFO: Pod pod-secrets-f7da5207-273e-481e-b402-e5d5d2d1fb57 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:19:34.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7894" for this suite. 03/29/23 08:19:34.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:19:34.174
Mar 29 08:19:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 08:19:34.174
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:34.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:34.181
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2758 03/29/23 08:19:34.183
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/29/23 08:19:34.185
STEP: Creating stateful set ss in namespace statefulset-2758 03/29/23 08:19:34.186
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2758 03/29/23 08:19:34.189
Mar 29 08:19:34.190: INFO: Found 0 stateful pods, waiting for 1
Mar 29 08:19:44.193: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/29/23 08:19:44.193
Mar 29 08:19:44.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:19:44.283: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:19:44.283: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:19:44.283: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:19:44.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 29 08:19:54.288: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:19:54.288: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:19:54.295: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999984s
Mar 29 08:19:55.297: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998582214s
Mar 29 08:19:56.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996205099s
Mar 29 08:19:57.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.994274454s
Mar 29 08:19:58.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991616675s
Mar 29 08:19:59.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988228388s
Mar 29 08:20:00.310: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.98635801s
Mar 29 08:20:01.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984240697s
Mar 29 08:20:02.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981871133s
Mar 29 08:20:03.316: INFO: Verifying statefulset ss doesn't scale past 1 for another 979.568726ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2758 03/29/23 08:20:04.317
Mar 29 08:20:04.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:20:04.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:20:04.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:20:04.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:20:04.401: INFO: Found 1 stateful pods, waiting for 3
Mar 29 08:20:14.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:20:14.405: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:20:14.405: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/29/23 08:20:14.405
STEP: Scale down will halt with unhealthy stateful pod 03/29/23 08:20:14.405
Mar 29 08:20:14.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:20:14.499: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:20:14.499: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:20:14.499: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:20:14.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:20:14.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:20:14.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:20:14.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:20:14.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:20:14.669: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:20:14.669: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:20:14.669: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:20:14.669: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:20:14.671: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 29 08:20:24.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:20:24.676: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:20:24.676: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:20:24.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999821s
Mar 29 08:20:25.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998383416s
Mar 29 08:20:26.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996124778s
Mar 29 08:20:27.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993144339s
Mar 29 08:20:28.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990839068s
Mar 29 08:20:29.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988754922s
Mar 29 08:20:30.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.986502536s
Mar 29 08:20:31.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.983459541s
Mar 29 08:20:32.701: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.981122084s
Mar 29 08:20:33.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.942143ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2758 03/29/23 08:20:34.703
Mar 29 08:20:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:20:34.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:20:34.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:20:34.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:20:34.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:20:34.883: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:20:34.883: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:20:34.883: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:20:34.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:20:34.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:20:34.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:20:34.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:20:34.963: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/29/23 08:20:44.97
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 08:20:44.971: INFO: Deleting all statefulset in ns statefulset-2758
Mar 29 08:20:44.972: INFO: Scaling statefulset ss to 0
Mar 29 08:20:44.976: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:20:44.977: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 08:20:44.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2758" for this suite. 03/29/23 08:20:44.984
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":107,"skipped":1929,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.813 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:19:34.174
    Mar 29 08:19:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 08:19:34.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:19:34.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:19:34.181
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2758 03/29/23 08:19:34.183
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/29/23 08:19:34.185
    STEP: Creating stateful set ss in namespace statefulset-2758 03/29/23 08:19:34.186
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2758 03/29/23 08:19:34.189
    Mar 29 08:19:34.190: INFO: Found 0 stateful pods, waiting for 1
    Mar 29 08:19:44.193: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/29/23 08:19:44.193
    Mar 29 08:19:44.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:19:44.283: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:19:44.283: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:19:44.283: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:19:44.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 29 08:19:54.288: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:19:54.288: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:19:54.295: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999984s
    Mar 29 08:19:55.297: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998582214s
    Mar 29 08:19:56.300: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996205099s
    Mar 29 08:19:57.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.994274454s
    Mar 29 08:19:58.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991616675s
    Mar 29 08:19:59.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988228388s
    Mar 29 08:20:00.310: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.98635801s
    Mar 29 08:20:01.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984240697s
    Mar 29 08:20:02.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981871133s
    Mar 29 08:20:03.316: INFO: Verifying statefulset ss doesn't scale past 1 for another 979.568726ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2758 03/29/23 08:20:04.317
    Mar 29 08:20:04.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:20:04.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:20:04.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:20:04.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:20:04.401: INFO: Found 1 stateful pods, waiting for 3
    Mar 29 08:20:14.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:20:14.405: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:20:14.405: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/29/23 08:20:14.405
    STEP: Scale down will halt with unhealthy stateful pod 03/29/23 08:20:14.405
    Mar 29 08:20:14.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:20:14.499: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:20:14.499: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:20:14.499: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:20:14.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:20:14.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:20:14.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:20:14.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:20:14.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:20:14.669: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:20:14.669: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:20:14.669: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:20:14.669: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:20:14.671: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar 29 08:20:24.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:20:24.676: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:20:24.676: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:20:24.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999821s
    Mar 29 08:20:25.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998383416s
    Mar 29 08:20:26.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996124778s
    Mar 29 08:20:27.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993144339s
    Mar 29 08:20:28.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990839068s
    Mar 29 08:20:29.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988754922s
    Mar 29 08:20:30.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.986502536s
    Mar 29 08:20:31.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.983459541s
    Mar 29 08:20:32.701: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.981122084s
    Mar 29 08:20:33.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.942143ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2758 03/29/23 08:20:34.703
    Mar 29 08:20:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:20:34.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:20:34.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:20:34.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:20:34.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:20:34.883: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:20:34.883: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:20:34.883: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:20:34.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-2758 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:20:34.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:20:34.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:20:34.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:20:34.963: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/29/23 08:20:44.97
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 08:20:44.971: INFO: Deleting all statefulset in ns statefulset-2758
    Mar 29 08:20:44.972: INFO: Scaling statefulset ss to 0
    Mar 29 08:20:44.976: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:20:44.977: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 08:20:44.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2758" for this suite. 03/29/23 08:20:44.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:20:44.988
Mar 29 08:20:44.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:20:44.988
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:20:44.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:20:44.996
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 29 08:20:45.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7386" for this suite. 03/29/23 08:20:45.002
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":108,"skipped":1943,"failed":0}
------------------------------
â€¢ [0.016 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:20:44.988
    Mar 29 08:20:44.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:20:44.988
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:20:44.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:20:44.996
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 29 08:20:45.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7386" for this suite. 03/29/23 08:20:45.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:20:45.005
Mar 29 08:20:45.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:20:45.005
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:20:45.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:20:45.012
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/29/23 08:20:45.013
Mar 29 08:20:45.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/29/23 08:20:53.212
Mar 29 08:20:53.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:20:55.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:21:02.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9053" for this suite. 03/29/23 08:21:02.768
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":109,"skipped":1958,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.766 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:20:45.005
    Mar 29 08:20:45.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:20:45.005
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:20:45.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:20:45.012
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/29/23 08:20:45.013
    Mar 29 08:20:45.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/29/23 08:20:53.212
    Mar 29 08:20:53.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:20:55.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:21:02.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9053" for this suite. 03/29/23 08:21:02.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:02.771
Mar 29 08:21:02.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:21:02.771
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:02.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:02.778
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/29/23 08:21:02.779
Mar 29 08:21:02.783: INFO: Waiting up to 5m0s for pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632" in namespace "pods-3294" to be "running and ready"
Mar 29 08:21:02.785: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632": Phase="Pending", Reason="", readiness=false. Elapsed: 1.171647ms
Mar 29 08:21:02.785: INFO: The phase of Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:21:04.788: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632": Phase="Running", Reason="", readiness=true. Elapsed: 2.004149656s
Mar 29 08:21:04.788: INFO: The phase of Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 is Running (Ready = true)
Mar 29 08:21:04.788: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632" satisfied condition "running and ready"
Mar 29 08:21:04.790: INFO: Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 has hostIP: 10.146.0.116
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 08:21:04.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3294" for this suite. 03/29/23 08:21:04.792
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":110,"skipped":1970,"failed":0}
------------------------------
â€¢ [2.023 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:02.771
    Mar 29 08:21:02.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:21:02.771
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:02.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:02.778
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/29/23 08:21:02.779
    Mar 29 08:21:02.783: INFO: Waiting up to 5m0s for pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632" in namespace "pods-3294" to be "running and ready"
    Mar 29 08:21:02.785: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632": Phase="Pending", Reason="", readiness=false. Elapsed: 1.171647ms
    Mar 29 08:21:02.785: INFO: The phase of Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:21:04.788: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632": Phase="Running", Reason="", readiness=true. Elapsed: 2.004149656s
    Mar 29 08:21:04.788: INFO: The phase of Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 is Running (Ready = true)
    Mar 29 08:21:04.788: INFO: Pod "pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632" satisfied condition "running and ready"
    Mar 29 08:21:04.790: INFO: Pod pod-hostip-e9b4f2f3-6a4b-49ef-9c1a-a2e4986bf632 has hostIP: 10.146.0.116
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 08:21:04.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3294" for this suite. 03/29/23 08:21:04.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:04.795
Mar 29 08:21:04.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:21:04.796
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:04.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:04.804
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-bd4673e2-670a-4723-bae4-3230ed73d070 03/29/23 08:21:04.805
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:21:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4475" for this suite. 03/29/23 08:21:04.807
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":111,"skipped":1990,"failed":0}
------------------------------
â€¢ [0.014 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:04.795
    Mar 29 08:21:04.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:21:04.796
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:04.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:04.804
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-bd4673e2-670a-4723-bae4-3230ed73d070 03/29/23 08:21:04.805
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:21:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4475" for this suite. 03/29/23 08:21:04.807
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:04.81
Mar 29 08:21:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-runtime 03/29/23 08:21:04.81
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:04.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:04.816
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/29/23 08:21:04.818
STEP: wait for the container to reach Succeeded 03/29/23 08:21:04.821
STEP: get the container status 03/29/23 08:21:08.829
STEP: the container should be terminated 03/29/23 08:21:08.831
STEP: the termination message should be set 03/29/23 08:21:08.831
Mar 29 08:21:08.831: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/29/23 08:21:08.831
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 29 08:21:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7182" for this suite. 03/29/23 08:21:08.84
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":112,"skipped":1991,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:04.81
    Mar 29 08:21:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-runtime 03/29/23 08:21:04.81
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:04.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:04.816
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/29/23 08:21:04.818
    STEP: wait for the container to reach Succeeded 03/29/23 08:21:04.821
    STEP: get the container status 03/29/23 08:21:08.829
    STEP: the container should be terminated 03/29/23 08:21:08.831
    STEP: the termination message should be set 03/29/23 08:21:08.831
    Mar 29 08:21:08.831: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/29/23 08:21:08.831
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 29 08:21:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7182" for this suite. 03/29/23 08:21:08.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:08.842
Mar 29 08:21:08.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:21:08.843
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:08.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:08.85
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/29/23 08:21:08.851
Mar 29 08:21:08.854: INFO: Waiting up to 5m0s for pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f" in namespace "downward-api-5735" to be "Succeeded or Failed"
Mar 29 08:21:08.855: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.176859ms
Mar 29 08:21:10.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00390389s
Mar 29 08:21:12.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00414468s
STEP: Saw pod success 03/29/23 08:21:12.858
Mar 29 08:21:12.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f" satisfied condition "Succeeded or Failed"
Mar 29 08:21:12.860: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:21:12.867
Mar 29 08:21:12.874: INFO: Waiting for pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f to disappear
Mar 29 08:21:12.876: INFO: Pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 29 08:21:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5735" for this suite. 03/29/23 08:21:12.878
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":113,"skipped":1998,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:08.842
    Mar 29 08:21:08.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:21:08.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:08.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:08.85
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/29/23 08:21:08.851
    Mar 29 08:21:08.854: INFO: Waiting up to 5m0s for pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f" in namespace "downward-api-5735" to be "Succeeded or Failed"
    Mar 29 08:21:08.855: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.176859ms
    Mar 29 08:21:10.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00390389s
    Mar 29 08:21:12.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00414468s
    STEP: Saw pod success 03/29/23 08:21:12.858
    Mar 29 08:21:12.858: INFO: Pod "downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f" satisfied condition "Succeeded or Failed"
    Mar 29 08:21:12.860: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:21:12.867
    Mar 29 08:21:12.874: INFO: Waiting for pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f to disappear
    Mar 29 08:21:12.876: INFO: Pod downward-api-eccc5aba-9f96-4ac9-b75d-93b8a06b2d3f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 29 08:21:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5735" for this suite. 03/29/23 08:21:12.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:12.881
Mar 29 08:21:12.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 08:21:12.882
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:12.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:12.891
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 in namespace container-probe-6072 03/29/23 08:21:12.892
Mar 29 08:21:12.896: INFO: Waiting up to 5m0s for pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1" in namespace "container-probe-6072" to be "not pending"
Mar 29 08:21:12.898: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.364788ms
Mar 29 08:21:14.900: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003320176s
Mar 29 08:21:14.900: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1" satisfied condition "not pending"
Mar 29 08:21:14.900: INFO: Started pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 in namespace container-probe-6072
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:21:14.9
Mar 29 08:21:14.901: INFO: Initial restart count of pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 is 0
Mar 29 08:21:34.931: INFO: Restart count of pod container-probe-6072/liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 is now 1 (20.029077017s elapsed)
STEP: deleting the pod 03/29/23 08:21:34.931
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 08:21:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6072" for this suite. 03/29/23 08:21:34.939
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":114,"skipped":2006,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.060 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:12.881
    Mar 29 08:21:12.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 08:21:12.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:12.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:12.891
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 in namespace container-probe-6072 03/29/23 08:21:12.892
    Mar 29 08:21:12.896: INFO: Waiting up to 5m0s for pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1" in namespace "container-probe-6072" to be "not pending"
    Mar 29 08:21:12.898: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.364788ms
    Mar 29 08:21:14.900: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003320176s
    Mar 29 08:21:14.900: INFO: Pod "liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1" satisfied condition "not pending"
    Mar 29 08:21:14.900: INFO: Started pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 in namespace container-probe-6072
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:21:14.9
    Mar 29 08:21:14.901: INFO: Initial restart count of pod liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 is 0
    Mar 29 08:21:34.931: INFO: Restart count of pod container-probe-6072/liveness-6d2ad1a1-55fb-4800-9bf5-dd25eb8f0fe1 is now 1 (20.029077017s elapsed)
    STEP: deleting the pod 03/29/23 08:21:34.931
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 08:21:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6072" for this suite. 03/29/23 08:21:34.939
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:34.942
Mar 29 08:21:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:21:34.943
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:34.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:34.949
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-a70e477b-b983-4d21-81bb-dc11290a5207 03/29/23 08:21:34.95
STEP: Creating a pod to test consume configMaps 03/29/23 08:21:34.953
Mar 29 08:21:34.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8" in namespace "configmap-7251" to be "Succeeded or Failed"
Mar 29 08:21:34.957: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.184924ms
Mar 29 08:21:36.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004385495s
Mar 29 08:21:38.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004017409s
STEP: Saw pod success 03/29/23 08:21:38.96
Mar 29 08:21:38.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8" satisfied condition "Succeeded or Failed"
Mar 29 08:21:38.961: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:21:38.969
Mar 29 08:21:38.975: INFO: Waiting for pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 to disappear
Mar 29 08:21:38.976: INFO: Pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:21:38.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7251" for this suite. 03/29/23 08:21:38.978
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":115,"skipped":2006,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:34.942
    Mar 29 08:21:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:21:34.943
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:34.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:34.949
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-a70e477b-b983-4d21-81bb-dc11290a5207 03/29/23 08:21:34.95
    STEP: Creating a pod to test consume configMaps 03/29/23 08:21:34.953
    Mar 29 08:21:34.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8" in namespace "configmap-7251" to be "Succeeded or Failed"
    Mar 29 08:21:34.957: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.184924ms
    Mar 29 08:21:36.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004385495s
    Mar 29 08:21:38.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004017409s
    STEP: Saw pod success 03/29/23 08:21:38.96
    Mar 29 08:21:38.960: INFO: Pod "pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8" satisfied condition "Succeeded or Failed"
    Mar 29 08:21:38.961: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:21:38.969
    Mar 29 08:21:38.975: INFO: Waiting for pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 to disappear
    Mar 29 08:21:38.976: INFO: Pod pod-configmaps-2c4d1341-bc70-4bd6-9808-639242c905e8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:21:38.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7251" for this suite. 03/29/23 08:21:38.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:38.98
Mar 29 08:21:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:21:38.981
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:38.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:38.988
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:21:38.995
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:21:38.998
Mar 29 08:21:39.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:21:39.001: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:21:40.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 08:21:40.004: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:21:41.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:21:41.005: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/29/23 08:21:41.006
Mar 29 08:21:41.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:21:41.015: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:21:42.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:21:42.018: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:21:43.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:21:43.019: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:21:44.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:21:44.018: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:21:44.019
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6999, will wait for the garbage collector to delete the pods 03/29/23 08:21:44.02
Mar 29 08:21:44.074: INFO: Deleting DaemonSet.extensions daemon-set took: 2.292056ms
Mar 29 08:21:44.174: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.560304ms
Mar 29 08:21:46.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:21:46.876: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 08:21:46.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9688"},"items":null}

Mar 29 08:21:46.879: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9688"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:21:46.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6999" for this suite. 03/29/23 08:21:46.885
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":116,"skipped":2013,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.907 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:38.98
    Mar 29 08:21:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:21:38.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:38.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:38.988
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:21:38.995
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:21:38.998
    Mar 29 08:21:39.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:21:39.001: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:21:40.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 08:21:40.004: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:21:41.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:21:41.005: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/29/23 08:21:41.006
    Mar 29 08:21:41.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:21:41.015: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:21:42.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:21:42.018: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:21:43.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:21:43.019: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:21:44.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:21:44.018: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:21:44.019
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6999, will wait for the garbage collector to delete the pods 03/29/23 08:21:44.02
    Mar 29 08:21:44.074: INFO: Deleting DaemonSet.extensions daemon-set took: 2.292056ms
    Mar 29 08:21:44.174: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.560304ms
    Mar 29 08:21:46.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:21:46.876: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 08:21:46.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9688"},"items":null}

    Mar 29 08:21:46.879: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9688"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:21:46.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6999" for this suite. 03/29/23 08:21:46.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:21:46.888
Mar 29 08:21:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 08:21:46.889
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:46.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:46.897
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/29/23 08:21:46.899
STEP: waiting for pod running 03/29/23 08:21:46.904
Mar 29 08:21:46.904: INFO: Waiting up to 2m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356" to be "running"
Mar 29 08:21:46.905: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204273ms
Mar 29 08:21:48.907: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003270436s
Mar 29 08:21:48.907: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" satisfied condition "running"
STEP: creating a file in subpath 03/29/23 08:21:48.907
Mar 29 08:21:48.909: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8356 PodName:var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:21:48.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:21:48.909: INFO: ExecWithOptions: Clientset creation
Mar 29 08:21:48.909: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/var-expansion-8356/pods/var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/29/23 08:21:48.95
Mar 29 08:21:48.951: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8356 PodName:var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:21:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:21:48.952: INFO: ExecWithOptions: Clientset creation
Mar 29 08:21:48.952: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/var-expansion-8356/pods/var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/29/23 08:21:48.99
Mar 29 08:21:49.497: INFO: Successfully updated pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3"
STEP: waiting for annotated pod running 03/29/23 08:21:49.497
Mar 29 08:21:49.497: INFO: Waiting up to 2m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356" to be "running"
Mar 29 08:21:49.499: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Running", Reason="", readiness=true. Elapsed: 1.480968ms
Mar 29 08:21:49.499: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" satisfied condition "running"
STEP: deleting the pod gracefully 03/29/23 08:21:49.499
Mar 29 08:21:49.499: INFO: Deleting pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356"
Mar 29 08:21:49.502: INFO: Wait up to 5m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 08:22:23.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8356" for this suite. 03/29/23 08:22:23.508
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":117,"skipped":2019,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.623 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:21:46.888
    Mar 29 08:21:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 08:21:46.889
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:21:46.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:21:46.897
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/29/23 08:21:46.899
    STEP: waiting for pod running 03/29/23 08:21:46.904
    Mar 29 08:21:46.904: INFO: Waiting up to 2m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356" to be "running"
    Mar 29 08:21:46.905: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204273ms
    Mar 29 08:21:48.907: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003270436s
    Mar 29 08:21:48.907: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" satisfied condition "running"
    STEP: creating a file in subpath 03/29/23 08:21:48.907
    Mar 29 08:21:48.909: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8356 PodName:var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:21:48.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:21:48.909: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:21:48.909: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/var-expansion-8356/pods/var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/29/23 08:21:48.95
    Mar 29 08:21:48.951: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8356 PodName:var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:21:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:21:48.952: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:21:48.952: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/var-expansion-8356/pods/var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/29/23 08:21:48.99
    Mar 29 08:21:49.497: INFO: Successfully updated pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3"
    STEP: waiting for annotated pod running 03/29/23 08:21:49.497
    Mar 29 08:21:49.497: INFO: Waiting up to 2m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356" to be "running"
    Mar 29 08:21:49.499: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3": Phase="Running", Reason="", readiness=true. Elapsed: 1.480968ms
    Mar 29 08:21:49.499: INFO: Pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" satisfied condition "running"
    STEP: deleting the pod gracefully 03/29/23 08:21:49.499
    Mar 29 08:21:49.499: INFO: Deleting pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" in namespace "var-expansion-8356"
    Mar 29 08:21:49.502: INFO: Wait up to 5m0s for pod "var-expansion-4443583e-db78-4489-ac25-ce7eb1e780c3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 08:22:23.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8356" for this suite. 03/29/23 08:22:23.508
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:23.512
Mar 29 08:22:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:22:23.512
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:23.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:23.519
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 29 08:22:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:22:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5578" for this suite. 03/29/23 08:22:24.044
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":118,"skipped":2021,"failed":0}
------------------------------
â€¢ [0.535 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:23.512
    Mar 29 08:22:23.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:22:23.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:23.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:23.519
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 29 08:22:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:22:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5578" for this suite. 03/29/23 08:22:24.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:24.047
Mar 29 08:22:24.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:22:24.048
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:24.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:24.055
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-4578/configmap-test-be9ba386-faf2-4aa1-a298-5af703f45892 03/29/23 08:22:24.056
STEP: Creating a pod to test consume configMaps 03/29/23 08:22:24.058
Mar 29 08:22:24.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52" in namespace "configmap-4578" to be "Succeeded or Failed"
Mar 29 08:22:24.063: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.168218ms
Mar 29 08:22:26.066: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0041226s
Mar 29 08:22:28.067: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004936091s
STEP: Saw pod success 03/29/23 08:22:28.067
Mar 29 08:22:28.067: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52" satisfied condition "Succeeded or Failed"
Mar 29 08:22:28.068: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 container env-test: <nil>
STEP: delete the pod 03/29/23 08:22:28.071
Mar 29 08:22:28.077: INFO: Waiting for pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 to disappear
Mar 29 08:22:28.079: INFO: Pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:22:28.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4578" for this suite. 03/29/23 08:22:28.081
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":119,"skipped":2042,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:24.047
    Mar 29 08:22:24.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:22:24.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:24.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:24.055
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-4578/configmap-test-be9ba386-faf2-4aa1-a298-5af703f45892 03/29/23 08:22:24.056
    STEP: Creating a pod to test consume configMaps 03/29/23 08:22:24.058
    Mar 29 08:22:24.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52" in namespace "configmap-4578" to be "Succeeded or Failed"
    Mar 29 08:22:24.063: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.168218ms
    Mar 29 08:22:26.066: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0041226s
    Mar 29 08:22:28.067: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004936091s
    STEP: Saw pod success 03/29/23 08:22:28.067
    Mar 29 08:22:28.067: INFO: Pod "pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52" satisfied condition "Succeeded or Failed"
    Mar 29 08:22:28.068: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 container env-test: <nil>
    STEP: delete the pod 03/29/23 08:22:28.071
    Mar 29 08:22:28.077: INFO: Waiting for pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 to disappear
    Mar 29 08:22:28.079: INFO: Pod pod-configmaps-c7438ef0-6044-42c7-8e2b-34e369be3f52 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:22:28.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4578" for this suite. 03/29/23 08:22:28.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:28.084
Mar 29 08:22:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:22:28.085
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:28.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:28.092
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar 29 08:22:28.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:22:30.914
Mar 29 08:22:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 create -f -'
Mar 29 08:22:31.336: INFO: stderr: ""
Mar 29 08:22:31.336: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 29 08:22:31.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 delete e2e-test-crd-publish-openapi-7886-crds test-cr'
Mar 29 08:22:31.379: INFO: stderr: ""
Mar 29 08:22:31.379: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 29 08:22:31.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 apply -f -'
Mar 29 08:22:31.506: INFO: stderr: ""
Mar 29 08:22:31.506: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 29 08:22:31.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 delete e2e-test-crd-publish-openapi-7886-crds test-cr'
Mar 29 08:22:31.550: INFO: stderr: ""
Mar 29 08:22:31.550: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/29/23 08:22:31.55
Mar 29 08:22:31.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 explain e2e-test-crd-publish-openapi-7886-crds'
Mar 29 08:22:31.668: INFO: stderr: ""
Mar 29 08:22:31.668: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7886-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:22:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4317" for this suite. 03/29/23 08:22:33.506
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":120,"skipped":2066,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.424 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:28.084
    Mar 29 08:22:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:22:28.085
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:28.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:28.092
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar 29 08:22:28.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:22:30.914
    Mar 29 08:22:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 create -f -'
    Mar 29 08:22:31.336: INFO: stderr: ""
    Mar 29 08:22:31.336: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 29 08:22:31.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 delete e2e-test-crd-publish-openapi-7886-crds test-cr'
    Mar 29 08:22:31.379: INFO: stderr: ""
    Mar 29 08:22:31.379: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 29 08:22:31.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 apply -f -'
    Mar 29 08:22:31.506: INFO: stderr: ""
    Mar 29 08:22:31.506: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 29 08:22:31.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 --namespace=crd-publish-openapi-4317 delete e2e-test-crd-publish-openapi-7886-crds test-cr'
    Mar 29 08:22:31.550: INFO: stderr: ""
    Mar 29 08:22:31.550: INFO: stdout: "e2e-test-crd-publish-openapi-7886-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/29/23 08:22:31.55
    Mar 29 08:22:31.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-4317 explain e2e-test-crd-publish-openapi-7886-crds'
    Mar 29 08:22:31.668: INFO: stderr: ""
    Mar 29 08:22:31.668: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7886-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:22:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4317" for this suite. 03/29/23 08:22:33.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:33.509
Mar 29 08:22:33.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:22:33.509
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:33.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:33.516
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar 29 08:22:33.526: INFO: Create a RollingUpdate DaemonSet
Mar 29 08:22:33.528: INFO: Check that daemon pods launch on every node of the cluster
Mar 29 08:22:33.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:22:33.531: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:22:34.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:22:34.536: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:22:35.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:22:35.536: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 29 08:22:35.536: INFO: Update the DaemonSet to trigger a rollout
Mar 29 08:22:35.540: INFO: Updating DaemonSet daemon-set
Mar 29 08:22:38.546: INFO: Roll back the DaemonSet before rollout is complete
Mar 29 08:22:38.551: INFO: Updating DaemonSet daemon-set
Mar 29 08:22:38.551: INFO: Make sure DaemonSet rollback is complete
Mar 29 08:22:38.553: INFO: Wrong image for pod: daemon-set-8hhbp. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar 29 08:22:38.553: INFO: Pod daemon-set-8hhbp is not available
Mar 29 08:22:41.558: INFO: Pod daemon-set-9qnn5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:22:41.562
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7547, will wait for the garbage collector to delete the pods 03/29/23 08:22:41.563
Mar 29 08:22:41.617: INFO: Deleting DaemonSet.extensions daemon-set took: 2.833732ms
Mar 29 08:22:41.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.513382ms
Mar 29 08:22:44.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:22:44.020: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 08:22:44.021: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10030"},"items":null}

Mar 29 08:22:44.022: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10030"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:22:44.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7547" for this suite. 03/29/23 08:22:44.029
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":121,"skipped":2074,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.523 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:33.509
    Mar 29 08:22:33.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:22:33.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:33.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:33.516
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar 29 08:22:33.526: INFO: Create a RollingUpdate DaemonSet
    Mar 29 08:22:33.528: INFO: Check that daemon pods launch on every node of the cluster
    Mar 29 08:22:33.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:22:33.531: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:22:34.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:22:34.536: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:22:35.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:22:35.536: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 29 08:22:35.536: INFO: Update the DaemonSet to trigger a rollout
    Mar 29 08:22:35.540: INFO: Updating DaemonSet daemon-set
    Mar 29 08:22:38.546: INFO: Roll back the DaemonSet before rollout is complete
    Mar 29 08:22:38.551: INFO: Updating DaemonSet daemon-set
    Mar 29 08:22:38.551: INFO: Make sure DaemonSet rollback is complete
    Mar 29 08:22:38.553: INFO: Wrong image for pod: daemon-set-8hhbp. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar 29 08:22:38.553: INFO: Pod daemon-set-8hhbp is not available
    Mar 29 08:22:41.558: INFO: Pod daemon-set-9qnn5 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:22:41.562
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7547, will wait for the garbage collector to delete the pods 03/29/23 08:22:41.563
    Mar 29 08:22:41.617: INFO: Deleting DaemonSet.extensions daemon-set took: 2.833732ms
    Mar 29 08:22:41.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.513382ms
    Mar 29 08:22:44.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:22:44.020: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 08:22:44.021: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10030"},"items":null}

    Mar 29 08:22:44.022: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10030"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:22:44.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7547" for this suite. 03/29/23 08:22:44.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:44.032
Mar 29 08:22:44.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:22:44.032
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:44.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:44.04
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/29/23 08:22:44.041
Mar 29 08:22:44.045: INFO: Waiting up to 5m0s for pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b" in namespace "downward-api-8843" to be "Succeeded or Failed"
Mar 29 08:22:44.047: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.271804ms
Mar 29 08:22:46.050: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004330535s
Mar 29 08:22:48.051: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005178133s
STEP: Saw pod success 03/29/23 08:22:48.051
Mar 29 08:22:48.051: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b" satisfied condition "Succeeded or Failed"
Mar 29 08:22:48.052: INFO: Trying to get logs from node 10.146.0.116 pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:22:48.055
Mar 29 08:22:48.060: INFO: Waiting for pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b to disappear
Mar 29 08:22:48.061: INFO: Pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 29 08:22:48.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8843" for this suite. 03/29/23 08:22:48.063
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":122,"skipped":2083,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:44.032
    Mar 29 08:22:44.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:22:44.032
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:44.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:44.04
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/29/23 08:22:44.041
    Mar 29 08:22:44.045: INFO: Waiting up to 5m0s for pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b" in namespace "downward-api-8843" to be "Succeeded or Failed"
    Mar 29 08:22:44.047: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.271804ms
    Mar 29 08:22:46.050: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004330535s
    Mar 29 08:22:48.051: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005178133s
    STEP: Saw pod success 03/29/23 08:22:48.051
    Mar 29 08:22:48.051: INFO: Pod "downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b" satisfied condition "Succeeded or Failed"
    Mar 29 08:22:48.052: INFO: Trying to get logs from node 10.146.0.116 pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:22:48.055
    Mar 29 08:22:48.060: INFO: Waiting for pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b to disappear
    Mar 29 08:22:48.061: INFO: Pod downward-api-44c69b29-509c-4705-bc91-80b6fd3b1f4b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 29 08:22:48.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8843" for this suite. 03/29/23 08:22:48.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:48.066
Mar 29 08:22:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:22:48.067
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:48.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:48.074
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 29 08:22:48.075: INFO: Creating simple deployment test-new-deployment
Mar 29 08:22:48.080: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/29/23 08:22:50.087
STEP: updating a scale subresource 03/29/23 08:22:50.088
STEP: verifying the deployment Spec.Replicas was modified 03/29/23 08:22:50.092
STEP: Patch a scale subresource 03/29/23 08:22:50.093
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:22:50.100: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6659  f2790bb9-ea70-4fc2-ae4d-1e53be54f779 10124 3 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c008f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 08:22:49 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-29 08:22:49 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 08:22:50.103: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6659  7af67681-e346-4e6d-ae3e-c671e22f7617 10129 2 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f2790bb9-ea70-4fc2-ae4d-1e53be54f779 0xc004ad2977 0xc004ad2978}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2790bb9-ea70-4fc2-ae4d-1e53be54f779\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ad2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:22:50.106: INFO: Pod "test-new-deployment-845c8977d9-k77vv" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-k77vv test-new-deployment-845c8977d9- deployment-6659  021414b7-7fbf-47f1-8386-5538e57ca9cb 10128 0 2023-03-29 08:22:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7af67681-e346-4e6d-ae3e-c671e22f7617 0xc004ad2f47 0xc004ad2f48}] [] [{kube-controller-manager Update v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af67681-e346-4e6d-ae3e-c671e22f7617\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6lcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6lcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:22:50.106: INFO: Pod "test-new-deployment-845c8977d9-wbrtf" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-wbrtf test-new-deployment-845c8977d9- deployment-6659  b40e3003-435e-416b-8aa6-51e3f2f8f12c 10118 0 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1717ab5228e61fe482b1f1b7bac0b8c055368c3954748d4d37a3adbcb3f34006 cni.projectcalico.org/podIP:192.168.30.6/32 cni.projectcalico.org/podIPs:192.168.30.6/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7af67681-e346-4e6d-ae3e-c671e22f7617 0xc004ad3190 0xc004ad3191}] [] [{calico Update v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af67681-e346-4e6d-ae3e-c671e22f7617\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:22:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4g8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4g8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.6,StartTime:2023-03-29 08:22:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:22:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0caa8604adfe4c784992b1fd0ec14aabfa5e516a322a5fbdb82ff4b978f9f8f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:22:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6659" for this suite. 03/29/23 08:22:50.108
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":123,"skipped":2133,"failed":0}
------------------------------
â€¢ [2.046 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:48.066
    Mar 29 08:22:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:22:48.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:48.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:48.074
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 29 08:22:48.075: INFO: Creating simple deployment test-new-deployment
    Mar 29 08:22:48.080: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/29/23 08:22:50.087
    STEP: updating a scale subresource 03/29/23 08:22:50.088
    STEP: verifying the deployment Spec.Replicas was modified 03/29/23 08:22:50.092
    STEP: Patch a scale subresource 03/29/23 08:22:50.093
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:22:50.100: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6659  f2790bb9-ea70-4fc2-ae4d-1e53be54f779 10124 3 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c008f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 08:22:49 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-29 08:22:49 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 29 08:22:50.103: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6659  7af67681-e346-4e6d-ae3e-c671e22f7617 10129 2 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f2790bb9-ea70-4fc2-ae4d-1e53be54f779 0xc004ad2977 0xc004ad2978}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2790bb9-ea70-4fc2-ae4d-1e53be54f779\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ad2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:22:50.106: INFO: Pod "test-new-deployment-845c8977d9-k77vv" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-k77vv test-new-deployment-845c8977d9- deployment-6659  021414b7-7fbf-47f1-8386-5538e57ca9cb 10128 0 2023-03-29 08:22:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7af67681-e346-4e6d-ae3e-c671e22f7617 0xc004ad2f47 0xc004ad2f48}] [] [{kube-controller-manager Update v1 2023-03-29 08:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af67681-e346-4e6d-ae3e-c671e22f7617\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6lcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6lcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:22:50.106: INFO: Pod "test-new-deployment-845c8977d9-wbrtf" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-wbrtf test-new-deployment-845c8977d9- deployment-6659  b40e3003-435e-416b-8aa6-51e3f2f8f12c 10118 0 2023-03-29 08:22:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1717ab5228e61fe482b1f1b7bac0b8c055368c3954748d4d37a3adbcb3f34006 cni.projectcalico.org/podIP:192.168.30.6/32 cni.projectcalico.org/podIPs:192.168.30.6/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 7af67681-e346-4e6d-ae3e-c671e22f7617 0xc004ad3190 0xc004ad3191}] [] [{calico Update v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:22:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7af67681-e346-4e6d-ae3e-c671e22f7617\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:22:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4g8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4g8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:22:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.6,StartTime:2023-03-29 08:22:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:22:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0caa8604adfe4c784992b1fd0ec14aabfa5e516a322a5fbdb82ff4b978f9f8f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:22:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6659" for this suite. 03/29/23 08:22:50.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:50.114
Mar 29 08:22:50.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:22:50.114
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:50.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:50.128
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 29 08:22:50.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3727" for this suite. 03/29/23 08:22:50.145
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":124,"skipped":2160,"failed":0}
------------------------------
â€¢ [0.034 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:50.114
    Mar 29 08:22:50.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubelet-test 03/29/23 08:22:50.114
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:50.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:50.128
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 29 08:22:50.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3727" for this suite. 03/29/23 08:22:50.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:50.149
Mar 29 08:22:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:22:50.149
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:50.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:50.156
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/29/23 08:22:50.157
Mar 29 08:22:50.160: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e" in namespace "emptydir-6277" to be "running"
Mar 29 08:22:50.161: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193653ms
Mar 29 08:22:52.164: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e": Phase="Running", Reason="", readiness=false. Elapsed: 2.003713088s
Mar 29 08:22:52.164: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/29/23 08:22:52.164
Mar 29 08:22:52.164: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6277 PodName:pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:22:52.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:22:52.164: INFO: ExecWithOptions: Clientset creation
Mar 29 08:22:52.164: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/emptydir-6277/pods/pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 29 08:22:52.214: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:22:52.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6277" for this suite. 03/29/23 08:22:52.216
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":125,"skipped":2177,"failed":0}
------------------------------
â€¢ [2.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:50.149
    Mar 29 08:22:50.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:22:50.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:50.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:50.156
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/29/23 08:22:50.157
    Mar 29 08:22:50.160: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e" in namespace "emptydir-6277" to be "running"
    Mar 29 08:22:50.161: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.193653ms
    Mar 29 08:22:52.164: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e": Phase="Running", Reason="", readiness=false. Elapsed: 2.003713088s
    Mar 29 08:22:52.164: INFO: Pod "pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/29/23 08:22:52.164
    Mar 29 08:22:52.164: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6277 PodName:pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:22:52.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:22:52.164: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:22:52.164: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/emptydir-6277/pods/pod-sharedvolume-3b364c37-b0f7-443f-ac69-72488920819e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 29 08:22:52.214: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:22:52.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6277" for this suite. 03/29/23 08:22:52.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:22:52.219
Mar 29 08:22:52.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:22:52.22
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:52.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:52.227
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-1941 03/29/23 08:22:52.228
Mar 29 08:22:52.232: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1941" to be "running and ready"
Mar 29 08:22:52.233: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.215105ms
Mar 29 08:22:52.233: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:22:54.235: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.003479223s
Mar 29 08:22:54.235: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 29 08:22:54.235: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 29 08:22:54.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 29 08:22:54.327: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 29 08:22:54.327: INFO: stdout: "ipvs"
Mar 29 08:22:54.327: INFO: proxyMode: ipvs
Mar 29 08:22:54.333: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 29 08:22:54.335: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1941 03/29/23 08:22:54.335
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1941 03/29/23 08:22:54.342
I0329 08:22:54.345488      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1941, replica count: 3
I0329 08:22:57.397546      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:22:57.402: INFO: Creating new exec pod
Mar 29 08:22:57.405: INFO: Waiting up to 5m0s for pod "execpod-affinity6rn9t" in namespace "services-1941" to be "running"
Mar 29 08:22:57.407: INFO: Pod "execpod-affinity6rn9t": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363715ms
Mar 29 08:22:59.409: INFO: Pod "execpod-affinity6rn9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.003582315s
Mar 29 08:22:59.409: INFO: Pod "execpod-affinity6rn9t" satisfied condition "running"
Mar 29 08:23:00.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 29 08:23:00.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 29 08:23:00.502: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:23:00.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.6.84 80'
Mar 29 08:23:00.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.6.84 80\nConnection to 10.100.6.84 80 port [tcp/http] succeeded!\n"
Mar 29 08:23:00.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:23:00.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 32427'
Mar 29 08:23:00.669: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 32427\nConnection to 10.146.0.117 32427 port [tcp/*] succeeded!\n"
Mar 29 08:23:00.669: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:23:00.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32427'
Mar 29 08:23:00.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32427\nConnection to 10.146.0.115 32427 port [tcp/*] succeeded!\n"
Mar 29 08:23:00.756: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:23:00.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:32427/ ; done'
Mar 29 08:23:00.881: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
Mar 29 08:23:00.881: INFO: stdout: "\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh"
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
Mar 29 08:23:00.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.146.0.115:32427/'
Mar 29 08:23:00.970: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
Mar 29 08:23:00.970: INFO: stdout: "affinity-nodeport-timeout-ctszh"
Mar 29 08:25:10.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.146.0.115:32427/'
Mar 29 08:25:11.063: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
Mar 29 08:25:11.063: INFO: stdout: "affinity-nodeport-timeout-hfgnh"
Mar 29 08:25:11.063: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1941, will wait for the garbage collector to delete the pods 03/29/23 08:25:11.07
Mar 29 08:25:11.127: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.526665ms
Mar 29 08:25:11.228: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.981106ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:25:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1941" for this suite. 03/29/23 08:25:13.241
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":126,"skipped":2186,"failed":0}
------------------------------
â€¢ [SLOW TEST] [141.024 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:22:52.219
    Mar 29 08:22:52.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:22:52.22
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:22:52.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:22:52.227
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-1941 03/29/23 08:22:52.228
    Mar 29 08:22:52.232: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1941" to be "running and ready"
    Mar 29 08:22:52.233: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 1.215105ms
    Mar 29 08:22:52.233: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:22:54.235: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.003479223s
    Mar 29 08:22:54.235: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 29 08:22:54.235: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 29 08:22:54.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 29 08:22:54.327: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 29 08:22:54.327: INFO: stdout: "ipvs"
    Mar 29 08:22:54.327: INFO: proxyMode: ipvs
    Mar 29 08:22:54.333: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 29 08:22:54.335: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-1941 03/29/23 08:22:54.335
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-1941 03/29/23 08:22:54.342
    I0329 08:22:54.345488      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1941, replica count: 3
    I0329 08:22:57.397546      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:22:57.402: INFO: Creating new exec pod
    Mar 29 08:22:57.405: INFO: Waiting up to 5m0s for pod "execpod-affinity6rn9t" in namespace "services-1941" to be "running"
    Mar 29 08:22:57.407: INFO: Pod "execpod-affinity6rn9t": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363715ms
    Mar 29 08:22:59.409: INFO: Pod "execpod-affinity6rn9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.003582315s
    Mar 29 08:22:59.409: INFO: Pod "execpod-affinity6rn9t" satisfied condition "running"
    Mar 29 08:23:00.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar 29 08:23:00.502: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar 29 08:23:00.502: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:23:00.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.6.84 80'
    Mar 29 08:23:00.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.6.84 80\nConnection to 10.100.6.84 80 port [tcp/http] succeeded!\n"
    Mar 29 08:23:00.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:23:00.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 32427'
    Mar 29 08:23:00.669: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 32427\nConnection to 10.146.0.117 32427 port [tcp/*] succeeded!\n"
    Mar 29 08:23:00.669: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:23:00.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32427'
    Mar 29 08:23:00.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32427\nConnection to 10.146.0.115 32427 port [tcp/*] succeeded!\n"
    Mar 29 08:23:00.756: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:23:00.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:32427/ ; done'
    Mar 29 08:23:00.881: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
    Mar 29 08:23:00.881: INFO: stdout: "\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh\naffinity-nodeport-timeout-ctszh"
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Received response from host: affinity-nodeport-timeout-ctszh
    Mar 29 08:23:00.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.146.0.115:32427/'
    Mar 29 08:23:00.970: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
    Mar 29 08:23:00.970: INFO: stdout: "affinity-nodeport-timeout-ctszh"
    Mar 29 08:25:10.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-1941 exec execpod-affinity6rn9t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.146.0.115:32427/'
    Mar 29 08:25:11.063: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.146.0.115:32427/\n"
    Mar 29 08:25:11.063: INFO: stdout: "affinity-nodeport-timeout-hfgnh"
    Mar 29 08:25:11.063: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1941, will wait for the garbage collector to delete the pods 03/29/23 08:25:11.07
    Mar 29 08:25:11.127: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.526665ms
    Mar 29 08:25:11.228: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.981106ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:25:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1941" for this suite. 03/29/23 08:25:13.241
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:13.244
Mar 29 08:25:13.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:25:13.245
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:13.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:13.252
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/29/23 08:25:13.254
STEP: Creating a ResourceQuota 03/29/23 08:25:18.256
STEP: Ensuring resource quota status is calculated 03/29/23 08:25:18.259
STEP: Creating a ReplicationController 03/29/23 08:25:20.262
STEP: Ensuring resource quota status captures replication controller creation 03/29/23 08:25:20.269
STEP: Deleting a ReplicationController 03/29/23 08:25:22.273
STEP: Ensuring resource quota status released usage 03/29/23 08:25:22.276
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:25:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5018" for this suite. 03/29/23 08:25:24.28
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":127,"skipped":2204,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:13.244
    Mar 29 08:25:13.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:25:13.245
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:13.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:13.252
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/29/23 08:25:13.254
    STEP: Creating a ResourceQuota 03/29/23 08:25:18.256
    STEP: Ensuring resource quota status is calculated 03/29/23 08:25:18.259
    STEP: Creating a ReplicationController 03/29/23 08:25:20.262
    STEP: Ensuring resource quota status captures replication controller creation 03/29/23 08:25:20.269
    STEP: Deleting a ReplicationController 03/29/23 08:25:22.273
    STEP: Ensuring resource quota status released usage 03/29/23 08:25:22.276
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:25:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5018" for this suite. 03/29/23 08:25:24.28
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:24.283
Mar 29 08:25:24.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replication-controller 03/29/23 08:25:24.283
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:24.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:24.29
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/29/23 08:25:24.292
STEP: When the matched label of one of its pods change 03/29/23 08:25:24.294
Mar 29 08:25:24.295: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 29 08:25:29.297: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/29/23 08:25:29.302
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 29 08:25:30.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5749" for this suite. 03/29/23 08:25:30.307
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":128,"skipped":2207,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.027 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:24.283
    Mar 29 08:25:24.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replication-controller 03/29/23 08:25:24.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:24.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:24.29
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/29/23 08:25:24.292
    STEP: When the matched label of one of its pods change 03/29/23 08:25:24.294
    Mar 29 08:25:24.295: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 29 08:25:29.297: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/29/23 08:25:29.302
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 29 08:25:30.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5749" for this suite. 03/29/23 08:25:30.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:30.31
Mar 29 08:25:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:25:30.311
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:30.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:30.318
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-e37f07a7-bd93-4866-a62b-529ef00cc1e5 03/29/23 08:25:30.319
STEP: Creating a pod to test consume configMaps 03/29/23 08:25:30.321
Mar 29 08:25:30.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db" in namespace "configmap-8597" to be "Succeeded or Failed"
Mar 29 08:25:30.326: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236193ms
Mar 29 08:25:32.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004364089s
Mar 29 08:25:34.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003881779s
STEP: Saw pod success 03/29/23 08:25:34.329
Mar 29 08:25:34.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db" satisfied condition "Succeeded or Failed"
Mar 29 08:25:34.330: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:25:34.339
Mar 29 08:25:34.344: INFO: Waiting for pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db to disappear
Mar 29 08:25:34.345: INFO: Pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:25:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8597" for this suite. 03/29/23 08:25:34.347
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":129,"skipped":2234,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:30.31
    Mar 29 08:25:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:25:30.311
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:30.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:30.318
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-e37f07a7-bd93-4866-a62b-529ef00cc1e5 03/29/23 08:25:30.319
    STEP: Creating a pod to test consume configMaps 03/29/23 08:25:30.321
    Mar 29 08:25:30.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db" in namespace "configmap-8597" to be "Succeeded or Failed"
    Mar 29 08:25:30.326: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.236193ms
    Mar 29 08:25:32.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004364089s
    Mar 29 08:25:34.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003881779s
    STEP: Saw pod success 03/29/23 08:25:34.329
    Mar 29 08:25:34.329: INFO: Pod "pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db" satisfied condition "Succeeded or Failed"
    Mar 29 08:25:34.330: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:25:34.339
    Mar 29 08:25:34.344: INFO: Waiting for pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db to disappear
    Mar 29 08:25:34.345: INFO: Pod pod-configmaps-f0ada076-79e7-4efc-88e2-8e0d5c6cc0db no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:25:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8597" for this suite. 03/29/23 08:25:34.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:34.35
Mar 29 08:25:34.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:25:34.35
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:34.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:34.357
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 29 08:25:34.370: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"09481a07-1ab3-48d8-a98e-02905b2a6c92", Controller:(*bool)(0xc0056ac06e), BlockOwnerDeletion:(*bool)(0xc0056ac06f)}}
Mar 29 08:25:34.377: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"94171cb8-a1c4-41d7-8ff0-d46d5bccfd0a", Controller:(*bool)(0xc0068af4de), BlockOwnerDeletion:(*bool)(0xc0068af4df)}}
Mar 29 08:25:34.380: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8651ddee-2a3b-45ce-bf3a-9baaae0d5eb5", Controller:(*bool)(0xc0068af776), BlockOwnerDeletion:(*bool)(0xc0068af777)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:25:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7152" for this suite. 03/29/23 08:25:39.387
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":130,"skipped":2246,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.040 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:34.35
    Mar 29 08:25:34.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:25:34.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:34.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:34.357
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 29 08:25:34.370: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"09481a07-1ab3-48d8-a98e-02905b2a6c92", Controller:(*bool)(0xc0056ac06e), BlockOwnerDeletion:(*bool)(0xc0056ac06f)}}
    Mar 29 08:25:34.377: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"94171cb8-a1c4-41d7-8ff0-d46d5bccfd0a", Controller:(*bool)(0xc0068af4de), BlockOwnerDeletion:(*bool)(0xc0068af4df)}}
    Mar 29 08:25:34.380: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8651ddee-2a3b-45ce-bf3a-9baaae0d5eb5", Controller:(*bool)(0xc0068af776), BlockOwnerDeletion:(*bool)(0xc0068af777)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:25:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7152" for this suite. 03/29/23 08:25:39.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:39.39
Mar 29 08:25:39.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 08:25:39.391
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:39.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:39.398
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/29/23 08:25:39.401
Mar 29 08:25:39.405: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4866" to be "running and ready"
Mar 29 08:25:39.406: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569444ms
Mar 29 08:25:39.406: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:25:41.409: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004437015s
Mar 29 08:25:41.409: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 29 08:25:41.409: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/29/23 08:25:41.411
Mar 29 08:25:41.413: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4866" to be "running and ready"
Mar 29 08:25:41.415: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.280285ms
Mar 29 08:25:41.415: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:25:43.417: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003869225s
Mar 29 08:25:43.417: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 29 08:25:43.417: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/29/23 08:25:43.418
STEP: delete the pod with lifecycle hook 03/29/23 08:25:43.421
Mar 29 08:25:43.425: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 08:25:43.426: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 08:25:45.427: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 08:25:45.429: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 29 08:25:47.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 29 08:25:47.430: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 29 08:25:47.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4866" for this suite. 03/29/23 08:25:47.432
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":131,"skipped":2256,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.045 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:39.39
    Mar 29 08:25:39.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 08:25:39.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:39.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:39.398
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/29/23 08:25:39.401
    Mar 29 08:25:39.405: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4866" to be "running and ready"
    Mar 29 08:25:39.406: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569444ms
    Mar 29 08:25:39.406: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:25:41.409: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004437015s
    Mar 29 08:25:41.409: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 29 08:25:41.409: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/29/23 08:25:41.411
    Mar 29 08:25:41.413: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4866" to be "running and ready"
    Mar 29 08:25:41.415: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.280285ms
    Mar 29 08:25:41.415: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:25:43.417: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003869225s
    Mar 29 08:25:43.417: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 29 08:25:43.417: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/29/23 08:25:43.418
    STEP: delete the pod with lifecycle hook 03/29/23 08:25:43.421
    Mar 29 08:25:43.425: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 29 08:25:43.426: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 29 08:25:45.427: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 29 08:25:45.429: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 29 08:25:47.428: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 29 08:25:47.430: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 29 08:25:47.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4866" for this suite. 03/29/23 08:25:47.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:47.435
Mar 29 08:25:47.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:25:47.436
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:47.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:47.445
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/29/23 08:25:47.446
Mar 29 08:25:47.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1933 create -f -'
Mar 29 08:25:47.875: INFO: stderr: ""
Mar 29 08:25:47.875: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/29/23 08:25:47.875
Mar 29 08:25:48.877: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 08:25:48.877: INFO: Found 1 / 1
Mar 29 08:25:48.877: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/29/23 08:25:48.877
Mar 29 08:25:48.879: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 08:25:48.879: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 08:25:48.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1933 patch pod agnhost-primary-b7dd9 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 29 08:25:48.923: INFO: stderr: ""
Mar 29 08:25:48.923: INFO: stdout: "pod/agnhost-primary-b7dd9 patched\n"
STEP: checking annotations 03/29/23 08:25:48.923
Mar 29 08:25:48.924: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 08:25:48.924: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:25:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1933" for this suite. 03/29/23 08:25:48.926
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":132,"skipped":2261,"failed":0}
------------------------------
â€¢ [1.493 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:47.435
    Mar 29 08:25:47.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:25:47.436
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:47.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:47.445
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/29/23 08:25:47.446
    Mar 29 08:25:47.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1933 create -f -'
    Mar 29 08:25:47.875: INFO: stderr: ""
    Mar 29 08:25:47.875: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/29/23 08:25:47.875
    Mar 29 08:25:48.877: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 08:25:48.877: INFO: Found 1 / 1
    Mar 29 08:25:48.877: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/29/23 08:25:48.877
    Mar 29 08:25:48.879: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 08:25:48.879: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 29 08:25:48.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1933 patch pod agnhost-primary-b7dd9 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 29 08:25:48.923: INFO: stderr: ""
    Mar 29 08:25:48.923: INFO: stdout: "pod/agnhost-primary-b7dd9 patched\n"
    STEP: checking annotations 03/29/23 08:25:48.923
    Mar 29 08:25:48.924: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 08:25:48.924: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:25:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1933" for this suite. 03/29/23 08:25:48.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:48.929
Mar 29 08:25:48.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename tables 03/29/23 08:25:48.93
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:48.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:48.937
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar 29 08:25:48.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7071" for this suite. 03/29/23 08:25:48.941
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":133,"skipped":2272,"failed":0}
------------------------------
â€¢ [0.015 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:48.929
    Mar 29 08:25:48.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename tables 03/29/23 08:25:48.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:48.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:48.937
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar 29 08:25:48.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7071" for this suite. 03/29/23 08:25:48.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:48.945
Mar 29 08:25:48.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:25:48.946
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:48.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:48.952
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-1989/configmap-test-697151a0-93e7-4511-9ff5-415f6325d1d0 03/29/23 08:25:48.953
STEP: Creating a pod to test consume configMaps 03/29/23 08:25:48.955
Mar 29 08:25:48.960: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29" in namespace "configmap-1989" to be "Succeeded or Failed"
Mar 29 08:25:48.961: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318159ms
Mar 29 08:25:50.964: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Running", Reason="", readiness=false. Elapsed: 2.004319535s
Mar 29 08:25:52.965: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005088653s
STEP: Saw pod success 03/29/23 08:25:52.965
Mar 29 08:25:52.965: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29" satisfied condition "Succeeded or Failed"
Mar 29 08:25:52.967: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 container env-test: <nil>
STEP: delete the pod 03/29/23 08:25:52.969
Mar 29 08:25:52.975: INFO: Waiting for pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 to disappear
Mar 29 08:25:52.976: INFO: Pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:25:52.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1989" for this suite. 03/29/23 08:25:52.978
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":134,"skipped":2296,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:48.945
    Mar 29 08:25:48.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:25:48.946
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:48.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:48.952
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-1989/configmap-test-697151a0-93e7-4511-9ff5-415f6325d1d0 03/29/23 08:25:48.953
    STEP: Creating a pod to test consume configMaps 03/29/23 08:25:48.955
    Mar 29 08:25:48.960: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29" in namespace "configmap-1989" to be "Succeeded or Failed"
    Mar 29 08:25:48.961: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318159ms
    Mar 29 08:25:50.964: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Running", Reason="", readiness=false. Elapsed: 2.004319535s
    Mar 29 08:25:52.965: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005088653s
    STEP: Saw pod success 03/29/23 08:25:52.965
    Mar 29 08:25:52.965: INFO: Pod "pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29" satisfied condition "Succeeded or Failed"
    Mar 29 08:25:52.967: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 container env-test: <nil>
    STEP: delete the pod 03/29/23 08:25:52.969
    Mar 29 08:25:52.975: INFO: Waiting for pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 to disappear
    Mar 29 08:25:52.976: INFO: Pod pod-configmaps-8a01e5d2-64da-48e7-8751-236f1ae87a29 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:25:52.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1989" for this suite. 03/29/23 08:25:52.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:52.982
Mar 29 08:25:52.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:25:52.983
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:52.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:52.989
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/29/23 08:25:52.99
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:25:52.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3933" for this suite. 03/29/23 08:25:52.993
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":135,"skipped":2332,"failed":0}
------------------------------
â€¢ [0.013 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:52.982
    Mar 29 08:25:52.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:25:52.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:52.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:52.989
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/29/23 08:25:52.99
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:25:52.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3933" for this suite. 03/29/23 08:25:52.993
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:52.996
Mar 29 08:25:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:25:52.997
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:53.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:53.004
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/29/23 08:25:53.006
STEP: Verify that the required pods have come up. 03/29/23 08:25:53.008
Mar 29 08:25:53.009: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 29 08:25:58.012: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 08:25:58.012
STEP: Getting /status 03/29/23 08:25:58.012
Mar 29 08:25:58.013: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/29/23 08:25:58.013
Mar 29 08:25:58.018: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/29/23 08:25:58.018
Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: ADDED
Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.019: INFO: Found replicaset test-rs in namespace replicaset-9319 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 29 08:25:58.019: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/29/23 08:25:58.019
Mar 29 08:25:58.019: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 29 08:25:58.023: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/29/23 08:25:58.023
Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: ADDED
Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.024: INFO: Observed replicaset test-rs in namespace replicaset-9319 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
Mar 29 08:25:58.024: INFO: Found replicaset test-rs in namespace replicaset-9319 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 29 08:25:58.024: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:25:58.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9319" for this suite. 03/29/23 08:25:58.026
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":136,"skipped":2347,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.033 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:52.996
    Mar 29 08:25:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:25:52.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:53.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:53.004
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/29/23 08:25:53.006
    STEP: Verify that the required pods have come up. 03/29/23 08:25:53.008
    Mar 29 08:25:53.009: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 29 08:25:58.012: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 08:25:58.012
    STEP: Getting /status 03/29/23 08:25:58.012
    Mar 29 08:25:58.013: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/29/23 08:25:58.013
    Mar 29 08:25:58.018: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/29/23 08:25:58.018
    Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: ADDED
    Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.019: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.019: INFO: Found replicaset test-rs in namespace replicaset-9319 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 29 08:25:58.019: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/29/23 08:25:58.019
    Mar 29 08:25:58.019: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 29 08:25:58.023: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/29/23 08:25:58.023
    Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: ADDED
    Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.024: INFO: Observed replicaset test-rs in namespace replicaset-9319 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 29 08:25:58.024: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 29 08:25:58.024: INFO: Found replicaset test-rs in namespace replicaset-9319 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 29 08:25:58.024: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:25:58.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9319" for this suite. 03/29/23 08:25:58.026
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:25:58.03
Mar 29 08:25:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:25:58.031
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:58.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:58.038
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/29/23 08:25:58.039
STEP: Creating a ResourceQuota 03/29/23 08:26:03.042
STEP: Ensuring resource quota status is calculated 03/29/23 08:26:03.047
STEP: Creating a Service 03/29/23 08:26:05.049
STEP: Creating a NodePort Service 03/29/23 08:26:05.058
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/29/23 08:26:05.068
STEP: Ensuring resource quota status captures service creation 03/29/23 08:26:05.077
STEP: Deleting Services 03/29/23 08:26:07.079
STEP: Ensuring resource quota status released usage 03/29/23 08:26:07.094
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:26:09.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9431" for this suite. 03/29/23 08:26:09.099
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":137,"skipped":2351,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.072 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:25:58.03
    Mar 29 08:25:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:25:58.031
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:25:58.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:25:58.038
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/29/23 08:25:58.039
    STEP: Creating a ResourceQuota 03/29/23 08:26:03.042
    STEP: Ensuring resource quota status is calculated 03/29/23 08:26:03.047
    STEP: Creating a Service 03/29/23 08:26:05.049
    STEP: Creating a NodePort Service 03/29/23 08:26:05.058
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/29/23 08:26:05.068
    STEP: Ensuring resource quota status captures service creation 03/29/23 08:26:05.077
    STEP: Deleting Services 03/29/23 08:26:07.079
    STEP: Ensuring resource quota status released usage 03/29/23 08:26:07.094
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:26:09.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9431" for this suite. 03/29/23 08:26:09.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:26:09.102
Mar 29 08:26:09.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 08:26:09.103
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:09.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:09.111
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/29/23 08:26:09.112
Mar 29 08:26:09.116: INFO: Waiting up to 5m0s for pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963" in namespace "var-expansion-6933" to be "Succeeded or Failed"
Mar 29 08:26:09.118: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226481ms
Mar 29 08:26:11.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003300592s
Mar 29 08:26:13.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003861057s
STEP: Saw pod success 03/29/23 08:26:13.12
Mar 29 08:26:13.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963" satisfied condition "Succeeded or Failed"
Mar 29 08:26:13.122: INFO: Trying to get logs from node 10.146.0.115 pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:26:13.125
Mar 29 08:26:13.132: INFO: Waiting for pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 to disappear
Mar 29 08:26:13.134: INFO: Pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 08:26:13.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6933" for this suite. 03/29/23 08:26:13.135
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":138,"skipped":2364,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:26:09.102
    Mar 29 08:26:09.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 08:26:09.103
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:09.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:09.111
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/29/23 08:26:09.112
    Mar 29 08:26:09.116: INFO: Waiting up to 5m0s for pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963" in namespace "var-expansion-6933" to be "Succeeded or Failed"
    Mar 29 08:26:09.118: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226481ms
    Mar 29 08:26:11.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003300592s
    Mar 29 08:26:13.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003861057s
    STEP: Saw pod success 03/29/23 08:26:13.12
    Mar 29 08:26:13.120: INFO: Pod "var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963" satisfied condition "Succeeded or Failed"
    Mar 29 08:26:13.122: INFO: Trying to get logs from node 10.146.0.115 pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:26:13.125
    Mar 29 08:26:13.132: INFO: Waiting for pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 to disappear
    Mar 29 08:26:13.134: INFO: Pod var-expansion-4a8f114a-f979-4106-86a0-50b6d0745963 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 08:26:13.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6933" for this suite. 03/29/23 08:26:13.135
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:26:13.138
Mar 29 08:26:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:26:13.139
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:13.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:13.145
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:26:13.147
Mar 29 08:26:13.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 29 08:26:13.193: INFO: stderr: ""
Mar 29 08:26:13.193: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/29/23 08:26:13.193
Mar 29 08:26:13.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar 29 08:26:13.323: INFO: stderr: ""
Mar 29 08:26:13.323: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:26:13.323
Mar 29 08:26:13.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 delete pods e2e-test-httpd-pod'
Mar 29 08:26:15.534: INFO: stderr: ""
Mar 29 08:26:15.534: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:26:15.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6087" for this suite. 03/29/23 08:26:15.536
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":139,"skipped":2368,"failed":0}
------------------------------
â€¢ [2.401 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:26:13.138
    Mar 29 08:26:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:26:13.139
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:13.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:13.145
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:26:13.147
    Mar 29 08:26:13.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 29 08:26:13.193: INFO: stderr: ""
    Mar 29 08:26:13.193: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/29/23 08:26:13.193
    Mar 29 08:26:13.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar 29 08:26:13.323: INFO: stderr: ""
    Mar 29 08:26:13.323: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/29/23 08:26:13.323
    Mar 29 08:26:13.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-6087 delete pods e2e-test-httpd-pod'
    Mar 29 08:26:15.534: INFO: stderr: ""
    Mar 29 08:26:15.534: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:26:15.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6087" for this suite. 03/29/23 08:26:15.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:26:15.54
Mar 29 08:26:15.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 08:26:15.541
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:15.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:15.547
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9499 03/29/23 08:26:15.549
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-9499 03/29/23 08:26:15.551
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9499 03/29/23 08:26:15.555
Mar 29 08:26:15.556: INFO: Found 0 stateful pods, waiting for 1
Mar 29 08:26:25.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/29/23 08:26:25.559
Mar 29 08:26:25.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:26:25.647: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:26:25.647: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:26:25.647: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:26:25.648: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 29 08:26:35.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:26:35.653: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:26:35.661: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 29 08:26:35.661: INFO: ss-0  10.146.0.116  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  }]
Mar 29 08:26:35.661: INFO: 
Mar 29 08:26:35.661: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 29 08:26:36.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99861143s
Mar 29 08:26:37.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996794328s
Mar 29 08:26:38.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.994476873s
Mar 29 08:26:39.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.99215939s
Mar 29 08:26:40.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.989798178s
Mar 29 08:26:41.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.987557999s
Mar 29 08:26:42.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.985297612s
Mar 29 08:26:43.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98208131s
Mar 29 08:26:44.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 979.796179ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9499 03/29/23 08:26:45.682
Mar 29 08:26:45.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:26:45.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:26:45.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:26:45.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:26:45.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:26:45.856: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 29 08:26:45.856: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:26:45.856: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:26:45.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:26:45.943: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 29 08:26:45.943: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:26:45.943: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 29 08:26:45.944: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:26:45.944: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:26:45.944: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/29/23 08:26:45.944
Mar 29 08:26:45.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:26:46.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:26:46.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:26:46.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:26:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:26:46.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:26:46.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:26:46.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:26:46.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:26:46.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:26:46.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:26:46.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:26:46.202: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:26:46.203: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 29 08:26:56.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:26:56.207: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:26:56.207: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 29 08:26:56.212: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 29 08:26:56.212: INFO: ss-0  10.146.0.116  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  }]
Mar 29 08:26:56.212: INFO: ss-1  10.146.0.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
Mar 29 08:26:56.212: INFO: ss-2  10.146.0.117  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
Mar 29 08:26:56.212: INFO: 
Mar 29 08:26:56.212: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 29 08:26:57.214: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 29 08:26:57.215: INFO: ss-2  10.146.0.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
Mar 29 08:26:57.215: INFO: 
Mar 29 08:26:57.215: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 29 08:26:58.216: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.996141114s
Mar 29 08:26:59.218: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.994287151s
Mar 29 08:27:00.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.992370579s
Mar 29 08:27:01.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.990457962s
Mar 29 08:27:02.225: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.987872256s
Mar 29 08:27:03.227: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.985586068s
Mar 29 08:27:04.229: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.983818981s
Mar 29 08:27:05.231: INFO: Verifying statefulset ss doesn't scale past 0 for another 981.53873ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9499 03/29/23 08:27:06.231
Mar 29 08:27:06.234: INFO: Scaling statefulset ss to 0
Mar 29 08:27:06.238: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 08:27:06.239: INFO: Deleting all statefulset in ns statefulset-9499
Mar 29 08:27:06.240: INFO: Scaling statefulset ss to 0
Mar 29 08:27:06.244: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:27:06.245: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 08:27:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9499" for this suite. 03/29/23 08:27:06.252
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":140,"skipped":2410,"failed":0}
------------------------------
â€¢ [SLOW TEST] [50.714 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:26:15.54
    Mar 29 08:26:15.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 08:26:15.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:26:15.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:26:15.547
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9499 03/29/23 08:26:15.549
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-9499 03/29/23 08:26:15.551
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9499 03/29/23 08:26:15.555
    Mar 29 08:26:15.556: INFO: Found 0 stateful pods, waiting for 1
    Mar 29 08:26:25.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/29/23 08:26:25.559
    Mar 29 08:26:25.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:26:25.647: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:26:25.647: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:26:25.647: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:26:25.648: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 29 08:26:35.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:26:35.653: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:26:35.661: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Mar 29 08:26:35.661: INFO: ss-0  10.146.0.116  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  }]
    Mar 29 08:26:35.661: INFO: 
    Mar 29 08:26:35.661: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar 29 08:26:36.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99861143s
    Mar 29 08:26:37.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996794328s
    Mar 29 08:26:38.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.994476873s
    Mar 29 08:26:39.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.99215939s
    Mar 29 08:26:40.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.989798178s
    Mar 29 08:26:41.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.987557999s
    Mar 29 08:26:42.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.985297612s
    Mar 29 08:26:43.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98208131s
    Mar 29 08:26:44.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 979.796179ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9499 03/29/23 08:26:45.682
    Mar 29 08:26:45.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:26:45.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:26:45.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:26:45.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:26:45.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:26:45.856: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 29 08:26:45.856: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:26:45.856: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:26:45.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:26:45.943: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 29 08:26:45.943: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:26:45.943: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 29 08:26:45.944: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:26:45.944: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:26:45.944: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/29/23 08:26:45.944
    Mar 29 08:26:45.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:26:46.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:26:46.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:26:46.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:26:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:26:46.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:26:46.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:26:46.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:26:46.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-9499 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:26:46.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:26:46.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:26:46.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:26:46.202: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:26:46.203: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 29 08:26:56.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:26:56.207: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:26:56.207: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 29 08:26:56.212: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Mar 29 08:26:56.212: INFO: ss-0  10.146.0.116  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:15 +0000 UTC  }]
    Mar 29 08:26:56.212: INFO: ss-1  10.146.0.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
    Mar 29 08:26:56.212: INFO: ss-2  10.146.0.117  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
    Mar 29 08:26:56.212: INFO: 
    Mar 29 08:26:56.212: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 29 08:26:57.214: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Mar 29 08:26:57.215: INFO: ss-2  10.146.0.117  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:26:35 +0000 UTC  }]
    Mar 29 08:26:57.215: INFO: 
    Mar 29 08:26:57.215: INFO: StatefulSet ss has not reached scale 0, at 1
    Mar 29 08:26:58.216: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.996141114s
    Mar 29 08:26:59.218: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.994287151s
    Mar 29 08:27:00.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.992370579s
    Mar 29 08:27:01.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.990457962s
    Mar 29 08:27:02.225: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.987872256s
    Mar 29 08:27:03.227: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.985586068s
    Mar 29 08:27:04.229: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.983818981s
    Mar 29 08:27:05.231: INFO: Verifying statefulset ss doesn't scale past 0 for another 981.53873ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9499 03/29/23 08:27:06.231
    Mar 29 08:27:06.234: INFO: Scaling statefulset ss to 0
    Mar 29 08:27:06.238: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 08:27:06.239: INFO: Deleting all statefulset in ns statefulset-9499
    Mar 29 08:27:06.240: INFO: Scaling statefulset ss to 0
    Mar 29 08:27:06.244: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:27:06.245: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 08:27:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9499" for this suite. 03/29/23 08:27:06.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:06.255
Mar 29 08:27:06.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:27:06.255
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:06.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:06.264
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:27:06.265
Mar 29 08:27:06.269: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674" in namespace "downward-api-4571" to be "Succeeded or Failed"
Mar 29 08:27:06.270: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Pending", Reason="", readiness=false. Elapsed: 1.191095ms
Mar 29 08:27:08.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579047s
Mar 29 08:27:10.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004469439s
STEP: Saw pod success 03/29/23 08:27:10.273
Mar 29 08:27:10.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674" satisfied condition "Succeeded or Failed"
Mar 29 08:27:10.275: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 container client-container: <nil>
STEP: delete the pod 03/29/23 08:27:10.277
Mar 29 08:27:10.284: INFO: Waiting for pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 to disappear
Mar 29 08:27:10.286: INFO: Pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:27:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4571" for this suite. 03/29/23 08:27:10.287
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":141,"skipped":2416,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:06.255
    Mar 29 08:27:06.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:27:06.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:06.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:06.264
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:27:06.265
    Mar 29 08:27:06.269: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674" in namespace "downward-api-4571" to be "Succeeded or Failed"
    Mar 29 08:27:06.270: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Pending", Reason="", readiness=false. Elapsed: 1.191095ms
    Mar 29 08:27:08.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579047s
    Mar 29 08:27:10.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004469439s
    STEP: Saw pod success 03/29/23 08:27:10.273
    Mar 29 08:27:10.273: INFO: Pod "downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674" satisfied condition "Succeeded or Failed"
    Mar 29 08:27:10.275: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:27:10.277
    Mar 29 08:27:10.284: INFO: Waiting for pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 to disappear
    Mar 29 08:27:10.286: INFO: Pod downwardapi-volume-1d82061a-ccfe-44ad-b241-94017ef93674 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:27:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4571" for this suite. 03/29/23 08:27:10.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:10.291
Mar 29 08:27:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replication-controller 03/29/23 08:27:10.292
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:10.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:10.298
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92 03/29/23 08:27:10.299
Mar 29 08:27:10.303: INFO: Pod name my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Found 0 pods out of 1
Mar 29 08:27:15.305: INFO: Pod name my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Found 1 pods out of 1
Mar 29 08:27:15.305: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92" are running
Mar 29 08:27:15.305: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" in namespace "replication-controller-8488" to be "running"
Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h": Phase="Running", Reason="", readiness=true. Elapsed: 1.159373ms
Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" satisfied condition "running"
Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:10 +0000 UTC Reason: Message:}])
Mar 29 08:27:15.306: INFO: Trying to dial the pod
Mar 29 08:27:20.313: INFO: Controller my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Got expected result from replica 1 [my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h]: "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 29 08:27:20.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8488" for this suite. 03/29/23 08:27:20.315
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":142,"skipped":2464,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.028 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:10.291
    Mar 29 08:27:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replication-controller 03/29/23 08:27:10.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:10.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:10.298
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92 03/29/23 08:27:10.299
    Mar 29 08:27:10.303: INFO: Pod name my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Found 0 pods out of 1
    Mar 29 08:27:15.305: INFO: Pod name my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Found 1 pods out of 1
    Mar 29 08:27:15.305: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92" are running
    Mar 29 08:27:15.305: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" in namespace "replication-controller-8488" to be "running"
    Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h": Phase="Running", Reason="", readiness=true. Elapsed: 1.159373ms
    Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" satisfied condition "running"
    Mar 29 08:27:15.306: INFO: Pod "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-29 08:27:10 +0000 UTC Reason: Message:}])
    Mar 29 08:27:15.306: INFO: Trying to dial the pod
    Mar 29 08:27:20.313: INFO: Controller my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92: Got expected result from replica 1 [my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h]: "my-hostname-basic-3dcbff20-6fef-4b61-85c3-208473466a92-dwg5h", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 29 08:27:20.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8488" for this suite. 03/29/23 08:27:20.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:20.32
Mar 29 08:27:20.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:27:20.321
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:20.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:20.328
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/29/23 08:27:20.33
STEP: Wait for the Deployment to create new ReplicaSet 03/29/23 08:27:20.334
STEP: delete the deployment 03/29/23 08:27:20.838
STEP: wait for all rs to be garbage collected 03/29/23 08:27:20.841
STEP: expected 0 rs, got 1 rs 03/29/23 08:27:20.844
STEP: expected 0 pods, got 2 pods 03/29/23 08:27:20.845
STEP: Gathering metrics 03/29/23 08:27:21.35
W0329 08:27:21.353085      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 08:27:21.353: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:27:21.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5710" for this suite. 03/29/23 08:27:21.354
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":143,"skipped":2492,"failed":0}
------------------------------
â€¢ [1.036 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:20.32
    Mar 29 08:27:20.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:27:20.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:20.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:20.328
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/29/23 08:27:20.33
    STEP: Wait for the Deployment to create new ReplicaSet 03/29/23 08:27:20.334
    STEP: delete the deployment 03/29/23 08:27:20.838
    STEP: wait for all rs to be garbage collected 03/29/23 08:27:20.841
    STEP: expected 0 rs, got 1 rs 03/29/23 08:27:20.844
    STEP: expected 0 pods, got 2 pods 03/29/23 08:27:20.845
    STEP: Gathering metrics 03/29/23 08:27:21.35
    W0329 08:27:21.353085      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 08:27:21.353: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:27:21.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5710" for this suite. 03/29/23 08:27:21.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:21.357
Mar 29 08:27:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename watch 03/29/23 08:27:21.358
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:21.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:21.365
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/29/23 08:27:21.367
STEP: creating a new configmap 03/29/23 08:27:21.367
STEP: modifying the configmap once 03/29/23 08:27:21.369
STEP: closing the watch once it receives two notifications 03/29/23 08:27:21.372
Mar 29 08:27:21.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11465 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:27:21.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11466 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/29/23 08:27:21.372
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/29/23 08:27:21.376
STEP: deleting the configmap 03/29/23 08:27:21.376
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/29/23 08:27:21.378
Mar 29 08:27:21.378: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11467 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:27:21.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11468 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 29 08:27:21.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3432" for this suite. 03/29/23 08:27:21.38
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":144,"skipped":2502,"failed":0}
------------------------------
â€¢ [0.025 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:21.357
    Mar 29 08:27:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename watch 03/29/23 08:27:21.358
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:21.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:21.365
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/29/23 08:27:21.367
    STEP: creating a new configmap 03/29/23 08:27:21.367
    STEP: modifying the configmap once 03/29/23 08:27:21.369
    STEP: closing the watch once it receives two notifications 03/29/23 08:27:21.372
    Mar 29 08:27:21.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11465 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:27:21.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11466 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/29/23 08:27:21.372
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/29/23 08:27:21.376
    STEP: deleting the configmap 03/29/23 08:27:21.376
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/29/23 08:27:21.378
    Mar 29 08:27:21.378: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11467 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:27:21.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3432  97ff62d2-9bd7-449a-915b-2d5a749cb2ea 11468 0 2023-03-29 08:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-29 08:27:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 29 08:27:21.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3432" for this suite. 03/29/23 08:27:21.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:21.383
Mar 29 08:27:21.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:27:21.383
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:21.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:21.389
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-8f1e1cf3-ad3f-40f0-b3f3-902e1872dcee 03/29/23 08:27:21.391
STEP: Creating a pod to test consume configMaps 03/29/23 08:27:21.392
Mar 29 08:27:21.395: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9" in namespace "configmap-7081" to be "Succeeded or Failed"
Mar 29 08:27:21.396: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198625ms
Mar 29 08:27:23.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003659312s
Mar 29 08:27:25.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004018018s
STEP: Saw pod success 03/29/23 08:27:25.399
Mar 29 08:27:25.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9" satisfied condition "Succeeded or Failed"
Mar 29 08:27:25.401: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:27:25.404
Mar 29 08:27:25.409: INFO: Waiting for pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 to disappear
Mar 29 08:27:25.411: INFO: Pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:27:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7081" for this suite. 03/29/23 08:27:25.412
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":145,"skipped":2520,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:21.383
    Mar 29 08:27:21.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:27:21.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:21.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:21.389
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-8f1e1cf3-ad3f-40f0-b3f3-902e1872dcee 03/29/23 08:27:21.391
    STEP: Creating a pod to test consume configMaps 03/29/23 08:27:21.392
    Mar 29 08:27:21.395: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9" in namespace "configmap-7081" to be "Succeeded or Failed"
    Mar 29 08:27:21.396: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.198625ms
    Mar 29 08:27:23.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003659312s
    Mar 29 08:27:25.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004018018s
    STEP: Saw pod success 03/29/23 08:27:25.399
    Mar 29 08:27:25.399: INFO: Pod "pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9" satisfied condition "Succeeded or Failed"
    Mar 29 08:27:25.401: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:27:25.404
    Mar 29 08:27:25.409: INFO: Waiting for pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 to disappear
    Mar 29 08:27:25.411: INFO: Pod pod-configmaps-f9610a9f-cb99-482b-b07d-aa09e8bcc0d9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:27:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7081" for this suite. 03/29/23 08:27:25.412
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:25.415
Mar 29 08:27:25.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename subpath 03/29/23 08:27:25.416
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:25.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:25.422
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/29/23 08:27:25.424
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-sjq2 03/29/23 08:27:25.431
STEP: Creating a pod to test atomic-volume-subpath 03/29/23 08:27:25.431
Mar 29 08:27:25.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sjq2" in namespace "subpath-6781" to be "Succeeded or Failed"
Mar 29 08:27:25.437: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248984ms
Mar 29 08:27:27.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004372405s
Mar 29 08:27:29.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00322832s
Mar 29 08:27:31.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 6.003201024s
Mar 29 08:27:33.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 8.003879342s
Mar 29 08:27:35.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 10.003639431s
Mar 29 08:27:37.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 12.003978649s
Mar 29 08:27:39.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 14.003661827s
Mar 29 08:27:41.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 16.004452567s
Mar 29 08:27:43.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 18.003342436s
Mar 29 08:27:45.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 20.003158048s
Mar 29 08:27:47.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=false. Elapsed: 22.003309944s
Mar 29 08:27:49.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003495232s
STEP: Saw pod success 03/29/23 08:27:49.439
Mar 29 08:27:49.439: INFO: Pod "pod-subpath-test-configmap-sjq2" satisfied condition "Succeeded or Failed"
Mar 29 08:27:49.441: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-configmap-sjq2 container test-container-subpath-configmap-sjq2: <nil>
STEP: delete the pod 03/29/23 08:27:49.444
Mar 29 08:27:49.450: INFO: Waiting for pod pod-subpath-test-configmap-sjq2 to disappear
Mar 29 08:27:49.451: INFO: Pod pod-subpath-test-configmap-sjq2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sjq2 03/29/23 08:27:49.451
Mar 29 08:27:49.451: INFO: Deleting pod "pod-subpath-test-configmap-sjq2" in namespace "subpath-6781"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 29 08:27:49.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6781" for this suite. 03/29/23 08:27:49.454
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":146,"skipped":2520,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.042 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:25.415
    Mar 29 08:27:25.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename subpath 03/29/23 08:27:25.416
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:25.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:25.422
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/29/23 08:27:25.424
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-sjq2 03/29/23 08:27:25.431
    STEP: Creating a pod to test atomic-volume-subpath 03/29/23 08:27:25.431
    Mar 29 08:27:25.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sjq2" in namespace "subpath-6781" to be "Succeeded or Failed"
    Mar 29 08:27:25.437: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248984ms
    Mar 29 08:27:27.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004372405s
    Mar 29 08:27:29.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00322832s
    Mar 29 08:27:31.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 6.003201024s
    Mar 29 08:27:33.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 8.003879342s
    Mar 29 08:27:35.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 10.003639431s
    Mar 29 08:27:37.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 12.003978649s
    Mar 29 08:27:39.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 14.003661827s
    Mar 29 08:27:41.440: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 16.004452567s
    Mar 29 08:27:43.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 18.003342436s
    Mar 29 08:27:45.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=true. Elapsed: 20.003158048s
    Mar 29 08:27:47.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Running", Reason="", readiness=false. Elapsed: 22.003309944s
    Mar 29 08:27:49.439: INFO: Pod "pod-subpath-test-configmap-sjq2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003495232s
    STEP: Saw pod success 03/29/23 08:27:49.439
    Mar 29 08:27:49.439: INFO: Pod "pod-subpath-test-configmap-sjq2" satisfied condition "Succeeded or Failed"
    Mar 29 08:27:49.441: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-configmap-sjq2 container test-container-subpath-configmap-sjq2: <nil>
    STEP: delete the pod 03/29/23 08:27:49.444
    Mar 29 08:27:49.450: INFO: Waiting for pod pod-subpath-test-configmap-sjq2 to disappear
    Mar 29 08:27:49.451: INFO: Pod pod-subpath-test-configmap-sjq2 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-sjq2 03/29/23 08:27:49.451
    Mar 29 08:27:49.451: INFO: Deleting pod "pod-subpath-test-configmap-sjq2" in namespace "subpath-6781"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 29 08:27:49.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6781" for this suite. 03/29/23 08:27:49.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:49.458
Mar 29 08:27:49.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:27:49.458
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:49.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:49.466
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-21426e43-bbb4-4421-a239-d68c01916f65 03/29/23 08:27:49.467
STEP: Creating a pod to test consume secrets 03/29/23 08:27:49.469
Mar 29 08:27:49.473: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f" in namespace "projected-8955" to be "Succeeded or Failed"
Mar 29 08:27:49.474: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.274361ms
Mar 29 08:27:51.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004225919s
Mar 29 08:27:53.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00390234s
STEP: Saw pod success 03/29/23 08:27:53.477
Mar 29 08:27:53.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f" satisfied condition "Succeeded or Failed"
Mar 29 08:27:53.478: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f container projected-secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:27:53.486
Mar 29 08:27:53.504: INFO: Waiting for pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f to disappear
Mar 29 08:27:53.508: INFO: Pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 08:27:53.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8955" for this suite. 03/29/23 08:27:53.51
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2534,"failed":0}
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:49.458
    Mar 29 08:27:49.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:27:49.458
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:49.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:49.466
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-21426e43-bbb4-4421-a239-d68c01916f65 03/29/23 08:27:49.467
    STEP: Creating a pod to test consume secrets 03/29/23 08:27:49.469
    Mar 29 08:27:49.473: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f" in namespace "projected-8955" to be "Succeeded or Failed"
    Mar 29 08:27:49.474: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.274361ms
    Mar 29 08:27:51.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004225919s
    Mar 29 08:27:53.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00390234s
    STEP: Saw pod success 03/29/23 08:27:53.477
    Mar 29 08:27:53.477: INFO: Pod "pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f" satisfied condition "Succeeded or Failed"
    Mar 29 08:27:53.478: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:27:53.486
    Mar 29 08:27:53.504: INFO: Waiting for pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f to disappear
    Mar 29 08:27:53.508: INFO: Pod pod-projected-secrets-5a3eedcd-291f-42ff-8bdd-1427f1d6de2f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 08:27:53.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8955" for this suite. 03/29/23 08:27:53.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:53.514
Mar 29 08:27:53.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:27:53.515
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:53.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:53.522
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-031482e4-2dd7-4c1f-8fc2-0fb4f26e2558 03/29/23 08:27:53.523
STEP: Creating a pod to test consume configMaps 03/29/23 08:27:53.525
Mar 29 08:27:53.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce" in namespace "configmap-4209" to be "Succeeded or Failed"
Mar 29 08:27:53.530: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.156745ms
Mar 29 08:27:55.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003695975s
Mar 29 08:27:57.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004338447s
STEP: Saw pod success 03/29/23 08:27:57.533
Mar 29 08:27:57.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce" satisfied condition "Succeeded or Failed"
Mar 29 08:27:57.535: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:27:57.537
Mar 29 08:27:57.543: INFO: Waiting for pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce to disappear
Mar 29 08:27:57.544: INFO: Pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:27:57.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4209" for this suite. 03/29/23 08:27:57.546
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":148,"skipped":2579,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:53.514
    Mar 29 08:27:53.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:27:53.515
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:53.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:53.522
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-031482e4-2dd7-4c1f-8fc2-0fb4f26e2558 03/29/23 08:27:53.523
    STEP: Creating a pod to test consume configMaps 03/29/23 08:27:53.525
    Mar 29 08:27:53.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce" in namespace "configmap-4209" to be "Succeeded or Failed"
    Mar 29 08:27:53.530: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.156745ms
    Mar 29 08:27:55.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003695975s
    Mar 29 08:27:57.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004338447s
    STEP: Saw pod success 03/29/23 08:27:57.533
    Mar 29 08:27:57.533: INFO: Pod "pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce" satisfied condition "Succeeded or Failed"
    Mar 29 08:27:57.535: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:27:57.537
    Mar 29 08:27:57.543: INFO: Waiting for pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce to disappear
    Mar 29 08:27:57.544: INFO: Pod pod-configmaps-73db131f-c153-4e74-85f0-7202c3143bce no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:27:57.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4209" for this suite. 03/29/23 08:27:57.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:27:57.549
Mar 29 08:27:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 08:27:57.55
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:57.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:57.556
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 08:28:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4827" for this suite. 03/29/23 08:28:57.565
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":149,"skipped":2589,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.019 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:27:57.549
    Mar 29 08:27:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 08:27:57.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:27:57.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:27:57.556
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 08:28:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4827" for this suite. 03/29/23 08:28:57.565
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:28:57.568
Mar 29 08:28:57.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 08:28:57.569
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:28:57.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:28:57.576
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 08:28:57.582
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:28:57.966
STEP: Deploying the webhook pod 03/29/23 08:28:57.97
STEP: Wait for the deployment to be ready 03/29/23 08:28:57.977
Mar 29 08:28:57.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 08:28:59.985
STEP: Verifying the service has paired with the endpoint 03/29/23 08:28:59.991
Mar 29 08:29:00.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/29/23 08:29:00.993
STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.001
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/29/23 08:29:01.005
STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.01
STEP: Patching a validating webhook configuration's rules to include the create operation 03/29/23 08:29:01.014
STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.018
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:29:01.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2918" for this suite. 03/29/23 08:29:01.023
STEP: Destroying namespace "webhook-2918-markers" for this suite. 03/29/23 08:29:01.025
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":150,"skipped":2593,"failed":0}
------------------------------
â€¢ [3.476 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:28:57.568
    Mar 29 08:28:57.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 08:28:57.569
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:28:57.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:28:57.576
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 08:28:57.582
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:28:57.966
    STEP: Deploying the webhook pod 03/29/23 08:28:57.97
    STEP: Wait for the deployment to be ready 03/29/23 08:28:57.977
    Mar 29 08:28:57.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 08:28:59.985
    STEP: Verifying the service has paired with the endpoint 03/29/23 08:28:59.991
    Mar 29 08:29:00.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/29/23 08:29:00.993
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.001
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/29/23 08:29:01.005
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.01
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/29/23 08:29:01.014
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 08:29:01.018
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:29:01.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2918" for this suite. 03/29/23 08:29:01.023
    STEP: Destroying namespace "webhook-2918-markers" for this suite. 03/29/23 08:29:01.025
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:29:01.045
Mar 29 08:29:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:29:01.046
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:29:01.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:29:01.058
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar 29 08:29:01.066: INFO: Waiting up to 5m0s for pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6" in namespace "svcaccounts-2727" to be "running"
Mar 29 08:29:01.068: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083485ms
Mar 29 08:29:03.071: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004681557s
Mar 29 08:29:03.071: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6" satisfied condition "running"
STEP: reading a file in the container 03/29/23 08:29:03.071
Mar 29 08:29:03.071: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/29/23 08:29:03.153
Mar 29 08:29:03.153: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/29/23 08:29:03.242
Mar 29 08:29:03.242: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 29 08:29:03.325: INFO: Got root ca configmap in namespace "svcaccounts-2727"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 08:29:03.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2727" for this suite. 03/29/23 08:29:03.329
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":151,"skipped":2614,"failed":0}
------------------------------
â€¢ [2.286 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:29:01.045
    Mar 29 08:29:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:29:01.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:29:01.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:29:01.058
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar 29 08:29:01.066: INFO: Waiting up to 5m0s for pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6" in namespace "svcaccounts-2727" to be "running"
    Mar 29 08:29:01.068: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083485ms
    Mar 29 08:29:03.071: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004681557s
    Mar 29 08:29:03.071: INFO: Pod "pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6" satisfied condition "running"
    STEP: reading a file in the container 03/29/23 08:29:03.071
    Mar 29 08:29:03.071: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/29/23 08:29:03.153
    Mar 29 08:29:03.153: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/29/23 08:29:03.242
    Mar 29 08:29:03.242: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2727 pod-service-account-bee0ddf1-e68f-4174-ae87-ebcb719954e6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 29 08:29:03.325: INFO: Got root ca configmap in namespace "svcaccounts-2727"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 08:29:03.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2727" for this suite. 03/29/23 08:29:03.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:29:03.332
Mar 29 08:29:03.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 08:29:03.333
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:29:03.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:29:03.34
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 in namespace container-probe-4238 03/29/23 08:29:03.341
Mar 29 08:29:03.344: INFO: Waiting up to 5m0s for pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2" in namespace "container-probe-4238" to be "not pending"
Mar 29 08:29:03.346: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265181ms
Mar 29 08:29:05.348: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003439925s
Mar 29 08:29:05.348: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2" satisfied condition "not pending"
Mar 29 08:29:05.348: INFO: Started pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 in namespace container-probe-4238
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:29:05.348
Mar 29 08:29:05.349: INFO: Initial restart count of pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 is 0
STEP: deleting the pod 03/29/23 08:33:05.667
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 08:33:05.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4238" for this suite. 03/29/23 08:33:05.678
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":152,"skipped":2641,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.349 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:29:03.332
    Mar 29 08:29:03.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 08:29:03.333
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:29:03.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:29:03.34
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 in namespace container-probe-4238 03/29/23 08:29:03.341
    Mar 29 08:29:03.344: INFO: Waiting up to 5m0s for pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2" in namespace "container-probe-4238" to be "not pending"
    Mar 29 08:29:03.346: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265181ms
    Mar 29 08:29:05.348: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003439925s
    Mar 29 08:29:05.348: INFO: Pod "liveness-32e8a994-496a-4ab5-aa44-87514a9598c2" satisfied condition "not pending"
    Mar 29 08:29:05.348: INFO: Started pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 in namespace container-probe-4238
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:29:05.348
    Mar 29 08:29:05.349: INFO: Initial restart count of pod liveness-32e8a994-496a-4ab5-aa44-87514a9598c2 is 0
    STEP: deleting the pod 03/29/23 08:33:05.667
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 08:33:05.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4238" for this suite. 03/29/23 08:33:05.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:33:05.681
Mar 29 08:33:05.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:33:05.682
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:05.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:05.696
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/29/23 08:33:05.699
STEP: delete the rc 03/29/23 08:33:10.705
STEP: wait for the rc to be deleted 03/29/23 08:33:10.709
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/29/23 08:33:15.712
STEP: Gathering metrics 03/29/23 08:33:45.721
W0329 08:33:45.724514      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 08:33:45.724: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 29 08:33:45.724: INFO: Deleting pod "simpletest.rc-22tfh" in namespace "gc-8763"
Mar 29 08:33:45.730: INFO: Deleting pod "simpletest.rc-2pw27" in namespace "gc-8763"
Mar 29 08:33:45.735: INFO: Deleting pod "simpletest.rc-2qxkj" in namespace "gc-8763"
Mar 29 08:33:45.741: INFO: Deleting pod "simpletest.rc-2rdln" in namespace "gc-8763"
Mar 29 08:33:45.747: INFO: Deleting pod "simpletest.rc-2x245" in namespace "gc-8763"
Mar 29 08:33:45.751: INFO: Deleting pod "simpletest.rc-42xgj" in namespace "gc-8763"
Mar 29 08:33:45.756: INFO: Deleting pod "simpletest.rc-44hdj" in namespace "gc-8763"
Mar 29 08:33:45.765: INFO: Deleting pod "simpletest.rc-47bjc" in namespace "gc-8763"
Mar 29 08:33:45.771: INFO: Deleting pod "simpletest.rc-4rc8p" in namespace "gc-8763"
Mar 29 08:33:45.778: INFO: Deleting pod "simpletest.rc-4vhcv" in namespace "gc-8763"
Mar 29 08:33:45.784: INFO: Deleting pod "simpletest.rc-58x6r" in namespace "gc-8763"
Mar 29 08:33:45.796: INFO: Deleting pod "simpletest.rc-58x94" in namespace "gc-8763"
Mar 29 08:33:45.805: INFO: Deleting pod "simpletest.rc-5xh5q" in namespace "gc-8763"
Mar 29 08:33:45.815: INFO: Deleting pod "simpletest.rc-67c6n" in namespace "gc-8763"
Mar 29 08:33:45.824: INFO: Deleting pod "simpletest.rc-6bhdw" in namespace "gc-8763"
Mar 29 08:33:45.838: INFO: Deleting pod "simpletest.rc-6g8r6" in namespace "gc-8763"
Mar 29 08:33:45.847: INFO: Deleting pod "simpletest.rc-6pf85" in namespace "gc-8763"
Mar 29 08:33:45.854: INFO: Deleting pod "simpletest.rc-6qmhq" in namespace "gc-8763"
Mar 29 08:33:45.860: INFO: Deleting pod "simpletest.rc-6skmb" in namespace "gc-8763"
Mar 29 08:33:45.869: INFO: Deleting pod "simpletest.rc-6vdqz" in namespace "gc-8763"
Mar 29 08:33:45.878: INFO: Deleting pod "simpletest.rc-76fnq" in namespace "gc-8763"
Mar 29 08:33:45.890: INFO: Deleting pod "simpletest.rc-78jnq" in namespace "gc-8763"
Mar 29 08:33:45.901: INFO: Deleting pod "simpletest.rc-8gtgm" in namespace "gc-8763"
Mar 29 08:33:45.936: INFO: Deleting pod "simpletest.rc-8kmv6" in namespace "gc-8763"
Mar 29 08:33:45.954: INFO: Deleting pod "simpletest.rc-8th9d" in namespace "gc-8763"
Mar 29 08:33:45.976: INFO: Deleting pod "simpletest.rc-8tj4z" in namespace "gc-8763"
Mar 29 08:33:45.998: INFO: Deleting pod "simpletest.rc-8whmx" in namespace "gc-8763"
Mar 29 08:33:46.014: INFO: Deleting pod "simpletest.rc-8xfcw" in namespace "gc-8763"
Mar 29 08:33:46.023: INFO: Deleting pod "simpletest.rc-8zdtx" in namespace "gc-8763"
Mar 29 08:33:46.041: INFO: Deleting pod "simpletest.rc-995lv" in namespace "gc-8763"
Mar 29 08:33:46.059: INFO: Deleting pod "simpletest.rc-9pn7j" in namespace "gc-8763"
Mar 29 08:33:46.075: INFO: Deleting pod "simpletest.rc-9vx69" in namespace "gc-8763"
Mar 29 08:33:46.088: INFO: Deleting pod "simpletest.rc-b5pxr" in namespace "gc-8763"
Mar 29 08:33:46.106: INFO: Deleting pod "simpletest.rc-b7pvl" in namespace "gc-8763"
Mar 29 08:33:46.118: INFO: Deleting pod "simpletest.rc-b9hmv" in namespace "gc-8763"
Mar 29 08:33:46.125: INFO: Deleting pod "simpletest.rc-cjjxj" in namespace "gc-8763"
Mar 29 08:33:46.138: INFO: Deleting pod "simpletest.rc-d2h9g" in namespace "gc-8763"
Mar 29 08:33:46.151: INFO: Deleting pod "simpletest.rc-d7xgz" in namespace "gc-8763"
Mar 29 08:33:46.169: INFO: Deleting pod "simpletest.rc-dfrdv" in namespace "gc-8763"
Mar 29 08:33:46.206: INFO: Deleting pod "simpletest.rc-dzjjr" in namespace "gc-8763"
Mar 29 08:33:46.225: INFO: Deleting pod "simpletest.rc-f27sl" in namespace "gc-8763"
Mar 29 08:33:46.237: INFO: Deleting pod "simpletest.rc-f8429" in namespace "gc-8763"
Mar 29 08:33:46.265: INFO: Deleting pod "simpletest.rc-fg4jl" in namespace "gc-8763"
Mar 29 08:33:46.296: INFO: Deleting pod "simpletest.rc-fj88p" in namespace "gc-8763"
Mar 29 08:33:46.317: INFO: Deleting pod "simpletest.rc-fpsjm" in namespace "gc-8763"
Mar 29 08:33:46.330: INFO: Deleting pod "simpletest.rc-ft5jp" in namespace "gc-8763"
Mar 29 08:33:46.341: INFO: Deleting pod "simpletest.rc-g2fhp" in namespace "gc-8763"
Mar 29 08:33:46.364: INFO: Deleting pod "simpletest.rc-gjx8b" in namespace "gc-8763"
Mar 29 08:33:46.400: INFO: Deleting pod "simpletest.rc-gnk8r" in namespace "gc-8763"
Mar 29 08:33:46.427: INFO: Deleting pod "simpletest.rc-gpmsz" in namespace "gc-8763"
Mar 29 08:33:46.442: INFO: Deleting pod "simpletest.rc-gpsg9" in namespace "gc-8763"
Mar 29 08:33:46.454: INFO: Deleting pod "simpletest.rc-hpr5d" in namespace "gc-8763"
Mar 29 08:33:46.465: INFO: Deleting pod "simpletest.rc-hrcpx" in namespace "gc-8763"
Mar 29 08:33:46.481: INFO: Deleting pod "simpletest.rc-j2w9p" in namespace "gc-8763"
Mar 29 08:33:46.502: INFO: Deleting pod "simpletest.rc-j5cvc" in namespace "gc-8763"
Mar 29 08:33:46.514: INFO: Deleting pod "simpletest.rc-jdzw6" in namespace "gc-8763"
Mar 29 08:33:46.526: INFO: Deleting pod "simpletest.rc-k2dkf" in namespace "gc-8763"
Mar 29 08:33:46.536: INFO: Deleting pod "simpletest.rc-k8q8n" in namespace "gc-8763"
Mar 29 08:33:46.556: INFO: Deleting pod "simpletest.rc-kj4v9" in namespace "gc-8763"
Mar 29 08:33:46.578: INFO: Deleting pod "simpletest.rc-kjb55" in namespace "gc-8763"
Mar 29 08:33:46.597: INFO: Deleting pod "simpletest.rc-ksqkq" in namespace "gc-8763"
Mar 29 08:33:46.605: INFO: Deleting pod "simpletest.rc-llnft" in namespace "gc-8763"
Mar 29 08:33:46.622: INFO: Deleting pod "simpletest.rc-mb99f" in namespace "gc-8763"
Mar 29 08:33:46.637: INFO: Deleting pod "simpletest.rc-n5qjl" in namespace "gc-8763"
Mar 29 08:33:46.652: INFO: Deleting pod "simpletest.rc-n6hwk" in namespace "gc-8763"
Mar 29 08:33:46.663: INFO: Deleting pod "simpletest.rc-n78ms" in namespace "gc-8763"
Mar 29 08:33:46.683: INFO: Deleting pod "simpletest.rc-nd7rd" in namespace "gc-8763"
Mar 29 08:33:46.697: INFO: Deleting pod "simpletest.rc-nzz7w" in namespace "gc-8763"
Mar 29 08:33:46.742: INFO: Deleting pod "simpletest.rc-pp864" in namespace "gc-8763"
Mar 29 08:33:46.781: INFO: Deleting pod "simpletest.rc-psfhw" in namespace "gc-8763"
Mar 29 08:33:46.831: INFO: Deleting pod "simpletest.rc-psm6n" in namespace "gc-8763"
Mar 29 08:33:46.894: INFO: Deleting pod "simpletest.rc-q4lfd" in namespace "gc-8763"
Mar 29 08:33:46.961: INFO: Deleting pod "simpletest.rc-q75qn" in namespace "gc-8763"
Mar 29 08:33:46.984: INFO: Deleting pod "simpletest.rc-qrh66" in namespace "gc-8763"
Mar 29 08:33:47.025: INFO: Deleting pod "simpletest.rc-qxnrb" in namespace "gc-8763"
Mar 29 08:33:47.084: INFO: Deleting pod "simpletest.rc-rg4hd" in namespace "gc-8763"
Mar 29 08:33:47.127: INFO: Deleting pod "simpletest.rc-rjmb7" in namespace "gc-8763"
Mar 29 08:33:47.176: INFO: Deleting pod "simpletest.rc-rvff7" in namespace "gc-8763"
Mar 29 08:33:47.224: INFO: Deleting pod "simpletest.rc-sc872" in namespace "gc-8763"
Mar 29 08:33:47.273: INFO: Deleting pod "simpletest.rc-sq6dh" in namespace "gc-8763"
Mar 29 08:33:47.324: INFO: Deleting pod "simpletest.rc-t5l2w" in namespace "gc-8763"
Mar 29 08:33:47.387: INFO: Deleting pod "simpletest.rc-t7k9r" in namespace "gc-8763"
Mar 29 08:33:47.420: INFO: Deleting pod "simpletest.rc-tg2x5" in namespace "gc-8763"
Mar 29 08:33:47.474: INFO: Deleting pod "simpletest.rc-th98f" in namespace "gc-8763"
Mar 29 08:33:47.526: INFO: Deleting pod "simpletest.rc-tn54t" in namespace "gc-8763"
Mar 29 08:33:47.573: INFO: Deleting pod "simpletest.rc-v2w5n" in namespace "gc-8763"
Mar 29 08:33:47.626: INFO: Deleting pod "simpletest.rc-v7q47" in namespace "gc-8763"
Mar 29 08:33:47.674: INFO: Deleting pod "simpletest.rc-v8w99" in namespace "gc-8763"
Mar 29 08:33:47.730: INFO: Deleting pod "simpletest.rc-vmgrq" in namespace "gc-8763"
Mar 29 08:33:47.774: INFO: Deleting pod "simpletest.rc-vnhtt" in namespace "gc-8763"
Mar 29 08:33:47.828: INFO: Deleting pod "simpletest.rc-vx7js" in namespace "gc-8763"
Mar 29 08:33:47.881: INFO: Deleting pod "simpletest.rc-wh6r4" in namespace "gc-8763"
Mar 29 08:33:47.923: INFO: Deleting pod "simpletest.rc-wq44z" in namespace "gc-8763"
Mar 29 08:33:47.979: INFO: Deleting pod "simpletest.rc-wtt6g" in namespace "gc-8763"
Mar 29 08:33:48.020: INFO: Deleting pod "simpletest.rc-x7qqn" in namespace "gc-8763"
Mar 29 08:33:48.072: INFO: Deleting pod "simpletest.rc-xl57w" in namespace "gc-8763"
Mar 29 08:33:48.121: INFO: Deleting pod "simpletest.rc-xt56h" in namespace "gc-8763"
Mar 29 08:33:48.171: INFO: Deleting pod "simpletest.rc-zb57k" in namespace "gc-8763"
Mar 29 08:33:48.223: INFO: Deleting pod "simpletest.rc-zw67h" in namespace "gc-8763"
Mar 29 08:33:48.270: INFO: Deleting pod "simpletest.rc-zz7xr" in namespace "gc-8763"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:33:48.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8763" for this suite. 03/29/23 08:33:48.366
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":153,"skipped":2646,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.737 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:33:05.681
    Mar 29 08:33:05.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:33:05.682
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:05.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:05.696
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/29/23 08:33:05.699
    STEP: delete the rc 03/29/23 08:33:10.705
    STEP: wait for the rc to be deleted 03/29/23 08:33:10.709
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/29/23 08:33:15.712
    STEP: Gathering metrics 03/29/23 08:33:45.721
    W0329 08:33:45.724514      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 08:33:45.724: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 29 08:33:45.724: INFO: Deleting pod "simpletest.rc-22tfh" in namespace "gc-8763"
    Mar 29 08:33:45.730: INFO: Deleting pod "simpletest.rc-2pw27" in namespace "gc-8763"
    Mar 29 08:33:45.735: INFO: Deleting pod "simpletest.rc-2qxkj" in namespace "gc-8763"
    Mar 29 08:33:45.741: INFO: Deleting pod "simpletest.rc-2rdln" in namespace "gc-8763"
    Mar 29 08:33:45.747: INFO: Deleting pod "simpletest.rc-2x245" in namespace "gc-8763"
    Mar 29 08:33:45.751: INFO: Deleting pod "simpletest.rc-42xgj" in namespace "gc-8763"
    Mar 29 08:33:45.756: INFO: Deleting pod "simpletest.rc-44hdj" in namespace "gc-8763"
    Mar 29 08:33:45.765: INFO: Deleting pod "simpletest.rc-47bjc" in namespace "gc-8763"
    Mar 29 08:33:45.771: INFO: Deleting pod "simpletest.rc-4rc8p" in namespace "gc-8763"
    Mar 29 08:33:45.778: INFO: Deleting pod "simpletest.rc-4vhcv" in namespace "gc-8763"
    Mar 29 08:33:45.784: INFO: Deleting pod "simpletest.rc-58x6r" in namespace "gc-8763"
    Mar 29 08:33:45.796: INFO: Deleting pod "simpletest.rc-58x94" in namespace "gc-8763"
    Mar 29 08:33:45.805: INFO: Deleting pod "simpletest.rc-5xh5q" in namespace "gc-8763"
    Mar 29 08:33:45.815: INFO: Deleting pod "simpletest.rc-67c6n" in namespace "gc-8763"
    Mar 29 08:33:45.824: INFO: Deleting pod "simpletest.rc-6bhdw" in namespace "gc-8763"
    Mar 29 08:33:45.838: INFO: Deleting pod "simpletest.rc-6g8r6" in namespace "gc-8763"
    Mar 29 08:33:45.847: INFO: Deleting pod "simpletest.rc-6pf85" in namespace "gc-8763"
    Mar 29 08:33:45.854: INFO: Deleting pod "simpletest.rc-6qmhq" in namespace "gc-8763"
    Mar 29 08:33:45.860: INFO: Deleting pod "simpletest.rc-6skmb" in namespace "gc-8763"
    Mar 29 08:33:45.869: INFO: Deleting pod "simpletest.rc-6vdqz" in namespace "gc-8763"
    Mar 29 08:33:45.878: INFO: Deleting pod "simpletest.rc-76fnq" in namespace "gc-8763"
    Mar 29 08:33:45.890: INFO: Deleting pod "simpletest.rc-78jnq" in namespace "gc-8763"
    Mar 29 08:33:45.901: INFO: Deleting pod "simpletest.rc-8gtgm" in namespace "gc-8763"
    Mar 29 08:33:45.936: INFO: Deleting pod "simpletest.rc-8kmv6" in namespace "gc-8763"
    Mar 29 08:33:45.954: INFO: Deleting pod "simpletest.rc-8th9d" in namespace "gc-8763"
    Mar 29 08:33:45.976: INFO: Deleting pod "simpletest.rc-8tj4z" in namespace "gc-8763"
    Mar 29 08:33:45.998: INFO: Deleting pod "simpletest.rc-8whmx" in namespace "gc-8763"
    Mar 29 08:33:46.014: INFO: Deleting pod "simpletest.rc-8xfcw" in namespace "gc-8763"
    Mar 29 08:33:46.023: INFO: Deleting pod "simpletest.rc-8zdtx" in namespace "gc-8763"
    Mar 29 08:33:46.041: INFO: Deleting pod "simpletest.rc-995lv" in namespace "gc-8763"
    Mar 29 08:33:46.059: INFO: Deleting pod "simpletest.rc-9pn7j" in namespace "gc-8763"
    Mar 29 08:33:46.075: INFO: Deleting pod "simpletest.rc-9vx69" in namespace "gc-8763"
    Mar 29 08:33:46.088: INFO: Deleting pod "simpletest.rc-b5pxr" in namespace "gc-8763"
    Mar 29 08:33:46.106: INFO: Deleting pod "simpletest.rc-b7pvl" in namespace "gc-8763"
    Mar 29 08:33:46.118: INFO: Deleting pod "simpletest.rc-b9hmv" in namespace "gc-8763"
    Mar 29 08:33:46.125: INFO: Deleting pod "simpletest.rc-cjjxj" in namespace "gc-8763"
    Mar 29 08:33:46.138: INFO: Deleting pod "simpletest.rc-d2h9g" in namespace "gc-8763"
    Mar 29 08:33:46.151: INFO: Deleting pod "simpletest.rc-d7xgz" in namespace "gc-8763"
    Mar 29 08:33:46.169: INFO: Deleting pod "simpletest.rc-dfrdv" in namespace "gc-8763"
    Mar 29 08:33:46.206: INFO: Deleting pod "simpletest.rc-dzjjr" in namespace "gc-8763"
    Mar 29 08:33:46.225: INFO: Deleting pod "simpletest.rc-f27sl" in namespace "gc-8763"
    Mar 29 08:33:46.237: INFO: Deleting pod "simpletest.rc-f8429" in namespace "gc-8763"
    Mar 29 08:33:46.265: INFO: Deleting pod "simpletest.rc-fg4jl" in namespace "gc-8763"
    Mar 29 08:33:46.296: INFO: Deleting pod "simpletest.rc-fj88p" in namespace "gc-8763"
    Mar 29 08:33:46.317: INFO: Deleting pod "simpletest.rc-fpsjm" in namespace "gc-8763"
    Mar 29 08:33:46.330: INFO: Deleting pod "simpletest.rc-ft5jp" in namespace "gc-8763"
    Mar 29 08:33:46.341: INFO: Deleting pod "simpletest.rc-g2fhp" in namespace "gc-8763"
    Mar 29 08:33:46.364: INFO: Deleting pod "simpletest.rc-gjx8b" in namespace "gc-8763"
    Mar 29 08:33:46.400: INFO: Deleting pod "simpletest.rc-gnk8r" in namespace "gc-8763"
    Mar 29 08:33:46.427: INFO: Deleting pod "simpletest.rc-gpmsz" in namespace "gc-8763"
    Mar 29 08:33:46.442: INFO: Deleting pod "simpletest.rc-gpsg9" in namespace "gc-8763"
    Mar 29 08:33:46.454: INFO: Deleting pod "simpletest.rc-hpr5d" in namespace "gc-8763"
    Mar 29 08:33:46.465: INFO: Deleting pod "simpletest.rc-hrcpx" in namespace "gc-8763"
    Mar 29 08:33:46.481: INFO: Deleting pod "simpletest.rc-j2w9p" in namespace "gc-8763"
    Mar 29 08:33:46.502: INFO: Deleting pod "simpletest.rc-j5cvc" in namespace "gc-8763"
    Mar 29 08:33:46.514: INFO: Deleting pod "simpletest.rc-jdzw6" in namespace "gc-8763"
    Mar 29 08:33:46.526: INFO: Deleting pod "simpletest.rc-k2dkf" in namespace "gc-8763"
    Mar 29 08:33:46.536: INFO: Deleting pod "simpletest.rc-k8q8n" in namespace "gc-8763"
    Mar 29 08:33:46.556: INFO: Deleting pod "simpletest.rc-kj4v9" in namespace "gc-8763"
    Mar 29 08:33:46.578: INFO: Deleting pod "simpletest.rc-kjb55" in namespace "gc-8763"
    Mar 29 08:33:46.597: INFO: Deleting pod "simpletest.rc-ksqkq" in namespace "gc-8763"
    Mar 29 08:33:46.605: INFO: Deleting pod "simpletest.rc-llnft" in namespace "gc-8763"
    Mar 29 08:33:46.622: INFO: Deleting pod "simpletest.rc-mb99f" in namespace "gc-8763"
    Mar 29 08:33:46.637: INFO: Deleting pod "simpletest.rc-n5qjl" in namespace "gc-8763"
    Mar 29 08:33:46.652: INFO: Deleting pod "simpletest.rc-n6hwk" in namespace "gc-8763"
    Mar 29 08:33:46.663: INFO: Deleting pod "simpletest.rc-n78ms" in namespace "gc-8763"
    Mar 29 08:33:46.683: INFO: Deleting pod "simpletest.rc-nd7rd" in namespace "gc-8763"
    Mar 29 08:33:46.697: INFO: Deleting pod "simpletest.rc-nzz7w" in namespace "gc-8763"
    Mar 29 08:33:46.742: INFO: Deleting pod "simpletest.rc-pp864" in namespace "gc-8763"
    Mar 29 08:33:46.781: INFO: Deleting pod "simpletest.rc-psfhw" in namespace "gc-8763"
    Mar 29 08:33:46.831: INFO: Deleting pod "simpletest.rc-psm6n" in namespace "gc-8763"
    Mar 29 08:33:46.894: INFO: Deleting pod "simpletest.rc-q4lfd" in namespace "gc-8763"
    Mar 29 08:33:46.961: INFO: Deleting pod "simpletest.rc-q75qn" in namespace "gc-8763"
    Mar 29 08:33:46.984: INFO: Deleting pod "simpletest.rc-qrh66" in namespace "gc-8763"
    Mar 29 08:33:47.025: INFO: Deleting pod "simpletest.rc-qxnrb" in namespace "gc-8763"
    Mar 29 08:33:47.084: INFO: Deleting pod "simpletest.rc-rg4hd" in namespace "gc-8763"
    Mar 29 08:33:47.127: INFO: Deleting pod "simpletest.rc-rjmb7" in namespace "gc-8763"
    Mar 29 08:33:47.176: INFO: Deleting pod "simpletest.rc-rvff7" in namespace "gc-8763"
    Mar 29 08:33:47.224: INFO: Deleting pod "simpletest.rc-sc872" in namespace "gc-8763"
    Mar 29 08:33:47.273: INFO: Deleting pod "simpletest.rc-sq6dh" in namespace "gc-8763"
    Mar 29 08:33:47.324: INFO: Deleting pod "simpletest.rc-t5l2w" in namespace "gc-8763"
    Mar 29 08:33:47.387: INFO: Deleting pod "simpletest.rc-t7k9r" in namespace "gc-8763"
    Mar 29 08:33:47.420: INFO: Deleting pod "simpletest.rc-tg2x5" in namespace "gc-8763"
    Mar 29 08:33:47.474: INFO: Deleting pod "simpletest.rc-th98f" in namespace "gc-8763"
    Mar 29 08:33:47.526: INFO: Deleting pod "simpletest.rc-tn54t" in namespace "gc-8763"
    Mar 29 08:33:47.573: INFO: Deleting pod "simpletest.rc-v2w5n" in namespace "gc-8763"
    Mar 29 08:33:47.626: INFO: Deleting pod "simpletest.rc-v7q47" in namespace "gc-8763"
    Mar 29 08:33:47.674: INFO: Deleting pod "simpletest.rc-v8w99" in namespace "gc-8763"
    Mar 29 08:33:47.730: INFO: Deleting pod "simpletest.rc-vmgrq" in namespace "gc-8763"
    Mar 29 08:33:47.774: INFO: Deleting pod "simpletest.rc-vnhtt" in namespace "gc-8763"
    Mar 29 08:33:47.828: INFO: Deleting pod "simpletest.rc-vx7js" in namespace "gc-8763"
    Mar 29 08:33:47.881: INFO: Deleting pod "simpletest.rc-wh6r4" in namespace "gc-8763"
    Mar 29 08:33:47.923: INFO: Deleting pod "simpletest.rc-wq44z" in namespace "gc-8763"
    Mar 29 08:33:47.979: INFO: Deleting pod "simpletest.rc-wtt6g" in namespace "gc-8763"
    Mar 29 08:33:48.020: INFO: Deleting pod "simpletest.rc-x7qqn" in namespace "gc-8763"
    Mar 29 08:33:48.072: INFO: Deleting pod "simpletest.rc-xl57w" in namespace "gc-8763"
    Mar 29 08:33:48.121: INFO: Deleting pod "simpletest.rc-xt56h" in namespace "gc-8763"
    Mar 29 08:33:48.171: INFO: Deleting pod "simpletest.rc-zb57k" in namespace "gc-8763"
    Mar 29 08:33:48.223: INFO: Deleting pod "simpletest.rc-zw67h" in namespace "gc-8763"
    Mar 29 08:33:48.270: INFO: Deleting pod "simpletest.rc-zz7xr" in namespace "gc-8763"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:33:48.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8763" for this suite. 03/29/23 08:33:48.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:33:48.419
Mar 29 08:33:48.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:33:48.42
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:48.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:48.434
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 29 08:33:48.448: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 29 08:33:53.450: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 08:33:53.45
STEP: Scaling up "test-rs" replicaset  03/29/23 08:33:53.45
Mar 29 08:33:53.454: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/29/23 08:33:53.454
W0329 08:33:53.458097      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 29 08:33:53.459: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
Mar 29 08:33:53.466: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
Mar 29 08:33:53.473: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
Mar 29 08:33:53.476: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
Mar 29 08:33:54.369: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 2, AvailableReplicas 2
Mar 29 08:33:55.015: INFO: observed Replicaset test-rs in namespace replicaset-5620 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:33:55.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5620" for this suite. 03/29/23 08:33:55.017
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":154,"skipped":2658,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.600 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:33:48.419
    Mar 29 08:33:48.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:33:48.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:48.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:48.434
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 29 08:33:48.448: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 29 08:33:53.450: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 08:33:53.45
    STEP: Scaling up "test-rs" replicaset  03/29/23 08:33:53.45
    Mar 29 08:33:53.454: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/29/23 08:33:53.454
    W0329 08:33:53.458097      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 29 08:33:53.459: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
    Mar 29 08:33:53.466: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
    Mar 29 08:33:53.473: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
    Mar 29 08:33:53.476: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 1, AvailableReplicas 1
    Mar 29 08:33:54.369: INFO: observed ReplicaSet test-rs in namespace replicaset-5620 with ReadyReplicas 2, AvailableReplicas 2
    Mar 29 08:33:55.015: INFO: observed Replicaset test-rs in namespace replicaset-5620 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:33:55.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5620" for this suite. 03/29/23 08:33:55.017
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:33:55.02
Mar 29 08:33:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:33:55.02
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:55.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:55.027
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/29/23 08:33:55.029
STEP: delete the rc 03/29/23 08:34:00.034
STEP: wait for the rc to be deleted 03/29/23 08:34:00.038
Mar 29 08:34:01.051: INFO: 80 pods remaining
Mar 29 08:34:01.051: INFO: 80 pods has nil DeletionTimestamp
Mar 29 08:34:01.051: INFO: 
Mar 29 08:34:02.054: INFO: 71 pods remaining
Mar 29 08:34:02.054: INFO: 71 pods has nil DeletionTimestamp
Mar 29 08:34:02.054: INFO: 
Mar 29 08:34:03.044: INFO: 60 pods remaining
Mar 29 08:34:03.044: INFO: 60 pods has nil DeletionTimestamp
Mar 29 08:34:03.044: INFO: 
Mar 29 08:34:04.046: INFO: 40 pods remaining
Mar 29 08:34:04.046: INFO: 40 pods has nil DeletionTimestamp
Mar 29 08:34:04.046: INFO: 
Mar 29 08:34:05.044: INFO: 31 pods remaining
Mar 29 08:34:05.044: INFO: 31 pods has nil DeletionTimestamp
Mar 29 08:34:05.044: INFO: 
Mar 29 08:34:06.042: INFO: 20 pods remaining
Mar 29 08:34:06.042: INFO: 20 pods has nil DeletionTimestamp
Mar 29 08:34:06.042: INFO: 
STEP: Gathering metrics 03/29/23 08:34:07.042
W0329 08:34:07.045004      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 08:34:07.045: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:34:07.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4364" for this suite. 03/29/23 08:34:07.046
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":155,"skipped":2661,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.030 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:33:55.02
    Mar 29 08:33:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:33:55.02
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:33:55.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:33:55.027
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/29/23 08:33:55.029
    STEP: delete the rc 03/29/23 08:34:00.034
    STEP: wait for the rc to be deleted 03/29/23 08:34:00.038
    Mar 29 08:34:01.051: INFO: 80 pods remaining
    Mar 29 08:34:01.051: INFO: 80 pods has nil DeletionTimestamp
    Mar 29 08:34:01.051: INFO: 
    Mar 29 08:34:02.054: INFO: 71 pods remaining
    Mar 29 08:34:02.054: INFO: 71 pods has nil DeletionTimestamp
    Mar 29 08:34:02.054: INFO: 
    Mar 29 08:34:03.044: INFO: 60 pods remaining
    Mar 29 08:34:03.044: INFO: 60 pods has nil DeletionTimestamp
    Mar 29 08:34:03.044: INFO: 
    Mar 29 08:34:04.046: INFO: 40 pods remaining
    Mar 29 08:34:04.046: INFO: 40 pods has nil DeletionTimestamp
    Mar 29 08:34:04.046: INFO: 
    Mar 29 08:34:05.044: INFO: 31 pods remaining
    Mar 29 08:34:05.044: INFO: 31 pods has nil DeletionTimestamp
    Mar 29 08:34:05.044: INFO: 
    Mar 29 08:34:06.042: INFO: 20 pods remaining
    Mar 29 08:34:06.042: INFO: 20 pods has nil DeletionTimestamp
    Mar 29 08:34:06.042: INFO: 
    STEP: Gathering metrics 03/29/23 08:34:07.042
    W0329 08:34:07.045004      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 08:34:07.045: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:34:07.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4364" for this suite. 03/29/23 08:34:07.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:07.05
Mar 29 08:34:07.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:34:07.051
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:07.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:07.059
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-06f1cfb9-5cb9-40ed-9323-b230588c1ee0 03/29/23 08:34:07.06
STEP: Creating a pod to test consume secrets 03/29/23 08:34:07.062
Mar 29 08:34:07.066: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542" in namespace "projected-5981" to be "Succeeded or Failed"
Mar 29 08:34:07.068: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.168777ms
Mar 29 08:34:09.069: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002869023s
Mar 29 08:34:11.071: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004323026s
Mar 29 08:34:13.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003999803s
Mar 29 08:34:15.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.003244095s
STEP: Saw pod success 03/29/23 08:34:15.07
Mar 29 08:34:15.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542" satisfied condition "Succeeded or Failed"
Mar 29 08:34:15.071: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:34:15.08
Mar 29 08:34:15.086: INFO: Waiting for pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 to disappear
Mar 29 08:34:15.087: INFO: Pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 08:34:15.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5981" for this suite. 03/29/23 08:34:15.089
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":156,"skipped":2676,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.042 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:07.05
    Mar 29 08:34:07.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:34:07.051
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:07.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:07.059
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-06f1cfb9-5cb9-40ed-9323-b230588c1ee0 03/29/23 08:34:07.06
    STEP: Creating a pod to test consume secrets 03/29/23 08:34:07.062
    Mar 29 08:34:07.066: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542" in namespace "projected-5981" to be "Succeeded or Failed"
    Mar 29 08:34:07.068: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.168777ms
    Mar 29 08:34:09.069: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002869023s
    Mar 29 08:34:11.071: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004323026s
    Mar 29 08:34:13.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003999803s
    Mar 29 08:34:15.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.003244095s
    STEP: Saw pod success 03/29/23 08:34:15.07
    Mar 29 08:34:15.070: INFO: Pod "pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542" satisfied condition "Succeeded or Failed"
    Mar 29 08:34:15.071: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:34:15.08
    Mar 29 08:34:15.086: INFO: Waiting for pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 to disappear
    Mar 29 08:34:15.087: INFO: Pod pod-projected-secrets-27239256-fd39-4b07-b039-669db39e6542 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 08:34:15.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5981" for this suite. 03/29/23 08:34:15.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:15.093
Mar 29 08:34:15.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 08:34:15.094
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:15.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:15.1
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/29/23 08:34:15.102
STEP: Ensure pods equal to paralellism count is attached to the job 03/29/23 08:34:15.105
STEP: patching /status 03/29/23 08:34:17.107
STEP: updating /status 03/29/23 08:34:17.112
STEP: get /status 03/29/23 08:34:17.131
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 08:34:17.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9573" for this suite. 03/29/23 08:34:17.134
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":157,"skipped":2684,"failed":0}
------------------------------
â€¢ [2.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:15.093
    Mar 29 08:34:15.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 08:34:15.094
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:15.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:15.1
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/29/23 08:34:15.102
    STEP: Ensure pods equal to paralellism count is attached to the job 03/29/23 08:34:15.105
    STEP: patching /status 03/29/23 08:34:17.107
    STEP: updating /status 03/29/23 08:34:17.112
    STEP: get /status 03/29/23 08:34:17.131
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 08:34:17.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9573" for this suite. 03/29/23 08:34:17.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:17.138
Mar 29 08:34:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename subpath 03/29/23 08:34:17.138
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:17.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:17.146
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/29/23 08:34:17.148
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-jm6x 03/29/23 08:34:17.152
STEP: Creating a pod to test atomic-volume-subpath 03/29/23 08:34:17.152
Mar 29 08:34:17.156: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jm6x" in namespace "subpath-816" to be "Succeeded or Failed"
Mar 29 08:34:17.157: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.262974ms
Mar 29 08:34:19.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 2.003876748s
Mar 29 08:34:21.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.003847648s
Mar 29 08:34:23.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 6.003335293s
Mar 29 08:34:25.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 8.003447652s
Mar 29 08:34:27.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.004749424s
Mar 29 08:34:29.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 12.00389221s
Mar 29 08:34:31.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 14.004371029s
Mar 29 08:34:33.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 16.003718588s
Mar 29 08:34:35.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 18.004990838s
Mar 29 08:34:37.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 20.004887236s
Mar 29 08:34:39.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=false. Elapsed: 22.004044499s
Mar 29 08:34:41.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003256386s
STEP: Saw pod success 03/29/23 08:34:41.159
Mar 29 08:34:41.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x" satisfied condition "Succeeded or Failed"
Mar 29 08:34:41.161: INFO: Trying to get logs from node 10.146.0.116 pod pod-subpath-test-downwardapi-jm6x container test-container-subpath-downwardapi-jm6x: <nil>
STEP: delete the pod 03/29/23 08:34:41.164
Mar 29 08:34:41.170: INFO: Waiting for pod pod-subpath-test-downwardapi-jm6x to disappear
Mar 29 08:34:41.171: INFO: Pod pod-subpath-test-downwardapi-jm6x no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jm6x 03/29/23 08:34:41.171
Mar 29 08:34:41.171: INFO: Deleting pod "pod-subpath-test-downwardapi-jm6x" in namespace "subpath-816"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 29 08:34:41.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-816" for this suite. 03/29/23 08:34:41.174
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":158,"skipped":2707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.038 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:17.138
    Mar 29 08:34:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename subpath 03/29/23 08:34:17.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:17.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:17.146
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/29/23 08:34:17.148
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-jm6x 03/29/23 08:34:17.152
    STEP: Creating a pod to test atomic-volume-subpath 03/29/23 08:34:17.152
    Mar 29 08:34:17.156: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jm6x" in namespace "subpath-816" to be "Succeeded or Failed"
    Mar 29 08:34:17.157: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.262974ms
    Mar 29 08:34:19.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 2.003876748s
    Mar 29 08:34:21.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.003847648s
    Mar 29 08:34:23.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 6.003335293s
    Mar 29 08:34:25.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 8.003447652s
    Mar 29 08:34:27.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.004749424s
    Mar 29 08:34:29.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 12.00389221s
    Mar 29 08:34:31.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 14.004371029s
    Mar 29 08:34:33.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 16.003718588s
    Mar 29 08:34:35.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 18.004990838s
    Mar 29 08:34:37.161: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=true. Elapsed: 20.004887236s
    Mar 29 08:34:39.160: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Running", Reason="", readiness=false. Elapsed: 22.004044499s
    Mar 29 08:34:41.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003256386s
    STEP: Saw pod success 03/29/23 08:34:41.159
    Mar 29 08:34:41.159: INFO: Pod "pod-subpath-test-downwardapi-jm6x" satisfied condition "Succeeded or Failed"
    Mar 29 08:34:41.161: INFO: Trying to get logs from node 10.146.0.116 pod pod-subpath-test-downwardapi-jm6x container test-container-subpath-downwardapi-jm6x: <nil>
    STEP: delete the pod 03/29/23 08:34:41.164
    Mar 29 08:34:41.170: INFO: Waiting for pod pod-subpath-test-downwardapi-jm6x to disappear
    Mar 29 08:34:41.171: INFO: Pod pod-subpath-test-downwardapi-jm6x no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-jm6x 03/29/23 08:34:41.171
    Mar 29 08:34:41.171: INFO: Deleting pod "pod-subpath-test-downwardapi-jm6x" in namespace "subpath-816"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 29 08:34:41.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-816" for this suite. 03/29/23 08:34:41.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:41.177
Mar 29 08:34:41.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:34:41.177
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:41.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:41.186
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:34:41.188
Mar 29 08:34:41.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7" in namespace "projected-9348" to be "Succeeded or Failed"
Mar 29 08:34:41.193: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.202847ms
Mar 29 08:34:43.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004295712s
Mar 29 08:34:45.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003864581s
STEP: Saw pod success 03/29/23 08:34:45.196
Mar 29 08:34:45.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7" satisfied condition "Succeeded or Failed"
Mar 29 08:34:45.197: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 container client-container: <nil>
STEP: delete the pod 03/29/23 08:34:45.201
Mar 29 08:34:45.207: INFO: Waiting for pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 to disappear
Mar 29 08:34:45.210: INFO: Pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:34:45.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9348" for this suite. 03/29/23 08:34:45.212
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":159,"skipped":2724,"failed":0}
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:41.177
    Mar 29 08:34:41.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:34:41.177
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:41.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:41.186
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:34:41.188
    Mar 29 08:34:41.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7" in namespace "projected-9348" to be "Succeeded or Failed"
    Mar 29 08:34:41.193: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.202847ms
    Mar 29 08:34:43.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004295712s
    Mar 29 08:34:45.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003864581s
    STEP: Saw pod success 03/29/23 08:34:45.196
    Mar 29 08:34:45.196: INFO: Pod "downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7" satisfied condition "Succeeded or Failed"
    Mar 29 08:34:45.197: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:34:45.201
    Mar 29 08:34:45.207: INFO: Waiting for pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 to disappear
    Mar 29 08:34:45.210: INFO: Pod downwardapi-volume-541dbcd9-c07a-47de-993d-5585e3c63fa7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:34:45.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9348" for this suite. 03/29/23 08:34:45.212
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:45.218
Mar 29 08:34:45.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:34:45.218
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:45.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:45.229
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/29/23 08:34:45.232
Mar 29 08:34:45.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 create -f -'
Mar 29 08:34:45.373: INFO: stderr: ""
Mar 29 08:34:45.373: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:45.373
Mar 29 08:34:45.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:34:45.418: INFO: stderr: ""
Mar 29 08:34:45.419: INFO: stdout: "update-demo-nautilus-6rwxn update-demo-nautilus-f22gr "
Mar 29 08:34:45.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:45.460: INFO: stderr: ""
Mar 29 08:34:45.460: INFO: stdout: ""
Mar 29 08:34:45.460: INFO: update-demo-nautilus-6rwxn is created but not running
Mar 29 08:34:50.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:34:50.504: INFO: stderr: ""
Mar 29 08:34:50.504: INFO: stdout: "update-demo-nautilus-6rwxn update-demo-nautilus-f22gr "
Mar 29 08:34:50.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:50.545: INFO: stderr: ""
Mar 29 08:34:50.545: INFO: stdout: "true"
Mar 29 08:34:50.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:34:50.585: INFO: stderr: ""
Mar 29 08:34:50.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:34:50.585: INFO: validating pod update-demo-nautilus-6rwxn
Mar 29 08:34:50.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:34:50.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:34:50.587: INFO: update-demo-nautilus-6rwxn is verified up and running
Mar 29 08:34:50.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-f22gr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:50.627: INFO: stderr: ""
Mar 29 08:34:50.627: INFO: stdout: "true"
Mar 29 08:34:50.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-f22gr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:34:50.667: INFO: stderr: ""
Mar 29 08:34:50.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:34:50.667: INFO: validating pod update-demo-nautilus-f22gr
Mar 29 08:34:50.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:34:50.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:34:50.670: INFO: update-demo-nautilus-f22gr is verified up and running
STEP: scaling down the replication controller 03/29/23 08:34:50.67
Mar 29 08:34:50.671: INFO: scanned /root for discovery docs: <nil>
Mar 29 08:34:50.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 29 08:34:51.722: INFO: stderr: ""
Mar 29 08:34:51.722: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:51.722
Mar 29 08:34:51.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:34:51.765: INFO: stderr: ""
Mar 29 08:34:51.765: INFO: stdout: "update-demo-nautilus-6rwxn "
Mar 29 08:34:51.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:51.805: INFO: stderr: ""
Mar 29 08:34:51.805: INFO: stdout: "true"
Mar 29 08:34:51.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:34:51.848: INFO: stderr: ""
Mar 29 08:34:51.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:34:51.848: INFO: validating pod update-demo-nautilus-6rwxn
Mar 29 08:34:51.849: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:34:51.849: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:34:51.849: INFO: update-demo-nautilus-6rwxn is verified up and running
STEP: scaling up the replication controller 03/29/23 08:34:51.849
Mar 29 08:34:51.850: INFO: scanned /root for discovery docs: <nil>
Mar 29 08:34:51.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 29 08:34:52.901: INFO: stderr: ""
Mar 29 08:34:52.901: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:52.901
Mar 29 08:34:52.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 29 08:34:52.944: INFO: stderr: ""
Mar 29 08:34:52.944: INFO: stdout: "update-demo-nautilus-478gl update-demo-nautilus-6rwxn "
Mar 29 08:34:52.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-478gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:52.985: INFO: stderr: ""
Mar 29 08:34:52.985: INFO: stdout: "true"
Mar 29 08:34:52.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-478gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:34:53.026: INFO: stderr: ""
Mar 29 08:34:53.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:34:53.026: INFO: validating pod update-demo-nautilus-478gl
Mar 29 08:34:53.028: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:34:53.028: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:34:53.028: INFO: update-demo-nautilus-478gl is verified up and running
Mar 29 08:34:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 29 08:34:53.069: INFO: stderr: ""
Mar 29 08:34:53.069: INFO: stdout: "true"
Mar 29 08:34:53.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 29 08:34:53.111: INFO: stderr: ""
Mar 29 08:34:53.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 29 08:34:53.111: INFO: validating pod update-demo-nautilus-6rwxn
Mar 29 08:34:53.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 29 08:34:53.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 29 08:34:53.112: INFO: update-demo-nautilus-6rwxn is verified up and running
STEP: using delete to clean up resources 03/29/23 08:34:53.112
Mar 29 08:34:53.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 delete --grace-period=0 --force -f -'
Mar 29 08:34:53.153: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 08:34:53.153: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 29 08:34:53.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get rc,svc -l name=update-demo --no-headers'
Mar 29 08:34:53.197: INFO: stderr: "No resources found in kubectl-7052 namespace.\n"
Mar 29 08:34:53.197: INFO: stdout: ""
Mar 29 08:34:53.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 08:34:53.241: INFO: stderr: ""
Mar 29 08:34:53.241: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:34:53.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7052" for this suite. 03/29/23 08:34:53.243
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":160,"skipped":2726,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.028 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:45.218
    Mar 29 08:34:45.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:34:45.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:45.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:45.229
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/29/23 08:34:45.232
    Mar 29 08:34:45.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 create -f -'
    Mar 29 08:34:45.373: INFO: stderr: ""
    Mar 29 08:34:45.373: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:45.373
    Mar 29 08:34:45.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:34:45.418: INFO: stderr: ""
    Mar 29 08:34:45.419: INFO: stdout: "update-demo-nautilus-6rwxn update-demo-nautilus-f22gr "
    Mar 29 08:34:45.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:45.460: INFO: stderr: ""
    Mar 29 08:34:45.460: INFO: stdout: ""
    Mar 29 08:34:45.460: INFO: update-demo-nautilus-6rwxn is created but not running
    Mar 29 08:34:50.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:34:50.504: INFO: stderr: ""
    Mar 29 08:34:50.504: INFO: stdout: "update-demo-nautilus-6rwxn update-demo-nautilus-f22gr "
    Mar 29 08:34:50.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:50.545: INFO: stderr: ""
    Mar 29 08:34:50.545: INFO: stdout: "true"
    Mar 29 08:34:50.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:34:50.585: INFO: stderr: ""
    Mar 29 08:34:50.585: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:34:50.585: INFO: validating pod update-demo-nautilus-6rwxn
    Mar 29 08:34:50.587: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:34:50.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:34:50.587: INFO: update-demo-nautilus-6rwxn is verified up and running
    Mar 29 08:34:50.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-f22gr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:50.627: INFO: stderr: ""
    Mar 29 08:34:50.627: INFO: stdout: "true"
    Mar 29 08:34:50.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-f22gr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:34:50.667: INFO: stderr: ""
    Mar 29 08:34:50.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:34:50.667: INFO: validating pod update-demo-nautilus-f22gr
    Mar 29 08:34:50.670: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:34:50.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:34:50.670: INFO: update-demo-nautilus-f22gr is verified up and running
    STEP: scaling down the replication controller 03/29/23 08:34:50.67
    Mar 29 08:34:50.671: INFO: scanned /root for discovery docs: <nil>
    Mar 29 08:34:50.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 29 08:34:51.722: INFO: stderr: ""
    Mar 29 08:34:51.722: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:51.722
    Mar 29 08:34:51.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:34:51.765: INFO: stderr: ""
    Mar 29 08:34:51.765: INFO: stdout: "update-demo-nautilus-6rwxn "
    Mar 29 08:34:51.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:51.805: INFO: stderr: ""
    Mar 29 08:34:51.805: INFO: stdout: "true"
    Mar 29 08:34:51.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:34:51.848: INFO: stderr: ""
    Mar 29 08:34:51.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:34:51.848: INFO: validating pod update-demo-nautilus-6rwxn
    Mar 29 08:34:51.849: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:34:51.849: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:34:51.849: INFO: update-demo-nautilus-6rwxn is verified up and running
    STEP: scaling up the replication controller 03/29/23 08:34:51.849
    Mar 29 08:34:51.850: INFO: scanned /root for discovery docs: <nil>
    Mar 29 08:34:51.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 29 08:34:52.901: INFO: stderr: ""
    Mar 29 08:34:52.901: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/29/23 08:34:52.901
    Mar 29 08:34:52.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 29 08:34:52.944: INFO: stderr: ""
    Mar 29 08:34:52.944: INFO: stdout: "update-demo-nautilus-478gl update-demo-nautilus-6rwxn "
    Mar 29 08:34:52.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-478gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:52.985: INFO: stderr: ""
    Mar 29 08:34:52.985: INFO: stdout: "true"
    Mar 29 08:34:52.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-478gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:34:53.026: INFO: stderr: ""
    Mar 29 08:34:53.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:34:53.026: INFO: validating pod update-demo-nautilus-478gl
    Mar 29 08:34:53.028: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:34:53.028: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:34:53.028: INFO: update-demo-nautilus-478gl is verified up and running
    Mar 29 08:34:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 29 08:34:53.069: INFO: stderr: ""
    Mar 29 08:34:53.069: INFO: stdout: "true"
    Mar 29 08:34:53.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods update-demo-nautilus-6rwxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 29 08:34:53.111: INFO: stderr: ""
    Mar 29 08:34:53.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 29 08:34:53.111: INFO: validating pod update-demo-nautilus-6rwxn
    Mar 29 08:34:53.112: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 29 08:34:53.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 29 08:34:53.112: INFO: update-demo-nautilus-6rwxn is verified up and running
    STEP: using delete to clean up resources 03/29/23 08:34:53.112
    Mar 29 08:34:53.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 delete --grace-period=0 --force -f -'
    Mar 29 08:34:53.153: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 08:34:53.153: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 29 08:34:53.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get rc,svc -l name=update-demo --no-headers'
    Mar 29 08:34:53.197: INFO: stderr: "No resources found in kubectl-7052 namespace.\n"
    Mar 29 08:34:53.197: INFO: stdout: ""
    Mar 29 08:34:53.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7052 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 29 08:34:53.241: INFO: stderr: ""
    Mar 29 08:34:53.241: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:34:53.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7052" for this suite. 03/29/23 08:34:53.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:53.247
Mar 29 08:34:53.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:34:53.248
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:53.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:53.255
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/29/23 08:34:53.257
Mar 29 08:34:53.260: INFO: Waiting up to 5m0s for pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1" in namespace "downward-api-6005" to be "running and ready"
Mar 29 08:34:53.261: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.256829ms
Mar 29 08:34:53.261: INFO: The phase of Pod annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:34:55.264: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004299689s
Mar 29 08:34:55.264: INFO: The phase of Pod annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1 is Running (Ready = true)
Mar 29 08:34:55.264: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1" satisfied condition "running and ready"
Mar 29 08:34:55.780: INFO: Successfully updated pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:34:59.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6005" for this suite. 03/29/23 08:34:59.793
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":161,"skipped":2770,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.549 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:53.247
    Mar 29 08:34:53.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:34:53.248
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:53.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:53.255
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/29/23 08:34:53.257
    Mar 29 08:34:53.260: INFO: Waiting up to 5m0s for pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1" in namespace "downward-api-6005" to be "running and ready"
    Mar 29 08:34:53.261: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.256829ms
    Mar 29 08:34:53.261: INFO: The phase of Pod annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:34:55.264: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004299689s
    Mar 29 08:34:55.264: INFO: The phase of Pod annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1 is Running (Ready = true)
    Mar 29 08:34:55.264: INFO: Pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1" satisfied condition "running and ready"
    Mar 29 08:34:55.780: INFO: Successfully updated pod "annotationupdatedb236401-bc88-4d4b-b6c9-52813ff82de1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:34:59.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6005" for this suite. 03/29/23 08:34:59.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:34:59.797
Mar 29 08:34:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:34:59.798
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:59.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:59.805
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9532 03/29/23 08:34:59.806
STEP: creating a selector 03/29/23 08:34:59.806
STEP: Creating the service pods in kubernetes 03/29/23 08:34:59.807
Mar 29 08:34:59.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 08:34:59.819: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9532" to be "running and ready"
Mar 29 08:34:59.821: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.940639ms
Mar 29 08:34:59.821: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:35:01.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004932538s
Mar 29 08:35:01.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:35:03.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00462489s
Mar 29 08:35:03.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:35:05.825: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005709015s
Mar 29 08:35:05.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:35:07.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005452921s
Mar 29 08:35:07.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:35:09.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004626137s
Mar 29 08:35:09.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:35:11.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005092775s
Mar 29 08:35:11.824: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 29 08:35:11.824: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 29 08:35:11.826: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9532" to be "running and ready"
Mar 29 08:35:11.827: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.255534ms
Mar 29 08:35:11.827: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 29 08:35:11.827: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 29 08:35:11.828: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9532" to be "running and ready"
Mar 29 08:35:11.829: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.08641ms
Mar 29 08:35:11.829: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 29 08:35:11.829: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/29/23 08:35:11.83
Mar 29 08:35:11.836: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9532" to be "running"
Mar 29 08:35:11.837: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359459ms
Mar 29 08:35:13.839: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003108279s
Mar 29 08:35:13.839: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 29 08:35:13.840: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9532" to be "running"
Mar 29 08:35:13.842: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.167805ms
Mar 29 08:35:13.842: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 29 08:35:13.843: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 29 08:35:13.843: INFO: Going to poll 192.168.30.47 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:35:13.844: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.30.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:35:13.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:35:13.844: INFO: ExecWithOptions: Clientset creation
Mar 29 08:35:13.844: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.30.47+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:35:14.890: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 29 08:35:14.890: INFO: Going to poll 192.168.219.169 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:35:14.891: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.219.169 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:35:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:35:14.892: INFO: ExecWithOptions: Clientset creation
Mar 29 08:35:14.892: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.219.169+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:35:15.934: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 29 08:35:15.934: INFO: Going to poll 192.168.87.228 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:35:15.936: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.87.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:35:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:35:15.937: INFO: ExecWithOptions: Clientset creation
Mar 29 08:35:15.937: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.87.228+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:35:16.980: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 29 08:35:16.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9532" for this suite. 03/29/23 08:35:16.982
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":162,"skipped":2792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.188 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:34:59.797
    Mar 29 08:34:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:34:59.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:34:59.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:34:59.805
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9532 03/29/23 08:34:59.806
    STEP: creating a selector 03/29/23 08:34:59.806
    STEP: Creating the service pods in kubernetes 03/29/23 08:34:59.807
    Mar 29 08:34:59.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 29 08:34:59.819: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9532" to be "running and ready"
    Mar 29 08:34:59.821: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.940639ms
    Mar 29 08:34:59.821: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:35:01.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004932538s
    Mar 29 08:35:01.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:35:03.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00462489s
    Mar 29 08:35:03.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:35:05.825: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005709015s
    Mar 29 08:35:05.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:35:07.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005452921s
    Mar 29 08:35:07.825: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:35:09.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004626137s
    Mar 29 08:35:09.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:35:11.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005092775s
    Mar 29 08:35:11.824: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 29 08:35:11.824: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 29 08:35:11.826: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9532" to be "running and ready"
    Mar 29 08:35:11.827: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.255534ms
    Mar 29 08:35:11.827: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 29 08:35:11.827: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 29 08:35:11.828: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9532" to be "running and ready"
    Mar 29 08:35:11.829: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.08641ms
    Mar 29 08:35:11.829: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 29 08:35:11.829: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/29/23 08:35:11.83
    Mar 29 08:35:11.836: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9532" to be "running"
    Mar 29 08:35:11.837: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359459ms
    Mar 29 08:35:13.839: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003108279s
    Mar 29 08:35:13.839: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 29 08:35:13.840: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9532" to be "running"
    Mar 29 08:35:13.842: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.167805ms
    Mar 29 08:35:13.842: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 29 08:35:13.843: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 29 08:35:13.843: INFO: Going to poll 192.168.30.47 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:35:13.844: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.30.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:35:13.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:35:13.844: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:35:13.844: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.30.47+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:35:14.890: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 29 08:35:14.890: INFO: Going to poll 192.168.219.169 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:35:14.891: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.219.169 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:35:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:35:14.892: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:35:14.892: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.219.169+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:35:15.934: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 29 08:35:15.934: INFO: Going to poll 192.168.87.228 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:35:15.936: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.87.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9532 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:35:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:35:15.937: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:35:15.937: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-9532/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.87.228+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:35:16.980: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 29 08:35:16.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9532" for this suite. 03/29/23 08:35:16.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:16.985
Mar 29 08:35:16.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:35:16.986
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:16.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:16.993
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-189 03/29/23 08:35:16.994
STEP: creating service affinity-nodeport in namespace services-189 03/29/23 08:35:16.994
STEP: creating replication controller affinity-nodeport in namespace services-189 03/29/23 08:35:17.001
I0329 08:35:17.004629      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-189, replica count: 3
I0329 08:35:20.056639      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:35:20.061: INFO: Creating new exec pod
Mar 29 08:35:20.063: INFO: Waiting up to 5m0s for pod "execpod-affinitygk4xh" in namespace "services-189" to be "running"
Mar 29 08:35:20.065: INFO: Pod "execpod-affinitygk4xh": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291969ms
Mar 29 08:35:22.067: INFO: Pod "execpod-affinitygk4xh": Phase="Running", Reason="", readiness=true. Elapsed: 2.003294139s
Mar 29 08:35:22.067: INFO: Pod "execpod-affinitygk4xh" satisfied condition "running"
Mar 29 08:35:23.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 29 08:35:23.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 29 08:35:23.160: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:35:23.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.76.228 80'
Mar 29 08:35:23.243: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.76.228 80\nConnection to 10.100.76.228 80 port [tcp/http] succeeded!\n"
Mar 29 08:35:23.243: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:35:23.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32534'
Mar 29 08:35:23.327: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32534\nConnection to 10.146.0.116 32534 port [tcp/*] succeeded!\n"
Mar 29 08:35:23.327: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:35:23.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32534'
Mar 29 08:35:23.411: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32534\nConnection to 10.146.0.115 32534 port [tcp/*] succeeded!\n"
Mar 29 08:35:23.411: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:35:23.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:32534/ ; done'
Mar 29 08:35:23.543: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n"
Mar 29 08:35:23.543: INFO: stdout: "\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9"
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
Mar 29 08:35:23.543: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-189, will wait for the garbage collector to delete the pods 03/29/23 08:35:23.55
Mar 29 08:35:23.605: INFO: Deleting ReplicationController affinity-nodeport took: 3.210668ms
Mar 29 08:35:23.705: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.075014ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:35:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-189" for this suite. 03/29/23 08:35:25.717
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":163,"skipped":2799,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.734 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:16.985
    Mar 29 08:35:16.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:35:16.986
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:16.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:16.993
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-189 03/29/23 08:35:16.994
    STEP: creating service affinity-nodeport in namespace services-189 03/29/23 08:35:16.994
    STEP: creating replication controller affinity-nodeport in namespace services-189 03/29/23 08:35:17.001
    I0329 08:35:17.004629      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-189, replica count: 3
    I0329 08:35:20.056639      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:35:20.061: INFO: Creating new exec pod
    Mar 29 08:35:20.063: INFO: Waiting up to 5m0s for pod "execpod-affinitygk4xh" in namespace "services-189" to be "running"
    Mar 29 08:35:20.065: INFO: Pod "execpod-affinitygk4xh": Phase="Pending", Reason="", readiness=false. Elapsed: 1.291969ms
    Mar 29 08:35:22.067: INFO: Pod "execpod-affinitygk4xh": Phase="Running", Reason="", readiness=true. Elapsed: 2.003294139s
    Mar 29 08:35:22.067: INFO: Pod "execpod-affinitygk4xh" satisfied condition "running"
    Mar 29 08:35:23.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar 29 08:35:23.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 29 08:35:23.160: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:35:23.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.76.228 80'
    Mar 29 08:35:23.243: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.76.228 80\nConnection to 10.100.76.228 80 port [tcp/http] succeeded!\n"
    Mar 29 08:35:23.243: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:35:23.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32534'
    Mar 29 08:35:23.327: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32534\nConnection to 10.146.0.116 32534 port [tcp/*] succeeded!\n"
    Mar 29 08:35:23.327: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:35:23.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32534'
    Mar 29 08:35:23.411: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32534\nConnection to 10.146.0.115 32534 port [tcp/*] succeeded!\n"
    Mar 29 08:35:23.411: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:35:23.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-189 exec execpod-affinitygk4xh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:32534/ ; done'
    Mar 29 08:35:23.543: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:32534/\n"
    Mar 29 08:35:23.543: INFO: stdout: "\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9\naffinity-nodeport-7chg9"
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Received response from host: affinity-nodeport-7chg9
    Mar 29 08:35:23.543: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-189, will wait for the garbage collector to delete the pods 03/29/23 08:35:23.55
    Mar 29 08:35:23.605: INFO: Deleting ReplicationController affinity-nodeport took: 3.210668ms
    Mar 29 08:35:23.705: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.075014ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:35:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-189" for this suite. 03/29/23 08:35:25.717
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:25.719
Mar 29 08:35:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename containers 03/29/23 08:35:25.72
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:25.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:25.73
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/29/23 08:35:25.731
Mar 29 08:35:25.735: INFO: Waiting up to 5m0s for pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1" in namespace "containers-4245" to be "Succeeded or Failed"
Mar 29 08:35:25.736: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.354422ms
Mar 29 08:35:27.738: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00349253s
Mar 29 08:35:29.739: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004126241s
STEP: Saw pod success 03/29/23 08:35:29.739
Mar 29 08:35:29.739: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1" satisfied condition "Succeeded or Failed"
Mar 29 08:35:29.740: INFO: Trying to get logs from node 10.146.0.115 pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:35:29.743
Mar 29 08:35:29.751: INFO: Waiting for pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 to disappear
Mar 29 08:35:29.753: INFO: Pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 29 08:35:29.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4245" for this suite. 03/29/23 08:35:29.755
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":164,"skipped":2801,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:25.719
    Mar 29 08:35:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename containers 03/29/23 08:35:25.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:25.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:25.73
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/29/23 08:35:25.731
    Mar 29 08:35:25.735: INFO: Waiting up to 5m0s for pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1" in namespace "containers-4245" to be "Succeeded or Failed"
    Mar 29 08:35:25.736: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.354422ms
    Mar 29 08:35:27.738: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00349253s
    Mar 29 08:35:29.739: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004126241s
    STEP: Saw pod success 03/29/23 08:35:29.739
    Mar 29 08:35:29.739: INFO: Pod "client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1" satisfied condition "Succeeded or Failed"
    Mar 29 08:35:29.740: INFO: Trying to get logs from node 10.146.0.115 pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:35:29.743
    Mar 29 08:35:29.751: INFO: Waiting for pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 to disappear
    Mar 29 08:35:29.753: INFO: Pod client-containers-16421d70-12c9-4af4-95c5-c668f9a8bdd1 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 29 08:35:29.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4245" for this suite. 03/29/23 08:35:29.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:29.758
Mar 29 08:35:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename containers 03/29/23 08:35:29.759
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:29.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:29.767
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/29/23 08:35:29.768
Mar 29 08:35:29.772: INFO: Waiting up to 5m0s for pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837" in namespace "containers-3714" to be "Succeeded or Failed"
Mar 29 08:35:29.774: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259387ms
Mar 29 08:35:31.776: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003877893s
Mar 29 08:35:33.777: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329773s
STEP: Saw pod success 03/29/23 08:35:33.777
Mar 29 08:35:33.777: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837" satisfied condition "Succeeded or Failed"
Mar 29 08:35:33.778: INFO: Trying to get logs from node 10.146.0.116 pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:35:33.781
Mar 29 08:35:33.787: INFO: Waiting for pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 to disappear
Mar 29 08:35:33.788: INFO: Pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 29 08:35:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3714" for this suite. 03/29/23 08:35:33.789
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":165,"skipped":2845,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:29.758
    Mar 29 08:35:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename containers 03/29/23 08:35:29.759
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:29.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:29.767
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/29/23 08:35:29.768
    Mar 29 08:35:29.772: INFO: Waiting up to 5m0s for pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837" in namespace "containers-3714" to be "Succeeded or Failed"
    Mar 29 08:35:29.774: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259387ms
    Mar 29 08:35:31.776: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003877893s
    Mar 29 08:35:33.777: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329773s
    STEP: Saw pod success 03/29/23 08:35:33.777
    Mar 29 08:35:33.777: INFO: Pod "client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837" satisfied condition "Succeeded or Failed"
    Mar 29 08:35:33.778: INFO: Trying to get logs from node 10.146.0.116 pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:35:33.781
    Mar 29 08:35:33.787: INFO: Waiting for pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 to disappear
    Mar 29 08:35:33.788: INFO: Pod client-containers-d22b205b-92aa-4b9d-8ffb-1d405f24c837 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 29 08:35:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3714" for this suite. 03/29/23 08:35:33.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:33.793
Mar 29 08:35:33.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:35:33.794
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:33.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:33.801
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-30f7e78f-c904-4daf-bd33-a01c01da4d39 03/29/23 08:35:33.803
STEP: Creating secret with name s-test-opt-upd-abc3b1d7-6d3f-4bf1-a8d4-09233525f616 03/29/23 08:35:33.805
STEP: Creating the pod 03/29/23 08:35:33.807
Mar 29 08:35:33.811: INFO: Waiting up to 5m0s for pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0" in namespace "secrets-6725" to be "running and ready"
Mar 29 08:35:33.813: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.194725ms
Mar 29 08:35:33.813: INFO: The phase of Pod pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:35:35.815: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0": Phase="Running", Reason="", readiness=true. Elapsed: 2.003155544s
Mar 29 08:35:35.815: INFO: The phase of Pod pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 is Running (Ready = true)
Mar 29 08:35:35.815: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-30f7e78f-c904-4daf-bd33-a01c01da4d39 03/29/23 08:35:35.823
STEP: Updating secret s-test-opt-upd-abc3b1d7-6d3f-4bf1-a8d4-09233525f616 03/29/23 08:35:35.826
STEP: Creating secret with name s-test-opt-create-0ee44aa3-55ef-45c5-8c85-1585e3fcced1 03/29/23 08:35:35.828
STEP: waiting to observe update in volume 03/29/23 08:35:35.83
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:35:37.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6725" for this suite. 03/29/23 08:35:37.842
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":166,"skipped":2865,"failed":0}
------------------------------
â€¢ [4.052 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:33.793
    Mar 29 08:35:33.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:35:33.794
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:33.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:33.801
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-30f7e78f-c904-4daf-bd33-a01c01da4d39 03/29/23 08:35:33.803
    STEP: Creating secret with name s-test-opt-upd-abc3b1d7-6d3f-4bf1-a8d4-09233525f616 03/29/23 08:35:33.805
    STEP: Creating the pod 03/29/23 08:35:33.807
    Mar 29 08:35:33.811: INFO: Waiting up to 5m0s for pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0" in namespace "secrets-6725" to be "running and ready"
    Mar 29 08:35:33.813: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.194725ms
    Mar 29 08:35:33.813: INFO: The phase of Pod pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:35:35.815: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0": Phase="Running", Reason="", readiness=true. Elapsed: 2.003155544s
    Mar 29 08:35:35.815: INFO: The phase of Pod pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 is Running (Ready = true)
    Mar 29 08:35:35.815: INFO: Pod "pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-30f7e78f-c904-4daf-bd33-a01c01da4d39 03/29/23 08:35:35.823
    STEP: Updating secret s-test-opt-upd-abc3b1d7-6d3f-4bf1-a8d4-09233525f616 03/29/23 08:35:35.826
    STEP: Creating secret with name s-test-opt-create-0ee44aa3-55ef-45c5-8c85-1585e3fcced1 03/29/23 08:35:35.828
    STEP: waiting to observe update in volume 03/29/23 08:35:35.83
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:35:37.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6725" for this suite. 03/29/23 08:35:37.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:37.845
Mar 29 08:35:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-pred 03/29/23 08:35:37.846
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:37.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:37.853
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 29 08:35:37.854: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 29 08:35:37.857: INFO: Waiting for terminating namespaces to be deleted...
Mar 29 08:35:37.858: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.115 before test
Mar 29 08:35:37.861: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.861: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:35:37.861: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.861: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:35:37.861: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:35:37.861: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:35:37.861: INFO: pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 from secrets-6725 started at 2023-03-29 08:35:33 +0000 UTC (3 container statuses recorded)
Mar 29 08:35:37.861: INFO: 	Container creates-volume-test ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 	Container dels-volume-test ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 	Container upds-volume-test ready: true, restart count 0
Mar 29 08:35:37.861: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:35:37.861: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:35:37.861: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.116 before test
Mar 29 08:35:37.864: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.864: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:35:37.864: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.864: INFO: 	Container coredns ready: true, restart count 0
Mar 29 08:35:37.864: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:35:37.864: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:35:37.864: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:35:37.864: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:35:37.864: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:35:37.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:35:37.864: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 29 08:35:37.864: INFO: 
Logging pods the apiserver thinks is on node 10.146.0.117 before test
Mar 29 08:35:37.866: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 29 08:35:37.866: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container calico-node ready: true, restart count 0
Mar 29 08:35:37.866: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container exporter ready: true, restart count 0
Mar 29 08:35:37.866: INFO: 	Container reload ready: true, restart count 0
Mar 29 08:35:37.866: INFO: 	Container unbound ready: true, restart count 0
Mar 29 08:35:37.866: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 29 08:35:37.866: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container e2e ready: true, restart count 0
Mar 29 08:35:37.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:35:37.866: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
Mar 29 08:35:37.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 29 08:35:37.866: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:35:37.866
Mar 29 08:35:37.869: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5059" to be "running"
Mar 29 08:35:37.870: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.137003ms
Mar 29 08:35:39.873: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003277752s
Mar 29 08:35:39.873: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:35:39.874
STEP: Trying to apply a random label on the found node. 03/29/23 08:35:39.879
STEP: verifying the node has the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 42 03/29/23 08:35:39.884
STEP: Trying to relaunch the pod, now with labels. 03/29/23 08:35:39.886
Mar 29 08:35:39.888: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5059" to be "not pending"
Mar 29 08:35:39.889: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253557ms
Mar 29 08:35:41.892: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004256949s
Mar 29 08:35:41.893: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 off the node 10.146.0.116 03/29/23 08:35:41.894
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 03/29/23 08:35:41.9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:35:41.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5059" for this suite. 03/29/23 08:35:41.904
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":167,"skipped":2871,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:37.845
    Mar 29 08:35:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-pred 03/29/23 08:35:37.846
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:37.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:37.853
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 29 08:35:37.854: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 29 08:35:37.857: INFO: Waiting for terminating namespaces to be deleted...
    Mar 29 08:35:37.858: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.115 before test
    Mar 29 08:35:37.861: INFO: calico-node-8mtw5 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.861: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: cluster-dns-77bdb56774-p98xx from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.861: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: node-dns-jqb4n from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:35:37.861: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: pod-secrets-2fd04027-aa63-4673-b4a6-00d25e21aed0 from secrets-6725 started at 2023-03-29 08:35:33 +0000 UTC (3 container statuses recorded)
    Mar 29 08:35:37.861: INFO: 	Container creates-volume-test ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 	Container dels-volume-test ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 	Container upds-volume-test ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-xkjhm from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:35:37.861: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:35:37.861: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.116 before test
    Mar 29 08:35:37.864: INFO: calico-node-l2g4d from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.864: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: cluster-dns-77bdb56774-pt7gn from kube-system started at 2023-03-29 07:44:22 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.864: INFO: 	Container coredns ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: node-dns-hb64l from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:35:37.864: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-ngsb2 from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:35:37.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 29 08:35:37.864: INFO: 
    Logging pods the apiserver thinks is on node 10.146.0.117 before test
    Mar 29 08:35:37.866: INFO: calico-kube-controllers-6747f75cdc-ngkn7 from kube-system started at 2023-03-29 07:44:04 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: calico-node-nj5q6 from kube-system started at 2023-03-29 07:43:44 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container calico-node ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: node-dns-nldsw from kube-system started at 2023-03-29 07:43:32 +0000 UTC (3 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container exporter ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: 	Container reload ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: 	Container unbound ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: sonobuoy from sonobuoy started at 2023-03-29 07:44:29 +0000 UTC (1 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: sonobuoy-e2e-job-733d8bf70035488f from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container e2e ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: sonobuoy-systemd-logs-daemon-set-562bdf8b95cd4a68-7rjnh from sonobuoy started at 2023-03-29 07:44:35 +0000 UTC (2 container statuses recorded)
    Mar 29 08:35:37.866: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 29 08:35:37.866: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/29/23 08:35:37.866
    Mar 29 08:35:37.869: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5059" to be "running"
    Mar 29 08:35:37.870: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.137003ms
    Mar 29 08:35:39.873: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003277752s
    Mar 29 08:35:39.873: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/29/23 08:35:39.874
    STEP: Trying to apply a random label on the found node. 03/29/23 08:35:39.879
    STEP: verifying the node has the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 42 03/29/23 08:35:39.884
    STEP: Trying to relaunch the pod, now with labels. 03/29/23 08:35:39.886
    Mar 29 08:35:39.888: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5059" to be "not pending"
    Mar 29 08:35:39.889: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253557ms
    Mar 29 08:35:41.892: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004256949s
    Mar 29 08:35:41.893: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 off the node 10.146.0.116 03/29/23 08:35:41.894
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d2df1493-4ee4-4a23-a583-64bdfa8ada92 03/29/23 08:35:41.9
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:35:41.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5059" for this suite. 03/29/23 08:35:41.904
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:41.907
Mar 29 08:35:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:35:41.908
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:41.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:41.914
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 29 08:35:41.920: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1331 to be scheduled
Mar 29 08:35:41.922: INFO: 1 pods are not scheduled: [runtimeclass-1331/test-runtimeclass-runtimeclass-1331-preconfigured-handler-qdhhc(87e13698-b660-473a-971e-9bcd9db472df)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 29 08:35:43.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1331" for this suite. 03/29/23 08:35:43.927
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":168,"skipped":2897,"failed":0}
------------------------------
â€¢ [2.023 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:41.907
    Mar 29 08:35:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:35:41.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:41.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:41.914
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 29 08:35:41.920: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1331 to be scheduled
    Mar 29 08:35:41.922: INFO: 1 pods are not scheduled: [runtimeclass-1331/test-runtimeclass-runtimeclass-1331-preconfigured-handler-qdhhc(87e13698-b660-473a-971e-9bcd9db472df)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 29 08:35:43.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1331" for this suite. 03/29/23 08:35:43.927
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:43.93
Mar 29 08:35:43.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:35:43.931
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:43.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:43.937
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-63d7c4b3-397d-4848-87a5-c14cf950a54b 03/29/23 08:35:43.946
STEP: Creating a pod to test consume secrets 03/29/23 08:35:43.947
Mar 29 08:35:43.950: INFO: Waiting up to 5m0s for pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4" in namespace "secrets-3750" to be "Succeeded or Failed"
Mar 29 08:35:43.952: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226181ms
Mar 29 08:35:45.954: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004083913s
Mar 29 08:35:47.954: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004061131s
STEP: Saw pod success 03/29/23 08:35:47.954
Mar 29 08:35:47.955: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4" satisfied condition "Succeeded or Failed"
Mar 29 08:35:47.956: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:35:47.958
Mar 29 08:35:47.965: INFO: Waiting for pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 to disappear
Mar 29 08:35:47.966: INFO: Pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:35:47.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3750" for this suite. 03/29/23 08:35:47.967
STEP: Destroying namespace "secret-namespace-3202" for this suite. 03/29/23 08:35:47.971
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":169,"skipped":2900,"failed":0}
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:43.93
    Mar 29 08:35:43.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:35:43.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:43.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:43.937
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-63d7c4b3-397d-4848-87a5-c14cf950a54b 03/29/23 08:35:43.946
    STEP: Creating a pod to test consume secrets 03/29/23 08:35:43.947
    Mar 29 08:35:43.950: INFO: Waiting up to 5m0s for pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4" in namespace "secrets-3750" to be "Succeeded or Failed"
    Mar 29 08:35:43.952: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226181ms
    Mar 29 08:35:45.954: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004083913s
    Mar 29 08:35:47.954: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004061131s
    STEP: Saw pod success 03/29/23 08:35:47.954
    Mar 29 08:35:47.955: INFO: Pod "pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4" satisfied condition "Succeeded or Failed"
    Mar 29 08:35:47.956: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:35:47.958
    Mar 29 08:35:47.965: INFO: Waiting for pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 to disappear
    Mar 29 08:35:47.966: INFO: Pod pod-secrets-92f67a9c-724a-4217-8b4d-80070cd085f4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:35:47.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3750" for this suite. 03/29/23 08:35:47.967
    STEP: Destroying namespace "secret-namespace-3202" for this suite. 03/29/23 08:35:47.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:47.974
Mar 29 08:35:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename watch 03/29/23 08:35:47.974
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:47.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:47.984
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/29/23 08:35:47.986
STEP: modifying the configmap once 03/29/23 08:35:47.989
STEP: modifying the configmap a second time 03/29/23 08:35:47.992
STEP: deleting the configmap 03/29/23 08:35:47.995
STEP: creating a watch on configmaps from the resource version returned by the first update 03/29/23 08:35:47.997
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/29/23 08:35:47.997
Mar 29 08:35:47.998: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-218  51732b9d-32e4-4fb7-9cd8-3655a5c41070 17426 0 2023-03-29 08:35:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 08:35:47.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-218  51732b9d-32e4-4fb7-9cd8-3655a5c41070 17427 0 2023-03-29 08:35:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 29 08:35:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-218" for this suite. 03/29/23 08:35:47.999
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":170,"skipped":2910,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:47.974
    Mar 29 08:35:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename watch 03/29/23 08:35:47.974
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:47.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:47.984
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/29/23 08:35:47.986
    STEP: modifying the configmap once 03/29/23 08:35:47.989
    STEP: modifying the configmap a second time 03/29/23 08:35:47.992
    STEP: deleting the configmap 03/29/23 08:35:47.995
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/29/23 08:35:47.997
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/29/23 08:35:47.997
    Mar 29 08:35:47.998: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-218  51732b9d-32e4-4fb7-9cd8-3655a5c41070 17426 0 2023-03-29 08:35:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 08:35:47.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-218  51732b9d-32e4-4fb7-9cd8-3655a5c41070 17427 0 2023-03-29 08:35:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 29 08:35:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-218" for this suite. 03/29/23 08:35:47.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:48.002
Mar 29 08:35:48.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context 03/29/23 08:35:48.002
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:48.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:48.009
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/29/23 08:35:48.01
Mar 29 08:35:48.014: INFO: Waiting up to 5m0s for pod "security-context-b51fa055-63e6-4a0c-a074-851404705581" in namespace "security-context-2031" to be "Succeeded or Failed"
Mar 29 08:35:48.015: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Pending", Reason="", readiness=false. Elapsed: 1.124174ms
Mar 29 08:35:50.018: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004010639s
Mar 29 08:35:52.017: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003655483s
STEP: Saw pod success 03/29/23 08:35:52.017
Mar 29 08:35:52.017: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581" satisfied condition "Succeeded or Failed"
Mar 29 08:35:52.019: INFO: Trying to get logs from node 10.146.0.115 pod security-context-b51fa055-63e6-4a0c-a074-851404705581 container test-container: <nil>
STEP: delete the pod 03/29/23 08:35:52.021
Mar 29 08:35:52.027: INFO: Waiting for pod security-context-b51fa055-63e6-4a0c-a074-851404705581 to disappear
Mar 29 08:35:52.028: INFO: Pod security-context-b51fa055-63e6-4a0c-a074-851404705581 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 08:35:52.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2031" for this suite. 03/29/23 08:35:52.03
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":171,"skipped":2918,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:48.002
    Mar 29 08:35:48.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context 03/29/23 08:35:48.002
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:48.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:48.009
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/29/23 08:35:48.01
    Mar 29 08:35:48.014: INFO: Waiting up to 5m0s for pod "security-context-b51fa055-63e6-4a0c-a074-851404705581" in namespace "security-context-2031" to be "Succeeded or Failed"
    Mar 29 08:35:48.015: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Pending", Reason="", readiness=false. Elapsed: 1.124174ms
    Mar 29 08:35:50.018: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004010639s
    Mar 29 08:35:52.017: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003655483s
    STEP: Saw pod success 03/29/23 08:35:52.017
    Mar 29 08:35:52.017: INFO: Pod "security-context-b51fa055-63e6-4a0c-a074-851404705581" satisfied condition "Succeeded or Failed"
    Mar 29 08:35:52.019: INFO: Trying to get logs from node 10.146.0.115 pod security-context-b51fa055-63e6-4a0c-a074-851404705581 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:35:52.021
    Mar 29 08:35:52.027: INFO: Waiting for pod security-context-b51fa055-63e6-4a0c-a074-851404705581 to disappear
    Mar 29 08:35:52.028: INFO: Pod security-context-b51fa055-63e6-4a0c-a074-851404705581 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 08:35:52.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2031" for this suite. 03/29/23 08:35:52.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:52.034
Mar 29 08:35:52.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 08:35:52.035
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:52.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:52.041
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/29/23 08:35:52.043
Mar 29 08:35:52.047: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-523  de7b9b05-5ebb-4339-89e0-4424aa783787 17472 0 2023-03-29 08:35:52 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjwvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjwvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:35:52.047: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-523" to be "running and ready"
Mar 29 08:35:52.048: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231341ms
Mar 29 08:35:52.048: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:35:54.050: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.002902984s
Mar 29 08:35:54.050: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 29 08:35:54.050: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/29/23 08:35:54.05
Mar 29 08:35:54.050: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-523 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:35:54.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:35:54.050: INFO: ExecWithOptions: Clientset creation
Mar 29 08:35:54.050: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/dns-523/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/29/23 08:35:54.106
Mar 29 08:35:54.106: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-523 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:35:54.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:35:54.106: INFO: ExecWithOptions: Clientset creation
Mar 29 08:35:54.106: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/dns-523/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:35:54.155: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 08:35:54.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-523" for this suite. 03/29/23 08:35:54.163
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":172,"skipped":2944,"failed":0}
------------------------------
â€¢ [2.131 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:52.034
    Mar 29 08:35:52.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 08:35:52.035
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:52.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:52.041
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/29/23 08:35:52.043
    Mar 29 08:35:52.047: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-523  de7b9b05-5ebb-4339-89e0-4424aa783787 17472 0 2023-03-29 08:35:52 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-29 08:35:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjwvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjwvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:35:52.047: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-523" to be "running and ready"
    Mar 29 08:35:52.048: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.231341ms
    Mar 29 08:35:52.048: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:35:54.050: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.002902984s
    Mar 29 08:35:54.050: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 29 08:35:54.050: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/29/23 08:35:54.05
    Mar 29 08:35:54.050: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-523 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:35:54.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:35:54.050: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:35:54.050: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/dns-523/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/29/23 08:35:54.106
    Mar 29 08:35:54.106: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-523 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:35:54.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:35:54.106: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:35:54.106: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/dns-523/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:35:54.155: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 08:35:54.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-523" for this suite. 03/29/23 08:35:54.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:54.167
Mar 29 08:35:54.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:35:54.167
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:54.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:54.174
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:35:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-61" for this suite. 03/29/23 08:35:54.191
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":173,"skipped":2990,"failed":0}
------------------------------
â€¢ [0.027 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:54.167
    Mar 29 08:35:54.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:35:54.167
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:54.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:54.174
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:35:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-61" for this suite. 03/29/23 08:35:54.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:54.195
Mar 29 08:35:54.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:35:54.196
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:54.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:54.202
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/29/23 08:35:54.203
Mar 29 08:35:54.207: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6415" to be "running and ready"
Mar 29 08:35:54.208: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195468ms
Mar 29 08:35:54.208: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:35:56.210: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003530301s
Mar 29 08:35:56.210: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 29 08:35:56.210: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/29/23 08:35:56.211
STEP: Then the orphan pod is adopted 03/29/23 08:35:56.214
STEP: When the matched label of one of its pods change 03/29/23 08:35:57.217
Mar 29 08:35:57.219: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/29/23 08:35:57.223
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:35:58.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6415" for this suite. 03/29/23 08:35:58.23
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":174,"skipped":3051,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:54.195
    Mar 29 08:35:54.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:35:54.196
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:54.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:54.202
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/29/23 08:35:54.203
    Mar 29 08:35:54.207: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6415" to be "running and ready"
    Mar 29 08:35:54.208: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.195468ms
    Mar 29 08:35:54.208: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:35:56.210: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003530301s
    Mar 29 08:35:56.210: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 29 08:35:56.210: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/29/23 08:35:56.211
    STEP: Then the orphan pod is adopted 03/29/23 08:35:56.214
    STEP: When the matched label of one of its pods change 03/29/23 08:35:57.217
    Mar 29 08:35:57.219: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/29/23 08:35:57.223
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:35:58.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6415" for this suite. 03/29/23 08:35:58.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:35:58.234
Mar 29 08:35:58.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:35:58.235
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:58.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:58.242
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:35:58.253
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:35:58.256
Mar 29 08:35:58.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:35:58.258: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:35:59.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 08:35:59.263: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
Mar 29 08:36:00.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:36:00.265: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/29/23 08:36:00.266
Mar 29 08:36:00.268: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/29/23 08:36:00.268
Mar 29 08:36:00.272: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/29/23 08:36:00.272
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: ADDED
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.273: INFO: Found daemon set daemon-set in namespace daemonsets-9560 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 29 08:36:00.273: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/29/23 08:36:00.273
STEP: watching for the daemon set status to be patched 03/29/23 08:36:00.277
Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: ADDED
Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.279: INFO: Observed daemon set daemon-set in namespace daemonsets-9560 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
Mar 29 08:36:00.279: INFO: Found daemon set daemon-set in namespace daemonsets-9560 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 29 08:36:00.279: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:36:00.28
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9560, will wait for the garbage collector to delete the pods 03/29/23 08:36:00.28
Mar 29 08:36:00.335: INFO: Deleting DaemonSet.extensions daemon-set took: 2.758914ms
Mar 29 08:36:00.435: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.588539ms
Mar 29 08:36:02.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:36:02.837: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 08:36:02.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17666"},"items":null}

Mar 29 08:36:02.839: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17666"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:36:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9560" for this suite. 03/29/23 08:36:02.846
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":175,"skipped":3089,"failed":0}
------------------------------
â€¢ [4.614 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:35:58.234
    Mar 29 08:35:58.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:35:58.235
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:35:58.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:35:58.242
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:35:58.253
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:35:58.256
    Mar 29 08:35:58.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:35:58.258: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:35:59.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 08:35:59.263: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
    Mar 29 08:36:00.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:36:00.265: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/29/23 08:36:00.266
    Mar 29 08:36:00.268: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/29/23 08:36:00.268
    Mar 29 08:36:00.272: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/29/23 08:36:00.272
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: ADDED
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.273: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.273: INFO: Found daemon set daemon-set in namespace daemonsets-9560 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 29 08:36:00.273: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/29/23 08:36:00.273
    STEP: watching for the daemon set status to be patched 03/29/23 08:36:00.277
    Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: ADDED
    Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.278: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.279: INFO: Observed daemon set daemon-set in namespace daemonsets-9560 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 29 08:36:00.279: INFO: Observed &DaemonSet event: MODIFIED
    Mar 29 08:36:00.279: INFO: Found daemon set daemon-set in namespace daemonsets-9560 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 29 08:36:00.279: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:36:00.28
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9560, will wait for the garbage collector to delete the pods 03/29/23 08:36:00.28
    Mar 29 08:36:00.335: INFO: Deleting DaemonSet.extensions daemon-set took: 2.758914ms
    Mar 29 08:36:00.435: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.588539ms
    Mar 29 08:36:02.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:36:02.837: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 08:36:02.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17666"},"items":null}

    Mar 29 08:36:02.839: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17666"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:36:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9560" for this suite. 03/29/23 08:36:02.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:02.849
Mar 29 08:36:02.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename watch 03/29/23 08:36:02.85
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:02.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:02.857
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/29/23 08:36:02.858
STEP: starting a background goroutine to produce watch events 03/29/23 08:36:02.86
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/29/23 08:36:02.86
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 29 08:36:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4595" for this suite. 03/29/23 08:36:05.702
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":176,"skipped":3111,"failed":0}
------------------------------
â€¢ [2.904 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:02.849
    Mar 29 08:36:02.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename watch 03/29/23 08:36:02.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:02.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:02.857
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/29/23 08:36:02.858
    STEP: starting a background goroutine to produce watch events 03/29/23 08:36:02.86
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/29/23 08:36:02.86
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 29 08:36:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4595" for this suite. 03/29/23 08:36:05.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:05.755
Mar 29 08:36:05.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename discovery 03/29/23 08:36:05.755
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:05.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:05.763
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/29/23 08:36:05.764
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 29 08:36:06.014: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 29 08:36:06.015: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 29 08:36:06.015: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 29 08:36:06.015: INFO: Checking APIGroup: apps
Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 29 08:36:06.015: INFO: Versions found [{apps/v1 v1}]
Mar 29 08:36:06.015: INFO: apps/v1 matches apps/v1
Mar 29 08:36:06.015: INFO: Checking APIGroup: events.k8s.io
Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 29 08:36:06.015: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 29 08:36:06.015: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 29 08:36:06.015: INFO: Checking APIGroup: authentication.k8s.io
Mar 29 08:36:06.016: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 29 08:36:06.016: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 29 08:36:06.016: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 29 08:36:06.016: INFO: Checking APIGroup: authorization.k8s.io
Mar 29 08:36:06.016: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 29 08:36:06.016: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 29 08:36:06.016: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 29 08:36:06.016: INFO: Checking APIGroup: autoscaling
Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 29 08:36:06.017: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar 29 08:36:06.017: INFO: autoscaling/v2 matches autoscaling/v2
Mar 29 08:36:06.017: INFO: Checking APIGroup: batch
Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 29 08:36:06.017: INFO: Versions found [{batch/v1 v1}]
Mar 29 08:36:06.017: INFO: batch/v1 matches batch/v1
Mar 29 08:36:06.017: INFO: Checking APIGroup: certificates.k8s.io
Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 29 08:36:06.017: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 29 08:36:06.017: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 29 08:36:06.017: INFO: Checking APIGroup: networking.k8s.io
Mar 29 08:36:06.018: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 29 08:36:06.018: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 29 08:36:06.018: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 29 08:36:06.018: INFO: Checking APIGroup: policy
Mar 29 08:36:06.018: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 29 08:36:06.018: INFO: Versions found [{policy/v1 v1}]
Mar 29 08:36:06.018: INFO: policy/v1 matches policy/v1
Mar 29 08:36:06.018: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 29 08:36:06.019: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 29 08:36:06.019: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 29 08:36:06.019: INFO: Checking APIGroup: storage.k8s.io
Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 29 08:36:06.019: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 29 08:36:06.019: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 29 08:36:06.019: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 29 08:36:06.019: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 29 08:36:06.019: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 29 08:36:06.019: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 29 08:36:06.020: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 29 08:36:06.020: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 29 08:36:06.020: INFO: Checking APIGroup: scheduling.k8s.io
Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 29 08:36:06.020: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 29 08:36:06.020: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 29 08:36:06.020: INFO: Checking APIGroup: coordination.k8s.io
Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 29 08:36:06.020: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 29 08:36:06.020: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 29 08:36:06.020: INFO: Checking APIGroup: node.k8s.io
Mar 29 08:36:06.021: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 29 08:36:06.021: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 29 08:36:06.021: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 29 08:36:06.021: INFO: Checking APIGroup: discovery.k8s.io
Mar 29 08:36:06.022: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 29 08:36:06.022: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 29 08:36:06.022: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 29 08:36:06.022: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 29 08:36:06.023: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar 29 08:36:06.023: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 29 08:36:06.023: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar 29 08:36:06.023: INFO: Checking APIGroup: crd.projectcalico.org
Mar 29 08:36:06.023: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 29 08:36:06.023: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 29 08:36:06.023: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar 29 08:36:06.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1860" for this suite. 03/29/23 08:36:06.025
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":177,"skipped":3121,"failed":0}
------------------------------
â€¢ [0.273 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:05.755
    Mar 29 08:36:05.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename discovery 03/29/23 08:36:05.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:05.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:05.763
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/29/23 08:36:05.764
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 29 08:36:06.014: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 29 08:36:06.015: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 29 08:36:06.015: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 29 08:36:06.015: INFO: Checking APIGroup: apps
    Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 29 08:36:06.015: INFO: Versions found [{apps/v1 v1}]
    Mar 29 08:36:06.015: INFO: apps/v1 matches apps/v1
    Mar 29 08:36:06.015: INFO: Checking APIGroup: events.k8s.io
    Mar 29 08:36:06.015: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 29 08:36:06.015: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 29 08:36:06.015: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 29 08:36:06.015: INFO: Checking APIGroup: authentication.k8s.io
    Mar 29 08:36:06.016: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 29 08:36:06.016: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 29 08:36:06.016: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 29 08:36:06.016: INFO: Checking APIGroup: authorization.k8s.io
    Mar 29 08:36:06.016: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 29 08:36:06.016: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 29 08:36:06.016: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 29 08:36:06.016: INFO: Checking APIGroup: autoscaling
    Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 29 08:36:06.017: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar 29 08:36:06.017: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 29 08:36:06.017: INFO: Checking APIGroup: batch
    Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 29 08:36:06.017: INFO: Versions found [{batch/v1 v1}]
    Mar 29 08:36:06.017: INFO: batch/v1 matches batch/v1
    Mar 29 08:36:06.017: INFO: Checking APIGroup: certificates.k8s.io
    Mar 29 08:36:06.017: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 29 08:36:06.017: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 29 08:36:06.017: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 29 08:36:06.017: INFO: Checking APIGroup: networking.k8s.io
    Mar 29 08:36:06.018: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 29 08:36:06.018: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 29 08:36:06.018: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 29 08:36:06.018: INFO: Checking APIGroup: policy
    Mar 29 08:36:06.018: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 29 08:36:06.018: INFO: Versions found [{policy/v1 v1}]
    Mar 29 08:36:06.018: INFO: policy/v1 matches policy/v1
    Mar 29 08:36:06.018: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 29 08:36:06.019: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Checking APIGroup: storage.k8s.io
    Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 29 08:36:06.019: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 29 08:36:06.019: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 29 08:36:06.019: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 29 08:36:06.019: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 29 08:36:06.020: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 29 08:36:06.020: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Checking APIGroup: coordination.k8s.io
    Mar 29 08:36:06.020: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 29 08:36:06.020: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 29 08:36:06.020: INFO: Checking APIGroup: node.k8s.io
    Mar 29 08:36:06.021: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 29 08:36:06.021: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 29 08:36:06.021: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 29 08:36:06.021: INFO: Checking APIGroup: discovery.k8s.io
    Mar 29 08:36:06.022: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 29 08:36:06.022: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 29 08:36:06.022: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 29 08:36:06.022: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 29 08:36:06.023: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar 29 08:36:06.023: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar 29 08:36:06.023: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar 29 08:36:06.023: INFO: Checking APIGroup: crd.projectcalico.org
    Mar 29 08:36:06.023: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar 29 08:36:06.023: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar 29 08:36:06.023: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar 29 08:36:06.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-1860" for this suite. 03/29/23 08:36:06.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:06.029
Mar 29 08:36:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename controllerrevisions 03/29/23 08:36:06.029
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:06.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:06.036
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-5n9cx-daemon-set" 03/29/23 08:36:06.044
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:36:06.047
Mar 29 08:36:06.050: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 0
Mar 29 08:36:06.050: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:36:07.054: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 2
Mar 29 08:36:07.054: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 08:36:08.054: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 3
Mar 29 08:36:08.054: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5n9cx-daemon-set
STEP: Confirm DaemonSet "e2e-5n9cx-daemon-set" successfully created with "daemonset-name=e2e-5n9cx-daemon-set" label 03/29/23 08:36:08.055
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5n9cx-daemon-set" 03/29/23 08:36:08.059
Mar 29 08:36:08.061: INFO: Located ControllerRevision: "e2e-5n9cx-daemon-set-77c6f594b"
STEP: Patching ControllerRevision "e2e-5n9cx-daemon-set-77c6f594b" 03/29/23 08:36:08.062
Mar 29 08:36:08.065: INFO: e2e-5n9cx-daemon-set-77c6f594b has been patched
STEP: Create a new ControllerRevision 03/29/23 08:36:08.065
Mar 29 08:36:08.067: INFO: Created ControllerRevision: e2e-5n9cx-daemon-set-84f5948fd8
STEP: Confirm that there are two ControllerRevisions 03/29/23 08:36:08.067
Mar 29 08:36:08.067: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 29 08:36:08.069: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-5n9cx-daemon-set-77c6f594b" 03/29/23 08:36:08.069
STEP: Confirm that there is only one ControllerRevision 03/29/23 08:36:08.071
Mar 29 08:36:08.071: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 29 08:36:08.072: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-5n9cx-daemon-set-84f5948fd8" 03/29/23 08:36:08.073
Mar 29 08:36:08.076: INFO: e2e-5n9cx-daemon-set-84f5948fd8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/29/23 08:36:08.076
W0329 08:36:08.079656      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/29/23 08:36:08.079
Mar 29 08:36:08.079: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 29 08:36:09.081: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 29 08:36:09.083: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5n9cx-daemon-set-84f5948fd8=updated" 03/29/23 08:36:09.083
STEP: Confirm that there is only one ControllerRevision 03/29/23 08:36:09.086
Mar 29 08:36:09.086: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 29 08:36:09.087: INFO: Found 1 ControllerRevisions
Mar 29 08:36:09.088: INFO: ControllerRevision "e2e-5n9cx-daemon-set-6987bdd9bf" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-5n9cx-daemon-set" 03/29/23 08:36:09.089
STEP: deleting DaemonSet.extensions e2e-5n9cx-daemon-set in namespace controllerrevisions-3517, will wait for the garbage collector to delete the pods 03/29/23 08:36:09.089
Mar 29 08:36:09.144: INFO: Deleting DaemonSet.extensions e2e-5n9cx-daemon-set took: 2.585776ms
Mar 29 08:36:09.244: INFO: Terminating DaemonSet.extensions e2e-5n9cx-daemon-set pods took: 100.076617ms
Mar 29 08:36:10.446: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 0
Mar 29 08:36:10.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5n9cx-daemon-set
Mar 29 08:36:10.447: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17921"},"items":null}

Mar 29 08:36:10.448: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17921"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:36:10.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3517" for this suite. 03/29/23 08:36:10.455
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":178,"skipped":3164,"failed":0}
------------------------------
â€¢ [4.428 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:06.029
    Mar 29 08:36:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename controllerrevisions 03/29/23 08:36:06.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:06.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:06.036
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-5n9cx-daemon-set" 03/29/23 08:36:06.044
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:36:06.047
    Mar 29 08:36:06.050: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 0
    Mar 29 08:36:06.050: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:36:07.054: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 2
    Mar 29 08:36:07.054: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 08:36:08.054: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 3
    Mar 29 08:36:08.054: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5n9cx-daemon-set
    STEP: Confirm DaemonSet "e2e-5n9cx-daemon-set" successfully created with "daemonset-name=e2e-5n9cx-daemon-set" label 03/29/23 08:36:08.055
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5n9cx-daemon-set" 03/29/23 08:36:08.059
    Mar 29 08:36:08.061: INFO: Located ControllerRevision: "e2e-5n9cx-daemon-set-77c6f594b"
    STEP: Patching ControllerRevision "e2e-5n9cx-daemon-set-77c6f594b" 03/29/23 08:36:08.062
    Mar 29 08:36:08.065: INFO: e2e-5n9cx-daemon-set-77c6f594b has been patched
    STEP: Create a new ControllerRevision 03/29/23 08:36:08.065
    Mar 29 08:36:08.067: INFO: Created ControllerRevision: e2e-5n9cx-daemon-set-84f5948fd8
    STEP: Confirm that there are two ControllerRevisions 03/29/23 08:36:08.067
    Mar 29 08:36:08.067: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 29 08:36:08.069: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-5n9cx-daemon-set-77c6f594b" 03/29/23 08:36:08.069
    STEP: Confirm that there is only one ControllerRevision 03/29/23 08:36:08.071
    Mar 29 08:36:08.071: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 29 08:36:08.072: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-5n9cx-daemon-set-84f5948fd8" 03/29/23 08:36:08.073
    Mar 29 08:36:08.076: INFO: e2e-5n9cx-daemon-set-84f5948fd8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/29/23 08:36:08.076
    W0329 08:36:08.079656      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/29/23 08:36:08.079
    Mar 29 08:36:08.079: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 29 08:36:09.081: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 29 08:36:09.083: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5n9cx-daemon-set-84f5948fd8=updated" 03/29/23 08:36:09.083
    STEP: Confirm that there is only one ControllerRevision 03/29/23 08:36:09.086
    Mar 29 08:36:09.086: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 29 08:36:09.087: INFO: Found 1 ControllerRevisions
    Mar 29 08:36:09.088: INFO: ControllerRevision "e2e-5n9cx-daemon-set-6987bdd9bf" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-5n9cx-daemon-set" 03/29/23 08:36:09.089
    STEP: deleting DaemonSet.extensions e2e-5n9cx-daemon-set in namespace controllerrevisions-3517, will wait for the garbage collector to delete the pods 03/29/23 08:36:09.089
    Mar 29 08:36:09.144: INFO: Deleting DaemonSet.extensions e2e-5n9cx-daemon-set took: 2.585776ms
    Mar 29 08:36:09.244: INFO: Terminating DaemonSet.extensions e2e-5n9cx-daemon-set pods took: 100.076617ms
    Mar 29 08:36:10.446: INFO: Number of nodes with available pods controlled by daemonset e2e-5n9cx-daemon-set: 0
    Mar 29 08:36:10.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5n9cx-daemon-set
    Mar 29 08:36:10.447: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17921"},"items":null}

    Mar 29 08:36:10.448: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17921"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:36:10.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3517" for this suite. 03/29/23 08:36:10.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:10.458
Mar 29 08:36:10.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:36:10.459
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:10.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:10.465
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-3383 03/29/23 08:36:10.466
STEP: creating replication controller nodeport-test in namespace services-3383 03/29/23 08:36:10.472
I0329 08:36:10.477345      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3383, replica count: 2
I0329 08:36:13.528079      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:36:13.528: INFO: Creating new exec pod
Mar 29 08:36:13.532: INFO: Waiting up to 5m0s for pod "execpodfmlk2" in namespace "services-3383" to be "running"
Mar 29 08:36:13.533: INFO: Pod "execpodfmlk2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4975ms
Mar 29 08:36:15.536: INFO: Pod "execpodfmlk2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003992724s
Mar 29 08:36:15.536: INFO: Pod "execpodfmlk2" satisfied condition "running"
Mar 29 08:36:16.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 29 08:36:16.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 29 08:36:16.626: INFO: stdout: "nodeport-test-wtv2m"
Mar 29 08:36:16.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.138.7 80'
Mar 29 08:36:16.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.138.7 80\nConnection to 10.100.138.7 80 port [tcp/http] succeeded!\n"
Mar 29 08:36:16.715: INFO: stdout: "nodeport-test-m6hhq"
Mar 29 08:36:16.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32345'
Mar 29 08:36:16.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32345\nConnection to 10.146.0.115 32345 port [tcp/*] succeeded!\n"
Mar 29 08:36:16.802: INFO: stdout: "nodeport-test-wtv2m"
Mar 29 08:36:16.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32345'
Mar 29 08:36:16.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32345\nConnection to 10.146.0.116 32345 port [tcp/*] succeeded!\n"
Mar 29 08:36:16.890: INFO: stdout: "nodeport-test-wtv2m"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:36:16.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3383" for this suite. 03/29/23 08:36:16.892
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":179,"skipped":3176,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.437 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:10.458
    Mar 29 08:36:10.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:36:10.459
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:10.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:10.465
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-3383 03/29/23 08:36:10.466
    STEP: creating replication controller nodeport-test in namespace services-3383 03/29/23 08:36:10.472
    I0329 08:36:10.477345      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3383, replica count: 2
    I0329 08:36:13.528079      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:36:13.528: INFO: Creating new exec pod
    Mar 29 08:36:13.532: INFO: Waiting up to 5m0s for pod "execpodfmlk2" in namespace "services-3383" to be "running"
    Mar 29 08:36:13.533: INFO: Pod "execpodfmlk2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4975ms
    Mar 29 08:36:15.536: INFO: Pod "execpodfmlk2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003992724s
    Mar 29 08:36:15.536: INFO: Pod "execpodfmlk2" satisfied condition "running"
    Mar 29 08:36:16.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar 29 08:36:16.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 29 08:36:16.626: INFO: stdout: "nodeport-test-wtv2m"
    Mar 29 08:36:16.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.138.7 80'
    Mar 29 08:36:16.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.138.7 80\nConnection to 10.100.138.7 80 port [tcp/http] succeeded!\n"
    Mar 29 08:36:16.715: INFO: stdout: "nodeport-test-m6hhq"
    Mar 29 08:36:16.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.115 32345'
    Mar 29 08:36:16.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.115 32345\nConnection to 10.146.0.115 32345 port [tcp/*] succeeded!\n"
    Mar 29 08:36:16.802: INFO: stdout: "nodeport-test-wtv2m"
    Mar 29 08:36:16.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-3383 exec execpodfmlk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32345'
    Mar 29 08:36:16.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32345\nConnection to 10.146.0.116 32345 port [tcp/*] succeeded!\n"
    Mar 29 08:36:16.890: INFO: stdout: "nodeport-test-wtv2m"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:36:16.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3383" for this suite. 03/29/23 08:36:16.892
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:16.895
Mar 29 08:36:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:36:16.896
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:16.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:16.904
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/29/23 08:36:16.906
STEP: Getting a ResourceQuota 03/29/23 08:36:16.907
STEP: Listing all ResourceQuotas with LabelSelector 03/29/23 08:36:16.909
STEP: Patching the ResourceQuota 03/29/23 08:36:16.91
STEP: Deleting a Collection of ResourceQuotas 03/29/23 08:36:16.913
STEP: Verifying the deleted ResourceQuota 03/29/23 08:36:16.917
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:36:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1182" for this suite. 03/29/23 08:36:16.92
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":180,"skipped":3179,"failed":0}
------------------------------
â€¢ [0.027 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:16.895
    Mar 29 08:36:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:36:16.896
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:16.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:16.904
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/29/23 08:36:16.906
    STEP: Getting a ResourceQuota 03/29/23 08:36:16.907
    STEP: Listing all ResourceQuotas with LabelSelector 03/29/23 08:36:16.909
    STEP: Patching the ResourceQuota 03/29/23 08:36:16.91
    STEP: Deleting a Collection of ResourceQuotas 03/29/23 08:36:16.913
    STEP: Verifying the deleted ResourceQuota 03/29/23 08:36:16.917
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:36:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1182" for this suite. 03/29/23 08:36:16.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:16.922
Mar 29 08:36:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:36:16.923
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:16.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:16.929
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/29/23 08:36:16.937
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:36:16.94
Mar 29 08:36:16.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:36:16.945: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:36:17.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 08:36:17.950: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:36:18.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:36:18.949: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/29/23 08:36:18.95
Mar 29 08:36:18.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:36:18.961: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
Mar 29 08:36:19.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:36:19.965: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/29/23 08:36:19.965
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:36:19.968
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6247, will wait for the garbage collector to delete the pods 03/29/23 08:36:19.968
Mar 29 08:36:20.022: INFO: Deleting DaemonSet.extensions daemon-set took: 2.507592ms
Mar 29 08:36:20.123: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.722825ms
Mar 29 08:36:22.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:36:22.825: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 08:36:22.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18175"},"items":null}

Mar 29 08:36:22.827: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18175"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:36:22.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6247" for this suite. 03/29/23 08:36:22.834
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":181,"skipped":3187,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.915 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:16.922
    Mar 29 08:36:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:36:16.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:16.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:16.929
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/29/23 08:36:16.937
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:36:16.94
    Mar 29 08:36:16.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:36:16.945: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:36:17.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 08:36:17.950: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:36:18.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:36:18.949: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/29/23 08:36:18.95
    Mar 29 08:36:18.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:36:18.961: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
    Mar 29 08:36:19.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:36:19.965: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/29/23 08:36:19.965
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 08:36:19.968
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6247, will wait for the garbage collector to delete the pods 03/29/23 08:36:19.968
    Mar 29 08:36:20.022: INFO: Deleting DaemonSet.extensions daemon-set took: 2.507592ms
    Mar 29 08:36:20.123: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.722825ms
    Mar 29 08:36:22.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:36:22.825: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 08:36:22.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18175"},"items":null}

    Mar 29 08:36:22.827: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18175"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:36:22.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6247" for this suite. 03/29/23 08:36:22.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:22.837
Mar 29 08:36:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename aggregator 03/29/23 08:36:22.838
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:22.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:22.845
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 29 08:36:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/29/23 08:36:22.847
Mar 29 08:36:23.104: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar 29 08:36:25.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:27.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:29.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:31.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:33.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:35.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:37.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:39.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:41.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:43.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:45.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 08:36:47.242: INFO: Waited 110.247234ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/29/23 08:36:47.262
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/29/23 08:36:47.263
STEP: List APIServices 03/29/23 08:36:47.267
Mar 29 08:36:47.270: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar 29 08:36:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1937" for this suite. 03/29/23 08:36:47.509
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":182,"skipped":3202,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.860 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:22.837
    Mar 29 08:36:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename aggregator 03/29/23 08:36:22.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:22.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:22.845
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 29 08:36:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/29/23 08:36:22.847
    Mar 29 08:36:23.104: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Mar 29 08:36:25.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:27.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:29.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:31.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:33.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:35.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:37.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:39.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:41.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:43.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:45.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 36, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 08:36:47.242: INFO: Waited 110.247234ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/29/23 08:36:47.262
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/29/23 08:36:47.263
    STEP: List APIServices 03/29/23 08:36:47.267
    Mar 29 08:36:47.270: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar 29 08:36:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-1937" for this suite. 03/29/23 08:36:47.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:36:47.698
Mar 29 08:36:47.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-runtime 03/29/23 08:36:47.699
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:48.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:48.713
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/29/23 08:36:48.718
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/29/23 08:37:05.754
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/29/23 08:37:05.755
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/29/23 08:37:05.758
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/29/23 08:37:05.758
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/29/23 08:37:05.769
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/29/23 08:37:08.776
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/29/23 08:37:10.781
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/29/23 08:37:10.784
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/29/23 08:37:10.784
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/29/23 08:37:10.794
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/29/23 08:37:11.797
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/29/23 08:37:14.804
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/29/23 08:37:14.806
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/29/23 08:37:14.806
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 29 08:37:14.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7304" for this suite. 03/29/23 08:37:14.817
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":183,"skipped":3225,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.121 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:36:47.698
    Mar 29 08:36:47.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-runtime 03/29/23 08:36:47.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:36:48.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:36:48.713
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/29/23 08:36:48.718
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/29/23 08:37:05.754
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/29/23 08:37:05.755
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/29/23 08:37:05.758
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/29/23 08:37:05.758
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/29/23 08:37:05.769
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/29/23 08:37:08.776
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/29/23 08:37:10.781
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/29/23 08:37:10.784
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/29/23 08:37:10.784
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/29/23 08:37:10.794
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/29/23 08:37:11.797
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/29/23 08:37:14.804
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/29/23 08:37:14.806
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/29/23 08:37:14.806
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 29 08:37:14.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7304" for this suite. 03/29/23 08:37:14.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:14.821
Mar 29 08:37:14.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:37:14.821
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:14.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:14.829
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/29/23 08:37:14.83
STEP: submitting the pod to kubernetes 03/29/23 08:37:14.83
STEP: verifying QOS class is set on the pod 03/29/23 08:37:14.833
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar 29 08:37:14.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1922" for this suite. 03/29/23 08:37:14.837
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":184,"skipped":3253,"failed":0}
------------------------------
â€¢ [0.019 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:14.821
    Mar 29 08:37:14.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:37:14.821
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:14.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:14.829
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/29/23 08:37:14.83
    STEP: submitting the pod to kubernetes 03/29/23 08:37:14.83
    STEP: verifying QOS class is set on the pod 03/29/23 08:37:14.833
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar 29 08:37:14.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1922" for this suite. 03/29/23 08:37:14.837
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:14.84
Mar 29 08:37:14.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename endpointslice 03/29/23 08:37:14.84
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:14.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:14.899
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 29 08:37:18.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2763" for this suite. 03/29/23 08:37:18.923
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":185,"skipped":3257,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:14.84
    Mar 29 08:37:14.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename endpointslice 03/29/23 08:37:14.84
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:14.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:14.899
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 29 08:37:18.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2763" for this suite. 03/29/23 08:37:18.923
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:18.925
Mar 29 08:37:18.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:37:18.926
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.933
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/29/23 08:37:18.934
STEP: Getting a ResourceQuota 03/29/23 08:37:18.936
STEP: Updating a ResourceQuota 03/29/23 08:37:18.937
STEP: Verifying a ResourceQuota was modified 03/29/23 08:37:18.941
STEP: Deleting a ResourceQuota 03/29/23 08:37:18.942
STEP: Verifying the deleted ResourceQuota 03/29/23 08:37:18.944
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:37:18.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9997" for this suite. 03/29/23 08:37:18.947
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":186,"skipped":3257,"failed":0}
------------------------------
â€¢ [0.024 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:18.925
    Mar 29 08:37:18.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:37:18.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.933
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/29/23 08:37:18.934
    STEP: Getting a ResourceQuota 03/29/23 08:37:18.936
    STEP: Updating a ResourceQuota 03/29/23 08:37:18.937
    STEP: Verifying a ResourceQuota was modified 03/29/23 08:37:18.941
    STEP: Deleting a ResourceQuota 03/29/23 08:37:18.942
    STEP: Verifying the deleted ResourceQuota 03/29/23 08:37:18.944
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:37:18.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9997" for this suite. 03/29/23 08:37:18.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:18.95
Mar 29 08:37:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename podtemplate 03/29/23 08:37:18.951
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.957
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 29 08:37:18.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-317" for this suite. 03/29/23 08:37:18.97
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":187,"skipped":3289,"failed":0}
------------------------------
â€¢ [0.023 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:18.95
    Mar 29 08:37:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename podtemplate 03/29/23 08:37:18.951
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.957
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 29 08:37:18.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-317" for this suite. 03/29/23 08:37:18.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:18.974
Mar 29 08:37:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:37:18.974
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.98
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/29/23 08:37:18.984
Mar 29 08:37:18.984: INFO: Creating simple deployment test-deployment-t8zmw
Mar 29 08:37:18.990: INFO: deployment "test-deployment-t8zmw" doesn't have the required revision set
STEP: Getting /status 03/29/23 08:37:20.996
Mar 29 08:37:20.998: INFO: Deployment test-deployment-t8zmw has Conditions: [{Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/29/23 08:37:20.998
Mar 29 08:37:21.003: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 37, 18, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-t8zmw-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/29/23 08:37:21.003
Mar 29 08:37:21.004: INFO: Observed &Deployment event: ADDED
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t8zmw-777898ffcc" is progressing.}
Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
Mar 29 08:37:21.004: INFO: Found Deployment test-deployment-t8zmw in namespace deployment-1266 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 29 08:37:21.004: INFO: Deployment test-deployment-t8zmw has an updated status
STEP: patching the Statefulset Status 03/29/23 08:37:21.004
Mar 29 08:37:21.004: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 29 08:37:21.007: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/29/23 08:37:21.007
Mar 29 08:37:21.008: INFO: Observed &Deployment event: ADDED
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t8zmw-777898ffcc" is progressing.}
Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
Mar 29 08:37:21.008: INFO: Found deployment test-deployment-t8zmw in namespace deployment-1266 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 29 08:37:21.008: INFO: Deployment test-deployment-t8zmw has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:37:21.012: INFO: Deployment "test-deployment-t8zmw":
&Deployment{ObjectMeta:{test-deployment-t8zmw  deployment-1266  6936e8ba-9bd8-45dd-90c6-42b523de67aa 18548 1 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-29 08:37:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6b8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 08:37:21.013: INFO: New ReplicaSet "test-deployment-t8zmw-777898ffcc" of Deployment "test-deployment-t8zmw":
&ReplicaSet{ObjectMeta:{test-deployment-t8zmw-777898ffcc  deployment-1266  2acd200e-1e62-4456-86d8-8d5ccae7a4f0 18527 1 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-t8zmw 6936e8ba-9bd8-45dd-90c6-42b523de67aa 0xc00049ebb0 0xc00049ebb1}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6936e8ba-9bd8-45dd-90c6-42b523de67aa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00049ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:37:21.014: INFO: Pod "test-deployment-t8zmw-777898ffcc-hw2dc" is available:
&Pod{ObjectMeta:{test-deployment-t8zmw-777898ffcc-hw2dc test-deployment-t8zmw-777898ffcc- deployment-1266  10d7f170-35c1-4922-902e-d065d94a9547 18524 0 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:451cc6eca9cba2e5c439301545ebd31a742f352952daaa6aeae60b18c1a9a5b3 cni.projectcalico.org/podIP:192.168.219.146/32 cni.projectcalico.org/podIPs:192.168.219.146/32] [{apps/v1 ReplicaSet test-deployment-t8zmw-777898ffcc 2acd200e-1e62-4456-86d8-8d5ccae7a4f0 0xc003d71a80 0xc003d71a81}] [] [{kube-controller-manager Update v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2acd200e-1e62-4456-86d8-8d5ccae7a4f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwmr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwmr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.146,StartTime:2023-03-29 08:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:37:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6755fb902c92f426a5a258717ef03558f173d6043ca99f6e3387304f783303b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:37:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1266" for this suite. 03/29/23 08:37:21.016
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":188,"skipped":3307,"failed":0}
------------------------------
â€¢ [2.045 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:18.974
    Mar 29 08:37:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:37:18.974
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:18.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:18.98
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/29/23 08:37:18.984
    Mar 29 08:37:18.984: INFO: Creating simple deployment test-deployment-t8zmw
    Mar 29 08:37:18.990: INFO: deployment "test-deployment-t8zmw" doesn't have the required revision set
    STEP: Getting /status 03/29/23 08:37:20.996
    Mar 29 08:37:20.998: INFO: Deployment test-deployment-t8zmw has Conditions: [{Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/29/23 08:37:20.998
    Mar 29 08:37:21.003: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 8, 37, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 8, 37, 18, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-t8zmw-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/29/23 08:37:21.003
    Mar 29 08:37:21.004: INFO: Observed &Deployment event: ADDED
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
    Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t8zmw-777898ffcc" is progressing.}
    Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
    Mar 29 08:37:21.004: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 29 08:37:21.004: INFO: Observed Deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
    Mar 29 08:37:21.004: INFO: Found Deployment test-deployment-t8zmw in namespace deployment-1266 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 29 08:37:21.004: INFO: Deployment test-deployment-t8zmw has an updated status
    STEP: patching the Statefulset Status 03/29/23 08:37:21.004
    Mar 29 08:37:21.004: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 29 08:37:21.007: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/29/23 08:37:21.007
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: ADDED
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t8zmw-777898ffcc"}
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:18 +0000 UTC 2023-03-29 08:37:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t8zmw-777898ffcc" is progressing.}
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-29 08:37:19 +0000 UTC 2023-03-29 08:37:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t8zmw-777898ffcc" has successfully progressed.}
    Mar 29 08:37:21.008: INFO: Observed deployment test-deployment-t8zmw in namespace deployment-1266 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 29 08:37:21.008: INFO: Observed &Deployment event: MODIFIED
    Mar 29 08:37:21.008: INFO: Found deployment test-deployment-t8zmw in namespace deployment-1266 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 29 08:37:21.008: INFO: Deployment test-deployment-t8zmw has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:37:21.012: INFO: Deployment "test-deployment-t8zmw":
    &Deployment{ObjectMeta:{test-deployment-t8zmw  deployment-1266  6936e8ba-9bd8-45dd-90c6-42b523de67aa 18548 1 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-29 08:37:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6b8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 29 08:37:21.013: INFO: New ReplicaSet "test-deployment-t8zmw-777898ffcc" of Deployment "test-deployment-t8zmw":
    &ReplicaSet{ObjectMeta:{test-deployment-t8zmw-777898ffcc  deployment-1266  2acd200e-1e62-4456-86d8-8d5ccae7a4f0 18527 1 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-t8zmw 6936e8ba-9bd8-45dd-90c6-42b523de67aa 0xc00049ebb0 0xc00049ebb1}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6936e8ba-9bd8-45dd-90c6-42b523de67aa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00049ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:37:21.014: INFO: Pod "test-deployment-t8zmw-777898ffcc-hw2dc" is available:
    &Pod{ObjectMeta:{test-deployment-t8zmw-777898ffcc-hw2dc test-deployment-t8zmw-777898ffcc- deployment-1266  10d7f170-35c1-4922-902e-d065d94a9547 18524 0 2023-03-29 08:37:18 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:451cc6eca9cba2e5c439301545ebd31a742f352952daaa6aeae60b18c1a9a5b3 cni.projectcalico.org/podIP:192.168.219.146/32 cni.projectcalico.org/podIPs:192.168.219.146/32] [{apps/v1 ReplicaSet test-deployment-t8zmw-777898ffcc 2acd200e-1e62-4456-86d8-8d5ccae7a4f0 0xc003d71a80 0xc003d71a81}] [] [{kube-controller-manager Update v1 2023-03-29 08:37:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2acd200e-1e62-4456-86d8-8d5ccae7a4f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwmr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwmr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:37:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.146,StartTime:2023-03-29 08:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:37:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6755fb902c92f426a5a258717ef03558f173d6043ca99f6e3387304f783303b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:37:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1266" for this suite. 03/29/23 08:37:21.016
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:21.019
Mar 29 08:37:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename events 03/29/23 08:37:21.02
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.026
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/29/23 08:37:21.028
STEP: listing events in all namespaces 03/29/23 08:37:21.03
STEP: listing events in test namespace 03/29/23 08:37:21.033
STEP: listing events with field selection filtering on source 03/29/23 08:37:21.034
STEP: listing events with field selection filtering on reportingController 03/29/23 08:37:21.035
STEP: getting the test event 03/29/23 08:37:21.037
STEP: patching the test event 03/29/23 08:37:21.038
STEP: getting the test event 03/29/23 08:37:21.04
STEP: updating the test event 03/29/23 08:37:21.041
STEP: getting the test event 03/29/23 08:37:21.044
STEP: deleting the test event 03/29/23 08:37:21.045
STEP: listing events in all namespaces 03/29/23 08:37:21.047
STEP: listing events in test namespace 03/29/23 08:37:21.05
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 29 08:37:21.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7516" for this suite. 03/29/23 08:37:21.053
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":189,"skipped":3310,"failed":0}
------------------------------
â€¢ [0.035 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:21.019
    Mar 29 08:37:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename events 03/29/23 08:37:21.02
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.026
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/29/23 08:37:21.028
    STEP: listing events in all namespaces 03/29/23 08:37:21.03
    STEP: listing events in test namespace 03/29/23 08:37:21.033
    STEP: listing events with field selection filtering on source 03/29/23 08:37:21.034
    STEP: listing events with field selection filtering on reportingController 03/29/23 08:37:21.035
    STEP: getting the test event 03/29/23 08:37:21.037
    STEP: patching the test event 03/29/23 08:37:21.038
    STEP: getting the test event 03/29/23 08:37:21.04
    STEP: updating the test event 03/29/23 08:37:21.041
    STEP: getting the test event 03/29/23 08:37:21.044
    STEP: deleting the test event 03/29/23 08:37:21.045
    STEP: listing events in all namespaces 03/29/23 08:37:21.047
    STEP: listing events in test namespace 03/29/23 08:37:21.05
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 29 08:37:21.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7516" for this suite. 03/29/23 08:37:21.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:21.055
Mar 29 08:37:21.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:37:21.056
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.064
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/29/23 08:37:21.065
Mar 29 08:37:21.065: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7494 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/29/23 08:37:21.099
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:37:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7494" for this suite. 03/29/23 08:37:21.105
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":190,"skipped":3321,"failed":0}
------------------------------
â€¢ [0.052 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:21.055
    Mar 29 08:37:21.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:37:21.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.064
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/29/23 08:37:21.065
    Mar 29 08:37:21.065: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-7494 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/29/23 08:37:21.099
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:37:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7494" for this suite. 03/29/23 08:37:21.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:37:21.107
Mar 29 08:37:21.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:37:21.109
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.117
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 29 08:37:21.123: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 08:38:21.135: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:38:21.136
Mar 29 08:38:21.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption-path 03/29/23 08:38:21.137
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:21.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:21.144
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar 29 08:38:21.152: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 29 08:38:21.153: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar 29 08:38:21.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8369" for this suite. 03/29/23 08:38:21.162
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:38:21.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3579" for this suite. 03/29/23 08:38:21.169
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":191,"skipped":3327,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.086 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:37:21.107
    Mar 29 08:37:21.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:37:21.109
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:37:21.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:37:21.117
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 29 08:37:21.123: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 08:38:21.135: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:38:21.136
    Mar 29 08:38:21.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption-path 03/29/23 08:38:21.137
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:21.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:21.144
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar 29 08:38:21.152: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 29 08:38:21.153: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar 29 08:38:21.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8369" for this suite. 03/29/23 08:38:21.162
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:38:21.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3579" for this suite. 03/29/23 08:38:21.169
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:38:21.195
Mar 29 08:38:21.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:38:21.196
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:21.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:21.202
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:38:21.204
Mar 29 08:38:21.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877" in namespace "projected-2046" to be "Succeeded or Failed"
Mar 29 08:38:21.209: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Pending", Reason="", readiness=false. Elapsed: 1.228084ms
Mar 29 08:38:23.212: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00434119s
Mar 29 08:38:25.211: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003814681s
STEP: Saw pod success 03/29/23 08:38:25.211
Mar 29 08:38:25.211: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877" satisfied condition "Succeeded or Failed"
Mar 29 08:38:25.213: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 container client-container: <nil>
STEP: delete the pod 03/29/23 08:38:25.222
Mar 29 08:38:25.229: INFO: Waiting for pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 to disappear
Mar 29 08:38:25.231: INFO: Pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:38:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2046" for this suite. 03/29/23 08:38:25.233
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":192,"skipped":3364,"failed":0}
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:38:21.195
    Mar 29 08:38:21.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:38:21.196
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:21.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:21.202
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:38:21.204
    Mar 29 08:38:21.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877" in namespace "projected-2046" to be "Succeeded or Failed"
    Mar 29 08:38:21.209: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Pending", Reason="", readiness=false. Elapsed: 1.228084ms
    Mar 29 08:38:23.212: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00434119s
    Mar 29 08:38:25.211: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003814681s
    STEP: Saw pod success 03/29/23 08:38:25.211
    Mar 29 08:38:25.211: INFO: Pod "downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877" satisfied condition "Succeeded or Failed"
    Mar 29 08:38:25.213: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:38:25.222
    Mar 29 08:38:25.229: INFO: Waiting for pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 to disappear
    Mar 29 08:38:25.231: INFO: Pod downwardapi-volume-9ca1f058-95c1-4bb5-851c-90c7a87ba877 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:38:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2046" for this suite. 03/29/23 08:38:25.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:38:25.238
Mar 29 08:38:25.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename endpointslice 03/29/23 08:38:25.239
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:25.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:25.246
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/29/23 08:38:30.279
STEP: referencing matching pods with named port 03/29/23 08:38:35.283
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/29/23 08:38:40.288
STEP: recreating EndpointSlices after they've been deleted 03/29/23 08:38:45.292
Mar 29 08:38:45.300: INFO: EndpointSlice for Service endpointslice-7183/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 29 08:38:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7183" for this suite. 03/29/23 08:38:55.308
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":193,"skipped":3382,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.073 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:38:25.238
    Mar 29 08:38:25.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename endpointslice 03/29/23 08:38:25.239
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:25.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:25.246
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/29/23 08:38:30.279
    STEP: referencing matching pods with named port 03/29/23 08:38:35.283
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/29/23 08:38:40.288
    STEP: recreating EndpointSlices after they've been deleted 03/29/23 08:38:45.292
    Mar 29 08:38:45.300: INFO: EndpointSlice for Service endpointslice-7183/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 29 08:38:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7183" for this suite. 03/29/23 08:38:55.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:38:55.311
Mar 29 08:38:55.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:38:55.312
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:55.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:55.32
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-a1ea33e2-1255-47cd-a36d-5f1246582c94 03/29/23 08:38:55.321
STEP: Creating a pod to test consume configMaps 03/29/23 08:38:55.323
Mar 29 08:38:55.326: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2" in namespace "projected-877" to be "Succeeded or Failed"
Mar 29 08:38:55.328: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.223864ms
Mar 29 08:38:57.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003693165s
Mar 29 08:38:59.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621869s
STEP: Saw pod success 03/29/23 08:38:59.33
Mar 29 08:38:59.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2" satisfied condition "Succeeded or Failed"
Mar 29 08:38:59.331: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:38:59.334
Mar 29 08:38:59.340: INFO: Waiting for pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 to disappear
Mar 29 08:38:59.341: INFO: Pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:38:59.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-877" for this suite. 03/29/23 08:38:59.343
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":194,"skipped":3391,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:38:55.311
    Mar 29 08:38:55.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:38:55.312
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:55.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:55.32
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-a1ea33e2-1255-47cd-a36d-5f1246582c94 03/29/23 08:38:55.321
    STEP: Creating a pod to test consume configMaps 03/29/23 08:38:55.323
    Mar 29 08:38:55.326: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2" in namespace "projected-877" to be "Succeeded or Failed"
    Mar 29 08:38:55.328: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.223864ms
    Mar 29 08:38:57.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003693165s
    Mar 29 08:38:59.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621869s
    STEP: Saw pod success 03/29/23 08:38:59.33
    Mar 29 08:38:59.330: INFO: Pod "pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2" satisfied condition "Succeeded or Failed"
    Mar 29 08:38:59.331: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:38:59.334
    Mar 29 08:38:59.340: INFO: Waiting for pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 to disappear
    Mar 29 08:38:59.341: INFO: Pod pod-projected-configmaps-c618074c-7534-40f0-b020-c927440aafa2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:38:59.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-877" for this suite. 03/29/23 08:38:59.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:38:59.347
Mar 29 08:38:59.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:38:59.347
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:59.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:59.354
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/29/23 08:38:59.356
Mar 29 08:38:59.359: INFO: created test-pod-1
Mar 29 08:38:59.363: INFO: created test-pod-2
Mar 29 08:38:59.366: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/29/23 08:38:59.366
Mar 29 08:38:59.366: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-690' to be running and ready
Mar 29 08:38:59.372: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 29 08:38:59.372: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 29 08:38:59.372: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 29 08:38:59.372: INFO: 0 / 3 pods in namespace 'pods-690' are running and ready (0 seconds elapsed)
Mar 29 08:38:59.372: INFO: expected 0 pod replicas in namespace 'pods-690', 0 are Running and Ready.
Mar 29 08:38:59.372: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Mar 29 08:38:59.372: INFO: test-pod-1  10.146.0.116  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
Mar 29 08:38:59.372: INFO: test-pod-2  10.146.0.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
Mar 29 08:38:59.372: INFO: test-pod-3  10.146.0.116  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
Mar 29 08:38:59.372: INFO: 
Mar 29 08:39:01.376: INFO: 3 / 3 pods in namespace 'pods-690' are running and ready (2 seconds elapsed)
Mar 29 08:39:01.376: INFO: expected 0 pod replicas in namespace 'pods-690', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/29/23 08:39:01.387
Mar 29 08:39:01.389: INFO: Pod quantity 3 is different from expected quantity 0
Mar 29 08:39:02.391: INFO: Pod quantity 3 is different from expected quantity 0
Mar 29 08:39:03.391: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 08:39:04.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-690" for this suite. 03/29/23 08:39:04.392
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":195,"skipped":3455,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:38:59.347
    Mar 29 08:38:59.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:38:59.347
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:38:59.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:38:59.354
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/29/23 08:38:59.356
    Mar 29 08:38:59.359: INFO: created test-pod-1
    Mar 29 08:38:59.363: INFO: created test-pod-2
    Mar 29 08:38:59.366: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/29/23 08:38:59.366
    Mar 29 08:38:59.366: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-690' to be running and ready
    Mar 29 08:38:59.372: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 29 08:38:59.372: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 29 08:38:59.372: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 29 08:38:59.372: INFO: 0 / 3 pods in namespace 'pods-690' are running and ready (0 seconds elapsed)
    Mar 29 08:38:59.372: INFO: expected 0 pod replicas in namespace 'pods-690', 0 are Running and Ready.
    Mar 29 08:38:59.372: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Mar 29 08:38:59.372: INFO: test-pod-1  10.146.0.116  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
    Mar 29 08:38:59.372: INFO: test-pod-2  10.146.0.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
    Mar 29 08:38:59.372: INFO: test-pod-3  10.146.0.116  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:38:59 +0000 UTC  }]
    Mar 29 08:38:59.372: INFO: 
    Mar 29 08:39:01.376: INFO: 3 / 3 pods in namespace 'pods-690' are running and ready (2 seconds elapsed)
    Mar 29 08:39:01.376: INFO: expected 0 pod replicas in namespace 'pods-690', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/29/23 08:39:01.387
    Mar 29 08:39:01.389: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 29 08:39:02.391: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 29 08:39:03.391: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 08:39:04.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-690" for this suite. 03/29/23 08:39:04.392
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:04.396
Mar 29 08:39:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:39:04.397
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:04.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:04.404
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-6014/secret-test-647a7817-60f6-49d9-9689-df57b2b3a85f 03/29/23 08:39:04.406
STEP: Creating a pod to test consume secrets 03/29/23 08:39:04.408
Mar 29 08:39:04.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4" in namespace "secrets-6014" to be "Succeeded or Failed"
Mar 29 08:39:04.414: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.836475ms
Mar 29 08:39:06.417: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004940074s
Mar 29 08:39:08.415: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003717204s
STEP: Saw pod success 03/29/23 08:39:08.415
Mar 29 08:39:08.415: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4" satisfied condition "Succeeded or Failed"
Mar 29 08:39:08.417: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 container env-test: <nil>
STEP: delete the pod 03/29/23 08:39:08.419
Mar 29 08:39:08.425: INFO: Waiting for pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 to disappear
Mar 29 08:39:08.427: INFO: Pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:39:08.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6014" for this suite. 03/29/23 08:39:08.428
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":196,"skipped":3456,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:04.396
    Mar 29 08:39:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:39:04.397
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:04.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:04.404
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-6014/secret-test-647a7817-60f6-49d9-9689-df57b2b3a85f 03/29/23 08:39:04.406
    STEP: Creating a pod to test consume secrets 03/29/23 08:39:04.408
    Mar 29 08:39:04.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4" in namespace "secrets-6014" to be "Succeeded or Failed"
    Mar 29 08:39:04.414: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.836475ms
    Mar 29 08:39:06.417: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004940074s
    Mar 29 08:39:08.415: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003717204s
    STEP: Saw pod success 03/29/23 08:39:08.415
    Mar 29 08:39:08.415: INFO: Pod "pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4" satisfied condition "Succeeded or Failed"
    Mar 29 08:39:08.417: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 container env-test: <nil>
    STEP: delete the pod 03/29/23 08:39:08.419
    Mar 29 08:39:08.425: INFO: Waiting for pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 to disappear
    Mar 29 08:39:08.427: INFO: Pod pod-configmaps-d2911834-a384-410e-8a3a-d2ef84128cb4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:39:08.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6014" for this suite. 03/29/23 08:39:08.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:08.432
Mar 29 08:39:08.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:39:08.433
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:08.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:08.44
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 29 08:39:08.442: INFO: Creating deployment "webserver-deployment"
Mar 29 08:39:08.445: INFO: Waiting for observed generation 1
Mar 29 08:39:10.448: INFO: Waiting for all required pods to come up
Mar 29 08:39:10.451: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/29/23 08:39:10.451
Mar 29 08:39:10.451: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wn5sb" in namespace "deployment-495" to be "running"
Mar 29 08:39:10.451: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mfwmw" in namespace "deployment-495" to be "running"
Mar 29 08:39:10.452: INFO: Pod "webserver-deployment-845c8977d9-wn5sb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36029ms
Mar 29 08:39:10.452: INFO: Pod "webserver-deployment-845c8977d9-mfwmw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.357181ms
Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-mfwmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.004254387s
Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-mfwmw" satisfied condition "running"
Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-wn5sb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004361117s
Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-wn5sb" satisfied condition "running"
Mar 29 08:39:12.455: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 29 08:39:12.457: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 29 08:39:12.463: INFO: Updating deployment webserver-deployment
Mar 29 08:39:12.463: INFO: Waiting for observed generation 2
Mar 29 08:39:14.465: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 29 08:39:14.475: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 29 08:39:14.477: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 29 08:39:14.482: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 29 08:39:14.482: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 29 08:39:14.483: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 29 08:39:14.486: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 29 08:39:14.486: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 29 08:39:14.492: INFO: Updating deployment webserver-deployment
Mar 29 08:39:14.492: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 29 08:39:14.495: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 29 08:39:14.497: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:39:16.503: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-495  82ad1cbc-4a74-49e6-a358-f8ea77a743da 19397 3 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d719a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-29 08:39:14 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-29 08:39:14 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 29 08:39:16.505: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-495  368fde90-6a68-48ef-8e92-ac9f9c02a7ad 19396 3 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 82ad1cbc-4a74-49e6-a358-f8ea77a743da 0xc003d71d87 0xc003d71d88}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82ad1cbc-4a74-49e6-a358-f8ea77a743da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d71e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:39:16.505: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 29 08:39:16.505: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-495  c751e20a-9130-446b-8853-8db1a7bc7d60 19365 3 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 82ad1cbc-4a74-49e6-a358-f8ea77a743da 0xc003d71e87 0xc003d71e88}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82ad1cbc-4a74-49e6-a358-f8ea77a743da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d71f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-6lh45" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6lh45 webserver-deployment-69b7448995- deployment-495  c28c6597-062f-4b89-9747-a909c6ed6990 19515 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:22dd85df342427c4314e305a185e5751467e0ad7e36da6a37388fa32d5e6738f cni.projectcalico.org/podIP:192.168.219.188/32 cni.projectcalico.org/podIPs:192.168.219.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28077 0xc003c28078}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv58s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv58s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-9skqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9skqr webserver-deployment-69b7448995- deployment-495  5e5abf2f-3342-4371-b66f-652031098f67 19267 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0aeb4f1a6442901ceef898c6b8079a88e672c08cd5a945a96552948594d3e541 cni.projectcalico.org/podIP:192.168.87.240/32 cni.projectcalico.org/podIPs:192.168.87.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28210 0xc003c28211}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgfqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgfqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-b849s" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-b849s webserver-deployment-69b7448995- deployment-495  c09da5a2-91fe-4a27-904e-65e237a592cd 19271 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:88c9ff23eb4e9149d8af1ef482bac6908415c4de79b1a93f485d0728888af14d cni.projectcalico.org/podIP:192.168.30.36/32 cni.projectcalico.org/podIPs:192.168.30.36/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28437 0xc003c28438}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wwrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wwrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-cwqdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cwqdr webserver-deployment-69b7448995- deployment-495  5073632f-44ce-45ba-8e81-78ba40801e8b 19512 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5cdd79450bc7e8ed84f7b7556ee278f6bc5f58d76d2f9b80e23927f385f3044a cni.projectcalico.org/podIP:192.168.219.160/32 cni.projectcalico.org/podIPs:192.168.219.160/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28637 0xc003c28638}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmw8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmw8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-hnfc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hnfc5 webserver-deployment-69b7448995- deployment-495  e11473bf-00b4-4709-a359-77e05a9920a2 19266 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:24f9fb5814cdb11d6ae80c1c63dba4cb56cada77ab5f880af31d4fb1c2bd09a9 cni.projectcalico.org/podIP:192.168.219.132/32 cni.projectcalico.org/podIPs:192.168.219.132/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28847 0xc003c28848}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4g4b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4g4b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-jcjp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jcjp8 webserver-deployment-69b7448995- deployment-495  7d3e1f5a-5cda-45a2-920e-8575cdce596b 19272 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b0da7e65e905b53b1741043e8bcbb383ccdb8a14a56a7f51cfa266ca4aa694e3 cni.projectcalico.org/podIP:192.168.219.186/32 cni.projectcalico.org/podIPs:192.168.219.186/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28a47 0xc003c28a48}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqjmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqjmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-lqrqg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lqrqg webserver-deployment-69b7448995- deployment-495  15f30957-d06a-41ef-b242-79ed3b78284b 19513 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:20c1b7da08cdf685a72adbf1ca7d3937f53511862013f1b23ca8171091c229ec cni.projectcalico.org/podIP:192.168.87.232/32 cni.projectcalico.org/podIPs:192.168.87.232/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28c67 0xc003c28c68}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqhfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqhfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-lwzm7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lwzm7 webserver-deployment-69b7448995- deployment-495  018c4cc9-eee8-407f-af20-339f3263fde6 19297 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:535565e090118a46aa330b60d6b5ff94a8d8a4da01b4119816fd74d51295f744 cni.projectcalico.org/podIP:192.168.30.43/32 cni.projectcalico.org/podIPs:192.168.30.43/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28eb7 0xc003c28eb8}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzpr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzpr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.43,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-mlptm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mlptm webserver-deployment-69b7448995- deployment-495  24b09101-c719-48af-b167-d3cc3bc9079f 19423 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b300ecdb6759e700ccd08c41d1f0307d36923c5d0a5869b97c373ab936f59a20 cni.projectcalico.org/podIP:192.168.219.180/32 cni.projectcalico.org/podIPs:192.168.219.180/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c290e7 0xc003c290e8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpmcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpmcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-mq8mm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mq8mm webserver-deployment-69b7448995- deployment-495  dac75880-6dfc-4d93-b312-1ed42f794563 19471 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:895411649f451c3384bad12d5ea2a90c5163dba8ee9655af15923d78bf89ad6e cni.projectcalico.org/podIP:192.168.87.229/32 cni.projectcalico.org/podIPs:192.168.87.229/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29307 0xc003c29308}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5f4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5f4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-w474s" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-w474s webserver-deployment-69b7448995- deployment-495  54598b3a-f8da-49a5-9df8-a3aee735e927 19465 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:c0cacf2ec3813ee2568ab0555fac6887a176a7f74f52d0a227983c0ef563dc20 cni.projectcalico.org/podIP:192.168.87.237/32 cni.projectcalico.org/podIPs:192.168.87.237/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29517 0xc003c29518}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdq9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdq9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-x77k2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-x77k2 webserver-deployment-69b7448995- deployment-495  136dcf57-ee45-406f-9ebf-eeaeb76914fe 19516 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1188f194cc85321c8f9a08f4a03c227bcd6bb3a8ddfa98e77c1ec1c0de192cc5 cni.projectcalico.org/podIP:192.168.30.59/32 cni.projectcalico.org/podIPs:192.168.30.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29747 0xc003c29748}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d547,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d547,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-z88tf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-z88tf webserver-deployment-69b7448995- deployment-495  0ddaa74d-f1e4-4829-9195-cfd79eaa1491 19446 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:495d9de723e48cde99c3c72efacbcd0899435ec5cc7484656a9464ed3aa93f31 cni.projectcalico.org/podIP:192.168.30.37/32 cni.projectcalico.org/podIPs:192.168.30.37/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29977 0xc003c29978}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hcrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hcrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-6s5h4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6s5h4 webserver-deployment-845c8977d9- deployment-495  9772a218-5f2f-4c9e-9de8-f5779e6345b8 19427 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99920b77f245e4c5cfdc7876a898917b063d8de17c4860937b1e4ca7870db182 cni.projectcalico.org/podIP:192.168.87.224/32 cni.projectcalico.org/podIPs:192.168.87.224/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29b87 0xc003c29b88}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq5bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq5bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-9d5xh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9d5xh webserver-deployment-845c8977d9- deployment-495  dd351100-aa9c-4df3-9ae3-afad6bed4ae9 19467 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3e01fac694dbff2a7f1cc5c355bf5257137810516eb9bb67c8909d99a922ab5f cni.projectcalico.org/podIP:192.168.219.183/32 cni.projectcalico.org/podIPs:192.168.219.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29d77 0xc003c29d78}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tcmjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tcmjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-9vw2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vw2p webserver-deployment-845c8977d9- deployment-495  6b575f6b-1368-4e26-bec9-cc529875642f 19495 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c1c551ae619d389af2df6978fb3cbdb9a493d6d123a32765e89c6bf859e7277 cni.projectcalico.org/podIP:192.168.219.137/32 cni.projectcalico.org/podIPs:192.168.219.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29f77 0xc003c29f78}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnz46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnz46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-bxf9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bxf9m webserver-deployment-845c8977d9- deployment-495  ebbccffb-5a38-4133-8cfd-7e0f90163fa2 19447 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:aec3dd1d9034093969762090eb656b38ab9d36bf3bcb527fd826facde64e8b83 cni.projectcalico.org/podIP:192.168.219.154/32 cni.projectcalico.org/podIPs:192.168.219.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e3a0 0xc00411e3a1}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdfc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdfc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-c8cs9" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-c8cs9 webserver-deployment-845c8977d9- deployment-495  8bca0b75-87b5-4dcc-ae52-420b098a78d9 19153 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:448115d4994978f9c95ddd9715e272244896413542da5f332d07cbef145d4665 cni.projectcalico.org/podIP:192.168.87.251/32 cni.projectcalico.org/podIPs:192.168.87.251/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e687 0xc00411e688}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgrrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgrrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.251,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2e769aba2d2a3d65b43a3390e8d28289e921276cb091becfdbc10570f1b1da11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-frbgw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-frbgw webserver-deployment-845c8977d9- deployment-495  c637f2cc-6436-40e5-bff3-f210016b08d0 19159 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:481e0d491e05622069258ea9a983aa14340d81a52c305ec68aa6014967f3ac09 cni.projectcalico.org/podIP:192.168.87.222/32 cni.projectcalico.org/podIPs:192.168.87.222/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e887 0xc00411e888}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8hvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8hvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.222,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9b227b6a871f1ca33a442d1cb3499f0975d3ff4f63c65ffd5bf83ce3d1a52eff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-gdsc9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gdsc9 webserver-deployment-845c8977d9- deployment-495  19132d0d-79f4-4f95-a919-bf5242d10d85 19472 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:97a8d9b31cf7b339b90515e92d11b51ede7f0d7dc3adda1f5ca361230de5e034 cni.projectcalico.org/podIP:192.168.30.46/32 cni.projectcalico.org/podIPs:192.168.30.46/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411eab7 0xc00411eab8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdzk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdzk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-gzpt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gzpt8 webserver-deployment-845c8977d9- deployment-495  e4278cb6-8787-44ec-8032-00c3bb05aa0c 19422 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:98cbb9d59e370fd654616c582b979d70faf6bf9795605655daba4984e19f573e cni.projectcalico.org/podIP:192.168.30.35/32 cni.projectcalico.org/podIPs:192.168.30.35/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411ecc7 0xc00411ecc8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vmht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vmht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-hj65q" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hj65q webserver-deployment-845c8977d9- deployment-495  3927442a-436a-4cb1-8ec9-c9df198f9f8a 19500 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:627f2f65fe69ddc3794e4214750e0f57dd46189a3c73ea558eee812ce5441593 cni.projectcalico.org/podIP:192.168.30.33/32 cni.projectcalico.org/podIPs:192.168.30.33/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411eef7 0xc00411eef8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtsc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtsc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-jsm77" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsm77 webserver-deployment-845c8977d9- deployment-495  02633656-f7d3-49d9-accc-17a1125096aa 19127 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0586c0b9b6734a4f2c4b73beb350ce9e39bbcff228f7c22bc0ffabd5b9a79c44 cni.projectcalico.org/podIP:192.168.30.53/32 cni.projectcalico.org/podIPs:192.168.30.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f107 0xc00411f108}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vswm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vswm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.53,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4ec0e79b7ef9c41fb7c1362e4f0af6a010a19071951e20ce3bec0c22f7eb1149,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-jtx5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jtx5s webserver-deployment-845c8977d9- deployment-495  ca177b6a-ea14-4d39-99e2-888db57121a7 19493 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5b6f8056b0ee537c2e3a47925f3f8e734bdda3c496342d96875704daf20ff183 cni.projectcalico.org/podIP:192.168.87.225/32 cni.projectcalico.org/podIPs:192.168.87.225/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f307 0xc00411f308}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lkn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lkn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-kddb4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kddb4 webserver-deployment-845c8977d9- deployment-495  f7a09c01-9600-4534-b124-56e7d2404519 19187 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c2470b2d48db0490b040a46687cc275b6bd64b068f7ce7a11622a26a8c04af8 cni.projectcalico.org/podIP:192.168.219.181/32 cni.projectcalico.org/podIPs:192.168.219.181/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f4f7 0xc00411f4f8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcf6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcf6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.181,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://864709069785427c6b2f2cdbf50df84f706409db61b71e4347ebb53a65e09b7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-rk4zb" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rk4zb webserver-deployment-845c8977d9- deployment-495  078a7e7a-ad45-4c25-a8b7-0849c3d403f6 19469 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:14ad0a8ced3b6ec3af9916958b42a3cfafb4db772ba2e16f24e92f492548159a cni.projectcalico.org/podIP:192.168.30.25/32 cni.projectcalico.org/podIPs:192.168.30.25/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f727 0xc00411f728}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps52c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps52c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-rstdn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rstdn webserver-deployment-845c8977d9- deployment-495  2e113f36-b63f-40dc-b0ff-acff39a1643b 19130 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6e7837bebb2b3cf4767d4c664f397a8b5aaa51dfc9fdc73b8738ff9b8cd87abd cni.projectcalico.org/podIP:192.168.30.49/32 cni.projectcalico.org/podIPs:192.168.30.49/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f937 0xc00411f938}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgchx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgchx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.49,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://49f73150f0cbd6d39225d93c31eedbd3b1bc3c6ed1676e0bba0e201a971e0a25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-sfgd8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfgd8 webserver-deployment-845c8977d9- deployment-495  b803cdf7-0714-41e0-8b76-6dad14f8e02b 19155 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2d8f593030d24e65c2638203128ab8fd4ed4016b69d84f47b4335020870ed8ef cni.projectcalico.org/podIP:192.168.87.241/32 cni.projectcalico.org/podIPs:192.168.87.241/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411fb37 0xc00411fb38}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dkm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dkm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.241,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://72ad5be66ef0de182a9053f371dc686573c8bb86bf178c3451b6a78f605fd671,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-vzpt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vzpt8 webserver-deployment-845c8977d9- deployment-495  eadfeadb-ffa8-4e48-a77e-fd316fe06791 19460 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4bd102c3cf1d73979596f840886d19c4690616a2c596ca7d6ea3116f415b06de cni.projectcalico.org/podIP:192.168.219.141/32 cni.projectcalico.org/podIPs:192.168.219.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411fd47 0xc00411fd48}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-db89k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-db89k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-wn5sb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wn5sb webserver-deployment-845c8977d9- deployment-495  1a9a61b8-d96e-4299-8cde-60a80499ce9b 19199 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:23bbd6726a10cd730b11e66f740ddc340a82daad9b8e60ed8e8a09d58a6a4a80 cni.projectcalico.org/podIP:192.168.219.179/32 cni.projectcalico.org/podIPs:192.168.219.179/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411ff37 0xc00411ff38}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vn4fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn4fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.179,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf9a971e9ca6c7c83f11c25ce52475127fe1ea1b9b305a71797cae436b37f9b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-wpqtg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wpqtg webserver-deployment-845c8977d9- deployment-495  e9124273-a44e-43f9-8878-9a1a3b029eb9 19358 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d44147 0xc003d44148}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mj588,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mj588,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-xgw6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgw6k webserver-deployment-845c8977d9- deployment-495  997a18fa-77ed-489f-8e2c-b29f00cc2dc2 19445 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e67c7a461963917e143764a7cf9e0da9a136199a240c99e1a27a913b187cb312 cni.projectcalico.org/podIP:192.168.87.234/32 cni.projectcalico.org/podIPs:192.168.87.234/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d442b0 0xc003d442b1}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqk7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqk7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-zhg5w" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zhg5w webserver-deployment-845c8977d9- deployment-495  4a0d175c-a33e-46f1-bfd9-659a755537e1 19160 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87b412a051842140ab01abb2b6a4e5f5766f3a5fe036f4bfd5ad55785f60b32d cni.projectcalico.org/podIP:192.168.30.58/32 cni.projectcalico.org/podIPs:192.168.30.58/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d444b7 0xc003d444b8}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrl4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrl4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.58,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e35bbb7f40aec3a979595179269cf742aea6f5ef83887f249343a3c38946351a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:39:16.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-495" for this suite. 03/29/23 08:39:16.519
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":197,"skipped":3480,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:08.432
    Mar 29 08:39:08.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:39:08.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:08.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:08.44
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 29 08:39:08.442: INFO: Creating deployment "webserver-deployment"
    Mar 29 08:39:08.445: INFO: Waiting for observed generation 1
    Mar 29 08:39:10.448: INFO: Waiting for all required pods to come up
    Mar 29 08:39:10.451: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/29/23 08:39:10.451
    Mar 29 08:39:10.451: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wn5sb" in namespace "deployment-495" to be "running"
    Mar 29 08:39:10.451: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mfwmw" in namespace "deployment-495" to be "running"
    Mar 29 08:39:10.452: INFO: Pod "webserver-deployment-845c8977d9-wn5sb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36029ms
    Mar 29 08:39:10.452: INFO: Pod "webserver-deployment-845c8977d9-mfwmw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.357181ms
    Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-mfwmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.004254387s
    Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-mfwmw" satisfied condition "running"
    Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-wn5sb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004361117s
    Mar 29 08:39:12.455: INFO: Pod "webserver-deployment-845c8977d9-wn5sb" satisfied condition "running"
    Mar 29 08:39:12.455: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 29 08:39:12.457: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 29 08:39:12.463: INFO: Updating deployment webserver-deployment
    Mar 29 08:39:12.463: INFO: Waiting for observed generation 2
    Mar 29 08:39:14.465: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 29 08:39:14.475: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 29 08:39:14.477: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 29 08:39:14.482: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 29 08:39:14.482: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 29 08:39:14.483: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 29 08:39:14.486: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 29 08:39:14.486: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 29 08:39:14.492: INFO: Updating deployment webserver-deployment
    Mar 29 08:39:14.492: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 29 08:39:14.495: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 29 08:39:14.497: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:39:16.503: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-495  82ad1cbc-4a74-49e6-a358-f8ea77a743da 19397 3 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d719a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-29 08:39:14 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-29 08:39:14 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 29 08:39:16.505: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-495  368fde90-6a68-48ef-8e92-ac9f9c02a7ad 19396 3 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 82ad1cbc-4a74-49e6-a358-f8ea77a743da 0xc003d71d87 0xc003d71d88}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82ad1cbc-4a74-49e6-a358-f8ea77a743da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d71e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:39:16.505: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 29 08:39:16.505: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-495  c751e20a-9130-446b-8853-8db1a7bc7d60 19365 3 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 82ad1cbc-4a74-49e6-a358-f8ea77a743da 0xc003d71e87 0xc003d71e88}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82ad1cbc-4a74-49e6-a358-f8ea77a743da\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d71f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-6lh45" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6lh45 webserver-deployment-69b7448995- deployment-495  c28c6597-062f-4b89-9747-a909c6ed6990 19515 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:22dd85df342427c4314e305a185e5751467e0ad7e36da6a37388fa32d5e6738f cni.projectcalico.org/podIP:192.168.219.188/32 cni.projectcalico.org/podIPs:192.168.219.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28077 0xc003c28078}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv58s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv58s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-9skqr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9skqr webserver-deployment-69b7448995- deployment-495  5e5abf2f-3342-4371-b66f-652031098f67 19267 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0aeb4f1a6442901ceef898c6b8079a88e672c08cd5a945a96552948594d3e541 cni.projectcalico.org/podIP:192.168.87.240/32 cni.projectcalico.org/podIPs:192.168.87.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28210 0xc003c28211}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgfqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgfqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-b849s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-b849s webserver-deployment-69b7448995- deployment-495  c09da5a2-91fe-4a27-904e-65e237a592cd 19271 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:88c9ff23eb4e9149d8af1ef482bac6908415c4de79b1a93f485d0728888af14d cni.projectcalico.org/podIP:192.168.30.36/32 cni.projectcalico.org/podIPs:192.168.30.36/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28437 0xc003c28438}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wwrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wwrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-cwqdr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cwqdr webserver-deployment-69b7448995- deployment-495  5073632f-44ce-45ba-8e81-78ba40801e8b 19512 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5cdd79450bc7e8ed84f7b7556ee278f6bc5f58d76d2f9b80e23927f385f3044a cni.projectcalico.org/podIP:192.168.219.160/32 cni.projectcalico.org/podIPs:192.168.219.160/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28637 0xc003c28638}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmw8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmw8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.509: INFO: Pod "webserver-deployment-69b7448995-hnfc5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hnfc5 webserver-deployment-69b7448995- deployment-495  e11473bf-00b4-4709-a359-77e05a9920a2 19266 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:24f9fb5814cdb11d6ae80c1c63dba4cb56cada77ab5f880af31d4fb1c2bd09a9 cni.projectcalico.org/podIP:192.168.219.132/32 cni.projectcalico.org/podIPs:192.168.219.132/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28847 0xc003c28848}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4g4b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4g4b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-jcjp8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jcjp8 webserver-deployment-69b7448995- deployment-495  7d3e1f5a-5cda-45a2-920e-8575cdce596b 19272 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b0da7e65e905b53b1741043e8bcbb383ccdb8a14a56a7f51cfa266ca4aa694e3 cni.projectcalico.org/podIP:192.168.219.186/32 cni.projectcalico.org/podIPs:192.168.219.186/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28a47 0xc003c28a48}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqjmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqjmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-lqrqg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lqrqg webserver-deployment-69b7448995- deployment-495  15f30957-d06a-41ef-b242-79ed3b78284b 19513 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:20c1b7da08cdf685a72adbf1ca7d3937f53511862013f1b23ca8171091c229ec cni.projectcalico.org/podIP:192.168.87.232/32 cni.projectcalico.org/podIPs:192.168.87.232/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28c67 0xc003c28c68}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqhfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqhfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-lwzm7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lwzm7 webserver-deployment-69b7448995- deployment-495  018c4cc9-eee8-407f-af20-339f3263fde6 19297 0 2023-03-29 08:39:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:535565e090118a46aa330b60d6b5ff94a8d8a4da01b4119816fd74d51295f744 cni.projectcalico.org/podIP:192.168.30.43/32 cni.projectcalico.org/podIPs:192.168.30.43/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c28eb7 0xc003c28eb8}] [] [{calico Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzpr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzpr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.43,StartTime:2023-03-29 08:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-mlptm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mlptm webserver-deployment-69b7448995- deployment-495  24b09101-c719-48af-b167-d3cc3bc9079f 19423 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b300ecdb6759e700ccd08c41d1f0307d36923c5d0a5869b97c373ab936f59a20 cni.projectcalico.org/podIP:192.168.219.180/32 cni.projectcalico.org/podIPs:192.168.219.180/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c290e7 0xc003c290e8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpmcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpmcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-mq8mm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mq8mm webserver-deployment-69b7448995- deployment-495  dac75880-6dfc-4d93-b312-1ed42f794563 19471 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:895411649f451c3384bad12d5ea2a90c5163dba8ee9655af15923d78bf89ad6e cni.projectcalico.org/podIP:192.168.87.229/32 cni.projectcalico.org/podIPs:192.168.87.229/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29307 0xc003c29308}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5f4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5f4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-w474s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-w474s webserver-deployment-69b7448995- deployment-495  54598b3a-f8da-49a5-9df8-a3aee735e927 19465 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:c0cacf2ec3813ee2568ab0555fac6887a176a7f74f52d0a227983c0ef563dc20 cni.projectcalico.org/podIP:192.168.87.237/32 cni.projectcalico.org/podIPs:192.168.87.237/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29517 0xc003c29518}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdq9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdq9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-x77k2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-x77k2 webserver-deployment-69b7448995- deployment-495  136dcf57-ee45-406f-9ebf-eeaeb76914fe 19516 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1188f194cc85321c8f9a08f4a03c227bcd6bb3a8ddfa98e77c1ec1c0de192cc5 cni.projectcalico.org/podIP:192.168.30.59/32 cni.projectcalico.org/podIPs:192.168.30.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29747 0xc003c29748}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d547,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d547,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.510: INFO: Pod "webserver-deployment-69b7448995-z88tf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-z88tf webserver-deployment-69b7448995- deployment-495  0ddaa74d-f1e4-4829-9195-cfd79eaa1491 19446 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:495d9de723e48cde99c3c72efacbcd0899435ec5cc7484656a9464ed3aa93f31 cni.projectcalico.org/podIP:192.168.30.37/32 cni.projectcalico.org/podIPs:192.168.30.37/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 368fde90-6a68-48ef-8e92-ac9f9c02a7ad 0xc003c29977 0xc003c29978}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368fde90-6a68-48ef-8e92-ac9f9c02a7ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hcrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hcrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-6s5h4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6s5h4 webserver-deployment-845c8977d9- deployment-495  9772a218-5f2f-4c9e-9de8-f5779e6345b8 19427 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99920b77f245e4c5cfdc7876a898917b063d8de17c4860937b1e4ca7870db182 cni.projectcalico.org/podIP:192.168.87.224/32 cni.projectcalico.org/podIPs:192.168.87.224/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29b87 0xc003c29b88}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zq5bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zq5bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-9d5xh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9d5xh webserver-deployment-845c8977d9- deployment-495  dd351100-aa9c-4df3-9ae3-afad6bed4ae9 19467 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3e01fac694dbff2a7f1cc5c355bf5257137810516eb9bb67c8909d99a922ab5f cni.projectcalico.org/podIP:192.168.219.183/32 cni.projectcalico.org/podIPs:192.168.219.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29d77 0xc003c29d78}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tcmjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tcmjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-9vw2p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vw2p webserver-deployment-845c8977d9- deployment-495  6b575f6b-1368-4e26-bec9-cc529875642f 19495 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c1c551ae619d389af2df6978fb3cbdb9a493d6d123a32765e89c6bf859e7277 cni.projectcalico.org/podIP:192.168.219.137/32 cni.projectcalico.org/podIPs:192.168.219.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003c29f77 0xc003c29f78}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnz46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnz46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-bxf9m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bxf9m webserver-deployment-845c8977d9- deployment-495  ebbccffb-5a38-4133-8cfd-7e0f90163fa2 19447 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:aec3dd1d9034093969762090eb656b38ab9d36bf3bcb527fd826facde64e8b83 cni.projectcalico.org/podIP:192.168.219.154/32 cni.projectcalico.org/podIPs:192.168.219.154/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e3a0 0xc00411e3a1}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdfc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdfc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-c8cs9" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-c8cs9 webserver-deployment-845c8977d9- deployment-495  8bca0b75-87b5-4dcc-ae52-420b098a78d9 19153 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:448115d4994978f9c95ddd9715e272244896413542da5f332d07cbef145d4665 cni.projectcalico.org/podIP:192.168.87.251/32 cni.projectcalico.org/podIPs:192.168.87.251/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e687 0xc00411e688}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgrrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgrrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.251,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2e769aba2d2a3d65b43a3390e8d28289e921276cb091becfdbc10570f1b1da11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-frbgw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-frbgw webserver-deployment-845c8977d9- deployment-495  c637f2cc-6436-40e5-bff3-f210016b08d0 19159 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:481e0d491e05622069258ea9a983aa14340d81a52c305ec68aa6014967f3ac09 cni.projectcalico.org/podIP:192.168.87.222/32 cni.projectcalico.org/podIPs:192.168.87.222/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411e887 0xc00411e888}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8hvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8hvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.222,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9b227b6a871f1ca33a442d1cb3499f0975d3ff4f63c65ffd5bf83ce3d1a52eff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-gdsc9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gdsc9 webserver-deployment-845c8977d9- deployment-495  19132d0d-79f4-4f95-a919-bf5242d10d85 19472 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:97a8d9b31cf7b339b90515e92d11b51ede7f0d7dc3adda1f5ca361230de5e034 cni.projectcalico.org/podIP:192.168.30.46/32 cni.projectcalico.org/podIPs:192.168.30.46/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411eab7 0xc00411eab8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdzk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdzk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.511: INFO: Pod "webserver-deployment-845c8977d9-gzpt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gzpt8 webserver-deployment-845c8977d9- deployment-495  e4278cb6-8787-44ec-8032-00c3bb05aa0c 19422 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:98cbb9d59e370fd654616c582b979d70faf6bf9795605655daba4984e19f573e cni.projectcalico.org/podIP:192.168.30.35/32 cni.projectcalico.org/podIPs:192.168.30.35/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411ecc7 0xc00411ecc8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vmht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vmht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-hj65q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hj65q webserver-deployment-845c8977d9- deployment-495  3927442a-436a-4cb1-8ec9-c9df198f9f8a 19500 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:627f2f65fe69ddc3794e4214750e0f57dd46189a3c73ea558eee812ce5441593 cni.projectcalico.org/podIP:192.168.30.33/32 cni.projectcalico.org/podIPs:192.168.30.33/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411eef7 0xc00411eef8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtsc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtsc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-jsm77" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsm77 webserver-deployment-845c8977d9- deployment-495  02633656-f7d3-49d9-accc-17a1125096aa 19127 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0586c0b9b6734a4f2c4b73beb350ce9e39bbcff228f7c22bc0ffabd5b9a79c44 cni.projectcalico.org/podIP:192.168.30.53/32 cni.projectcalico.org/podIPs:192.168.30.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f107 0xc00411f108}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vswm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vswm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.53,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4ec0e79b7ef9c41fb7c1362e4f0af6a010a19071951e20ce3bec0c22f7eb1149,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-jtx5s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jtx5s webserver-deployment-845c8977d9- deployment-495  ca177b6a-ea14-4d39-99e2-888db57121a7 19493 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5b6f8056b0ee537c2e3a47925f3f8e734bdda3c496342d96875704daf20ff183 cni.projectcalico.org/podIP:192.168.87.225/32 cni.projectcalico.org/podIPs:192.168.87.225/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f307 0xc00411f308}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lkn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lkn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-kddb4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kddb4 webserver-deployment-845c8977d9- deployment-495  f7a09c01-9600-4534-b124-56e7d2404519 19187 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c2470b2d48db0490b040a46687cc275b6bd64b068f7ce7a11622a26a8c04af8 cni.projectcalico.org/podIP:192.168.219.181/32 cni.projectcalico.org/podIPs:192.168.219.181/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f4f7 0xc00411f4f8}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcf6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcf6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.181,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://864709069785427c6b2f2cdbf50df84f706409db61b71e4347ebb53a65e09b7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-rk4zb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rk4zb webserver-deployment-845c8977d9- deployment-495  078a7e7a-ad45-4c25-a8b7-0849c3d403f6 19469 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:14ad0a8ced3b6ec3af9916958b42a3cfafb4db772ba2e16f24e92f492548159a cni.projectcalico.org/podIP:192.168.30.25/32 cni.projectcalico.org/podIPs:192.168.30.25/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f727 0xc00411f728}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps52c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps52c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-rstdn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rstdn webserver-deployment-845c8977d9- deployment-495  2e113f36-b63f-40dc-b0ff-acff39a1643b 19130 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6e7837bebb2b3cf4767d4c664f397a8b5aaa51dfc9fdc73b8738ff9b8cd87abd cni.projectcalico.org/podIP:192.168.30.49/32 cni.projectcalico.org/podIPs:192.168.30.49/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411f937 0xc00411f938}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgchx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgchx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.49,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://49f73150f0cbd6d39225d93c31eedbd3b1bc3c6ed1676e0bba0e201a971e0a25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.512: INFO: Pod "webserver-deployment-845c8977d9-sfgd8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfgd8 webserver-deployment-845c8977d9- deployment-495  b803cdf7-0714-41e0-8b76-6dad14f8e02b 19155 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2d8f593030d24e65c2638203128ab8fd4ed4016b69d84f47b4335020870ed8ef cni.projectcalico.org/podIP:192.168.87.241/32 cni.projectcalico.org/podIPs:192.168.87.241/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411fb37 0xc00411fb38}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dkm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dkm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.241,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://72ad5be66ef0de182a9053f371dc686573c8bb86bf178c3451b6a78f605fd671,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-vzpt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vzpt8 webserver-deployment-845c8977d9- deployment-495  eadfeadb-ffa8-4e48-a77e-fd316fe06791 19460 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4bd102c3cf1d73979596f840886d19c4690616a2c596ca7d6ea3116f415b06de cni.projectcalico.org/podIP:192.168.219.141/32 cni.projectcalico.org/podIPs:192.168.219.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411fd47 0xc00411fd48}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-db89k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-db89k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-wn5sb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wn5sb webserver-deployment-845c8977d9- deployment-495  1a9a61b8-d96e-4299-8cde-60a80499ce9b 19199 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:23bbd6726a10cd730b11e66f740ddc340a82daad9b8e60ed8e8a09d58a6a4a80 cni.projectcalico.org/podIP:192.168.219.179/32 cni.projectcalico.org/podIPs:192.168.219.179/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc00411ff37 0xc00411ff38}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:39:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vn4fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn4fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.179,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf9a971e9ca6c7c83f11c25ce52475127fe1ea1b9b305a71797cae436b37f9b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-wpqtg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wpqtg webserver-deployment-845c8977d9- deployment-495  e9124273-a44e-43f9-8878-9a1a3b029eb9 19358 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d44147 0xc003d44148}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mj588,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mj588,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-xgw6k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgw6k webserver-deployment-845c8977d9- deployment-495  997a18fa-77ed-489f-8e2c-b29f00cc2dc2 19445 0 2023-03-29 08:39:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e67c7a461963917e143764a7cf9e0da9a136199a240c99e1a27a913b187cb312 cni.projectcalico.org/podIP:192.168.87.234/32 cni.projectcalico.org/podIPs:192.168.87.234/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d442b0 0xc003d442b1}] [] [{kube-controller-manager Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-29 08:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqk7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqk7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:,StartTime:2023-03-29 08:39:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 29 08:39:16.513: INFO: Pod "webserver-deployment-845c8977d9-zhg5w" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zhg5w webserver-deployment-845c8977d9- deployment-495  4a0d175c-a33e-46f1-bfd9-659a755537e1 19160 0 2023-03-29 08:39:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87b412a051842140ab01abb2b6a4e5f5766f3a5fe036f4bfd5ad55785f60b32d cni.projectcalico.org/podIP:192.168.30.58/32 cni.projectcalico.org/podIPs:192.168.30.58/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c751e20a-9130-446b-8853-8db1a7bc7d60 0xc003d444b7 0xc003d444b8}] [] [{calico Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c751e20a-9130-446b-8853-8db1a7bc7d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:39:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrl4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrl4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:39:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.58,StartTime:2023-03-29 08:39:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:39:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e35bbb7f40aec3a979595179269cf742aea6f5ef83887f249343a3c38946351a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:39:16.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-495" for this suite. 03/29/23 08:39:16.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:16.526
Mar 29 08:39:16.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:39:16.526
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:16.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:16.538
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/29/23 08:39:16.539
Mar 29 08:39:16.543: INFO: Waiting up to 5m0s for pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55" in namespace "emptydir-5482" to be "Succeeded or Failed"
Mar 29 08:39:16.544: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457867ms
Mar 29 08:39:18.546: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00345512s
Mar 29 08:39:20.548: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005379559s
STEP: Saw pod success 03/29/23 08:39:20.548
Mar 29 08:39:20.549: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55" satisfied condition "Succeeded or Failed"
Mar 29 08:39:20.550: INFO: Trying to get logs from node 10.146.0.115 pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 container test-container: <nil>
STEP: delete the pod 03/29/23 08:39:20.557
Mar 29 08:39:20.564: INFO: Waiting for pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 to disappear
Mar 29 08:39:20.567: INFO: Pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:39:20.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5482" for this suite. 03/29/23 08:39:20.568
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":198,"skipped":3502,"failed":0}
------------------------------
â€¢ [4.045 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:16.526
    Mar 29 08:39:16.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:39:16.526
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:16.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:16.538
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/29/23 08:39:16.539
    Mar 29 08:39:16.543: INFO: Waiting up to 5m0s for pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55" in namespace "emptydir-5482" to be "Succeeded or Failed"
    Mar 29 08:39:16.544: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457867ms
    Mar 29 08:39:18.546: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00345512s
    Mar 29 08:39:20.548: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005379559s
    STEP: Saw pod success 03/29/23 08:39:20.548
    Mar 29 08:39:20.549: INFO: Pod "pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55" satisfied condition "Succeeded or Failed"
    Mar 29 08:39:20.550: INFO: Trying to get logs from node 10.146.0.115 pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:39:20.557
    Mar 29 08:39:20.564: INFO: Waiting for pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 to disappear
    Mar 29 08:39:20.567: INFO: Pod pod-465731b4-52b1-4a2c-b1e8-fc7c5b60ac55 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:39:20.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5482" for this suite. 03/29/23 08:39:20.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:20.572
Mar 29 08:39:20.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:39:20.572
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:20.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:20.579
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:39:20.58
Mar 29 08:39:20.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389" in namespace "downward-api-1721" to be "Succeeded or Failed"
Mar 29 08:39:20.585: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224418ms
Mar 29 08:39:22.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915808s
Mar 29 08:39:24.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003826501s
Mar 29 08:39:26.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003811631s
STEP: Saw pod success 03/29/23 08:39:26.587
Mar 29 08:39:26.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389" satisfied condition "Succeeded or Failed"
Mar 29 08:39:26.589: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 container client-container: <nil>
STEP: delete the pod 03/29/23 08:39:26.592
Mar 29 08:39:26.598: INFO: Waiting for pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 to disappear
Mar 29 08:39:26.601: INFO: Pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:39:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1721" for this suite. 03/29/23 08:39:26.603
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":199,"skipped":3523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.033 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:20.572
    Mar 29 08:39:20.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:39:20.572
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:20.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:20.579
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:39:20.58
    Mar 29 08:39:20.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389" in namespace "downward-api-1721" to be "Succeeded or Failed"
    Mar 29 08:39:20.585: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224418ms
    Mar 29 08:39:22.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915808s
    Mar 29 08:39:24.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003826501s
    Mar 29 08:39:26.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003811631s
    STEP: Saw pod success 03/29/23 08:39:26.587
    Mar 29 08:39:26.587: INFO: Pod "downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389" satisfied condition "Succeeded or Failed"
    Mar 29 08:39:26.589: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:39:26.592
    Mar 29 08:39:26.598: INFO: Waiting for pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 to disappear
    Mar 29 08:39:26.601: INFO: Pod downwardapi-volume-4a146c5a-f552-46ca-8066-aaa0f8755389 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:39:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1721" for this suite. 03/29/23 08:39:26.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:26.606
Mar 29 08:39:26.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 08:39:26.607
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:26.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:26.613
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:39:26.622
STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:39:26.625
Mar 29 08:39:26.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:39:26.629: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:39:27.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 08:39:27.639: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
Mar 29 08:39:28.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:39:28.634: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
Mar 29 08:39:29.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 29 08:39:29.634: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
Mar 29 08:39:30.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 29 08:39:30.633: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/29/23 08:39:30.634
STEP: DeleteCollection of the DaemonSets 03/29/23 08:39:30.636
STEP: Verify that ReplicaSets have been deleted 03/29/23 08:39:30.639
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar 29 08:39:30.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20017"},"items":null}

Mar 29 08:39:30.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20020"},"items":[{"metadata":{"name":"daemon-set-g5kvv","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"56b26dc4-4b23-48b4-89b7-1e0929d50a23","resourceVersion":"20018","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"788a49076a4fde9c7edfb00ddc1d1d755b6f9910322198e100652cb8a9e87f81","cni.projectcalico.org/podIP":"192.168.87.235/32","cni.projectcalico.org/podIPs":"192.168.87.235/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gtsz2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gtsz2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.117","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.117"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.117","podIP":"192.168.87.235","podIPs":[{"ip":"192.168.87.235"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6d81d79d5d66571f7246d42ff94dfbf3d7407d71d715616cba5bfc7c6125bdef","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hqb2c","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"4b050b1c-acd6-4858-82fa-ad93f08511bf","resourceVersion":"20019","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6f1e6b2e86e77035ab15000d7b2b8d7380b2e494f3ae3864032e01e1a43d436f","cni.projectcalico.org/podIP":"192.168.219.178/32","cni.projectcalico.org/podIPs":"192.168.219.178/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kszpg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kszpg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.116","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.116"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.116","podIP":"192.168.219.178","podIPs":[{"ip":"192.168.219.178"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d6e1c89db97993016236958a65d6b2e98aac95cf1b466c38a699e7c6173576b6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n5jbd","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"3c64808b-8af7-44b8-bde1-dcbc21f81eb7","resourceVersion":"20020","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"416d9c057e7983b417055bca63da9be29454dbce3e1d32e3493843a3e5d5f2f8","cni.projectcalico.org/podIP":"192.168.30.2/32","cni.projectcalico.org/podIPs":"192.168.30.2/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-f68ch","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-f68ch","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.115","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.115"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.115","podIP":"192.168.30.2","podIPs":[{"ip":"192.168.30.2"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://736513661b836d3fe6a37a75a76bca97f2483667763809ab44685511e69b30f3","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:39:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2278" for this suite. 03/29/23 08:39:30.657
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":200,"skipped":3565,"failed":0}
------------------------------
â€¢ [4.053 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:26.606
    Mar 29 08:39:26.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 08:39:26.607
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:26.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:26.613
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/29/23 08:39:26.622
    STEP: Check that daemon pods launch on every node of the cluster. 03/29/23 08:39:26.625
    Mar 29 08:39:26.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:39:26.629: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:39:27.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 08:39:27.639: INFO: Node 10.146.0.115 is running 0 daemon pod, expected 1
    Mar 29 08:39:28.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:39:28.634: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
    Mar 29 08:39:29.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 29 08:39:29.634: INFO: Node 10.146.0.116 is running 0 daemon pod, expected 1
    Mar 29 08:39:30.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 29 08:39:30.633: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/29/23 08:39:30.634
    STEP: DeleteCollection of the DaemonSets 03/29/23 08:39:30.636
    STEP: Verify that ReplicaSets have been deleted 03/29/23 08:39:30.639
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar 29 08:39:30.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20017"},"items":null}

    Mar 29 08:39:30.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20020"},"items":[{"metadata":{"name":"daemon-set-g5kvv","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"56b26dc4-4b23-48b4-89b7-1e0929d50a23","resourceVersion":"20018","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"788a49076a4fde9c7edfb00ddc1d1d755b6f9910322198e100652cb8a9e87f81","cni.projectcalico.org/podIP":"192.168.87.235/32","cni.projectcalico.org/podIPs":"192.168.87.235/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gtsz2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gtsz2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.117","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.117"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.117","podIP":"192.168.87.235","podIPs":[{"ip":"192.168.87.235"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6d81d79d5d66571f7246d42ff94dfbf3d7407d71d715616cba5bfc7c6125bdef","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hqb2c","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"4b050b1c-acd6-4858-82fa-ad93f08511bf","resourceVersion":"20019","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6f1e6b2e86e77035ab15000d7b2b8d7380b2e494f3ae3864032e01e1a43d436f","cni.projectcalico.org/podIP":"192.168.219.178/32","cni.projectcalico.org/podIPs":"192.168.219.178/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kszpg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kszpg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.116","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.116"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.116","podIP":"192.168.219.178","podIPs":[{"ip":"192.168.219.178"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d6e1c89db97993016236958a65d6b2e98aac95cf1b466c38a699e7c6173576b6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n5jbd","generateName":"daemon-set-","namespace":"daemonsets-2278","uid":"3c64808b-8af7-44b8-bde1-dcbc21f81eb7","resourceVersion":"20020","creationTimestamp":"2023-03-29T08:39:26Z","deletionTimestamp":"2023-03-29T08:40:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"416d9c057e7983b417055bca63da9be29454dbce3e1d32e3493843a3e5d5f2f8","cni.projectcalico.org/podIP":"192.168.30.2/32","cni.projectcalico.org/podIPs":"192.168.30.2/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c212f7fd-71c0-45e7-b753-32f761454a09","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c212f7fd-71c0-45e7-b753-32f761454a09\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-29T08:39:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-f68ch","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-f68ch","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.146.0.115","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.146.0.115"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-29T08:39:26Z"}],"hostIP":"10.146.0.115","podIP":"192.168.30.2","podIPs":[{"ip":"192.168.30.2"}],"startTime":"2023-03-29T08:39:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-29T08:39:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://736513661b836d3fe6a37a75a76bca97f2483667763809ab44685511e69b30f3","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:39:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2278" for this suite. 03/29/23 08:39:30.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:30.661
Mar 29 08:39:30.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:39:30.662
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:30.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:30.669
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7582 03/29/23 08:39:30.67
STEP: changing the ExternalName service to type=ClusterIP 03/29/23 08:39:30.672
STEP: creating replication controller externalname-service in namespace services-7582 03/29/23 08:39:30.682
I0329 08:39:30.685808      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7582, replica count: 2
I0329 08:39:33.736643      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:39:33.736: INFO: Creating new exec pod
Mar 29 08:39:33.740: INFO: Waiting up to 5m0s for pod "execpodj9bwz" in namespace "services-7582" to be "running"
Mar 29 08:39:33.742: INFO: Pod "execpodj9bwz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490371ms
Mar 29 08:39:35.744: INFO: Pod "execpodj9bwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004055776s
Mar 29 08:39:35.744: INFO: Pod "execpodj9bwz" satisfied condition "running"
Mar 29 08:39:36.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7582 exec execpodj9bwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 29 08:39:36.837: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 29 08:39:36.837: INFO: stdout: "externalname-service-cz6hv"
Mar 29 08:39:36.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7582 exec execpodj9bwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.135.245 80'
Mar 29 08:39:36.917: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.135.245 80\nConnection to 10.100.135.245 80 port [tcp/http] succeeded!\n"
Mar 29 08:39:36.917: INFO: stdout: "externalname-service-flplt"
Mar 29 08:39:36.917: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:39:36.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7582" for this suite. 03/29/23 08:39:36.928
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":201,"skipped":3608,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.270 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:30.661
    Mar 29 08:39:30.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:39:30.662
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:30.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:30.669
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7582 03/29/23 08:39:30.67
    STEP: changing the ExternalName service to type=ClusterIP 03/29/23 08:39:30.672
    STEP: creating replication controller externalname-service in namespace services-7582 03/29/23 08:39:30.682
    I0329 08:39:30.685808      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7582, replica count: 2
    I0329 08:39:33.736643      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:39:33.736: INFO: Creating new exec pod
    Mar 29 08:39:33.740: INFO: Waiting up to 5m0s for pod "execpodj9bwz" in namespace "services-7582" to be "running"
    Mar 29 08:39:33.742: INFO: Pod "execpodj9bwz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490371ms
    Mar 29 08:39:35.744: INFO: Pod "execpodj9bwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004055776s
    Mar 29 08:39:35.744: INFO: Pod "execpodj9bwz" satisfied condition "running"
    Mar 29 08:39:36.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7582 exec execpodj9bwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 29 08:39:36.837: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 29 08:39:36.837: INFO: stdout: "externalname-service-cz6hv"
    Mar 29 08:39:36.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7582 exec execpodj9bwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.135.245 80'
    Mar 29 08:39:36.917: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.135.245 80\nConnection to 10.100.135.245 80 port [tcp/http] succeeded!\n"
    Mar 29 08:39:36.917: INFO: stdout: "externalname-service-flplt"
    Mar 29 08:39:36.917: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:39:36.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7582" for this suite. 03/29/23 08:39:36.928
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:36.932
Mar 29 08:39:36.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:39:36.933
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:36.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:36.94
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:39:36.942
Mar 29 08:39:36.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a" in namespace "downward-api-2864" to be "Succeeded or Failed"
Mar 29 08:39:36.947: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471506ms
Mar 29 08:39:38.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003490865s
Mar 29 08:39:40.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003377796s
STEP: Saw pod success 03/29/23 08:39:40.949
Mar 29 08:39:40.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a" satisfied condition "Succeeded or Failed"
Mar 29 08:39:40.951: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a container client-container: <nil>
STEP: delete the pod 03/29/23 08:39:40.955
Mar 29 08:39:40.960: INFO: Waiting for pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a to disappear
Mar 29 08:39:40.962: INFO: Pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:39:40.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2864" for this suite. 03/29/23 08:39:40.964
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":202,"skipped":3655,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:36.932
    Mar 29 08:39:36.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:39:36.933
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:36.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:36.94
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:39:36.942
    Mar 29 08:39:36.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a" in namespace "downward-api-2864" to be "Succeeded or Failed"
    Mar 29 08:39:36.947: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471506ms
    Mar 29 08:39:38.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003490865s
    Mar 29 08:39:40.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003377796s
    STEP: Saw pod success 03/29/23 08:39:40.949
    Mar 29 08:39:40.949: INFO: Pod "downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a" satisfied condition "Succeeded or Failed"
    Mar 29 08:39:40.951: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a container client-container: <nil>
    STEP: delete the pod 03/29/23 08:39:40.955
    Mar 29 08:39:40.960: INFO: Waiting for pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a to disappear
    Mar 29 08:39:40.962: INFO: Pod downwardapi-volume-8634ecf9-b89d-4ea1-9a4c-05abc173a42a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:39:40.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2864" for this suite. 03/29/23 08:39:40.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:40.966
Mar 29 08:39:40.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:39:40.967
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:40.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:40.974
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-7056 03/29/23 08:39:40.975
STEP: creating service affinity-clusterip-transition in namespace services-7056 03/29/23 08:39:40.975
STEP: creating replication controller affinity-clusterip-transition in namespace services-7056 03/29/23 08:39:40.981
I0329 08:39:40.984415      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7056, replica count: 3
I0329 08:39:44.036186      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:39:44.038: INFO: Creating new exec pod
Mar 29 08:39:44.041: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8wq" in namespace "services-7056" to be "running"
Mar 29 08:39:44.042: INFO: Pod "execpod-affinitylt8wq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.243144ms
Mar 29 08:39:46.044: INFO: Pod "execpod-affinitylt8wq": Phase="Running", Reason="", readiness=true. Elapsed: 2.002887565s
Mar 29 08:39:46.044: INFO: Pod "execpod-affinitylt8wq" satisfied condition "running"
Mar 29 08:39:47.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 29 08:39:47.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 29 08:39:47.131: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:39:47.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.115.29 80'
Mar 29 08:39:47.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.115.29 80\nConnection to 10.100.115.29 80 port [tcp/http] succeeded!\n"
Mar 29 08:39:47.216: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:39:47.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.115.29:80/ ; done'
Mar 29 08:39:47.362: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n"
Mar 29 08:39:47.362: INFO: stdout: "\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc"
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
Mar 29 08:39:47.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.115.29:80/ ; done'
Mar 29 08:39:47.494: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n"
Mar 29 08:39:47.494: INFO: stdout: "\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh"
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
Mar 29 08:39:47.494: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7056, will wait for the garbage collector to delete the pods 03/29/23 08:39:47.501
Mar 29 08:39:47.558: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.446046ms
Mar 29 08:39:47.658: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.712781ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:39:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7056" for this suite. 03/29/23 08:39:49.77
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":203,"skipped":3669,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:40.966
    Mar 29 08:39:40.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:39:40.967
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:40.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:40.974
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-7056 03/29/23 08:39:40.975
    STEP: creating service affinity-clusterip-transition in namespace services-7056 03/29/23 08:39:40.975
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7056 03/29/23 08:39:40.981
    I0329 08:39:40.984415      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7056, replica count: 3
    I0329 08:39:44.036186      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:39:44.038: INFO: Creating new exec pod
    Mar 29 08:39:44.041: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8wq" in namespace "services-7056" to be "running"
    Mar 29 08:39:44.042: INFO: Pod "execpod-affinitylt8wq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.243144ms
    Mar 29 08:39:46.044: INFO: Pod "execpod-affinitylt8wq": Phase="Running", Reason="", readiness=true. Elapsed: 2.002887565s
    Mar 29 08:39:46.044: INFO: Pod "execpod-affinitylt8wq" satisfied condition "running"
    Mar 29 08:39:47.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar 29 08:39:47.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 29 08:39:47.131: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:39:47.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.115.29 80'
    Mar 29 08:39:47.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.115.29 80\nConnection to 10.100.115.29 80 port [tcp/http] succeeded!\n"
    Mar 29 08:39:47.216: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:39:47.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.115.29:80/ ; done'
    Mar 29 08:39:47.362: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n"
    Mar 29 08:39:47.362: INFO: stdout: "\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc\naffinity-clusterip-transition-c9ft5\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-vpbqc"
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-c9ft5
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.362: INFO: Received response from host: affinity-clusterip-transition-vpbqc
    Mar 29 08:39:47.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7056 exec execpod-affinitylt8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.115.29:80/ ; done'
    Mar 29 08:39:47.494: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.115.29:80/\n"
    Mar 29 08:39:47.494: INFO: stdout: "\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh\naffinity-clusterip-transition-xwwmh"
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Received response from host: affinity-clusterip-transition-xwwmh
    Mar 29 08:39:47.494: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7056, will wait for the garbage collector to delete the pods 03/29/23 08:39:47.501
    Mar 29 08:39:47.558: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.446046ms
    Mar 29 08:39:47.658: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.712781ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:39:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7056" for this suite. 03/29/23 08:39:49.77
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:49.774
Mar 29 08:39:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename prestop 03/29/23 08:39:49.774
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:49.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:49.782
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4131 03/29/23 08:39:49.783
STEP: Waiting for pods to come up. 03/29/23 08:39:49.787
Mar 29 08:39:49.787: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4131" to be "running"
Mar 29 08:39:49.789: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.336004ms
Mar 29 08:39:51.791: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00335524s
Mar 29 08:39:51.791: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4131 03/29/23 08:39:51.792
Mar 29 08:39:51.795: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4131" to be "running"
Mar 29 08:39:51.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3519ms
Mar 29 08:39:53.798: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003340729s
Mar 29 08:39:53.798: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/29/23 08:39:53.798
Mar 29 08:39:58.805: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/29/23 08:39:58.805
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar 29 08:39:58.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4131" for this suite. 03/29/23 08:39:58.813
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":204,"skipped":3709,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.045 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:49.774
    Mar 29 08:39:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename prestop 03/29/23 08:39:49.774
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:49.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:49.782
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4131 03/29/23 08:39:49.783
    STEP: Waiting for pods to come up. 03/29/23 08:39:49.787
    Mar 29 08:39:49.787: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4131" to be "running"
    Mar 29 08:39:49.789: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.336004ms
    Mar 29 08:39:51.791: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00335524s
    Mar 29 08:39:51.791: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4131 03/29/23 08:39:51.792
    Mar 29 08:39:51.795: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4131" to be "running"
    Mar 29 08:39:51.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3519ms
    Mar 29 08:39:53.798: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003340729s
    Mar 29 08:39:53.798: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/29/23 08:39:53.798
    Mar 29 08:39:58.805: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/29/23 08:39:58.805
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar 29 08:39:58.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-4131" for this suite. 03/29/23 08:39:58.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:39:58.819
Mar 29 08:39:58.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:39:58.82
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:58.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:58.831
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-d0cc32e7-9923-436a-8bf2-94fc14147a72 03/29/23 08:39:58.834
STEP: Creating secret with name s-test-opt-upd-818b33b3-df4f-45da-915d-fa79dcf47d30 03/29/23 08:39:58.836
STEP: Creating the pod 03/29/23 08:39:58.839
Mar 29 08:39:58.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c" in namespace "projected-8313" to be "running and ready"
Mar 29 08:39:58.850: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452381ms
Mar 29 08:39:58.850: INFO: The phase of Pod pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:40:00.852: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00374873s
Mar 29 08:40:00.852: INFO: The phase of Pod pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c is Running (Ready = true)
Mar 29 08:40:00.852: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d0cc32e7-9923-436a-8bf2-94fc14147a72 03/29/23 08:40:00.862
STEP: Updating secret s-test-opt-upd-818b33b3-df4f-45da-915d-fa79dcf47d30 03/29/23 08:40:00.866
STEP: Creating secret with name s-test-opt-create-80685586-0db2-405b-b7b1-78f96d6e3069 03/29/23 08:40:00.868
STEP: waiting to observe update in volume 03/29/23 08:40:00.871
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 08:40:04.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8313" for this suite. 03/29/23 08:40:04.888
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":205,"skipped":3735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:39:58.819
    Mar 29 08:39:58.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:39:58.82
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:39:58.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:39:58.831
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-d0cc32e7-9923-436a-8bf2-94fc14147a72 03/29/23 08:39:58.834
    STEP: Creating secret with name s-test-opt-upd-818b33b3-df4f-45da-915d-fa79dcf47d30 03/29/23 08:39:58.836
    STEP: Creating the pod 03/29/23 08:39:58.839
    Mar 29 08:39:58.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c" in namespace "projected-8313" to be "running and ready"
    Mar 29 08:39:58.850: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452381ms
    Mar 29 08:39:58.850: INFO: The phase of Pod pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:40:00.852: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00374873s
    Mar 29 08:40:00.852: INFO: The phase of Pod pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c is Running (Ready = true)
    Mar 29 08:40:00.852: INFO: Pod "pod-projected-secrets-e89c7f3a-5310-408f-a8bd-440b7ee08d7c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d0cc32e7-9923-436a-8bf2-94fc14147a72 03/29/23 08:40:00.862
    STEP: Updating secret s-test-opt-upd-818b33b3-df4f-45da-915d-fa79dcf47d30 03/29/23 08:40:00.866
    STEP: Creating secret with name s-test-opt-create-80685586-0db2-405b-b7b1-78f96d6e3069 03/29/23 08:40:00.868
    STEP: waiting to observe update in volume 03/29/23 08:40:00.871
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 08:40:04.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8313" for this suite. 03/29/23 08:40:04.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:04.891
Mar 29 08:40:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:40:04.892
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:04.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:04.899
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:40:04.901
Mar 29 08:40:04.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e" in namespace "downward-api-6562" to be "Succeeded or Failed"
Mar 29 08:40:04.906: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246505ms
Mar 29 08:40:06.908: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003474026s
Mar 29 08:40:08.909: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003983939s
STEP: Saw pod success 03/29/23 08:40:08.909
Mar 29 08:40:08.909: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e" satisfied condition "Succeeded or Failed"
Mar 29 08:40:08.910: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e container client-container: <nil>
STEP: delete the pod 03/29/23 08:40:08.913
Mar 29 08:40:08.919: INFO: Waiting for pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e to disappear
Mar 29 08:40:08.920: INFO: Pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:40:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6562" for this suite. 03/29/23 08:40:08.921
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":206,"skipped":3758,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:04.891
    Mar 29 08:40:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:40:04.892
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:04.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:04.899
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:40:04.901
    Mar 29 08:40:04.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e" in namespace "downward-api-6562" to be "Succeeded or Failed"
    Mar 29 08:40:04.906: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246505ms
    Mar 29 08:40:06.908: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003474026s
    Mar 29 08:40:08.909: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003983939s
    STEP: Saw pod success 03/29/23 08:40:08.909
    Mar 29 08:40:08.909: INFO: Pod "downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e" satisfied condition "Succeeded or Failed"
    Mar 29 08:40:08.910: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e container client-container: <nil>
    STEP: delete the pod 03/29/23 08:40:08.913
    Mar 29 08:40:08.919: INFO: Waiting for pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e to disappear
    Mar 29 08:40:08.920: INFO: Pod downwardapi-volume-ba111114-f301-4b66-a789-05f10321509e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:40:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6562" for this suite. 03/29/23 08:40:08.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:08.925
Mar 29 08:40:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:40:08.925
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:08.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:08.933
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/29/23 08:40:08.935
Mar 29 08:40:08.938: INFO: Waiting up to 5m0s for pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59" in namespace "downward-api-2696" to be "Succeeded or Failed"
Mar 29 08:40:08.939: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248219ms
Mar 29 08:40:10.942: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004239396s
Mar 29 08:40:12.943: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005119415s
STEP: Saw pod success 03/29/23 08:40:12.943
Mar 29 08:40:12.943: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59" satisfied condition "Succeeded or Failed"
Mar 29 08:40:12.945: INFO: Trying to get logs from node 10.146.0.116 pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:40:12.947
Mar 29 08:40:12.953: INFO: Waiting for pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 to disappear
Mar 29 08:40:12.954: INFO: Pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 29 08:40:12.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2696" for this suite. 03/29/23 08:40:12.956
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":207,"skipped":3771,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:08.925
    Mar 29 08:40:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:40:08.925
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:08.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:08.933
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/29/23 08:40:08.935
    Mar 29 08:40:08.938: INFO: Waiting up to 5m0s for pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59" in namespace "downward-api-2696" to be "Succeeded or Failed"
    Mar 29 08:40:08.939: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248219ms
    Mar 29 08:40:10.942: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004239396s
    Mar 29 08:40:12.943: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005119415s
    STEP: Saw pod success 03/29/23 08:40:12.943
    Mar 29 08:40:12.943: INFO: Pod "downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59" satisfied condition "Succeeded or Failed"
    Mar 29 08:40:12.945: INFO: Trying to get logs from node 10.146.0.116 pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:40:12.947
    Mar 29 08:40:12.953: INFO: Waiting for pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 to disappear
    Mar 29 08:40:12.954: INFO: Pod downward-api-66e6d5e9-b279-444d-8b17-9fc71b937c59 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 29 08:40:12.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2696" for this suite. 03/29/23 08:40:12.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:12.959
Mar 29 08:40:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:40:12.959
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:12.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:12.969
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/29/23 08:40:12.97
Mar 29 08:40:12.970: INFO: namespace kubectl-8599
Mar 29 08:40:12.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 create -f -'
Mar 29 08:40:13.533: INFO: stderr: ""
Mar 29 08:40:13.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/29/23 08:40:13.533
Mar 29 08:40:14.536: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 08:40:14.536: INFO: Found 1 / 1
Mar 29 08:40:14.536: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 29 08:40:14.537: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 29 08:40:14.537: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 29 08:40:14.537: INFO: wait on agnhost-primary startup in kubectl-8599 
Mar 29 08:40:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 logs agnhost-primary-tv6mp agnhost-primary'
Mar 29 08:40:14.582: INFO: stderr: ""
Mar 29 08:40:14.582: INFO: stdout: "Paused\n"
STEP: exposing RC 03/29/23 08:40:14.582
Mar 29 08:40:14.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 29 08:40:14.630: INFO: stderr: ""
Mar 29 08:40:14.630: INFO: stdout: "service/rm2 exposed\n"
Mar 29 08:40:14.632: INFO: Service rm2 in namespace kubectl-8599 found.
STEP: exposing service 03/29/23 08:40:16.636
Mar 29 08:40:16.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 29 08:40:16.683: INFO: stderr: ""
Mar 29 08:40:16.683: INFO: stdout: "service/rm3 exposed\n"
Mar 29 08:40:16.685: INFO: Service rm3 in namespace kubectl-8599 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:40:18.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8599" for this suite. 03/29/23 08:40:18.69
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":208,"skipped":3782,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.735 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:12.959
    Mar 29 08:40:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:40:12.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:12.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:12.969
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/29/23 08:40:12.97
    Mar 29 08:40:12.970: INFO: namespace kubectl-8599
    Mar 29 08:40:12.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 create -f -'
    Mar 29 08:40:13.533: INFO: stderr: ""
    Mar 29 08:40:13.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/29/23 08:40:13.533
    Mar 29 08:40:14.536: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 08:40:14.536: INFO: Found 1 / 1
    Mar 29 08:40:14.536: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 29 08:40:14.537: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 29 08:40:14.537: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 29 08:40:14.537: INFO: wait on agnhost-primary startup in kubectl-8599 
    Mar 29 08:40:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 logs agnhost-primary-tv6mp agnhost-primary'
    Mar 29 08:40:14.582: INFO: stderr: ""
    Mar 29 08:40:14.582: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/29/23 08:40:14.582
    Mar 29 08:40:14.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 29 08:40:14.630: INFO: stderr: ""
    Mar 29 08:40:14.630: INFO: stdout: "service/rm2 exposed\n"
    Mar 29 08:40:14.632: INFO: Service rm2 in namespace kubectl-8599 found.
    STEP: exposing service 03/29/23 08:40:16.636
    Mar 29 08:40:16.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8599 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 29 08:40:16.683: INFO: stderr: ""
    Mar 29 08:40:16.683: INFO: stdout: "service/rm3 exposed\n"
    Mar 29 08:40:16.685: INFO: Service rm3 in namespace kubectl-8599 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:40:18.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8599" for this suite. 03/29/23 08:40:18.69
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:18.694
Mar 29 08:40:18.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename proxy 03/29/23 08:40:18.695
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:18.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:18.703
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 29 08:40:18.704: INFO: Creating pod...
Mar 29 08:40:18.708: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8113" to be "running"
Mar 29 08:40:18.709: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230949ms
Mar 29 08:40:20.712: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004327354s
Mar 29 08:40:20.712: INFO: Pod "agnhost" satisfied condition "running"
Mar 29 08:40:20.712: INFO: Creating service...
Mar 29 08:40:20.718: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=DELETE
Mar 29 08:40:20.720: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 29 08:40:20.720: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=OPTIONS
Mar 29 08:40:20.723: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 29 08:40:20.723: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=PATCH
Mar 29 08:40:20.724: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 29 08:40:20.724: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=POST
Mar 29 08:40:20.727: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 29 08:40:20.727: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=PUT
Mar 29 08:40:20.728: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 29 08:40:20.728: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 29 08:40:20.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 29 08:40:20.730: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 29 08:40:20.732: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 29 08:40:20.732: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 29 08:40:20.734: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 29 08:40:20.734: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=POST
Mar 29 08:40:20.736: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 29 08:40:20.736: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=PUT
Mar 29 08:40:20.739: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 29 08:40:20.739: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=GET
Mar 29 08:40:20.740: INFO: http.Client request:GET StatusCode:301
Mar 29 08:40:20.740: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=GET
Mar 29 08:40:20.742: INFO: http.Client request:GET StatusCode:301
Mar 29 08:40:20.742: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=HEAD
Mar 29 08:40:20.743: INFO: http.Client request:HEAD StatusCode:301
Mar 29 08:40:20.743: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 29 08:40:20.744: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 29 08:40:20.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8113" for this suite. 03/29/23 08:40:20.746
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":209,"skipped":3786,"failed":0}
------------------------------
â€¢ [2.055 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:18.694
    Mar 29 08:40:18.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename proxy 03/29/23 08:40:18.695
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:18.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:18.703
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 29 08:40:18.704: INFO: Creating pod...
    Mar 29 08:40:18.708: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8113" to be "running"
    Mar 29 08:40:18.709: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.230949ms
    Mar 29 08:40:20.712: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004327354s
    Mar 29 08:40:20.712: INFO: Pod "agnhost" satisfied condition "running"
    Mar 29 08:40:20.712: INFO: Creating service...
    Mar 29 08:40:20.718: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=DELETE
    Mar 29 08:40:20.720: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 29 08:40:20.720: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=OPTIONS
    Mar 29 08:40:20.723: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 29 08:40:20.723: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=PATCH
    Mar 29 08:40:20.724: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 29 08:40:20.724: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=POST
    Mar 29 08:40:20.727: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 29 08:40:20.727: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=PUT
    Mar 29 08:40:20.728: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 29 08:40:20.728: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 29 08:40:20.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 29 08:40:20.730: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 29 08:40:20.732: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 29 08:40:20.732: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 29 08:40:20.734: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 29 08:40:20.734: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=POST
    Mar 29 08:40:20.736: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 29 08:40:20.736: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 29 08:40:20.739: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 29 08:40:20.739: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=GET
    Mar 29 08:40:20.740: INFO: http.Client request:GET StatusCode:301
    Mar 29 08:40:20.740: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=GET
    Mar 29 08:40:20.742: INFO: http.Client request:GET StatusCode:301
    Mar 29 08:40:20.742: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/pods/agnhost/proxy?method=HEAD
    Mar 29 08:40:20.743: INFO: http.Client request:HEAD StatusCode:301
    Mar 29 08:40:20.743: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-8113/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 29 08:40:20.744: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 29 08:40:20.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8113" for this suite. 03/29/23 08:40:20.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:20.749
Mar 29 08:40:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:40:20.75
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:20.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:20.758
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-d5bd9654-34f0-4618-903d-c2747b9f3b68 03/29/23 08:40:20.759
STEP: Creating a pod to test consume secrets 03/29/23 08:40:20.761
Mar 29 08:40:20.766: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839" in namespace "projected-7895" to be "Succeeded or Failed"
Mar 29 08:40:20.767: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Pending", Reason="", readiness=false. Elapsed: 1.383812ms
Mar 29 08:40:22.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004358695s
Mar 29 08:40:24.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004362167s
STEP: Saw pod success 03/29/23 08:40:24.77
Mar 29 08:40:24.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839" satisfied condition "Succeeded or Failed"
Mar 29 08:40:24.771: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:40:24.774
Mar 29 08:40:24.780: INFO: Waiting for pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 to disappear
Mar 29 08:40:24.782: INFO: Pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 08:40:24.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7895" for this suite. 03/29/23 08:40:24.783
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":210,"skipped":3802,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:20.749
    Mar 29 08:40:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:40:20.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:20.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:20.758
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-d5bd9654-34f0-4618-903d-c2747b9f3b68 03/29/23 08:40:20.759
    STEP: Creating a pod to test consume secrets 03/29/23 08:40:20.761
    Mar 29 08:40:20.766: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839" in namespace "projected-7895" to be "Succeeded or Failed"
    Mar 29 08:40:20.767: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Pending", Reason="", readiness=false. Elapsed: 1.383812ms
    Mar 29 08:40:22.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004358695s
    Mar 29 08:40:24.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004362167s
    STEP: Saw pod success 03/29/23 08:40:24.77
    Mar 29 08:40:24.770: INFO: Pod "pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839" satisfied condition "Succeeded or Failed"
    Mar 29 08:40:24.771: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:40:24.774
    Mar 29 08:40:24.780: INFO: Waiting for pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 to disappear
    Mar 29 08:40:24.782: INFO: Pod pod-projected-secrets-a7de363e-b175-438a-b65f-a9959eedd839 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 08:40:24.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7895" for this suite. 03/29/23 08:40:24.783
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:24.786
Mar 29 08:40:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:40:24.786
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:24.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:24.793
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7887 03/29/23 08:40:24.795
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/29/23 08:40:24.801
STEP: creating service externalsvc in namespace services-7887 03/29/23 08:40:24.801
STEP: creating replication controller externalsvc in namespace services-7887 03/29/23 08:40:24.806
I0329 08:40:24.809936      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7887, replica count: 2
I0329 08:40:27.862298      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/29/23 08:40:27.864
Mar 29 08:40:27.872: INFO: Creating new exec pod
Mar 29 08:40:27.875: INFO: Waiting up to 5m0s for pod "execpodgpxtp" in namespace "services-7887" to be "running"
Mar 29 08:40:27.878: INFO: Pod "execpodgpxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799889ms
Mar 29 08:40:29.880: INFO: Pod "execpodgpxtp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005182568s
Mar 29 08:40:29.881: INFO: Pod "execpodgpxtp" satisfied condition "running"
Mar 29 08:40:29.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7887 exec execpodgpxtp -- /bin/sh -x -c nslookup nodeport-service.services-7887.svc.cluster.local'
Mar 29 08:40:29.983: INFO: stderr: "+ nslookup nodeport-service.services-7887.svc.cluster.local\n"
Mar 29 08:40:29.983: INFO: stdout: "Server:\t\t10.146.0.115\nAddress:\t10.146.0.115#53\n\nNon-authoritative answer:\nnodeport-service.services-7887.svc.cluster.local\tcanonical name = externalsvc.services-7887.svc.cluster.local.\nName:\texternalsvc.services-7887.svc.cluster.local\nAddress: 10.100.183.240\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7887, will wait for the garbage collector to delete the pods 03/29/23 08:40:29.983
Mar 29 08:40:30.038: INFO: Deleting ReplicationController externalsvc took: 2.996932ms
Mar 29 08:40:30.140: INFO: Terminating ReplicationController externalsvc pods took: 101.09128ms
Mar 29 08:40:32.347: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:40:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7887" for this suite. 03/29/23 08:40:32.354
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":211,"skipped":3804,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.572 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:24.786
    Mar 29 08:40:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:40:24.786
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:24.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:24.793
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7887 03/29/23 08:40:24.795
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/29/23 08:40:24.801
    STEP: creating service externalsvc in namespace services-7887 03/29/23 08:40:24.801
    STEP: creating replication controller externalsvc in namespace services-7887 03/29/23 08:40:24.806
    I0329 08:40:24.809936      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7887, replica count: 2
    I0329 08:40:27.862298      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/29/23 08:40:27.864
    Mar 29 08:40:27.872: INFO: Creating new exec pod
    Mar 29 08:40:27.875: INFO: Waiting up to 5m0s for pod "execpodgpxtp" in namespace "services-7887" to be "running"
    Mar 29 08:40:27.878: INFO: Pod "execpodgpxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799889ms
    Mar 29 08:40:29.880: INFO: Pod "execpodgpxtp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005182568s
    Mar 29 08:40:29.881: INFO: Pod "execpodgpxtp" satisfied condition "running"
    Mar 29 08:40:29.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7887 exec execpodgpxtp -- /bin/sh -x -c nslookup nodeport-service.services-7887.svc.cluster.local'
    Mar 29 08:40:29.983: INFO: stderr: "+ nslookup nodeport-service.services-7887.svc.cluster.local\n"
    Mar 29 08:40:29.983: INFO: stdout: "Server:\t\t10.146.0.115\nAddress:\t10.146.0.115#53\n\nNon-authoritative answer:\nnodeport-service.services-7887.svc.cluster.local\tcanonical name = externalsvc.services-7887.svc.cluster.local.\nName:\texternalsvc.services-7887.svc.cluster.local\nAddress: 10.100.183.240\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7887, will wait for the garbage collector to delete the pods 03/29/23 08:40:29.983
    Mar 29 08:40:30.038: INFO: Deleting ReplicationController externalsvc took: 2.996932ms
    Mar 29 08:40:30.140: INFO: Terminating ReplicationController externalsvc pods took: 101.09128ms
    Mar 29 08:40:32.347: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:40:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7887" for this suite. 03/29/23 08:40:32.354
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:32.359
Mar 29 08:40:32.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:40:32.359
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:32.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:32.366
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/29/23 08:40:32.368
STEP: delete the rc 03/29/23 08:40:37.374
STEP: wait for all pods to be garbage collected 03/29/23 08:40:37.377
STEP: Gathering metrics 03/29/23 08:40:42.38
W0329 08:40:42.383226      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 08:40:42.383: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:40:42.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4911" for this suite. 03/29/23 08:40:42.384
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":212,"skipped":3847,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.028 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:32.359
    Mar 29 08:40:32.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:40:32.359
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:32.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:32.366
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/29/23 08:40:32.368
    STEP: delete the rc 03/29/23 08:40:37.374
    STEP: wait for all pods to be garbage collected 03/29/23 08:40:37.377
    STEP: Gathering metrics 03/29/23 08:40:42.38
    W0329 08:40:42.383226      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 08:40:42.383: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:40:42.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4911" for this suite. 03/29/23 08:40:42.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:42.387
Mar 29 08:40:42.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 08:40:42.388
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:42.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:42.395
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9309 03/29/23 08:40:42.397
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/29/23 08:40:42.399
STEP: Creating pod with conflicting port in namespace statefulset-9309 03/29/23 08:40:42.401
STEP: Waiting until pod test-pod will start running in namespace statefulset-9309 03/29/23 08:40:42.405
Mar 29 08:40:42.405: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9309" to be "running"
Mar 29 08:40:42.406: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.111353ms
Mar 29 08:40:44.408: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00333084s
Mar 29 08:40:44.408: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9309 03/29/23 08:40:44.408
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9309 03/29/23 08:40:44.412
Mar 29 08:40:44.423: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Pending. Waiting for statefulset controller to delete.
Mar 29 08:40:44.429: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Failed. Waiting for statefulset controller to delete.
Mar 29 08:40:44.446: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Failed. Waiting for statefulset controller to delete.
Mar 29 08:40:44.448: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9309
STEP: Removing pod with conflicting port in namespace statefulset-9309 03/29/23 08:40:44.448
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9309 and will be in running state 03/29/23 08:40:44.455
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 08:40:46.458: INFO: Deleting all statefulset in ns statefulset-9309
Mar 29 08:40:46.459: INFO: Scaling statefulset ss to 0
Mar 29 08:40:56.470: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:40:56.471: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 08:40:56.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9309" for this suite. 03/29/23 08:40:56.478
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":213,"skipped":3863,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.093 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:42.387
    Mar 29 08:40:42.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 08:40:42.388
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:42.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:42.395
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9309 03/29/23 08:40:42.397
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/29/23 08:40:42.399
    STEP: Creating pod with conflicting port in namespace statefulset-9309 03/29/23 08:40:42.401
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9309 03/29/23 08:40:42.405
    Mar 29 08:40:42.405: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9309" to be "running"
    Mar 29 08:40:42.406: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.111353ms
    Mar 29 08:40:44.408: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00333084s
    Mar 29 08:40:44.408: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9309 03/29/23 08:40:44.408
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9309 03/29/23 08:40:44.412
    Mar 29 08:40:44.423: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 29 08:40:44.429: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 29 08:40:44.446: INFO: Observed stateful pod in namespace: statefulset-9309, name: ss-0, uid: eab18551-a76c-4e42-be12-950bc701df23, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 29 08:40:44.448: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9309
    STEP: Removing pod with conflicting port in namespace statefulset-9309 03/29/23 08:40:44.448
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9309 and will be in running state 03/29/23 08:40:44.455
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 08:40:46.458: INFO: Deleting all statefulset in ns statefulset-9309
    Mar 29 08:40:46.459: INFO: Scaling statefulset ss to 0
    Mar 29 08:40:56.470: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:40:56.471: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 08:40:56.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9309" for this suite. 03/29/23 08:40:56.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:40:56.481
Mar 29 08:40:56.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context-test 03/29/23 08:40:56.482
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:56.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:56.488
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar 29 08:40:56.492: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938" in namespace "security-context-test-2680" to be "Succeeded or Failed"
Mar 29 08:40:56.494: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Pending", Reason="", readiness=false. Elapsed: 1.161243ms
Mar 29 08:40:58.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00338091s
Mar 29 08:41:00.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00329799s
Mar 29 08:41:00.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 08:41:00.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2680" for this suite. 03/29/23 08:41:00.497
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":214,"skipped":3888,"failed":0}
------------------------------
â€¢ [4.020 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:40:56.481
    Mar 29 08:40:56.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context-test 03/29/23 08:40:56.482
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:40:56.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:40:56.488
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar 29 08:40:56.492: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938" in namespace "security-context-test-2680" to be "Succeeded or Failed"
    Mar 29 08:40:56.494: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Pending", Reason="", readiness=false. Elapsed: 1.161243ms
    Mar 29 08:40:58.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00338091s
    Mar 29 08:41:00.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00329799s
    Mar 29 08:41:00.496: INFO: Pod "busybox-user-65534-c0a488ec-8325-44d5-b6b6-173274679938" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 08:41:00.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2680" for this suite. 03/29/23 08:41:00.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:00.502
Mar 29 08:41:00.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:41:00.502
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:00.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:00.509
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/29/23 08:41:00.512
STEP: waiting for Deployment to be created 03/29/23 08:41:00.515
STEP: waiting for all Replicas to be Ready 03/29/23 08:41:00.515
Mar 29 08:41:00.516: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.516: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.521: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.521: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.528: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.528: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.539: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:00.539: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 29 08:41:01.303: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 29 08:41:01.303: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 29 08:41:01.572: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/29/23 08:41:01.572
W0329 08:41:01.576418      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 29 08:41:01.577: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/29/23 08:41:01.577
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.583: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.583: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.592: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.592: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:01.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:01.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:01.603: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:01.603: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:02.580: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:02.580: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:02.588: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
STEP: listing Deployments 03/29/23 08:41:02.588
Mar 29 08:41:02.589: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/29/23 08:41:02.589
Mar 29 08:41:02.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/29/23 08:41:02.596
Mar 29 08:41:02.602: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:02.602: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:02.609: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:02.617: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:02.622: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:03.308: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:03.319: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:03.335: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:03.340: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 29 08:41:04.818: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/29/23 08:41:04.828
STEP: fetching the DeploymentStatus 03/29/23 08:41:04.832
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3
STEP: deleting the Deployment 03/29/23 08:41:04.835
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
Mar 29 08:41:04.839: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:41:04.841: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 29 08:41:04.842: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-453  a6a9319a-3e11-4b33-80ea-b8f1a0426680 21246 4 2023-03-29 08:41:01 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5637 0xc005ac5638}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac56c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 29 08:41:04.844: INFO: pod: "test-deployment-54cc775c4b-6z9qr":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-6z9qr test-deployment-54cc775c4b- deployment-453  03f6bf63-2ce5-40db-858a-10a926e82fd0 21241 0 2023-03-29 08:41:01 +0000 UTC 2023-03-29 08:41:05 +0000 UTC 0xc003def278 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:f703c9795b94b005e93e0a51e046742e78f13470170ea2c709245ed9448b4a9b cni.projectcalico.org/podIP:192.168.30.44/32 cni.projectcalico.org/podIPs:192.168.30.44/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a6a9319a-3e11-4b33-80ea-b8f1a0426680 0xc003def2c7 0xc003def2c8}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6a9319a-3e11-4b33-80ea-b8f1a0426680\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sznql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sznql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.44,StartTime:2023-03-29 08:41:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://99fb54407aee1550c9a5953546b89c7c70aecd65ae5117ffe49ed199e99f52cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 29 08:41:04.845: INFO: pod: "test-deployment-54cc775c4b-tfjqt":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-tfjqt test-deployment-54cc775c4b- deployment-453  3ee7b336-8509-4030-a615-0109a95f87ce 21198 0 2023-03-29 08:41:02 +0000 UTC 2023-03-29 08:41:04 +0000 UTC 0xc003def4c0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:976fc904bf849d680bb52e335aa5f023fb85ce5222dc89ab9e23b593190eb47a cni.projectcalico.org/podIP:192.168.219.185/32 cni.projectcalico.org/podIPs:192.168.219.185/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a6a9319a-3e11-4b33-80ea-b8f1a0426680 0xc003def4f7 0xc003def4f8}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6a9319a-3e11-4b33-80ea-b8f1a0426680\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4thzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4thzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.185,StartTime:2023-03-29 08:41:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://b2ad2df6428d83e2540fe0fb3e11ce1041095ffa960a5bdaaba0144a469a3efb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 29 08:41:04.845: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-453  77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 21238 2 2023-03-29 08:41:02 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5727 0xc005ac5728}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac57b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 29 08:41:04.846: INFO: pod: "test-deployment-7c7d8d58c8-2m6xq":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2m6xq test-deployment-7c7d8d58c8- deployment-453  6b388694-f176-4dbb-bc6f-d4ec969cfada 21237 0 2023-03-29 08:41:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1d954ba10f7d9b60e97fafd3f7ca998abce7e0ed3a4ec9a88a923db4c5d5752d cni.projectcalico.org/podIP:192.168.87.230/32 cni.projectcalico.org/podIPs:192.168.87.230/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 0xc004a94f07 0xc004a94f08}] [] [{calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b36aa6-7c99-4a7d-9da0-d8793c0b16f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6nkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6nkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.230,StartTime:2023-03-29 08:41:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f9c355724d515df8ac263a3ade41c976dc4bb899763ff3fe2d261fa73e8addbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 29 08:41:04.847: INFO: pod: "test-deployment-7c7d8d58c8-78twc":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-78twc test-deployment-7c7d8d58c8- deployment-453  aa78e18c-310c-4a6a-a5f1-549e57724d76 21193 0 2023-03-29 08:41:02 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bb70879d3db0f11064f86c5dfad5c5d7569b13415b248b5fb43a90419c2f28a8 cni.projectcalico.org/podIP:192.168.219.130/32 cni.projectcalico.org/podIPs:192.168.219.130/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 0xc004a95177 0xc004a95178}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b36aa6-7c99-4a7d-9da0-d8793c0b16f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwmt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwmt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.130,StartTime:2023-03-29 08:41:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5b29c65290147a3fef4412e56482fac756d19402810abb24d3fcda3054afc222,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 29 08:41:04.847: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-453  6a306262-4405-48ca-9a4a-16e2ef740edb 21152 3 2023-03-29 08:41:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5817 0xc005ac5818}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac58a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:41:04.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-453" for this suite. 03/29/23 08:41:04.85
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":215,"skipped":3894,"failed":0}
------------------------------
â€¢ [4.351 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:00.502
    Mar 29 08:41:00.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:41:00.502
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:00.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:00.509
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/29/23 08:41:00.512
    STEP: waiting for Deployment to be created 03/29/23 08:41:00.515
    STEP: waiting for all Replicas to be Ready 03/29/23 08:41:00.515
    Mar 29 08:41:00.516: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.516: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.521: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.521: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.528: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.528: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.539: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:00.539: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 29 08:41:01.303: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 29 08:41:01.303: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 29 08:41:01.572: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/29/23 08:41:01.572
    W0329 08:41:01.576418      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 29 08:41:01.577: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/29/23 08:41:01.577
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 0
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.578: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.583: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.583: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.592: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.592: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:01.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:01.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:01.603: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:01.603: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:02.580: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:02.580: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:02.588: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    STEP: listing Deployments 03/29/23 08:41:02.588
    Mar 29 08:41:02.589: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/29/23 08:41:02.589
    Mar 29 08:41:02.596: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/29/23 08:41:02.596
    Mar 29 08:41:02.602: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:02.602: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:02.609: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:02.617: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:02.622: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:03.308: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:03.319: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:03.335: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:03.340: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 29 08:41:04.818: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/29/23 08:41:04.828
    STEP: fetching the DeploymentStatus 03/29/23 08:41:04.832
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 1
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 2
    Mar 29 08:41:04.835: INFO: observed Deployment test-deployment in namespace deployment-453 with ReadyReplicas 3
    STEP: deleting the Deployment 03/29/23 08:41:04.835
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    Mar 29 08:41:04.839: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:41:04.841: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 29 08:41:04.842: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-453  a6a9319a-3e11-4b33-80ea-b8f1a0426680 21246 4 2023-03-29 08:41:01 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5637 0xc005ac5638}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac56c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 29 08:41:04.844: INFO: pod: "test-deployment-54cc775c4b-6z9qr":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-6z9qr test-deployment-54cc775c4b- deployment-453  03f6bf63-2ce5-40db-858a-10a926e82fd0 21241 0 2023-03-29 08:41:01 +0000 UTC 2023-03-29 08:41:05 +0000 UTC 0xc003def278 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:f703c9795b94b005e93e0a51e046742e78f13470170ea2c709245ed9448b4a9b cni.projectcalico.org/podIP:192.168.30.44/32 cni.projectcalico.org/podIPs:192.168.30.44/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a6a9319a-3e11-4b33-80ea-b8f1a0426680 0xc003def2c7 0xc003def2c8}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6a9319a-3e11-4b33-80ea-b8f1a0426680\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sznql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sznql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.44,StartTime:2023-03-29 08:41:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://99fb54407aee1550c9a5953546b89c7c70aecd65ae5117ffe49ed199e99f52cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 29 08:41:04.845: INFO: pod: "test-deployment-54cc775c4b-tfjqt":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-tfjqt test-deployment-54cc775c4b- deployment-453  3ee7b336-8509-4030-a615-0109a95f87ce 21198 0 2023-03-29 08:41:02 +0000 UTC 2023-03-29 08:41:04 +0000 UTC 0xc003def4c0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:976fc904bf849d680bb52e335aa5f023fb85ce5222dc89ab9e23b593190eb47a cni.projectcalico.org/podIP:192.168.219.185/32 cni.projectcalico.org/podIPs:192.168.219.185/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a6a9319a-3e11-4b33-80ea-b8f1a0426680 0xc003def4f7 0xc003def4f8}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6a9319a-3e11-4b33-80ea-b8f1a0426680\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4thzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4thzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.185,StartTime:2023-03-29 08:41:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://b2ad2df6428d83e2540fe0fb3e11ce1041095ffa960a5bdaaba0144a469a3efb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 29 08:41:04.845: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-453  77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 21238 2 2023-03-29 08:41:02 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5727 0xc005ac5728}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac57b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar 29 08:41:04.846: INFO: pod: "test-deployment-7c7d8d58c8-2m6xq":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2m6xq test-deployment-7c7d8d58c8- deployment-453  6b388694-f176-4dbb-bc6f-d4ec969cfada 21237 0 2023-03-29 08:41:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1d954ba10f7d9b60e97fafd3f7ca998abce7e0ed3a4ec9a88a923db4c5d5752d cni.projectcalico.org/podIP:192.168.87.230/32 cni.projectcalico.org/podIPs:192.168.87.230/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 0xc004a94f07 0xc004a94f08}] [] [{calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b36aa6-7c99-4a7d-9da0-d8793c0b16f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:41:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6nkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6nkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.117,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.117,PodIP:192.168.87.230,StartTime:2023-03-29 08:41:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f9c355724d515df8ac263a3ade41c976dc4bb899763ff3fe2d261fa73e8addbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 29 08:41:04.847: INFO: pod: "test-deployment-7c7d8d58c8-78twc":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-78twc test-deployment-7c7d8d58c8- deployment-453  aa78e18c-310c-4a6a-a5f1-549e57724d76 21193 0 2023-03-29 08:41:02 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bb70879d3db0f11064f86c5dfad5c5d7569b13415b248b5fb43a90419c2f28a8 cni.projectcalico.org/podIP:192.168.219.130/32 cni.projectcalico.org/podIPs:192.168.219.130/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 77b36aa6-7c99-4a7d-9da0-d8793c0b16f0 0xc004a95177 0xc004a95178}] [] [{kube-controller-manager Update v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b36aa6-7c99-4a7d-9da0-d8793c0b16f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-29 08:41:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.219.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwmt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwmt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:192.168.219.130,StartTime:2023-03-29 08:41:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 08:41:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5b29c65290147a3fef4412e56482fac756d19402810abb24d3fcda3054afc222,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.219.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 29 08:41:04.847: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-453  6a306262-4405-48ca-9a4a-16e2ef740edb 21152 3 2023-03-29 08:41:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 34ec72eb-57bb-4c77-aaca-50fe7633092d 0xc005ac5817 0xc005ac5818}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ec72eb-57bb-4c77-aaca-50fe7633092d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:41:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac58a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:41:04.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-453" for this suite. 03/29/23 08:41:04.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:04.853
Mar 29 08:41:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:41:04.854
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:04.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:04.863
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 29 08:41:04.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:41:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5338" for this suite. 03/29/23 08:41:10.988
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":216,"skipped":3903,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.139 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:04.853
    Mar 29 08:41:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:41:04.854
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:04.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:04.863
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 29 08:41:04.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:41:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5338" for this suite. 03/29/23 08:41:10.988
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:10.992
Mar 29 08:41:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:41:10.993
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:10.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:11
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-0fc8323f-1e59-4e8c-aea3-963f13945426 03/29/23 08:41:11.002
STEP: Creating a pod to test consume configMaps 03/29/23 08:41:11.005
Mar 29 08:41:11.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2" in namespace "projected-3624" to be "Succeeded or Failed"
Mar 29 08:41:11.009: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179485ms
Mar 29 08:41:13.012: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004128306s
Mar 29 08:41:15.011: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003519721s
STEP: Saw pod success 03/29/23 08:41:15.011
Mar 29 08:41:15.012: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2" satisfied condition "Succeeded or Failed"
Mar 29 08:41:15.013: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:41:15.016
Mar 29 08:41:15.022: INFO: Waiting for pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 to disappear
Mar 29 08:41:15.024: INFO: Pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:41:15.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3624" for this suite. 03/29/23 08:41:15.025
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":217,"skipped":3903,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:10.992
    Mar 29 08:41:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:41:10.993
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:10.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:11
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-0fc8323f-1e59-4e8c-aea3-963f13945426 03/29/23 08:41:11.002
    STEP: Creating a pod to test consume configMaps 03/29/23 08:41:11.005
    Mar 29 08:41:11.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2" in namespace "projected-3624" to be "Succeeded or Failed"
    Mar 29 08:41:11.009: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179485ms
    Mar 29 08:41:13.012: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004128306s
    Mar 29 08:41:15.011: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003519721s
    STEP: Saw pod success 03/29/23 08:41:15.011
    Mar 29 08:41:15.012: INFO: Pod "pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2" satisfied condition "Succeeded or Failed"
    Mar 29 08:41:15.013: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:41:15.016
    Mar 29 08:41:15.022: INFO: Waiting for pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 to disappear
    Mar 29 08:41:15.024: INFO: Pod pod-projected-configmaps-d6564680-3e90-4f6a-9a2f-752ed1ed43a2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:41:15.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3624" for this suite. 03/29/23 08:41:15.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:15.028
Mar 29 08:41:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename events 03/29/23 08:41:15.029
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.037
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/29/23 08:41:15.038
STEP: listing all events in all namespaces 03/29/23 08:41:15.041
STEP: patching the test event 03/29/23 08:41:15.044
STEP: fetching the test event 03/29/23 08:41:15.047
STEP: updating the test event 03/29/23 08:41:15.048
STEP: getting the test event 03/29/23 08:41:15.051
STEP: deleting the test event 03/29/23 08:41:15.052
STEP: listing all events in all namespaces 03/29/23 08:41:15.055
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 29 08:41:15.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6435" for this suite. 03/29/23 08:41:15.059
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":218,"skipped":3935,"failed":0}
------------------------------
â€¢ [0.033 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:15.028
    Mar 29 08:41:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename events 03/29/23 08:41:15.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.037
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/29/23 08:41:15.038
    STEP: listing all events in all namespaces 03/29/23 08:41:15.041
    STEP: patching the test event 03/29/23 08:41:15.044
    STEP: fetching the test event 03/29/23 08:41:15.047
    STEP: updating the test event 03/29/23 08:41:15.048
    STEP: getting the test event 03/29/23 08:41:15.051
    STEP: deleting the test event 03/29/23 08:41:15.052
    STEP: listing all events in all namespaces 03/29/23 08:41:15.055
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 29 08:41:15.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6435" for this suite. 03/29/23 08:41:15.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:15.062
Mar 29 08:41:15.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename namespaces 03/29/23 08:41:15.063
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.069
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/29/23 08:41:15.07
STEP: patching the Namespace 03/29/23 08:41:15.075
STEP: get the Namespace and ensuring it has the label 03/29/23 08:41:15.078
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:41:15.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-950" for this suite. 03/29/23 08:41:15.081
STEP: Destroying namespace "nspatchtest-566d72fb-7145-497b-a6fd-d86a661c59da-43" for this suite. 03/29/23 08:41:15.083
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":219,"skipped":3948,"failed":0}
------------------------------
â€¢ [0.023 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:15.062
    Mar 29 08:41:15.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename namespaces 03/29/23 08:41:15.063
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.069
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/29/23 08:41:15.07
    STEP: patching the Namespace 03/29/23 08:41:15.075
    STEP: get the Namespace and ensuring it has the label 03/29/23 08:41:15.078
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:41:15.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-950" for this suite. 03/29/23 08:41:15.081
    STEP: Destroying namespace "nspatchtest-566d72fb-7145-497b-a6fd-d86a661c59da-43" for this suite. 03/29/23 08:41:15.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:15.088
Mar 29 08:41:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:41:15.089
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.096
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4692 03/29/23 08:41:15.097
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/29/23 08:41:15.101
STEP: creating service externalsvc in namespace services-4692 03/29/23 08:41:15.101
STEP: creating replication controller externalsvc in namespace services-4692 03/29/23 08:41:15.107
I0329 08:41:15.111485      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4692, replica count: 2
I0329 08:41:18.162383      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/29/23 08:41:18.164
Mar 29 08:41:18.170: INFO: Creating new exec pod
Mar 29 08:41:18.173: INFO: Waiting up to 5m0s for pod "execpodwvxhr" in namespace "services-4692" to be "running"
Mar 29 08:41:18.174: INFO: Pod "execpodwvxhr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.350037ms
Mar 29 08:41:20.177: INFO: Pod "execpodwvxhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004133739s
Mar 29 08:41:20.177: INFO: Pod "execpodwvxhr" satisfied condition "running"
Mar 29 08:41:20.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4692 exec execpodwvxhr -- /bin/sh -x -c nslookup clusterip-service.services-4692.svc.cluster.local'
Mar 29 08:41:20.281: INFO: stderr: "+ nslookup clusterip-service.services-4692.svc.cluster.local\n"
Mar 29 08:41:20.281: INFO: stdout: "Server:\t\t10.146.0.115\nAddress:\t10.146.0.115#53\n\nNon-authoritative answer:\nclusterip-service.services-4692.svc.cluster.local\tcanonical name = externalsvc.services-4692.svc.cluster.local.\nName:\texternalsvc.services-4692.svc.cluster.local\nAddress: 10.100.177.219\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4692, will wait for the garbage collector to delete the pods 03/29/23 08:41:20.281
Mar 29 08:41:20.337: INFO: Deleting ReplicationController externalsvc took: 2.971667ms
Mar 29 08:41:20.437: INFO: Terminating ReplicationController externalsvc pods took: 100.380732ms
Mar 29 08:41:22.447: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:41:22.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4692" for this suite. 03/29/23 08:41:22.454
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":220,"skipped":4033,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.370 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:15.088
    Mar 29 08:41:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:41:15.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:15.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:15.096
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4692 03/29/23 08:41:15.097
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/29/23 08:41:15.101
    STEP: creating service externalsvc in namespace services-4692 03/29/23 08:41:15.101
    STEP: creating replication controller externalsvc in namespace services-4692 03/29/23 08:41:15.107
    I0329 08:41:15.111485      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4692, replica count: 2
    I0329 08:41:18.162383      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/29/23 08:41:18.164
    Mar 29 08:41:18.170: INFO: Creating new exec pod
    Mar 29 08:41:18.173: INFO: Waiting up to 5m0s for pod "execpodwvxhr" in namespace "services-4692" to be "running"
    Mar 29 08:41:18.174: INFO: Pod "execpodwvxhr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.350037ms
    Mar 29 08:41:20.177: INFO: Pod "execpodwvxhr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004133739s
    Mar 29 08:41:20.177: INFO: Pod "execpodwvxhr" satisfied condition "running"
    Mar 29 08:41:20.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4692 exec execpodwvxhr -- /bin/sh -x -c nslookup clusterip-service.services-4692.svc.cluster.local'
    Mar 29 08:41:20.281: INFO: stderr: "+ nslookup clusterip-service.services-4692.svc.cluster.local\n"
    Mar 29 08:41:20.281: INFO: stdout: "Server:\t\t10.146.0.115\nAddress:\t10.146.0.115#53\n\nNon-authoritative answer:\nclusterip-service.services-4692.svc.cluster.local\tcanonical name = externalsvc.services-4692.svc.cluster.local.\nName:\texternalsvc.services-4692.svc.cluster.local\nAddress: 10.100.177.219\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4692, will wait for the garbage collector to delete the pods 03/29/23 08:41:20.281
    Mar 29 08:41:20.337: INFO: Deleting ReplicationController externalsvc took: 2.971667ms
    Mar 29 08:41:20.437: INFO: Terminating ReplicationController externalsvc pods took: 100.380732ms
    Mar 29 08:41:22.447: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:41:22.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4692" for this suite. 03/29/23 08:41:22.454
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:22.463
Mar 29 08:41:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:41:22.463
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:22.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:22.474
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/29/23 08:41:22.476
Mar 29 08:41:22.480: INFO: Waiting up to 5m0s for pod "pod-zhzmk" in namespace "pods-9564" to be "running"
Mar 29 08:41:22.482: INFO: Pod "pod-zhzmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.533205ms
Mar 29 08:41:24.485: INFO: Pod "pod-zhzmk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004476062s
Mar 29 08:41:24.485: INFO: Pod "pod-zhzmk" satisfied condition "running"
STEP: patching /status 03/29/23 08:41:24.485
Mar 29 08:41:24.490: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 08:41:24.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9564" for this suite. 03/29/23 08:41:24.492
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":221,"skipped":4097,"failed":0}
------------------------------
â€¢ [2.034 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:22.463
    Mar 29 08:41:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:41:22.463
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:22.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:22.474
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/29/23 08:41:22.476
    Mar 29 08:41:22.480: INFO: Waiting up to 5m0s for pod "pod-zhzmk" in namespace "pods-9564" to be "running"
    Mar 29 08:41:22.482: INFO: Pod "pod-zhzmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.533205ms
    Mar 29 08:41:24.485: INFO: Pod "pod-zhzmk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004476062s
    Mar 29 08:41:24.485: INFO: Pod "pod-zhzmk" satisfied condition "running"
    STEP: patching /status 03/29/23 08:41:24.485
    Mar 29 08:41:24.490: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 08:41:24.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9564" for this suite. 03/29/23 08:41:24.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:24.497
Mar 29 08:41:24.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:41:24.498
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:24.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:24.505
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/29/23 08:41:24.506
Mar 29 08:41:24.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 create -f -'
Mar 29 08:41:25.082: INFO: stderr: ""
Mar 29 08:41:25.082: INFO: stdout: "pod/pause created\n"
Mar 29 08:41:25.082: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 29 08:41:25.082: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8239" to be "running and ready"
Mar 29 08:41:25.084: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19616ms
Mar 29 08:41:25.084: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.146.0.116' to be 'Running' but was 'Pending'
Mar 29 08:41:27.087: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005297925s
Mar 29 08:41:27.087: INFO: Pod "pause" satisfied condition "running and ready"
Mar 29 08:41:27.087: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/29/23 08:41:27.087
Mar 29 08:41:27.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 label pods pause testing-label=testing-label-value'
Mar 29 08:41:27.136: INFO: stderr: ""
Mar 29 08:41:27.136: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/29/23 08:41:27.136
Mar 29 08:41:27.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pod pause -L testing-label'
Mar 29 08:41:27.178: INFO: stderr: ""
Mar 29 08:41:27.178: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/29/23 08:41:27.178
Mar 29 08:41:27.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 label pods pause testing-label-'
Mar 29 08:41:27.224: INFO: stderr: ""
Mar 29 08:41:27.224: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/29/23 08:41:27.224
Mar 29 08:41:27.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pod pause -L testing-label'
Mar 29 08:41:27.265: INFO: stderr: ""
Mar 29 08:41:27.265: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/29/23 08:41:27.265
Mar 29 08:41:27.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 delete --grace-period=0 --force -f -'
Mar 29 08:41:27.311: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 29 08:41:27.311: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 29 08:41:27.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get rc,svc -l name=pause --no-headers'
Mar 29 08:41:27.355: INFO: stderr: "No resources found in kubectl-8239 namespace.\n"
Mar 29 08:41:27.355: INFO: stdout: ""
Mar 29 08:41:27.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 29 08:41:27.395: INFO: stderr: ""
Mar 29 08:41:27.395: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:41:27.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8239" for this suite. 03/29/23 08:41:27.397
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":222,"skipped":4125,"failed":0}
------------------------------
â€¢ [2.902 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:24.497
    Mar 29 08:41:24.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:41:24.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:24.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:24.505
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/29/23 08:41:24.506
    Mar 29 08:41:24.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 create -f -'
    Mar 29 08:41:25.082: INFO: stderr: ""
    Mar 29 08:41:25.082: INFO: stdout: "pod/pause created\n"
    Mar 29 08:41:25.082: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 29 08:41:25.082: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8239" to be "running and ready"
    Mar 29 08:41:25.084: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19616ms
    Mar 29 08:41:25.084: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.146.0.116' to be 'Running' but was 'Pending'
    Mar 29 08:41:27.087: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005297925s
    Mar 29 08:41:27.087: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 29 08:41:27.087: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/29/23 08:41:27.087
    Mar 29 08:41:27.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 label pods pause testing-label=testing-label-value'
    Mar 29 08:41:27.136: INFO: stderr: ""
    Mar 29 08:41:27.136: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/29/23 08:41:27.136
    Mar 29 08:41:27.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pod pause -L testing-label'
    Mar 29 08:41:27.178: INFO: stderr: ""
    Mar 29 08:41:27.178: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/29/23 08:41:27.178
    Mar 29 08:41:27.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 label pods pause testing-label-'
    Mar 29 08:41:27.224: INFO: stderr: ""
    Mar 29 08:41:27.224: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/29/23 08:41:27.224
    Mar 29 08:41:27.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pod pause -L testing-label'
    Mar 29 08:41:27.265: INFO: stderr: ""
    Mar 29 08:41:27.265: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/29/23 08:41:27.265
    Mar 29 08:41:27.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 delete --grace-period=0 --force -f -'
    Mar 29 08:41:27.311: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 29 08:41:27.311: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 29 08:41:27.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get rc,svc -l name=pause --no-headers'
    Mar 29 08:41:27.355: INFO: stderr: "No resources found in kubectl-8239 namespace.\n"
    Mar 29 08:41:27.355: INFO: stdout: ""
    Mar 29 08:41:27.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-8239 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 29 08:41:27.395: INFO: stderr: ""
    Mar 29 08:41:27.395: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:41:27.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8239" for this suite. 03/29/23 08:41:27.397
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:27.4
Mar 29 08:41:27.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:41:27.4
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:27.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:27.407
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/29/23 08:41:27.408
STEP: Creating a ResourceQuota 03/29/23 08:41:32.411
STEP: Ensuring resource quota status is calculated 03/29/23 08:41:32.413
STEP: Creating a ReplicaSet 03/29/23 08:41:34.415
STEP: Ensuring resource quota status captures replicaset creation 03/29/23 08:41:34.423
STEP: Deleting a ReplicaSet 03/29/23 08:41:36.426
STEP: Ensuring resource quota status released usage 03/29/23 08:41:36.429
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:41:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2870" for this suite. 03/29/23 08:41:38.433
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":223,"skipped":4126,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.036 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:27.4
    Mar 29 08:41:27.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:41:27.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:27.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:27.407
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/29/23 08:41:27.408
    STEP: Creating a ResourceQuota 03/29/23 08:41:32.411
    STEP: Ensuring resource quota status is calculated 03/29/23 08:41:32.413
    STEP: Creating a ReplicaSet 03/29/23 08:41:34.415
    STEP: Ensuring resource quota status captures replicaset creation 03/29/23 08:41:34.423
    STEP: Deleting a ReplicaSet 03/29/23 08:41:36.426
    STEP: Ensuring resource quota status released usage 03/29/23 08:41:36.429
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:41:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2870" for this suite. 03/29/23 08:41:38.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:38.436
Mar 29 08:41:38.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 08:41:38.437
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:38.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:38.446
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/29/23 08:41:38.447
Mar 29 08:41:38.451: INFO: Waiting up to 5m0s for pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36" in namespace "var-expansion-1818" to be "Succeeded or Failed"
Mar 29 08:41:38.452: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225717ms
Mar 29 08:41:40.454: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003847662s
Mar 29 08:41:42.455: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004840116s
STEP: Saw pod success 03/29/23 08:41:42.455
Mar 29 08:41:42.456: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36" satisfied condition "Succeeded or Failed"
Mar 29 08:41:42.457: INFO: Trying to get logs from node 10.146.0.116 pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:41:42.46
Mar 29 08:41:42.466: INFO: Waiting for pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 to disappear
Mar 29 08:41:42.467: INFO: Pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 08:41:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1818" for this suite. 03/29/23 08:41:42.468
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":224,"skipped":4160,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:38.436
    Mar 29 08:41:38.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 08:41:38.437
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:38.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:38.446
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/29/23 08:41:38.447
    Mar 29 08:41:38.451: INFO: Waiting up to 5m0s for pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36" in namespace "var-expansion-1818" to be "Succeeded or Failed"
    Mar 29 08:41:38.452: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225717ms
    Mar 29 08:41:40.454: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003847662s
    Mar 29 08:41:42.455: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004840116s
    STEP: Saw pod success 03/29/23 08:41:42.455
    Mar 29 08:41:42.456: INFO: Pod "var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36" satisfied condition "Succeeded or Failed"
    Mar 29 08:41:42.457: INFO: Trying to get logs from node 10.146.0.116 pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:41:42.46
    Mar 29 08:41:42.466: INFO: Waiting for pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 to disappear
    Mar 29 08:41:42.467: INFO: Pod var-expansion-184b71cf-798f-41e3-93b7-4f13dce92e36 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 08:41:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1818" for this suite. 03/29/23 08:41:42.468
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:42.473
Mar 29 08:41:42.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replicaset 03/29/23 08:41:42.474
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:42.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:42.481
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/29/23 08:41:42.482
STEP: Verify that the required pods have come up 03/29/23 08:41:42.485
Mar 29 08:41:42.486: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 29 08:41:47.490: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/29/23 08:41:47.49
Mar 29 08:41:47.491: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/29/23 08:41:47.491
STEP: DeleteCollection of the ReplicaSets 03/29/23 08:41:47.493
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/29/23 08:41:47.496
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 29 08:41:47.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9618" for this suite. 03/29/23 08:41:47.499
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":225,"skipped":4161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.032 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:42.473
    Mar 29 08:41:42.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replicaset 03/29/23 08:41:42.474
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:42.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:42.481
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/29/23 08:41:42.482
    STEP: Verify that the required pods have come up 03/29/23 08:41:42.485
    Mar 29 08:41:42.486: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 29 08:41:47.490: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/29/23 08:41:47.49
    Mar 29 08:41:47.491: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/29/23 08:41:47.491
    STEP: DeleteCollection of the ReplicaSets 03/29/23 08:41:47.493
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/29/23 08:41:47.496
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 29 08:41:47.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9618" for this suite. 03/29/23 08:41:47.499
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:41:47.505
Mar 29 08:41:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 08:41:47.506
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:47.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:47.517
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3380 03/29/23 08:41:47.52
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/29/23 08:41:47.523
Mar 29 08:41:47.527: INFO: Found 0 stateful pods, waiting for 3
Mar 29 08:41:57.530: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:41:57.530: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:41:57.530: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 08:41:57.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:41:57.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:41:57.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:41:57.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/29/23 08:42:07.634
Mar 29 08:42:07.648: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/29/23 08:42:07.648
STEP: Updating Pods in reverse ordinal order 03/29/23 08:42:17.657
Mar 29 08:42:17.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:42:17.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:42:17.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:42:17.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/29/23 08:42:27.758
Mar 29 08:42:27.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 29 08:42:27.844: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 29 08:42:27.844: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 29 08:42:27.844: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 29 08:42:37.870: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/29/23 08:42:47.877
Mar 29 08:42:47.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 29 08:42:47.962: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 29 08:42:47.962: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 29 08:42:47.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 08:42:57.973: INFO: Deleting all statefulset in ns statefulset-3380
Mar 29 08:42:57.974: INFO: Scaling statefulset ss2 to 0
Mar 29 08:43:07.986: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 08:43:07.987: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 08:43:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3380" for this suite. 03/29/23 08:43:07.995
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":226,"skipped":4161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.492 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:41:47.505
    Mar 29 08:41:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 08:41:47.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:41:47.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:41:47.517
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3380 03/29/23 08:41:47.52
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/29/23 08:41:47.523
    Mar 29 08:41:47.527: INFO: Found 0 stateful pods, waiting for 3
    Mar 29 08:41:57.530: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:41:57.530: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:41:57.530: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 08:41:57.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:41:57.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:41:57.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:41:57.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/29/23 08:42:07.634
    Mar 29 08:42:07.648: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/29/23 08:42:07.648
    STEP: Updating Pods in reverse ordinal order 03/29/23 08:42:17.657
    Mar 29 08:42:17.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:42:17.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:42:17.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:42:17.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/29/23 08:42:27.758
    Mar 29 08:42:27.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 29 08:42:27.844: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 29 08:42:27.844: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 29 08:42:27.844: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 29 08:42:37.870: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/29/23 08:42:47.877
    Mar 29 08:42:47.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=statefulset-3380 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 29 08:42:47.962: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 29 08:42:47.962: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 29 08:42:47.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 08:42:57.973: INFO: Deleting all statefulset in ns statefulset-3380
    Mar 29 08:42:57.974: INFO: Scaling statefulset ss2 to 0
    Mar 29 08:43:07.986: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 08:43:07.987: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 08:43:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3380" for this suite. 03/29/23 08:43:07.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:43:07.999
Mar 29 08:43:07.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 08:43:08
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:43:08.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:43:08.007
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/29/23 08:43:08.009
Mar 29 08:43:08.013: INFO: Waiting up to 2m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490" to be "running"
Mar 29 08:43:08.014: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30837ms
Mar 29 08:43:10.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004478075s
Mar 29 08:43:12.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004435087s
Mar 29 08:43:14.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003155757s
Mar 29 08:43:16.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004205328s
Mar 29 08:43:18.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003808238s
Mar 29 08:43:20.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004939831s
Mar 29 08:43:22.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004667256s
Mar 29 08:43:24.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.003714462s
Mar 29 08:43:26.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.003390601s
Mar 29 08:43:28.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004519962s
Mar 29 08:43:30.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004701159s
Mar 29 08:43:32.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003557853s
Mar 29 08:43:34.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004010422s
Mar 29 08:43:36.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004937929s
Mar 29 08:43:38.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005089558s
Mar 29 08:43:40.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0038336s
Mar 29 08:43:42.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004874788s
Mar 29 08:43:44.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.003560355s
Mar 29 08:43:46.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00355381s
Mar 29 08:43:48.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 40.003475378s
Mar 29 08:43:50.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004623927s
Mar 29 08:43:52.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004532459s
Mar 29 08:43:54.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.003504529s
Mar 29 08:43:56.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003589678s
Mar 29 08:43:58.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004417558s
Mar 29 08:44:00.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004333717s
Mar 29 08:44:02.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003596864s
Mar 29 08:44:04.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004004004s
Mar 29 08:44:06.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004071904s
Mar 29 08:44:08.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004877322s
Mar 29 08:44:10.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.003797606s
Mar 29 08:44:12.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004873827s
Mar 29 08:44:14.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.003600288s
Mar 29 08:44:16.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004467915s
Mar 29 08:44:18.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004231128s
Mar 29 08:44:20.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004238484s
Mar 29 08:44:22.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00513183s
Mar 29 08:44:24.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.003988744s
Mar 29 08:44:26.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003215748s
Mar 29 08:44:28.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004802213s
Mar 29 08:44:30.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.003005783s
Mar 29 08:44:32.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003048726s
Mar 29 08:44:34.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.003621303s
Mar 29 08:44:36.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004508813s
Mar 29 08:44:38.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004269231s
Mar 29 08:44:40.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.00440279s
Mar 29 08:44:42.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00435382s
Mar 29 08:44:44.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004237877s
Mar 29 08:44:46.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003775623s
Mar 29 08:44:48.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004607467s
Mar 29 08:44:50.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004696214s
Mar 29 08:44:52.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004612985s
Mar 29 08:44:54.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004122081s
Mar 29 08:44:56.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003758647s
Mar 29 08:44:58.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004577402s
Mar 29 08:45:00.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004605791s
Mar 29 08:45:02.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004516839s
Mar 29 08:45:04.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004041953s
Mar 29 08:45:06.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004945668s
Mar 29 08:45:08.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004680721s
Mar 29 08:45:08.019: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005995574s
STEP: updating the pod 03/29/23 08:45:08.019
Mar 29 08:45:08.527: INFO: Successfully updated pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4"
STEP: waiting for pod running 03/29/23 08:45:08.527
Mar 29 08:45:08.527: INFO: Waiting up to 2m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490" to be "running"
Mar 29 08:45:08.528: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457176ms
Mar 29 08:45:10.531: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004210364s
Mar 29 08:45:10.531: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" satisfied condition "running"
STEP: deleting the pod gracefully 03/29/23 08:45:10.531
Mar 29 08:45:10.531: INFO: Deleting pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490"
Mar 29 08:45:10.535: INFO: Wait up to 5m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 08:45:42.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-490" for this suite. 03/29/23 08:45:42.542
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":227,"skipped":4222,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.545 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:43:07.999
    Mar 29 08:43:07.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 08:43:08
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:43:08.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:43:08.007
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/29/23 08:43:08.009
    Mar 29 08:43:08.013: INFO: Waiting up to 2m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490" to be "running"
    Mar 29 08:43:08.014: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30837ms
    Mar 29 08:43:10.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004478075s
    Mar 29 08:43:12.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004435087s
    Mar 29 08:43:14.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003155757s
    Mar 29 08:43:16.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004205328s
    Mar 29 08:43:18.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003808238s
    Mar 29 08:43:20.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004939831s
    Mar 29 08:43:22.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004667256s
    Mar 29 08:43:24.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.003714462s
    Mar 29 08:43:26.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.003390601s
    Mar 29 08:43:28.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004519962s
    Mar 29 08:43:30.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004701159s
    Mar 29 08:43:32.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.003557853s
    Mar 29 08:43:34.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004010422s
    Mar 29 08:43:36.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004937929s
    Mar 29 08:43:38.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005089558s
    Mar 29 08:43:40.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0038336s
    Mar 29 08:43:42.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004874788s
    Mar 29 08:43:44.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.003560355s
    Mar 29 08:43:46.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00355381s
    Mar 29 08:43:48.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 40.003475378s
    Mar 29 08:43:50.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004623927s
    Mar 29 08:43:52.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004532459s
    Mar 29 08:43:54.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.003504529s
    Mar 29 08:43:56.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003589678s
    Mar 29 08:43:58.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004417558s
    Mar 29 08:44:00.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004333717s
    Mar 29 08:44:02.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003596864s
    Mar 29 08:44:04.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004004004s
    Mar 29 08:44:06.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004071904s
    Mar 29 08:44:08.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004877322s
    Mar 29 08:44:10.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.003797606s
    Mar 29 08:44:12.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004873827s
    Mar 29 08:44:14.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.003600288s
    Mar 29 08:44:16.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004467915s
    Mar 29 08:44:18.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004231128s
    Mar 29 08:44:20.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004238484s
    Mar 29 08:44:22.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00513183s
    Mar 29 08:44:24.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.003988744s
    Mar 29 08:44:26.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003215748s
    Mar 29 08:44:28.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004802213s
    Mar 29 08:44:30.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.003005783s
    Mar 29 08:44:32.016: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.003048726s
    Mar 29 08:44:34.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.003621303s
    Mar 29 08:44:36.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004508813s
    Mar 29 08:44:38.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004269231s
    Mar 29 08:44:40.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.00440279s
    Mar 29 08:44:42.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00435382s
    Mar 29 08:44:44.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004237877s
    Mar 29 08:44:46.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003775623s
    Mar 29 08:44:48.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004607467s
    Mar 29 08:44:50.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004696214s
    Mar 29 08:44:52.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004612985s
    Mar 29 08:44:54.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004122081s
    Mar 29 08:44:56.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003758647s
    Mar 29 08:44:58.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004577402s
    Mar 29 08:45:00.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004605791s
    Mar 29 08:45:02.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004516839s
    Mar 29 08:45:04.017: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004041953s
    Mar 29 08:45:06.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004945668s
    Mar 29 08:45:08.018: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004680721s
    Mar 29 08:45:08.019: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005995574s
    STEP: updating the pod 03/29/23 08:45:08.019
    Mar 29 08:45:08.527: INFO: Successfully updated pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4"
    STEP: waiting for pod running 03/29/23 08:45:08.527
    Mar 29 08:45:08.527: INFO: Waiting up to 2m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490" to be "running"
    Mar 29 08:45:08.528: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457176ms
    Mar 29 08:45:10.531: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004210364s
    Mar 29 08:45:10.531: INFO: Pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" satisfied condition "running"
    STEP: deleting the pod gracefully 03/29/23 08:45:10.531
    Mar 29 08:45:10.531: INFO: Deleting pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" in namespace "var-expansion-490"
    Mar 29 08:45:10.535: INFO: Wait up to 5m0s for pod "var-expansion-e7a3acf1-48a6-455e-80c0-54045a3fdea4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 08:45:42.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-490" for this suite. 03/29/23 08:45:42.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:45:42.545
Mar 29 08:45:42.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename replication-controller 03/29/23 08:45:42.546
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:42.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:42.555
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/29/23 08:45:42.558
STEP: waiting for RC to be added 03/29/23 08:45:42.56
STEP: waiting for available Replicas 03/29/23 08:45:42.56
STEP: patching ReplicationController 03/29/23 08:45:43.997
STEP: waiting for RC to be modified 03/29/23 08:45:44.002
STEP: patching ReplicationController status 03/29/23 08:45:44.002
STEP: waiting for RC to be modified 03/29/23 08:45:44.006
STEP: waiting for available Replicas 03/29/23 08:45:44.006
STEP: fetching ReplicationController status 03/29/23 08:45:44.009
STEP: patching ReplicationController scale 03/29/23 08:45:44.011
STEP: waiting for RC to be modified 03/29/23 08:45:44.014
STEP: waiting for ReplicationController's scale to be the max amount 03/29/23 08:45:44.014
STEP: fetching ReplicationController; ensuring that it's patched 03/29/23 08:45:44.732
STEP: updating ReplicationController status 03/29/23 08:45:44.734
STEP: waiting for RC to be modified 03/29/23 08:45:44.736
STEP: listing all ReplicationControllers 03/29/23 08:45:44.737
STEP: checking that ReplicationController has expected values 03/29/23 08:45:44.738
STEP: deleting ReplicationControllers by collection 03/29/23 08:45:44.738
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/29/23 08:45:44.741
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 29 08:45:44.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9170" for this suite. 03/29/23 08:45:44.775
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":228,"skipped":4228,"failed":0}
------------------------------
â€¢ [2.233 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:45:42.545
    Mar 29 08:45:42.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename replication-controller 03/29/23 08:45:42.546
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:42.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:42.555
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/29/23 08:45:42.558
    STEP: waiting for RC to be added 03/29/23 08:45:42.56
    STEP: waiting for available Replicas 03/29/23 08:45:42.56
    STEP: patching ReplicationController 03/29/23 08:45:43.997
    STEP: waiting for RC to be modified 03/29/23 08:45:44.002
    STEP: patching ReplicationController status 03/29/23 08:45:44.002
    STEP: waiting for RC to be modified 03/29/23 08:45:44.006
    STEP: waiting for available Replicas 03/29/23 08:45:44.006
    STEP: fetching ReplicationController status 03/29/23 08:45:44.009
    STEP: patching ReplicationController scale 03/29/23 08:45:44.011
    STEP: waiting for RC to be modified 03/29/23 08:45:44.014
    STEP: waiting for ReplicationController's scale to be the max amount 03/29/23 08:45:44.014
    STEP: fetching ReplicationController; ensuring that it's patched 03/29/23 08:45:44.732
    STEP: updating ReplicationController status 03/29/23 08:45:44.734
    STEP: waiting for RC to be modified 03/29/23 08:45:44.736
    STEP: listing all ReplicationControllers 03/29/23 08:45:44.737
    STEP: checking that ReplicationController has expected values 03/29/23 08:45:44.738
    STEP: deleting ReplicationControllers by collection 03/29/23 08:45:44.738
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/29/23 08:45:44.741
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 29 08:45:44.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9170" for this suite. 03/29/23 08:45:44.775
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:45:44.778
Mar 29 08:45:44.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:45:44.778
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:44.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:44.786
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-20df151b-6559-4d8d-8c38-2598282211d3 03/29/23 08:45:44.787
STEP: Creating a pod to test consume configMaps 03/29/23 08:45:44.789
Mar 29 08:45:44.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff" in namespace "configmap-6678" to be "Succeeded or Failed"
Mar 29 08:45:44.793: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.241957ms
Mar 29 08:45:46.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003763601s
Mar 29 08:45:48.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004163467s
STEP: Saw pod success 03/29/23 08:45:48.796
Mar 29 08:45:48.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff" satisfied condition "Succeeded or Failed"
Mar 29 08:45:48.798: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:45:48.805
Mar 29 08:45:48.812: INFO: Waiting for pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff to disappear
Mar 29 08:45:48.813: INFO: Pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:45:48.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6678" for this suite. 03/29/23 08:45:48.815
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":229,"skipped":4231,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:45:44.778
    Mar 29 08:45:44.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:45:44.778
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:44.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:44.786
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-20df151b-6559-4d8d-8c38-2598282211d3 03/29/23 08:45:44.787
    STEP: Creating a pod to test consume configMaps 03/29/23 08:45:44.789
    Mar 29 08:45:44.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff" in namespace "configmap-6678" to be "Succeeded or Failed"
    Mar 29 08:45:44.793: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.241957ms
    Mar 29 08:45:46.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003763601s
    Mar 29 08:45:48.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004163467s
    STEP: Saw pod success 03/29/23 08:45:48.796
    Mar 29 08:45:48.796: INFO: Pod "pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff" satisfied condition "Succeeded or Failed"
    Mar 29 08:45:48.798: INFO: Trying to get logs from node 10.146.0.115 pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:45:48.805
    Mar 29 08:45:48.812: INFO: Waiting for pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff to disappear
    Mar 29 08:45:48.813: INFO: Pod pod-configmaps-84a3a337-e7a9-462e-96b3-d8984e446fff no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:45:48.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6678" for this suite. 03/29/23 08:45:48.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:45:48.818
Mar 29 08:45:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:45:48.818
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:48.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:48.825
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar 29 08:45:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:45:50.695
Mar 29 08:45:50.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 create -f -'
Mar 29 08:45:51.165: INFO: stderr: ""
Mar 29 08:45:51.165: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 29 08:45:51.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-2517-crds test-cr'
Mar 29 08:45:51.210: INFO: stderr: ""
Mar 29 08:45:51.210: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 29 08:45:51.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 apply -f -'
Mar 29 08:45:51.338: INFO: stderr: ""
Mar 29 08:45:51.338: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 29 08:45:51.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-2517-crds test-cr'
Mar 29 08:45:51.380: INFO: stderr: ""
Mar 29 08:45:51.380: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/29/23 08:45:51.38
Mar 29 08:45:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 explain e2e-test-crd-publish-openapi-2517-crds'
Mar 29 08:45:51.499: INFO: stderr: ""
Mar 29 08:45:51.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2517-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:45:53.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2428" for this suite. 03/29/23 08:45:53.33
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":230,"skipped":4248,"failed":0}
------------------------------
â€¢ [4.514 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:45:48.818
    Mar 29 08:45:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:45:48.818
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:48.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:48.825
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar 29 08:45:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:45:50.695
    Mar 29 08:45:50.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 create -f -'
    Mar 29 08:45:51.165: INFO: stderr: ""
    Mar 29 08:45:51.165: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 29 08:45:51.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-2517-crds test-cr'
    Mar 29 08:45:51.210: INFO: stderr: ""
    Mar 29 08:45:51.210: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 29 08:45:51.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 apply -f -'
    Mar 29 08:45:51.338: INFO: stderr: ""
    Mar 29 08:45:51.338: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 29 08:45:51.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-2517-crds test-cr'
    Mar 29 08:45:51.380: INFO: stderr: ""
    Mar 29 08:45:51.380: INFO: stdout: "e2e-test-crd-publish-openapi-2517-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/29/23 08:45:51.38
    Mar 29 08:45:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-2428 explain e2e-test-crd-publish-openapi-2517-crds'
    Mar 29 08:45:51.499: INFO: stderr: ""
    Mar 29 08:45:51.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2517-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:45:53.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2428" for this suite. 03/29/23 08:45:53.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:45:53.333
Mar 29 08:45:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:45:53.334
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:53.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:53.342
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar 29 08:45:53.349: INFO: created pod
Mar 29 08:45:53.349: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4533" to be "Succeeded or Failed"
Mar 29 08:45:53.351: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.181318ms
Mar 29 08:45:55.353: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003487976s
Mar 29 08:45:57.353: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003622523s
STEP: Saw pod success 03/29/23 08:45:57.353
Mar 29 08:45:57.353: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 29 08:46:27.355: INFO: polling logs
Mar 29 08:46:27.364: INFO: Pod logs: 
I0329 08:45:53.918934       1 log.go:195] OK: Got token
I0329 08:45:53.918984       1 log.go:195] validating with in-cluster discovery
I0329 08:45:53.919191       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0329 08:45:53.919215       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4533:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680080153, NotBefore:1680079553, IssuedAt:1680079553, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4533", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2b33432-91a3-4607-b486-07494875eb1b"}}}
I0329 08:45:53.927267       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0329 08:45:53.931206       1 log.go:195] OK: Validated signature on JWT
I0329 08:45:53.931271       1 log.go:195] OK: Got valid claims from token!
I0329 08:45:53.931290       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4533:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680080153, NotBefore:1680079553, IssuedAt:1680079553, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4533", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2b33432-91a3-4607-b486-07494875eb1b"}}}

Mar 29 08:46:27.364: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 08:46:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4533" for this suite. 03/29/23 08:46:27.368
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":231,"skipped":4288,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.037 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:45:53.333
    Mar 29 08:45:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:45:53.334
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:45:53.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:45:53.342
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar 29 08:45:53.349: INFO: created pod
    Mar 29 08:45:53.349: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4533" to be "Succeeded or Failed"
    Mar 29 08:45:53.351: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.181318ms
    Mar 29 08:45:55.353: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003487976s
    Mar 29 08:45:57.353: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003622523s
    STEP: Saw pod success 03/29/23 08:45:57.353
    Mar 29 08:45:57.353: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 29 08:46:27.355: INFO: polling logs
    Mar 29 08:46:27.364: INFO: Pod logs: 
    I0329 08:45:53.918934       1 log.go:195] OK: Got token
    I0329 08:45:53.918984       1 log.go:195] validating with in-cluster discovery
    I0329 08:45:53.919191       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0329 08:45:53.919215       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4533:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680080153, NotBefore:1680079553, IssuedAt:1680079553, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4533", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2b33432-91a3-4607-b486-07494875eb1b"}}}
    I0329 08:45:53.927267       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0329 08:45:53.931206       1 log.go:195] OK: Validated signature on JWT
    I0329 08:45:53.931271       1 log.go:195] OK: Got valid claims from token!
    I0329 08:45:53.931290       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4533:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680080153, NotBefore:1680079553, IssuedAt:1680079553, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4533", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2b33432-91a3-4607-b486-07494875eb1b"}}}

    Mar 29 08:46:27.364: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 08:46:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4533" for this suite. 03/29/23 08:46:27.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:46:27.373
Mar 29 08:46:27.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename proxy 03/29/23 08:46:27.373
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:27.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:27.38
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 29 08:46:27.381: INFO: Creating pod...
Mar 29 08:46:27.385: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2846" to be "running"
Mar 29 08:46:27.386: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179168ms
Mar 29 08:46:29.389: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003816295s
Mar 29 08:46:29.389: INFO: Pod "agnhost" satisfied condition "running"
Mar 29 08:46:29.389: INFO: Creating service...
Mar 29 08:46:29.394: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/DELETE
Mar 29 08:46:29.397: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 29 08:46:29.397: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/GET
Mar 29 08:46:29.400: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 29 08:46:29.400: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/HEAD
Mar 29 08:46:29.401: INFO: http.Client request:HEAD | StatusCode:200
Mar 29 08:46:29.401: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 29 08:46:29.403: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 29 08:46:29.403: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/PATCH
Mar 29 08:46:29.404: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 29 08:46:29.404: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/POST
Mar 29 08:46:29.406: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 29 08:46:29.406: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/PUT
Mar 29 08:46:29.409: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 29 08:46:29.409: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/DELETE
Mar 29 08:46:29.411: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 29 08:46:29.411: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/GET
Mar 29 08:46:29.414: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 29 08:46:29.414: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/HEAD
Mar 29 08:46:29.416: INFO: http.Client request:HEAD | StatusCode:200
Mar 29 08:46:29.416: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/OPTIONS
Mar 29 08:46:29.419: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 29 08:46:29.419: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/PATCH
Mar 29 08:46:29.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 29 08:46:29.421: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/POST
Mar 29 08:46:29.423: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 29 08:46:29.423: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/PUT
Mar 29 08:46:29.425: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 29 08:46:29.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2846" for this suite. 03/29/23 08:46:29.427
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":232,"skipped":4352,"failed":0}
------------------------------
â€¢ [2.059 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:46:27.373
    Mar 29 08:46:27.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename proxy 03/29/23 08:46:27.373
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:27.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:27.38
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 29 08:46:27.381: INFO: Creating pod...
    Mar 29 08:46:27.385: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2846" to be "running"
    Mar 29 08:46:27.386: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179168ms
    Mar 29 08:46:29.389: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003816295s
    Mar 29 08:46:29.389: INFO: Pod "agnhost" satisfied condition "running"
    Mar 29 08:46:29.389: INFO: Creating service...
    Mar 29 08:46:29.394: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/DELETE
    Mar 29 08:46:29.397: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 29 08:46:29.397: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/GET
    Mar 29 08:46:29.400: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 29 08:46:29.400: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/HEAD
    Mar 29 08:46:29.401: INFO: http.Client request:HEAD | StatusCode:200
    Mar 29 08:46:29.401: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 29 08:46:29.403: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 29 08:46:29.403: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/PATCH
    Mar 29 08:46:29.404: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 29 08:46:29.404: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/POST
    Mar 29 08:46:29.406: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 29 08:46:29.406: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/pods/agnhost/proxy/some/path/with/PUT
    Mar 29 08:46:29.409: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 29 08:46:29.409: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/DELETE
    Mar 29 08:46:29.411: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 29 08:46:29.411: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/GET
    Mar 29 08:46:29.414: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 29 08:46:29.414: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/HEAD
    Mar 29 08:46:29.416: INFO: http.Client request:HEAD | StatusCode:200
    Mar 29 08:46:29.416: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/OPTIONS
    Mar 29 08:46:29.419: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 29 08:46:29.419: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/PATCH
    Mar 29 08:46:29.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 29 08:46:29.421: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/POST
    Mar 29 08:46:29.423: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 29 08:46:29.423: INFO: Starting http.Client for https://10.100.0.1:443/api/v1/namespaces/proxy-2846/services/test-service/proxy/some/path/with/PUT
    Mar 29 08:46:29.425: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 29 08:46:29.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2846" for this suite. 03/29/23 08:46:29.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:46:29.432
Mar 29 08:46:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:46:29.433
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:29.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:29.44
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/29/23 08:46:29.441
Mar 29 08:46:29.446: INFO: Waiting up to 5m0s for pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f" in namespace "emptydir-5884" to be "Succeeded or Failed"
Mar 29 08:46:29.447: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.280477ms
Mar 29 08:46:31.450: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432033s
Mar 29 08:46:33.449: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003438254s
STEP: Saw pod success 03/29/23 08:46:33.449
Mar 29 08:46:33.449: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f" satisfied condition "Succeeded or Failed"
Mar 29 08:46:33.450: INFO: Trying to get logs from node 10.146.0.115 pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f container test-container: <nil>
STEP: delete the pod 03/29/23 08:46:33.453
Mar 29 08:46:33.459: INFO: Waiting for pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f to disappear
Mar 29 08:46:33.462: INFO: Pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:46:33.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5884" for this suite. 03/29/23 08:46:33.463
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4361,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:46:29.432
    Mar 29 08:46:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:46:29.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:29.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:29.44
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/29/23 08:46:29.441
    Mar 29 08:46:29.446: INFO: Waiting up to 5m0s for pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f" in namespace "emptydir-5884" to be "Succeeded or Failed"
    Mar 29 08:46:29.447: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.280477ms
    Mar 29 08:46:31.450: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432033s
    Mar 29 08:46:33.449: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003438254s
    STEP: Saw pod success 03/29/23 08:46:33.449
    Mar 29 08:46:33.449: INFO: Pod "pod-07c3a369-a69e-4498-8fe3-b5cd0711441f" satisfied condition "Succeeded or Failed"
    Mar 29 08:46:33.450: INFO: Trying to get logs from node 10.146.0.115 pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f container test-container: <nil>
    STEP: delete the pod 03/29/23 08:46:33.453
    Mar 29 08:46:33.459: INFO: Waiting for pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f to disappear
    Mar 29 08:46:33.462: INFO: Pod pod-07c3a369-a69e-4498-8fe3-b5cd0711441f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:46:33.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5884" for this suite. 03/29/23 08:46:33.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:46:33.466
Mar 29 08:46:33.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:46:33.466
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:33.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:33.474
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:46:33.475
Mar 29 08:46:33.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba" in namespace "projected-3391" to be "Succeeded or Failed"
Mar 29 08:46:33.481: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.187332ms
Mar 29 08:46:35.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Running", Reason="", readiness=false. Elapsed: 2.003969343s
Mar 29 08:46:37.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003932558s
STEP: Saw pod success 03/29/23 08:46:37.483
Mar 29 08:46:37.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba" satisfied condition "Succeeded or Failed"
Mar 29 08:46:37.485: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba container client-container: <nil>
STEP: delete the pod 03/29/23 08:46:37.488
Mar 29 08:46:37.494: INFO: Waiting for pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba to disappear
Mar 29 08:46:37.496: INFO: Pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:46:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3391" for this suite. 03/29/23 08:46:37.497
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":234,"skipped":4368,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:46:33.466
    Mar 29 08:46:33.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:46:33.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:33.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:33.474
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:46:33.475
    Mar 29 08:46:33.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba" in namespace "projected-3391" to be "Succeeded or Failed"
    Mar 29 08:46:33.481: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.187332ms
    Mar 29 08:46:35.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Running", Reason="", readiness=false. Elapsed: 2.003969343s
    Mar 29 08:46:37.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003932558s
    STEP: Saw pod success 03/29/23 08:46:37.483
    Mar 29 08:46:37.483: INFO: Pod "downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba" satisfied condition "Succeeded or Failed"
    Mar 29 08:46:37.485: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba container client-container: <nil>
    STEP: delete the pod 03/29/23 08:46:37.488
    Mar 29 08:46:37.494: INFO: Waiting for pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba to disappear
    Mar 29 08:46:37.496: INFO: Pod downwardapi-volume-6ecd91e8-6905-406a-98c8-e2f878d64bba no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:46:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3391" for this suite. 03/29/23 08:46:37.497
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:46:37.5
Mar 29 08:46:37.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 08:46:37.501
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:37.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:37.507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 08:46:37.514
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:46:37.813
STEP: Deploying the webhook pod 03/29/23 08:46:37.816
STEP: Wait for the deployment to be ready 03/29/23 08:46:37.822
Mar 29 08:46:37.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 08:46:39.831
STEP: Verifying the service has paired with the endpoint 03/29/23 08:46:39.837
Mar 29 08:46:40.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/29/23 08:46:40.84
STEP: create a pod 03/29/23 08:46:40.849
Mar 29 08:46:40.852: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6595" to be "running"
Mar 29 08:46:40.853: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245077ms
Mar 29 08:46:42.856: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812505s
Mar 29 08:46:42.856: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/29/23 08:46:42.856
Mar 29 08:46:42.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=webhook-6595 attach --namespace=webhook-6595 to-be-attached-pod -i -c=container1'
Mar 29 08:46:42.906: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:46:42.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6595" for this suite. 03/29/23 08:46:42.924
STEP: Destroying namespace "webhook-6595-markers" for this suite. 03/29/23 08:46:42.927
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":235,"skipped":4370,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:46:37.5
    Mar 29 08:46:37.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 08:46:37.501
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:37.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:37.507
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 08:46:37.514
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:46:37.813
    STEP: Deploying the webhook pod 03/29/23 08:46:37.816
    STEP: Wait for the deployment to be ready 03/29/23 08:46:37.822
    Mar 29 08:46:37.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 08:46:39.831
    STEP: Verifying the service has paired with the endpoint 03/29/23 08:46:39.837
    Mar 29 08:46:40.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/29/23 08:46:40.84
    STEP: create a pod 03/29/23 08:46:40.849
    Mar 29 08:46:40.852: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6595" to be "running"
    Mar 29 08:46:40.853: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245077ms
    Mar 29 08:46:42.856: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003812505s
    Mar 29 08:46:42.856: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/29/23 08:46:42.856
    Mar 29 08:46:42.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=webhook-6595 attach --namespace=webhook-6595 to-be-attached-pod -i -c=container1'
    Mar 29 08:46:42.906: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:46:42.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6595" for this suite. 03/29/23 08:46:42.924
    STEP: Destroying namespace "webhook-6595-markers" for this suite. 03/29/23 08:46:42.927
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:46:42.953
Mar 29 08:46:42.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir-wrapper 03/29/23 08:46:42.953
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:42.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:42.961
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/29/23 08:46:42.962
STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:46:43.206
Mar 29 08:46:43.307: INFO: Pod name wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/29/23 08:46:43.307
Mar 29 08:46:43.307: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:46:43.356: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 48.707389ms
Mar 29 08:46:45.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052114863s
Mar 29 08:46:47.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051699714s
Mar 29 08:46:49.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053023621s
Mar 29 08:46:51.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052065352s
Mar 29 08:46:53.358: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050986944s
Mar 29 08:46:55.358: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 12.051138542s
Mar 29 08:46:57.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.052546425s
Mar 29 08:46:57.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m" satisfied condition "running"
Mar 29 08:46:57.360: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:46:57.361: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6": Phase="Running", Reason="", readiness=true. Elapsed: 1.712756ms
Mar 29 08:46:57.361: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6" satisfied condition "running"
Mar 29 08:46:57.361: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:46:57.363: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt": Phase="Running", Reason="", readiness=true. Elapsed: 1.547331ms
Mar 29 08:46:57.363: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt" satisfied condition "running"
Mar 29 08:46:57.363: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:46:57.364: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49": Phase="Running", Reason="", readiness=true. Elapsed: 1.407753ms
Mar 29 08:46:57.364: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49" satisfied condition "running"
Mar 29 08:46:57.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:46:57.366: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv": Phase="Running", Reason="", readiness=true. Elapsed: 1.425051ms
Mar 29 08:46:57.366: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:46:57.366
Mar 29 08:46:57.421: INFO: Deleting ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 took: 3.291672ms
Mar 29 08:46:57.522: INFO: Terminating ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 pods took: 101.069974ms
STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:47:00.725
Mar 29 08:47:00.732: INFO: Pod name wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203: Found 0 pods out of 5
Mar 29 08:47:05.737: INFO: Pod name wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/29/23 08:47:05.737
Mar 29 08:47:05.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:05.739: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643295ms
Mar 29 08:47:07.741: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003885882s
Mar 29 08:47:09.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005224668s
Mar 29 08:47:11.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005171218s
Mar 29 08:47:13.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004642519s
Mar 29 08:47:15.743: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Running", Reason="", readiness=true. Elapsed: 10.00544198s
Mar 29 08:47:15.743: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2" satisfied condition "running"
Mar 29 08:47:15.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:15.744: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v": Phase="Running", Reason="", readiness=true. Elapsed: 1.651581ms
Mar 29 08:47:15.744: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v" satisfied condition "running"
Mar 29 08:47:15.744: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:15.746: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625874ms
Mar 29 08:47:17.749: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005004615s
Mar 29 08:47:17.749: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm" satisfied condition "running"
Mar 29 08:47:17.749: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:17.751: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm": Phase="Running", Reason="", readiness=true. Elapsed: 1.715843ms
Mar 29 08:47:17.751: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm" satisfied condition "running"
Mar 29 08:47:17.751: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:17.753: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9": Phase="Running", Reason="", readiness=true. Elapsed: 1.566691ms
Mar 29 08:47:17.753: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:47:17.753
Mar 29 08:47:17.807: INFO: Deleting ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 took: 2.875904ms
Mar 29 08:47:17.908: INFO: Terminating ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 pods took: 100.871945ms
STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:47:21.311
Mar 29 08:47:21.330: INFO: Pod name wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8: Found 0 pods out of 5
Mar 29 08:47:26.335: INFO: Pod name wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/29/23 08:47:26.335
Mar 29 08:47:26.335: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:26.336: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604937ms
Mar 29 08:47:28.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004573549s
Mar 29 08:47:30.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005737482s
Mar 29 08:47:32.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004235511s
Mar 29 08:47:34.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004015366s
Mar 29 08:47:36.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Running", Reason="", readiness=true. Elapsed: 10.005179838s
Mar 29 08:47:36.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b" satisfied condition "running"
Mar 29 08:47:36.340: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:36.342: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788047ms
Mar 29 08:47:38.345: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004803284s
Mar 29 08:47:38.345: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp" satisfied condition "running"
Mar 29 08:47:38.345: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:38.346: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v": Phase="Running", Reason="", readiness=true. Elapsed: 1.669769ms
Mar 29 08:47:38.346: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v" satisfied condition "running"
Mar 29 08:47:38.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:38.348: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt": Phase="Running", Reason="", readiness=true. Elapsed: 1.699682ms
Mar 29 08:47:38.348: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt" satisfied condition "running"
Mar 29 08:47:38.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r" in namespace "emptydir-wrapper-63" to be "running"
Mar 29 08:47:38.350: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r": Phase="Running", Reason="", readiness=true. Elapsed: 1.567507ms
Mar 29 08:47:38.350: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:47:38.35
Mar 29 08:47:38.405: INFO: Deleting ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 took: 2.951367ms
Mar 29 08:47:38.506: INFO: Terminating ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 pods took: 100.986973ms
STEP: Cleaning up the configMaps 03/29/23 08:47:40.907
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 29 08:47:41.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-63" for this suite. 03/29/23 08:47:41.011
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":236,"skipped":4415,"failed":0}
------------------------------
â€¢ [SLOW TEST] [58.060 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:46:42.953
    Mar 29 08:46:42.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir-wrapper 03/29/23 08:46:42.953
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:46:42.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:46:42.961
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/29/23 08:46:42.962
    STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:46:43.206
    Mar 29 08:46:43.307: INFO: Pod name wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/29/23 08:46:43.307
    Mar 29 08:46:43.307: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:46:43.356: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 48.707389ms
    Mar 29 08:46:45.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052114863s
    Mar 29 08:46:47.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051699714s
    Mar 29 08:46:49.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053023621s
    Mar 29 08:46:51.359: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052065352s
    Mar 29 08:46:53.358: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050986944s
    Mar 29 08:46:55.358: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Pending", Reason="", readiness=false. Elapsed: 12.051138542s
    Mar 29 08:46:57.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.052546425s
    Mar 29 08:46:57.360: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-krg8m" satisfied condition "running"
    Mar 29 08:46:57.360: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:46:57.361: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6": Phase="Running", Reason="", readiness=true. Elapsed: 1.712756ms
    Mar 29 08:46:57.361: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-lqrv6" satisfied condition "running"
    Mar 29 08:46:57.361: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:46:57.363: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt": Phase="Running", Reason="", readiness=true. Elapsed: 1.547331ms
    Mar 29 08:46:57.363: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-pzptt" satisfied condition "running"
    Mar 29 08:46:57.363: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:46:57.364: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49": Phase="Running", Reason="", readiness=true. Elapsed: 1.407753ms
    Mar 29 08:46:57.364: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wcx49" satisfied condition "running"
    Mar 29 08:46:57.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:46:57.366: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv": Phase="Running", Reason="", readiness=true. Elapsed: 1.425051ms
    Mar 29 08:46:57.366: INFO: Pod "wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173-wvdmv" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:46:57.366
    Mar 29 08:46:57.421: INFO: Deleting ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 took: 3.291672ms
    Mar 29 08:46:57.522: INFO: Terminating ReplicationController wrapped-volume-race-29347996-4b8c-4ef3-a91e-8a45d3433173 pods took: 101.069974ms
    STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:47:00.725
    Mar 29 08:47:00.732: INFO: Pod name wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203: Found 0 pods out of 5
    Mar 29 08:47:05.737: INFO: Pod name wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/29/23 08:47:05.737
    Mar 29 08:47:05.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:05.739: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643295ms
    Mar 29 08:47:07.741: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003885882s
    Mar 29 08:47:09.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005224668s
    Mar 29 08:47:11.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005171218s
    Mar 29 08:47:13.742: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004642519s
    Mar 29 08:47:15.743: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2": Phase="Running", Reason="", readiness=true. Elapsed: 10.00544198s
    Mar 29 08:47:15.743: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-76jj2" satisfied condition "running"
    Mar 29 08:47:15.743: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:15.744: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v": Phase="Running", Reason="", readiness=true. Elapsed: 1.651581ms
    Mar 29 08:47:15.744: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-j2g2v" satisfied condition "running"
    Mar 29 08:47:15.744: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:15.746: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625874ms
    Mar 29 08:47:17.749: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm": Phase="Running", Reason="", readiness=true. Elapsed: 2.005004615s
    Mar 29 08:47:17.749: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k44tm" satisfied condition "running"
    Mar 29 08:47:17.749: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:17.751: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm": Phase="Running", Reason="", readiness=true. Elapsed: 1.715843ms
    Mar 29 08:47:17.751: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-k5mfm" satisfied condition "running"
    Mar 29 08:47:17.751: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:17.753: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9": Phase="Running", Reason="", readiness=true. Elapsed: 1.566691ms
    Mar 29 08:47:17.753: INFO: Pod "wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203-mrdj9" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:47:17.753
    Mar 29 08:47:17.807: INFO: Deleting ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 took: 2.875904ms
    Mar 29 08:47:17.908: INFO: Terminating ReplicationController wrapped-volume-race-053c65ea-d373-4074-8384-b22e3b39f203 pods took: 100.871945ms
    STEP: Creating RC which spawns configmap-volume pods 03/29/23 08:47:21.311
    Mar 29 08:47:21.330: INFO: Pod name wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8: Found 0 pods out of 5
    Mar 29 08:47:26.335: INFO: Pod name wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/29/23 08:47:26.335
    Mar 29 08:47:26.335: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:26.336: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604937ms
    Mar 29 08:47:28.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004573549s
    Mar 29 08:47:30.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005737482s
    Mar 29 08:47:32.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004235511s
    Mar 29 08:47:34.339: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004015366s
    Mar 29 08:47:36.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b": Phase="Running", Reason="", readiness=true. Elapsed: 10.005179838s
    Mar 29 08:47:36.340: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-7jx6b" satisfied condition "running"
    Mar 29 08:47:36.340: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:36.342: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788047ms
    Mar 29 08:47:38.345: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004803284s
    Mar 29 08:47:38.345: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-fvlmp" satisfied condition "running"
    Mar 29 08:47:38.345: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:38.346: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v": Phase="Running", Reason="", readiness=true. Elapsed: 1.669769ms
    Mar 29 08:47:38.346: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-kpd2v" satisfied condition "running"
    Mar 29 08:47:38.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:38.348: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt": Phase="Running", Reason="", readiness=true. Elapsed: 1.699682ms
    Mar 29 08:47:38.348: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-mjrqt" satisfied condition "running"
    Mar 29 08:47:38.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r" in namespace "emptydir-wrapper-63" to be "running"
    Mar 29 08:47:38.350: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r": Phase="Running", Reason="", readiness=true. Elapsed: 1.567507ms
    Mar 29 08:47:38.350: INFO: Pod "wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8-tnk4r" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 in namespace emptydir-wrapper-63, will wait for the garbage collector to delete the pods 03/29/23 08:47:38.35
    Mar 29 08:47:38.405: INFO: Deleting ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 took: 2.951367ms
    Mar 29 08:47:38.506: INFO: Terminating ReplicationController wrapped-volume-race-c7152169-1552-4828-9dc0-59bfd11d88d8 pods took: 100.986973ms
    STEP: Cleaning up the configMaps 03/29/23 08:47:40.907
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:47:41.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-63" for this suite. 03/29/23 08:47:41.011
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:47:41.013
Mar 29 08:47:41.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption 03/29/23 08:47:41.014
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:41.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:41.023
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/29/23 08:47:41.024
STEP: Waiting for the pdb to be processed 03/29/23 08:47:41.026
STEP: First trying to evict a pod which shouldn't be evictable 03/29/23 08:47:43.032
STEP: Waiting for all pods to be running 03/29/23 08:47:43.032
Mar 29 08:47:43.033: INFO: pods: 0 < 3
STEP: locating a running pod 03/29/23 08:47:45.036
STEP: Updating the pdb to allow a pod to be evicted 03/29/23 08:47:45.04
STEP: Waiting for the pdb to be processed 03/29/23 08:47:45.044
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/29/23 08:47:47.048
STEP: Waiting for all pods to be running 03/29/23 08:47:47.048
STEP: Waiting for the pdb to observed all healthy pods 03/29/23 08:47:47.049
STEP: Patching the pdb to disallow a pod to be evicted 03/29/23 08:47:47.058
STEP: Waiting for the pdb to be processed 03/29/23 08:47:47.066
STEP: Waiting for all pods to be running 03/29/23 08:47:49.069
STEP: locating a running pod 03/29/23 08:47:49.071
STEP: Deleting the pdb to allow a pod to be evicted 03/29/23 08:47:49.075
STEP: Waiting for the pdb to be deleted 03/29/23 08:47:49.077
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/29/23 08:47:49.078
STEP: Waiting for all pods to be running 03/29/23 08:47:49.078
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 29 08:47:49.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5620" for this suite. 03/29/23 08:47:49.085
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":237,"skipped":4418,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:47:41.013
    Mar 29 08:47:41.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption 03/29/23 08:47:41.014
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:41.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:41.023
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/29/23 08:47:41.024
    STEP: Waiting for the pdb to be processed 03/29/23 08:47:41.026
    STEP: First trying to evict a pod which shouldn't be evictable 03/29/23 08:47:43.032
    STEP: Waiting for all pods to be running 03/29/23 08:47:43.032
    Mar 29 08:47:43.033: INFO: pods: 0 < 3
    STEP: locating a running pod 03/29/23 08:47:45.036
    STEP: Updating the pdb to allow a pod to be evicted 03/29/23 08:47:45.04
    STEP: Waiting for the pdb to be processed 03/29/23 08:47:45.044
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/29/23 08:47:47.048
    STEP: Waiting for all pods to be running 03/29/23 08:47:47.048
    STEP: Waiting for the pdb to observed all healthy pods 03/29/23 08:47:47.049
    STEP: Patching the pdb to disallow a pod to be evicted 03/29/23 08:47:47.058
    STEP: Waiting for the pdb to be processed 03/29/23 08:47:47.066
    STEP: Waiting for all pods to be running 03/29/23 08:47:49.069
    STEP: locating a running pod 03/29/23 08:47:49.071
    STEP: Deleting the pdb to allow a pod to be evicted 03/29/23 08:47:49.075
    STEP: Waiting for the pdb to be deleted 03/29/23 08:47:49.077
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/29/23 08:47:49.078
    STEP: Waiting for all pods to be running 03/29/23 08:47:49.078
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 29 08:47:49.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5620" for this suite. 03/29/23 08:47:49.085
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:47:49.089
Mar 29 08:47:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:47:49.089
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:49.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:49.097
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:47:49.099
Mar 29 08:47:49.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a" in namespace "downward-api-7688" to be "Succeeded or Failed"
Mar 29 08:47:49.104: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224128ms
Mar 29 08:47:51.106: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002783429s
Mar 29 08:47:53.108: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004371823s
STEP: Saw pod success 03/29/23 08:47:53.108
Mar 29 08:47:53.108: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a" satisfied condition "Succeeded or Failed"
Mar 29 08:47:53.109: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a container client-container: <nil>
STEP: delete the pod 03/29/23 08:47:53.112
Mar 29 08:47:53.117: INFO: Waiting for pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a to disappear
Mar 29 08:47:53.119: INFO: Pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 08:47:53.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7688" for this suite. 03/29/23 08:47:53.121
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":238,"skipped":4421,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:47:49.089
    Mar 29 08:47:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:47:49.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:49.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:49.097
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:47:49.099
    Mar 29 08:47:49.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a" in namespace "downward-api-7688" to be "Succeeded or Failed"
    Mar 29 08:47:49.104: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.224128ms
    Mar 29 08:47:51.106: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002783429s
    Mar 29 08:47:53.108: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004371823s
    STEP: Saw pod success 03/29/23 08:47:53.108
    Mar 29 08:47:53.108: INFO: Pod "downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a" satisfied condition "Succeeded or Failed"
    Mar 29 08:47:53.109: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a container client-container: <nil>
    STEP: delete the pod 03/29/23 08:47:53.112
    Mar 29 08:47:53.117: INFO: Waiting for pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a to disappear
    Mar 29 08:47:53.119: INFO: Pod downwardapi-volume-5d21e04b-1ec0-43bc-90b5-598c43e0181a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 08:47:53.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7688" for this suite. 03/29/23 08:47:53.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:47:53.126
Mar 29 08:47:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:47:53.126
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:53.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:53.132
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:47:53.134
Mar 29 08:47:53.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97" in namespace "projected-3755" to be "Succeeded or Failed"
Mar 29 08:47:53.139: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.276092ms
Mar 29 08:47:55.142: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003932156s
Mar 29 08:47:57.143: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004746279s
STEP: Saw pod success 03/29/23 08:47:57.143
Mar 29 08:47:57.143: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97" satisfied condition "Succeeded or Failed"
Mar 29 08:47:57.144: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 container client-container: <nil>
STEP: delete the pod 03/29/23 08:47:57.147
Mar 29 08:47:57.152: INFO: Waiting for pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 to disappear
Mar 29 08:47:57.153: INFO: Pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:47:57.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3755" for this suite. 03/29/23 08:47:57.155
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":239,"skipped":4476,"failed":0}
------------------------------
â€¢ [4.031 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:47:53.126
    Mar 29 08:47:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:47:53.126
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:53.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:53.132
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:47:53.134
    Mar 29 08:47:53.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97" in namespace "projected-3755" to be "Succeeded or Failed"
    Mar 29 08:47:53.139: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.276092ms
    Mar 29 08:47:55.142: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003932156s
    Mar 29 08:47:57.143: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004746279s
    STEP: Saw pod success 03/29/23 08:47:57.143
    Mar 29 08:47:57.143: INFO: Pod "downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97" satisfied condition "Succeeded or Failed"
    Mar 29 08:47:57.144: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:47:57.147
    Mar 29 08:47:57.152: INFO: Waiting for pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 to disappear
    Mar 29 08:47:57.153: INFO: Pod downwardapi-volume-20e43eed-ce02-439a-9329-94c2d4d34f97 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:47:57.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3755" for this suite. 03/29/23 08:47:57.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:47:57.159
Mar 29 08:47:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 08:47:57.159
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:57.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:57.167
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 29 08:47:57.168: INFO: Creating deployment "test-recreate-deployment"
Mar 29 08:47:57.170: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 29 08:47:57.174: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 29 08:47:59.178: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 29 08:47:59.179: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 29 08:47:59.183: INFO: Updating deployment test-recreate-deployment
Mar 29 08:47:59.183: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 08:47:59.217: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3311  848db91f-f60f-4102-9bad-aaff52877d42 24276 2 2023-03-29 08:47:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038881a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-29 08:47:59 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-29 08:47:59 +0000 UTC,LastTransitionTime:2023-03-29 08:47:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 29 08:47:59.218: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3311  22d98ebf-331d-4db6-a286-4ef9325f323c 24273 1 2023-03-29 08:47:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 848db91f-f60f-4102-9bad-aaff52877d42 0xc005ac5af0 0xc005ac5af1}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"848db91f-f60f-4102-9bad-aaff52877d42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac5b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:47:59.218: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 29 08:47:59.218: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3311  b35eae68-76ef-4eee-bda2-a8025f001835 24264 2 2023-03-29 08:47:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 848db91f-f60f-4102-9bad-aaff52877d42 0xc005ac59d7 0xc005ac59d8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"848db91f-f60f-4102-9bad-aaff52877d42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac5a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 08:47:59.220: INFO: Pod "test-recreate-deployment-9d58999df-z8ssg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-z8ssg test-recreate-deployment-9d58999df- deployment-3311  398046fe-578f-4b0b-9e40-3913e2d347ae 24275 0 2023-03-29 08:47:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 22d98ebf-331d-4db6-a286-4ef9325f323c 0xc005ac5fe0 0xc005ac5fe1}] [] [{kube-controller-manager Update v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d98ebf-331d-4db6-a286-4ef9325f323c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-292z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-292z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 08:47:59.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3311" for this suite. 03/29/23 08:47:59.221
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":240,"skipped":4545,"failed":0}
------------------------------
â€¢ [2.065 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:47:57.159
    Mar 29 08:47:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 08:47:57.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:57.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:57.167
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 29 08:47:57.168: INFO: Creating deployment "test-recreate-deployment"
    Mar 29 08:47:57.170: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 29 08:47:57.174: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 29 08:47:59.178: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 29 08:47:59.179: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 29 08:47:59.183: INFO: Updating deployment test-recreate-deployment
    Mar 29 08:47:59.183: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 08:47:59.217: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3311  848db91f-f60f-4102-9bad-aaff52877d42 24276 2 2023-03-29 08:47:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038881a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-29 08:47:59 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-29 08:47:59 +0000 UTC,LastTransitionTime:2023-03-29 08:47:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 29 08:47:59.218: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3311  22d98ebf-331d-4db6-a286-4ef9325f323c 24273 1 2023-03-29 08:47:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 848db91f-f60f-4102-9bad-aaff52877d42 0xc005ac5af0 0xc005ac5af1}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"848db91f-f60f-4102-9bad-aaff52877d42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac5b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:47:59.218: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 29 08:47:59.218: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3311  b35eae68-76ef-4eee-bda2-a8025f001835 24264 2 2023-03-29 08:47:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 848db91f-f60f-4102-9bad-aaff52877d42 0xc005ac59d7 0xc005ac59d8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"848db91f-f60f-4102-9bad-aaff52877d42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac5a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 08:47:59.220: INFO: Pod "test-recreate-deployment-9d58999df-z8ssg" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-z8ssg test-recreate-deployment-9d58999df- deployment-3311  398046fe-578f-4b0b-9e40-3913e2d347ae 24275 0 2023-03-29 08:47:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 22d98ebf-331d-4db6-a286-4ef9325f323c 0xc005ac5fe0 0xc005ac5fe1}] [] [{kube-controller-manager Update v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d98ebf-331d-4db6-a286-4ef9325f323c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 08:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-292z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-292z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.116,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 08:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.116,PodIP:,StartTime:2023-03-29 08:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 08:47:59.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3311" for this suite. 03/29/23 08:47:59.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:47:59.225
Mar 29 08:47:59.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename init-container 03/29/23 08:47:59.226
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:59.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:59.232
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/29/23 08:47:59.235
Mar 29 08:47:59.235: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 08:48:04.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9302" for this suite. 03/29/23 08:48:04.216
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":241,"skipped":4599,"failed":0}
------------------------------
â€¢ [4.995 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:47:59.225
    Mar 29 08:47:59.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename init-container 03/29/23 08:47:59.226
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:47:59.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:47:59.232
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/29/23 08:47:59.235
    Mar 29 08:47:59.235: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 08:48:04.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9302" for this suite. 03/29/23 08:48:04.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:04.221
Mar 29 08:48:04.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 08:48:04.221
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:04.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:04.229
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/29/23 08:48:04.23
Mar 29 08:48:04.234: INFO: Waiting up to 5m0s for pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6" in namespace "downward-api-8644" to be "Succeeded or Failed"
Mar 29 08:48:04.235: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30016ms
Mar 29 08:48:06.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004217544s
Mar 29 08:48:08.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096609s
STEP: Saw pod success 03/29/23 08:48:08.238
Mar 29 08:48:08.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6" satisfied condition "Succeeded or Failed"
Mar 29 08:48:08.239: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 container dapi-container: <nil>
STEP: delete the pod 03/29/23 08:48:08.247
Mar 29 08:48:08.253: INFO: Waiting for pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 to disappear
Mar 29 08:48:08.255: INFO: Pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 29 08:48:08.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8644" for this suite. 03/29/23 08:48:08.256
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":242,"skipped":4622,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:04.221
    Mar 29 08:48:04.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 08:48:04.221
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:04.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:04.229
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/29/23 08:48:04.23
    Mar 29 08:48:04.234: INFO: Waiting up to 5m0s for pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6" in namespace "downward-api-8644" to be "Succeeded or Failed"
    Mar 29 08:48:04.235: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30016ms
    Mar 29 08:48:06.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004217544s
    Mar 29 08:48:08.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096609s
    STEP: Saw pod success 03/29/23 08:48:08.238
    Mar 29 08:48:08.238: INFO: Pod "downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6" satisfied condition "Succeeded or Failed"
    Mar 29 08:48:08.239: INFO: Trying to get logs from node 10.146.0.115 pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 container dapi-container: <nil>
    STEP: delete the pod 03/29/23 08:48:08.247
    Mar 29 08:48:08.253: INFO: Waiting for pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 to disappear
    Mar 29 08:48:08.255: INFO: Pod downward-api-6be8289d-a5b7-4ad9-b034-5b21142c0ba6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 29 08:48:08.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8644" for this suite. 03/29/23 08:48:08.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:08.259
Mar 29 08:48:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 08:48:08.26
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:08.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:08.267
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4967.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4967.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/29/23 08:48:08.268
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4967.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4967.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/29/23 08:48:08.268
STEP: creating a pod to probe /etc/hosts 03/29/23 08:48:08.268
STEP: submitting the pod to kubernetes 03/29/23 08:48:08.268
Mar 29 08:48:08.271: INFO: Waiting up to 15m0s for pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd" in namespace "dns-4967" to be "running"
Mar 29 08:48:08.273: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.241748ms
Mar 29 08:48:10.275: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003459706s
Mar 29 08:48:10.275: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd" satisfied condition "running"
STEP: retrieving the pod 03/29/23 08:48:10.275
STEP: looking for the results for each expected name from probers 03/29/23 08:48:10.276
Mar 29 08:48:10.283: INFO: DNS probes using dns-4967/dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd succeeded

STEP: deleting the pod 03/29/23 08:48:10.283
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 08:48:10.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4967" for this suite. 03/29/23 08:48:10.292
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":243,"skipped":4642,"failed":0}
------------------------------
â€¢ [2.035 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:08.259
    Mar 29 08:48:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 08:48:08.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:08.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:08.267
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4967.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4967.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/29/23 08:48:08.268
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4967.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4967.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/29/23 08:48:08.268
    STEP: creating a pod to probe /etc/hosts 03/29/23 08:48:08.268
    STEP: submitting the pod to kubernetes 03/29/23 08:48:08.268
    Mar 29 08:48:08.271: INFO: Waiting up to 15m0s for pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd" in namespace "dns-4967" to be "running"
    Mar 29 08:48:08.273: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.241748ms
    Mar 29 08:48:10.275: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003459706s
    Mar 29 08:48:10.275: INFO: Pod "dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 08:48:10.275
    STEP: looking for the results for each expected name from probers 03/29/23 08:48:10.276
    Mar 29 08:48:10.283: INFO: DNS probes using dns-4967/dns-test-0b786571-d200-47ad-9f69-c853c7a6fdcd succeeded

    STEP: deleting the pod 03/29/23 08:48:10.283
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 08:48:10.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4967" for this suite. 03/29/23 08:48:10.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:10.294
Mar 29 08:48:10.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 08:48:10.295
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:10.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:10.302
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/29/23 08:48:10.303
STEP: Counting existing ResourceQuota 03/29/23 08:48:15.306
STEP: Creating a ResourceQuota 03/29/23 08:48:20.308
STEP: Ensuring resource quota status is calculated 03/29/23 08:48:20.311
STEP: Creating a Secret 03/29/23 08:48:22.314
STEP: Ensuring resource quota status captures secret creation 03/29/23 08:48:22.322
STEP: Deleting a secret 03/29/23 08:48:24.325
STEP: Ensuring resource quota status released usage 03/29/23 08:48:24.328
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 08:48:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7899" for this suite. 03/29/23 08:48:26.331
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":244,"skipped":4652,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:10.294
    Mar 29 08:48:10.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 08:48:10.295
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:10.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:10.302
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/29/23 08:48:10.303
    STEP: Counting existing ResourceQuota 03/29/23 08:48:15.306
    STEP: Creating a ResourceQuota 03/29/23 08:48:20.308
    STEP: Ensuring resource quota status is calculated 03/29/23 08:48:20.311
    STEP: Creating a Secret 03/29/23 08:48:22.314
    STEP: Ensuring resource quota status captures secret creation 03/29/23 08:48:22.322
    STEP: Deleting a secret 03/29/23 08:48:24.325
    STEP: Ensuring resource quota status released usage 03/29/23 08:48:24.328
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 08:48:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7899" for this suite. 03/29/23 08:48:26.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:26.335
Mar 29 08:48:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:48:26.335
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:26.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:26.344
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/29/23 08:48:26.345
STEP: watching for the ServiceAccount to be added 03/29/23 08:48:26.349
STEP: patching the ServiceAccount 03/29/23 08:48:26.349
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/29/23 08:48:26.351
STEP: deleting the ServiceAccount 03/29/23 08:48:26.353
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 29 08:48:26.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7698" for this suite. 03/29/23 08:48:26.361
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":245,"skipped":4676,"failed":0}
------------------------------
â€¢ [0.029 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:26.335
    Mar 29 08:48:26.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svcaccounts 03/29/23 08:48:26.335
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:26.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:26.344
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/29/23 08:48:26.345
    STEP: watching for the ServiceAccount to be added 03/29/23 08:48:26.349
    STEP: patching the ServiceAccount 03/29/23 08:48:26.349
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/29/23 08:48:26.351
    STEP: deleting the ServiceAccount 03/29/23 08:48:26.353
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 29 08:48:26.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7698" for this suite. 03/29/23 08:48:26.361
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:26.364
Mar 29 08:48:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename init-container 03/29/23 08:48:26.364
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:26.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:26.371
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/29/23 08:48:26.372
Mar 29 08:48:26.372: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 08:48:30.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9790" for this suite. 03/29/23 08:48:30.005
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":246,"skipped":4680,"failed":0}
------------------------------
â€¢ [3.644 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:26.364
    Mar 29 08:48:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename init-container 03/29/23 08:48:26.364
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:26.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:26.371
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/29/23 08:48:26.372
    Mar 29 08:48:26.372: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 08:48:30.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9790" for this suite. 03/29/23 08:48:30.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:30.008
Mar 29 08:48:30.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 08:48:30.009
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:30.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:30.017
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/29/23 08:48:30.018
Mar 29 08:48:30.021: INFO: Waiting up to 5m0s for pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094" in namespace "emptydir-931" to be "Succeeded or Failed"
Mar 29 08:48:30.022: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245239ms
Mar 29 08:48:32.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00419347s
Mar 29 08:48:34.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003739372s
STEP: Saw pod success 03/29/23 08:48:34.025
Mar 29 08:48:34.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094" satisfied condition "Succeeded or Failed"
Mar 29 08:48:34.026: INFO: Trying to get logs from node 10.146.0.116 pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 container test-container: <nil>
STEP: delete the pod 03/29/23 08:48:34.029
Mar 29 08:48:34.035: INFO: Waiting for pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 to disappear
Mar 29 08:48:34.036: INFO: Pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 08:48:34.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-931" for this suite. 03/29/23 08:48:34.038
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4689,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:30.008
    Mar 29 08:48:30.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 08:48:30.009
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:30.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:30.017
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/29/23 08:48:30.018
    Mar 29 08:48:30.021: INFO: Waiting up to 5m0s for pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094" in namespace "emptydir-931" to be "Succeeded or Failed"
    Mar 29 08:48:30.022: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245239ms
    Mar 29 08:48:32.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00419347s
    Mar 29 08:48:34.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003739372s
    STEP: Saw pod success 03/29/23 08:48:34.025
    Mar 29 08:48:34.025: INFO: Pod "pod-dca18058-02b0-4754-b646-68f4ad5e9094" satisfied condition "Succeeded or Failed"
    Mar 29 08:48:34.026: INFO: Trying to get logs from node 10.146.0.116 pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 container test-container: <nil>
    STEP: delete the pod 03/29/23 08:48:34.029
    Mar 29 08:48:34.035: INFO: Waiting for pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 to disappear
    Mar 29 08:48:34.036: INFO: Pod pod-dca18058-02b0-4754-b646-68f4ad5e9094 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:48:34.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-931" for this suite. 03/29/23 08:48:34.038
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:34.041
Mar 29 08:48:34.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption 03/29/23 08:48:34.042
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:34.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:34.048
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:34.05
Mar 29 08:48:34.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption-2 03/29/23 08:48:34.05
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:34.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:34.057
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/29/23 08:48:34.06
STEP: Waiting for the pdb to be processed 03/29/23 08:48:36.068
STEP: Waiting for the pdb to be processed 03/29/23 08:48:38.075
STEP: listing a collection of PDBs across all namespaces 03/29/23 08:48:40.079
STEP: listing a collection of PDBs in namespace disruption-6473 03/29/23 08:48:40.081
STEP: deleting a collection of PDBs 03/29/23 08:48:40.082
STEP: Waiting for the PDB collection to be deleted 03/29/23 08:48:40.086
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar 29 08:48:40.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9496" for this suite. 03/29/23 08:48:40.089
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 29 08:48:40.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6473" for this suite. 03/29/23 08:48:40.093
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":248,"skipped":4690,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.054 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:34.041
    Mar 29 08:48:34.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption 03/29/23 08:48:34.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:34.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:34.048
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:34.05
    Mar 29 08:48:34.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption-2 03/29/23 08:48:34.05
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:34.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:34.057
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:34.06
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:36.068
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:38.075
    STEP: listing a collection of PDBs across all namespaces 03/29/23 08:48:40.079
    STEP: listing a collection of PDBs in namespace disruption-6473 03/29/23 08:48:40.081
    STEP: deleting a collection of PDBs 03/29/23 08:48:40.082
    STEP: Waiting for the PDB collection to be deleted 03/29/23 08:48:40.086
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar 29 08:48:40.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-9496" for this suite. 03/29/23 08:48:40.089
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 29 08:48:40.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6473" for this suite. 03/29/23 08:48:40.093
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:40.095
Mar 29 08:48:40.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:48:40.096
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:40.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:40.104
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/29/23 08:48:40.105
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/29/23 08:48:40.105
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/29/23 08:48:40.105
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/29/23 08:48:40.106
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/29/23 08:48:40.106
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/29/23 08:48:40.106
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/29/23 08:48:40.106
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:48:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7208" for this suite. 03/29/23 08:48:40.108
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":249,"skipped":4694,"failed":0}
------------------------------
â€¢ [0.015 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:40.095
    Mar 29 08:48:40.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 08:48:40.096
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:40.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:40.104
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/29/23 08:48:40.105
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/29/23 08:48:40.105
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/29/23 08:48:40.105
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/29/23 08:48:40.106
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/29/23 08:48:40.106
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/29/23 08:48:40.106
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/29/23 08:48:40.106
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:48:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7208" for this suite. 03/29/23 08:48:40.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:40.111
Mar 29 08:48:40.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:48:40.111
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:40.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:40.118
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-5b60fa10-225e-49cf-b8db-1455a8e4e1ed 03/29/23 08:48:40.119
STEP: Creating a pod to test consume secrets 03/29/23 08:48:40.121
Mar 29 08:48:40.124: INFO: Waiting up to 5m0s for pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a" in namespace "secrets-1694" to be "Succeeded or Failed"
Mar 29 08:48:40.125: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.22529ms
Mar 29 08:48:42.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003473367s
Mar 29 08:48:44.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003698205s
STEP: Saw pod success 03/29/23 08:48:44.128
Mar 29 08:48:44.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a" satisfied condition "Succeeded or Failed"
Mar 29 08:48:44.129: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:48:44.132
Mar 29 08:48:44.138: INFO: Waiting for pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a to disappear
Mar 29 08:48:44.139: INFO: Pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:48:44.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1694" for this suite. 03/29/23 08:48:44.141
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":250,"skipped":4720,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:40.111
    Mar 29 08:48:40.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:48:40.111
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:40.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:40.118
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-5b60fa10-225e-49cf-b8db-1455a8e4e1ed 03/29/23 08:48:40.119
    STEP: Creating a pod to test consume secrets 03/29/23 08:48:40.121
    Mar 29 08:48:40.124: INFO: Waiting up to 5m0s for pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a" in namespace "secrets-1694" to be "Succeeded or Failed"
    Mar 29 08:48:40.125: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.22529ms
    Mar 29 08:48:42.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003473367s
    Mar 29 08:48:44.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003698205s
    STEP: Saw pod success 03/29/23 08:48:44.128
    Mar 29 08:48:44.128: INFO: Pod "pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a" satisfied condition "Succeeded or Failed"
    Mar 29 08:48:44.129: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:48:44.132
    Mar 29 08:48:44.138: INFO: Waiting for pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a to disappear
    Mar 29 08:48:44.139: INFO: Pod pod-secrets-78e8d7e8-41e2-448a-b1aa-b2da483e7e7a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:48:44.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1694" for this suite. 03/29/23 08:48:44.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:44.145
Mar 29 08:48:44.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename disruption 03/29/23 08:48:44.146
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:44.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:44.152
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/29/23 08:48:44.153
STEP: Waiting for the pdb to be processed 03/29/23 08:48:44.155
STEP: updating the pdb 03/29/23 08:48:46.159
STEP: Waiting for the pdb to be processed 03/29/23 08:48:46.163
STEP: patching the pdb 03/29/23 08:48:48.168
STEP: Waiting for the pdb to be processed 03/29/23 08:48:48.173
STEP: Waiting for the pdb to be deleted 03/29/23 08:48:50.179
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 29 08:48:50.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3443" for this suite. 03/29/23 08:48:50.182
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":251,"skipped":4736,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.040 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:44.145
    Mar 29 08:48:44.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename disruption 03/29/23 08:48:44.146
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:44.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:44.152
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/29/23 08:48:44.153
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:44.155
    STEP: updating the pdb 03/29/23 08:48:46.159
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:46.163
    STEP: patching the pdb 03/29/23 08:48:48.168
    STEP: Waiting for the pdb to be processed 03/29/23 08:48:48.173
    STEP: Waiting for the pdb to be deleted 03/29/23 08:48:50.179
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 29 08:48:50.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3443" for this suite. 03/29/23 08:48:50.182
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:48:50.186
Mar 29 08:48:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 08:48:50.186
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:50.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:50.194
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 08:48:50.201
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:48:50.432
STEP: Deploying the webhook pod 03/29/23 08:48:50.436
STEP: Wait for the deployment to be ready 03/29/23 08:48:50.442
Mar 29 08:48:50.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 08:48:52.451
STEP: Verifying the service has paired with the endpoint 03/29/23 08:48:52.456
Mar 29 08:48:53.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/29/23 08:48:53.459
STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:53.459
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/29/23 08:48:53.468
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/29/23 08:48:54.474
STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:54.474
STEP: Having no error when timeout is longer than webhook latency 03/29/23 08:48:55.487
STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:55.487
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/29/23 08:49:00.503
STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:49:00.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:49:05.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9279" for this suite. 03/29/23 08:49:05.52
STEP: Destroying namespace "webhook-9279-markers" for this suite. 03/29/23 08:49:05.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":252,"skipped":4740,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.361 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:48:50.186
    Mar 29 08:48:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 08:48:50.186
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:48:50.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:48:50.194
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 08:48:50.201
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:48:50.432
    STEP: Deploying the webhook pod 03/29/23 08:48:50.436
    STEP: Wait for the deployment to be ready 03/29/23 08:48:50.442
    Mar 29 08:48:50.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 08:48:52.451
    STEP: Verifying the service has paired with the endpoint 03/29/23 08:48:52.456
    Mar 29 08:48:53.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/29/23 08:48:53.459
    STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:53.459
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/29/23 08:48:53.468
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/29/23 08:48:54.474
    STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:54.474
    STEP: Having no error when timeout is longer than webhook latency 03/29/23 08:48:55.487
    STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:48:55.487
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/29/23 08:49:00.503
    STEP: Registering slow webhook via the AdmissionRegistration API 03/29/23 08:49:00.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:49:05.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9279" for this suite. 03/29/23 08:49:05.52
    STEP: Destroying namespace "webhook-9279-markers" for this suite. 03/29/23 08:49:05.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:05.547
Mar 29 08:49:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:49:05.548
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:05.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:05.557
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar 29 08:49:05.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:49:07.381
Mar 29 08:49:07.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 create -f -'
Mar 29 08:49:07.852: INFO: stderr: ""
Mar 29 08:49:07.852: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 29 08:49:07.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 delete e2e-test-crd-publish-openapi-3475-crds test-cr'
Mar 29 08:49:07.896: INFO: stderr: ""
Mar 29 08:49:07.896: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 29 08:49:07.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 apply -f -'
Mar 29 08:49:08.021: INFO: stderr: ""
Mar 29 08:49:08.021: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 29 08:49:08.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 delete e2e-test-crd-publish-openapi-3475-crds test-cr'
Mar 29 08:49:08.064: INFO: stderr: ""
Mar 29 08:49:08.064: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/29/23 08:49:08.064
Mar 29 08:49:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 explain e2e-test-crd-publish-openapi-3475-crds'
Mar 29 08:49:08.187: INFO: stderr: ""
Mar 29 08:49:08.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3475-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:49:10.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7276" for this suite. 03/29/23 08:49:10.998
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":253,"skipped":4741,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.454 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:05.547
    Mar 29 08:49:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 08:49:05.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:05.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:05.557
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar 29 08:49:05.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/29/23 08:49:07.381
    Mar 29 08:49:07.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 create -f -'
    Mar 29 08:49:07.852: INFO: stderr: ""
    Mar 29 08:49:07.852: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 29 08:49:07.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 delete e2e-test-crd-publish-openapi-3475-crds test-cr'
    Mar 29 08:49:07.896: INFO: stderr: ""
    Mar 29 08:49:07.896: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 29 08:49:07.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 apply -f -'
    Mar 29 08:49:08.021: INFO: stderr: ""
    Mar 29 08:49:08.021: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 29 08:49:08.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 --namespace=crd-publish-openapi-7276 delete e2e-test-crd-publish-openapi-3475-crds test-cr'
    Mar 29 08:49:08.064: INFO: stderr: ""
    Mar 29 08:49:08.064: INFO: stdout: "e2e-test-crd-publish-openapi-3475-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/29/23 08:49:08.064
    Mar 29 08:49:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=crd-publish-openapi-7276 explain e2e-test-crd-publish-openapi-3475-crds'
    Mar 29 08:49:08.187: INFO: stderr: ""
    Mar 29 08:49:08.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3475-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:49:10.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7276" for this suite. 03/29/23 08:49:10.998
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:11.001
Mar 29 08:49:11.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename ingressclass 03/29/23 08:49:11.002
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:11.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:11.009
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/29/23 08:49:11.01
STEP: getting /apis/networking.k8s.io 03/29/23 08:49:11.011
STEP: getting /apis/networking.k8s.iov1 03/29/23 08:49:11.011
STEP: creating 03/29/23 08:49:11.012
STEP: getting 03/29/23 08:49:11.017
STEP: listing 03/29/23 08:49:11.018
STEP: watching 03/29/23 08:49:11.019
Mar 29 08:49:11.019: INFO: starting watch
STEP: patching 03/29/23 08:49:11.02
STEP: updating 03/29/23 08:49:11.023
Mar 29 08:49:11.025: INFO: waiting for watch events with expected annotations
Mar 29 08:49:11.025: INFO: saw patched and updated annotations
STEP: deleting 03/29/23 08:49:11.025
STEP: deleting a collection 03/29/23 08:49:11.031
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar 29 08:49:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-26" for this suite. 03/29/23 08:49:11.038
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":254,"skipped":4742,"failed":0}
------------------------------
â€¢ [0.039 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:11.001
    Mar 29 08:49:11.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename ingressclass 03/29/23 08:49:11.002
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:11.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:11.009
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/29/23 08:49:11.01
    STEP: getting /apis/networking.k8s.io 03/29/23 08:49:11.011
    STEP: getting /apis/networking.k8s.iov1 03/29/23 08:49:11.011
    STEP: creating 03/29/23 08:49:11.012
    STEP: getting 03/29/23 08:49:11.017
    STEP: listing 03/29/23 08:49:11.018
    STEP: watching 03/29/23 08:49:11.019
    Mar 29 08:49:11.019: INFO: starting watch
    STEP: patching 03/29/23 08:49:11.02
    STEP: updating 03/29/23 08:49:11.023
    Mar 29 08:49:11.025: INFO: waiting for watch events with expected annotations
    Mar 29 08:49:11.025: INFO: saw patched and updated annotations
    STEP: deleting 03/29/23 08:49:11.025
    STEP: deleting a collection 03/29/23 08:49:11.031
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar 29 08:49:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-26" for this suite. 03/29/23 08:49:11.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:11.04
Mar 29 08:49:11.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:49:11.041
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:11.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:11.047
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:49:11.049
Mar 29 08:49:11.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8" in namespace "projected-3176" to be "Succeeded or Failed"
Mar 29 08:49:11.053: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.138016ms
Mar 29 08:49:13.055: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00347278s
Mar 29 08:49:15.056: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004458003s
STEP: Saw pod success 03/29/23 08:49:15.056
Mar 29 08:49:15.056: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8" satisfied condition "Succeeded or Failed"
Mar 29 08:49:15.058: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 container client-container: <nil>
STEP: delete the pod 03/29/23 08:49:15.061
Mar 29 08:49:15.066: INFO: Waiting for pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 to disappear
Mar 29 08:49:15.068: INFO: Pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:49:15.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3176" for this suite. 03/29/23 08:49:15.069
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":255,"skipped":4760,"failed":0}
------------------------------
â€¢ [4.031 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:11.04
    Mar 29 08:49:11.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:49:11.041
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:11.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:11.047
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:49:11.049
    Mar 29 08:49:11.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8" in namespace "projected-3176" to be "Succeeded or Failed"
    Mar 29 08:49:11.053: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.138016ms
    Mar 29 08:49:13.055: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00347278s
    Mar 29 08:49:15.056: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004458003s
    STEP: Saw pod success 03/29/23 08:49:15.056
    Mar 29 08:49:15.056: INFO: Pod "downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8" satisfied condition "Succeeded or Failed"
    Mar 29 08:49:15.058: INFO: Trying to get logs from node 10.146.0.116 pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:49:15.061
    Mar 29 08:49:15.066: INFO: Waiting for pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 to disappear
    Mar 29 08:49:15.068: INFO: Pod downwardapi-volume-76b6b7d9-9cc8-4b95-9582-99c5153770e8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:49:15.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3176" for this suite. 03/29/23 08:49:15.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:15.072
Mar 29 08:49:15.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 08:49:15.072
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:15.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:15.079
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/29/23 08:49:15.08
STEP: Ensuring job reaches completions 03/29/23 08:49:15.083
STEP: Ensuring pods with index for job exist 03/29/23 08:49:23.086
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 08:49:23.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4561" for this suite. 03/29/23 08:49:23.09
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":256,"skipped":4769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.022 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:15.072
    Mar 29 08:49:15.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 08:49:15.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:15.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:15.079
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/29/23 08:49:15.08
    STEP: Ensuring job reaches completions 03/29/23 08:49:15.083
    STEP: Ensuring pods with index for job exist 03/29/23 08:49:23.086
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 08:49:23.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4561" for this suite. 03/29/23 08:49:23.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:23.097
Mar 29 08:49:23.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir-wrapper 03/29/23 08:49:23.098
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:23.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:23.105
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 29 08:49:23.114: INFO: Waiting up to 5m0s for pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838" in namespace "emptydir-wrapper-32" to be "running and ready"
Mar 29 08:49:23.115: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302589ms
Mar 29 08:49:23.115: INFO: The phase of Pod pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:49:25.117: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838": Phase="Running", Reason="", readiness=true. Elapsed: 2.003398193s
Mar 29 08:49:25.117: INFO: The phase of Pod pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838 is Running (Ready = true)
Mar 29 08:49:25.117: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/29/23 08:49:25.119
STEP: Cleaning up the configmap 03/29/23 08:49:25.121
STEP: Cleaning up the pod 03/29/23 08:49:25.124
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 29 08:49:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-32" for this suite. 03/29/23 08:49:25.131
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":257,"skipped":4866,"failed":0}
------------------------------
â€¢ [2.037 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:23.097
    Mar 29 08:49:23.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir-wrapper 03/29/23 08:49:23.098
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:23.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:23.105
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 29 08:49:23.114: INFO: Waiting up to 5m0s for pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838" in namespace "emptydir-wrapper-32" to be "running and ready"
    Mar 29 08:49:23.115: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838": Phase="Pending", Reason="", readiness=false. Elapsed: 1.302589ms
    Mar 29 08:49:23.115: INFO: The phase of Pod pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:49:25.117: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838": Phase="Running", Reason="", readiness=true. Elapsed: 2.003398193s
    Mar 29 08:49:25.117: INFO: The phase of Pod pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838 is Running (Ready = true)
    Mar 29 08:49:25.117: INFO: Pod "pod-secrets-de26303f-1af3-4b27-8223-06303fdf9838" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/29/23 08:49:25.119
    STEP: Cleaning up the configmap 03/29/23 08:49:25.121
    STEP: Cleaning up the pod 03/29/23 08:49:25.124
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 29 08:49:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-32" for this suite. 03/29/23 08:49:25.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:25.135
Mar 29 08:49:25.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename certificates 03/29/23 08:49:25.135
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:25.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:25.142
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/29/23 08:49:25.813
STEP: getting /apis/certificates.k8s.io 03/29/23 08:49:25.814
STEP: getting /apis/certificates.k8s.io/v1 03/29/23 08:49:25.815
STEP: creating 03/29/23 08:49:25.815
STEP: getting 03/29/23 08:49:25.824
STEP: listing 03/29/23 08:49:25.825
STEP: watching 03/29/23 08:49:25.827
Mar 29 08:49:25.827: INFO: starting watch
STEP: patching 03/29/23 08:49:25.827
STEP: updating 03/29/23 08:49:25.83
Mar 29 08:49:25.832: INFO: waiting for watch events with expected annotations
Mar 29 08:49:25.832: INFO: saw patched and updated annotations
STEP: getting /approval 03/29/23 08:49:25.832
STEP: patching /approval 03/29/23 08:49:25.834
STEP: updating /approval 03/29/23 08:49:25.836
STEP: getting /status 03/29/23 08:49:25.839
STEP: patching /status 03/29/23 08:49:25.841
STEP: updating /status 03/29/23 08:49:25.844
STEP: deleting 03/29/23 08:49:25.849
STEP: deleting a collection 03/29/23 08:49:25.854
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:49:25.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2010" for this suite. 03/29/23 08:49:25.861
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":258,"skipped":4873,"failed":0}
------------------------------
â€¢ [0.728 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:25.135
    Mar 29 08:49:25.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename certificates 03/29/23 08:49:25.135
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:25.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:25.142
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/29/23 08:49:25.813
    STEP: getting /apis/certificates.k8s.io 03/29/23 08:49:25.814
    STEP: getting /apis/certificates.k8s.io/v1 03/29/23 08:49:25.815
    STEP: creating 03/29/23 08:49:25.815
    STEP: getting 03/29/23 08:49:25.824
    STEP: listing 03/29/23 08:49:25.825
    STEP: watching 03/29/23 08:49:25.827
    Mar 29 08:49:25.827: INFO: starting watch
    STEP: patching 03/29/23 08:49:25.827
    STEP: updating 03/29/23 08:49:25.83
    Mar 29 08:49:25.832: INFO: waiting for watch events with expected annotations
    Mar 29 08:49:25.832: INFO: saw patched and updated annotations
    STEP: getting /approval 03/29/23 08:49:25.832
    STEP: patching /approval 03/29/23 08:49:25.834
    STEP: updating /approval 03/29/23 08:49:25.836
    STEP: getting /status 03/29/23 08:49:25.839
    STEP: patching /status 03/29/23 08:49:25.841
    STEP: updating /status 03/29/23 08:49:25.844
    STEP: deleting 03/29/23 08:49:25.849
    STEP: deleting a collection 03/29/23 08:49:25.854
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:49:25.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2010" for this suite. 03/29/23 08:49:25.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:25.863
Mar 29 08:49:25.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 08:49:25.864
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:25.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:25.872
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/29/23 08:49:25.876
STEP: watching for Pod to be ready 03/29/23 08:49:25.879
Mar 29 08:49:25.880: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 29 08:49:25.882: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
Mar 29 08:49:25.888: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
Mar 29 08:49:26.306: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
Mar 29 08:49:27.338: INFO: Found Pod pod-test in namespace pods-3982 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/29/23 08:49:27.34
STEP: getting the Pod and ensuring that it's patched 03/29/23 08:49:27.345
STEP: replacing the Pod's status Ready condition to False 03/29/23 08:49:27.349
STEP: check the Pod again to ensure its Ready conditions are False 03/29/23 08:49:27.356
STEP: deleting the Pod via a Collection with a LabelSelector 03/29/23 08:49:27.356
STEP: watching for the Pod to be deleted 03/29/23 08:49:27.361
Mar 29 08:49:27.362: INFO: observed event type MODIFIED
Mar 29 08:49:29.341: INFO: observed event type MODIFIED
Mar 29 08:49:29.446: INFO: observed event type MODIFIED
Mar 29 08:49:30.344: INFO: observed event type MODIFIED
Mar 29 08:49:30.346: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 08:49:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3982" for this suite. 03/29/23 08:49:30.352
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":259,"skipped":4880,"failed":0}
------------------------------
â€¢ [4.491 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:25.863
    Mar 29 08:49:25.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 08:49:25.864
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:25.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:25.872
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/29/23 08:49:25.876
    STEP: watching for Pod to be ready 03/29/23 08:49:25.879
    Mar 29 08:49:25.880: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 29 08:49:25.882: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
    Mar 29 08:49:25.888: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
    Mar 29 08:49:26.306: INFO: observed Pod pod-test in namespace pods-3982 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
    Mar 29 08:49:27.338: INFO: Found Pod pod-test in namespace pods-3982 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-29 08:49:25 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/29/23 08:49:27.34
    STEP: getting the Pod and ensuring that it's patched 03/29/23 08:49:27.345
    STEP: replacing the Pod's status Ready condition to False 03/29/23 08:49:27.349
    STEP: check the Pod again to ensure its Ready conditions are False 03/29/23 08:49:27.356
    STEP: deleting the Pod via a Collection with a LabelSelector 03/29/23 08:49:27.356
    STEP: watching for the Pod to be deleted 03/29/23 08:49:27.361
    Mar 29 08:49:27.362: INFO: observed event type MODIFIED
    Mar 29 08:49:29.341: INFO: observed event type MODIFIED
    Mar 29 08:49:29.446: INFO: observed event type MODIFIED
    Mar 29 08:49:30.344: INFO: observed event type MODIFIED
    Mar 29 08:49:30.346: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 08:49:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3982" for this suite. 03/29/23 08:49:30.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:49:30.355
Mar 29 08:49:30.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:49:30.356
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:30.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:30.363
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-0b77e571-9d7c-4d84-ba0c-beb85a471a4f 03/29/23 08:49:30.366
STEP: Creating configMap with name cm-test-opt-upd-24f67094-1e5b-4002-a73b-3c5cbc2a7a48 03/29/23 08:49:30.367
STEP: Creating the pod 03/29/23 08:49:30.369
Mar 29 08:49:30.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c" in namespace "projected-6992" to be "running and ready"
Mar 29 08:49:30.375: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.351596ms
Mar 29 08:49:30.375: INFO: The phase of Pod pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:49:32.378: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004669993s
Mar 29 08:49:32.378: INFO: The phase of Pod pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c is Running (Ready = true)
Mar 29 08:49:32.378: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0b77e571-9d7c-4d84-ba0c-beb85a471a4f 03/29/23 08:49:32.387
STEP: Updating configmap cm-test-opt-upd-24f67094-1e5b-4002-a73b-3c5cbc2a7a48 03/29/23 08:49:32.39
STEP: Creating configMap with name cm-test-opt-create-72f7c700-4e22-437d-bc4e-601babc07817 03/29/23 08:49:32.392
STEP: waiting to observe update in volume 03/29/23 08:49:32.395
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:50:36.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6992" for this suite. 03/29/23 08:50:36.54
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":260,"skipped":4890,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.187 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:49:30.355
    Mar 29 08:49:30.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:49:30.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:49:30.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:49:30.363
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-0b77e571-9d7c-4d84-ba0c-beb85a471a4f 03/29/23 08:49:30.366
    STEP: Creating configMap with name cm-test-opt-upd-24f67094-1e5b-4002-a73b-3c5cbc2a7a48 03/29/23 08:49:30.367
    STEP: Creating the pod 03/29/23 08:49:30.369
    Mar 29 08:49:30.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c" in namespace "projected-6992" to be "running and ready"
    Mar 29 08:49:30.375: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.351596ms
    Mar 29 08:49:30.375: INFO: The phase of Pod pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:49:32.378: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004669993s
    Mar 29 08:49:32.378: INFO: The phase of Pod pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c is Running (Ready = true)
    Mar 29 08:49:32.378: INFO: Pod "pod-projected-configmaps-fd0caab5-a551-4079-b09c-6b95ed71599c" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0b77e571-9d7c-4d84-ba0c-beb85a471a4f 03/29/23 08:49:32.387
    STEP: Updating configmap cm-test-opt-upd-24f67094-1e5b-4002-a73b-3c5cbc2a7a48 03/29/23 08:49:32.39
    STEP: Creating configMap with name cm-test-opt-create-72f7c700-4e22-437d-bc4e-601babc07817 03/29/23 08:49:32.392
    STEP: waiting to observe update in volume 03/29/23 08:49:32.395
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:50:36.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6992" for this suite. 03/29/23 08:50:36.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:50:36.543
Mar 29 08:50:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename secrets 03/29/23 08:50:36.544
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:36.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:36.553
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-d70b5c01-a6e4-4dff-92c4-923cc95dd143 03/29/23 08:50:36.554
STEP: Creating a pod to test consume secrets 03/29/23 08:50:36.556
Mar 29 08:50:36.560: INFO: Waiting up to 5m0s for pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861" in namespace "secrets-9815" to be "Succeeded or Failed"
Mar 29 08:50:36.561: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Pending", Reason="", readiness=false. Elapsed: 1.242753ms
Mar 29 08:50:38.563: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00285087s
Mar 29 08:50:40.565: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00471535s
STEP: Saw pod success 03/29/23 08:50:40.565
Mar 29 08:50:40.565: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861" satisfied condition "Succeeded or Failed"
Mar 29 08:50:40.566: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 container secret-volume-test: <nil>
STEP: delete the pod 03/29/23 08:50:40.573
Mar 29 08:50:40.579: INFO: Waiting for pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 to disappear
Mar 29 08:50:40.580: INFO: Pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 29 08:50:40.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9815" for this suite. 03/29/23 08:50:40.582
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":261,"skipped":4904,"failed":0}
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:50:36.543
    Mar 29 08:50:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename secrets 03/29/23 08:50:36.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:36.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:36.553
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-d70b5c01-a6e4-4dff-92c4-923cc95dd143 03/29/23 08:50:36.554
    STEP: Creating a pod to test consume secrets 03/29/23 08:50:36.556
    Mar 29 08:50:36.560: INFO: Waiting up to 5m0s for pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861" in namespace "secrets-9815" to be "Succeeded or Failed"
    Mar 29 08:50:36.561: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Pending", Reason="", readiness=false. Elapsed: 1.242753ms
    Mar 29 08:50:38.563: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00285087s
    Mar 29 08:50:40.565: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00471535s
    STEP: Saw pod success 03/29/23 08:50:40.565
    Mar 29 08:50:40.565: INFO: Pod "pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861" satisfied condition "Succeeded or Failed"
    Mar 29 08:50:40.566: INFO: Trying to get logs from node 10.146.0.115 pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 container secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:50:40.573
    Mar 29 08:50:40.579: INFO: Waiting for pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 to disappear
    Mar 29 08:50:40.580: INFO: Pod pod-secrets-77f56655-3a03-4fc4-bea8-8c6aea18f861 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 29 08:50:40.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9815" for this suite. 03/29/23 08:50:40.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:50:40.585
Mar 29 08:50:40.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename events 03/29/23 08:50:40.585
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:40.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:40.593
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/29/23 08:50:40.594
STEP: get a list of Events with a label in the current namespace 03/29/23 08:50:40.6
STEP: delete a list of events 03/29/23 08:50:40.601
Mar 29 08:50:40.601: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/29/23 08:50:40.608
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 29 08:50:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5334" for this suite. 03/29/23 08:50:40.61
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":262,"skipped":4928,"failed":0}
------------------------------
â€¢ [0.028 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:50:40.585
    Mar 29 08:50:40.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename events 03/29/23 08:50:40.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:40.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:40.593
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/29/23 08:50:40.594
    STEP: get a list of Events with a label in the current namespace 03/29/23 08:50:40.6
    STEP: delete a list of events 03/29/23 08:50:40.601
    Mar 29 08:50:40.601: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/29/23 08:50:40.608
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 29 08:50:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5334" for this suite. 03/29/23 08:50:40.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:50:40.613
Mar 29 08:50:40.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:50:40.614
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:40.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:40.621
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 29 08:50:40.628: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 29 08:51:40.640: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/29/23 08:51:40.641
Mar 29 08:51:40.653: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 29 08:51:40.655: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 29 08:51:40.664: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 29 08:51:40.667: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 29 08:51:40.674: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 29 08:51:40.677: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/29/23 08:51:40.677
Mar 29 08:51:40.677: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:40.679: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275629ms
Mar 29 08:51:42.682: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004375263s
Mar 29 08:51:44.682: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.004486681s
Mar 29 08:51:44.682: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 29 08:51:44.682: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:44.683: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.280698ms
Mar 29 08:51:44.683: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:51:44.683: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:44.684: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.127909ms
Mar 29 08:51:46.687: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004050895s
Mar 29 08:51:48.686: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002621026s
Mar 29 08:51:50.688: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.004637025s
Mar 29 08:51:50.688: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:51:50.688: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:50.689: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.431954ms
Mar 29 08:51:50.689: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:51:50.689: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:50.691: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.238087ms
Mar 29 08:51:50.691: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 29 08:51:50.691: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:50.692: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.277413ms
Mar 29 08:51:50.692: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/29/23 08:51:50.692
Mar 29 08:51:50.696: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7135" to be "running"
Mar 29 08:51:50.697: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258037ms
Mar 29 08:51:52.700: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004198531s
Mar 29 08:51:52.700: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 29 08:51:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7135" for this suite. 03/29/23 08:51:52.709
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":263,"skipped":4960,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.117 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:50:40.613
    Mar 29 08:50:40.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sched-preemption 03/29/23 08:50:40.614
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:50:40.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:50:40.621
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 29 08:50:40.628: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 29 08:51:40.640: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/29/23 08:51:40.641
    Mar 29 08:51:40.653: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 29 08:51:40.655: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 29 08:51:40.664: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 29 08:51:40.667: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 29 08:51:40.674: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 29 08:51:40.677: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/29/23 08:51:40.677
    Mar 29 08:51:40.677: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:40.679: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275629ms
    Mar 29 08:51:42.682: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004375263s
    Mar 29 08:51:44.682: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.004486681s
    Mar 29 08:51:44.682: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 29 08:51:44.682: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:44.683: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.280698ms
    Mar 29 08:51:44.683: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:51:44.683: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:44.684: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.127909ms
    Mar 29 08:51:46.687: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004050895s
    Mar 29 08:51:48.686: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002621026s
    Mar 29 08:51:50.688: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.004637025s
    Mar 29 08:51:50.688: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:51:50.688: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:50.689: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.431954ms
    Mar 29 08:51:50.689: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:51:50.689: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:50.691: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.238087ms
    Mar 29 08:51:50.691: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 29 08:51:50.691: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:50.692: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.277413ms
    Mar 29 08:51:50.692: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/29/23 08:51:50.692
    Mar 29 08:51:50.696: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7135" to be "running"
    Mar 29 08:51:50.697: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258037ms
    Mar 29 08:51:52.700: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004198531s
    Mar 29 08:51:52.700: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 08:51:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7135" for this suite. 03/29/23 08:51:52.709
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:51:52.731
Mar 29 08:51:52.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:51:52.732
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:52.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:52.74
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:51:52.741
Mar 29 08:51:52.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88" in namespace "projected-4568" to be "Succeeded or Failed"
Mar 29 08:51:52.745: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.239935ms
Mar 29 08:51:54.748: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004087978s
Mar 29 08:51:56.749: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005078166s
STEP: Saw pod success 03/29/23 08:51:56.749
Mar 29 08:51:56.749: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88" satisfied condition "Succeeded or Failed"
Mar 29 08:51:56.751: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 container client-container: <nil>
STEP: delete the pod 03/29/23 08:51:56.755
Mar 29 08:51:56.760: INFO: Waiting for pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 to disappear
Mar 29 08:51:56.761: INFO: Pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:51:56.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4568" for this suite. 03/29/23 08:51:56.762
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":264,"skipped":4970,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:51:52.731
    Mar 29 08:51:52.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:51:52.732
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:52.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:52.74
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:51:52.741
    Mar 29 08:51:52.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88" in namespace "projected-4568" to be "Succeeded or Failed"
    Mar 29 08:51:52.745: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.239935ms
    Mar 29 08:51:54.748: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004087978s
    Mar 29 08:51:56.749: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005078166s
    STEP: Saw pod success 03/29/23 08:51:56.749
    Mar 29 08:51:56.749: INFO: Pod "downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88" satisfied condition "Succeeded or Failed"
    Mar 29 08:51:56.751: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:51:56.755
    Mar 29 08:51:56.760: INFO: Waiting for pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 to disappear
    Mar 29 08:51:56.761: INFO: Pod downwardapi-volume-bfcd1679-aa53-489f-9546-c9b48ca34b88 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:51:56.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4568" for this suite. 03/29/23 08:51:56.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:51:56.766
Mar 29 08:51:56.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:51:56.766
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:56.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:56.774
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/29/23 08:51:56.776
STEP: waiting for available Endpoint 03/29/23 08:51:56.778
STEP: listing all Endpoints 03/29/23 08:51:56.779
STEP: updating the Endpoint 03/29/23 08:51:56.78
STEP: fetching the Endpoint 03/29/23 08:51:56.782
STEP: patching the Endpoint 03/29/23 08:51:56.783
STEP: fetching the Endpoint 03/29/23 08:51:56.788
STEP: deleting the Endpoint by Collection 03/29/23 08:51:56.789
STEP: waiting for Endpoint deletion 03/29/23 08:51:56.791
STEP: fetching the Endpoint 03/29/23 08:51:56.792
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:51:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5402" for this suite. 03/29/23 08:51:56.795
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":265,"skipped":5000,"failed":0}
------------------------------
â€¢ [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:51:56.766
    Mar 29 08:51:56.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:51:56.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:56.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:56.774
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/29/23 08:51:56.776
    STEP: waiting for available Endpoint 03/29/23 08:51:56.778
    STEP: listing all Endpoints 03/29/23 08:51:56.779
    STEP: updating the Endpoint 03/29/23 08:51:56.78
    STEP: fetching the Endpoint 03/29/23 08:51:56.782
    STEP: patching the Endpoint 03/29/23 08:51:56.783
    STEP: fetching the Endpoint 03/29/23 08:51:56.788
    STEP: deleting the Endpoint by Collection 03/29/23 08:51:56.789
    STEP: waiting for Endpoint deletion 03/29/23 08:51:56.791
    STEP: fetching the Endpoint 03/29/23 08:51:56.792
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:51:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5402" for this suite. 03/29/23 08:51:56.795
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:51:56.797
Mar 29 08:51:56.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 08:51:56.798
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:56.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:56.805
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e in namespace container-probe-2190 03/29/23 08:51:56.806
Mar 29 08:51:56.809: INFO: Waiting up to 5m0s for pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e" in namespace "container-probe-2190" to be "not pending"
Mar 29 08:51:56.811: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.173801ms
Mar 29 08:51:58.813: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003372967s
Mar 29 08:51:58.813: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e" satisfied condition "not pending"
Mar 29 08:51:58.813: INFO: Started pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e in namespace container-probe-2190
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:51:58.813
Mar 29 08:51:58.814: INFO: Initial restart count of pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e is 0
STEP: deleting the pod 03/29/23 08:55:59.131
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 08:55:59.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2190" for this suite. 03/29/23 08:55:59.143
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":266,"skipped":5007,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.350 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:51:56.797
    Mar 29 08:51:56.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 08:51:56.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:51:56.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:51:56.805
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e in namespace container-probe-2190 03/29/23 08:51:56.806
    Mar 29 08:51:56.809: INFO: Waiting up to 5m0s for pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e" in namespace "container-probe-2190" to be "not pending"
    Mar 29 08:51:56.811: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.173801ms
    Mar 29 08:51:58.813: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003372967s
    Mar 29 08:51:58.813: INFO: Pod "busybox-17813f4f-801c-4d03-a1d0-193485d8149e" satisfied condition "not pending"
    Mar 29 08:51:58.813: INFO: Started pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e in namespace container-probe-2190
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:51:58.813
    Mar 29 08:51:58.814: INFO: Initial restart count of pod busybox-17813f4f-801c-4d03-a1d0-193485d8149e is 0
    STEP: deleting the pod 03/29/23 08:55:59.131
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 08:55:59.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2190" for this suite. 03/29/23 08:55:59.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:55:59.148
Mar 29 08:55:59.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 08:55:59.148
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:55:59.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:55:59.157
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/29/23 08:55:59.158
STEP: fetching the ConfigMap 03/29/23 08:55:59.16
STEP: patching the ConfigMap 03/29/23 08:55:59.161
STEP: listing all ConfigMaps in all namespaces with a label selector 03/29/23 08:55:59.164
STEP: deleting the ConfigMap by collection with a label selector 03/29/23 08:55:59.165
STEP: listing all ConfigMaps in test namespace 03/29/23 08:55:59.168
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 08:55:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6626" for this suite. 03/29/23 08:55:59.171
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":267,"skipped":5030,"failed":0}
------------------------------
â€¢ [0.025 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:55:59.148
    Mar 29 08:55:59.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 08:55:59.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:55:59.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:55:59.157
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/29/23 08:55:59.158
    STEP: fetching the ConfigMap 03/29/23 08:55:59.16
    STEP: patching the ConfigMap 03/29/23 08:55:59.161
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/29/23 08:55:59.164
    STEP: deleting the ConfigMap by collection with a label selector 03/29/23 08:55:59.165
    STEP: listing all ConfigMaps in test namespace 03/29/23 08:55:59.168
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 08:55:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6626" for this suite. 03/29/23 08:55:59.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:55:59.173
Mar 29 08:55:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:55:59.174
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:55:59.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:55:59.181
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7025 03/29/23 08:55:59.183
STEP: changing the ExternalName service to type=NodePort 03/29/23 08:55:59.184
STEP: creating replication controller externalname-service in namespace services-7025 03/29/23 08:55:59.194
I0329 08:55:59.197300      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7025, replica count: 2
I0329 08:56:02.248627      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 08:56:02.248: INFO: Creating new exec pod
Mar 29 08:56:02.252: INFO: Waiting up to 5m0s for pod "execpod858cn" in namespace "services-7025" to be "running"
Mar 29 08:56:02.253: INFO: Pod "execpod858cn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306869ms
Mar 29 08:56:04.256: INFO: Pod "execpod858cn": Phase="Running", Reason="", readiness=true. Elapsed: 2.003477834s
Mar 29 08:56:04.256: INFO: Pod "execpod858cn" satisfied condition "running"
Mar 29 08:56:05.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 29 08:56:05.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:05.347: INFO: stdout: "externalname-service-qwpnf"
Mar 29 08:56:05.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.229.189 80'
Mar 29 08:56:05.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.229.189 80\nConnection to 10.100.229.189 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:05.429: INFO: stdout: "externalname-service-xcgmj"
Mar 29 08:56:05.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 32478'
Mar 29 08:56:05.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 32478\nConnection to 10.146.0.117 32478 port [tcp/*] succeeded!\n"
Mar 29 08:56:05.517: INFO: stdout: "externalname-service-qwpnf"
Mar 29 08:56:05.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32478'
Mar 29 08:56:05.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32478\nConnection to 10.146.0.116 32478 port [tcp/*] succeeded!\n"
Mar 29 08:56:05.601: INFO: stdout: "externalname-service-qwpnf"
Mar 29 08:56:05.601: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:56:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7025" for this suite. 03/29/23 08:56:05.615
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":268,"skipped":5040,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.444 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:55:59.173
    Mar 29 08:55:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:55:59.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:55:59.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:55:59.181
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7025 03/29/23 08:55:59.183
    STEP: changing the ExternalName service to type=NodePort 03/29/23 08:55:59.184
    STEP: creating replication controller externalname-service in namespace services-7025 03/29/23 08:55:59.194
    I0329 08:55:59.197300      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7025, replica count: 2
    I0329 08:56:02.248627      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 08:56:02.248: INFO: Creating new exec pod
    Mar 29 08:56:02.252: INFO: Waiting up to 5m0s for pod "execpod858cn" in namespace "services-7025" to be "running"
    Mar 29 08:56:02.253: INFO: Pod "execpod858cn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306869ms
    Mar 29 08:56:04.256: INFO: Pod "execpod858cn": Phase="Running", Reason="", readiness=true. Elapsed: 2.003477834s
    Mar 29 08:56:04.256: INFO: Pod "execpod858cn" satisfied condition "running"
    Mar 29 08:56:05.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 29 08:56:05.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:05.347: INFO: stdout: "externalname-service-qwpnf"
    Mar 29 08:56:05.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.229.189 80'
    Mar 29 08:56:05.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.229.189 80\nConnection to 10.100.229.189 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:05.429: INFO: stdout: "externalname-service-xcgmj"
    Mar 29 08:56:05.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 32478'
    Mar 29 08:56:05.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 32478\nConnection to 10.146.0.117 32478 port [tcp/*] succeeded!\n"
    Mar 29 08:56:05.517: INFO: stdout: "externalname-service-qwpnf"
    Mar 29 08:56:05.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-7025 exec execpod858cn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 32478'
    Mar 29 08:56:05.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 32478\nConnection to 10.146.0.116 32478 port [tcp/*] succeeded!\n"
    Mar 29 08:56:05.601: INFO: stdout: "externalname-service-qwpnf"
    Mar 29 08:56:05.601: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:56:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7025" for this suite. 03/29/23 08:56:05.615
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:05.618
Mar 29 08:56:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:56:05.618
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:05.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:05.626
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-4475 03/29/23 08:56:05.628
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[] 03/29/23 08:56:05.634
Mar 29 08:56:05.638: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4475 03/29/23 08:56:05.638
Mar 29 08:56:05.642: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4475" to be "running and ready"
Mar 29 08:56:05.643: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.336822ms
Mar 29 08:56:05.643: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:56:07.646: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319543s
Mar 29 08:56:07.646: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 29 08:56:07.646: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod1:[80]] 03/29/23 08:56:07.647
Mar 29 08:56:07.652: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/29/23 08:56:07.652
Mar 29 08:56:07.652: INFO: Creating new exec pod
Mar 29 08:56:07.655: INFO: Waiting up to 5m0s for pod "execpodl4fwm" in namespace "services-4475" to be "running"
Mar 29 08:56:07.656: INFO: Pod "execpodl4fwm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.178675ms
Mar 29 08:56:09.659: INFO: Pod "execpodl4fwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.003654569s
Mar 29 08:56:09.659: INFO: Pod "execpodl4fwm" satisfied condition "running"
Mar 29 08:56:10.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 29 08:56:10.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:10.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:56:10.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
Mar 29 08:56:10.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:10.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4475 03/29/23 08:56:10.84
Mar 29 08:56:10.843: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4475" to be "running and ready"
Mar 29 08:56:10.845: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556218ms
Mar 29 08:56:10.845: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:56:12.847: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003981841s
Mar 29 08:56:12.847: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 29 08:56:12.847: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod1:[80] pod2:[80]] 03/29/23 08:56:12.849
Mar 29 08:56:12.854: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/29/23 08:56:12.854
Mar 29 08:56:13.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 29 08:56:13.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:13.947: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:56:13.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
Mar 29 08:56:14.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:14.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4475 03/29/23 08:56:14.033
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod2:[80]] 03/29/23 08:56:14.04
Mar 29 08:56:15.048: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/29/23 08:56:15.048
Mar 29 08:56:16.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 29 08:56:16.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:16.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 08:56:16.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
Mar 29 08:56:16.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
Mar 29 08:56:16.220: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4475 03/29/23 08:56:16.22
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[] 03/29/23 08:56:16.228
Mar 29 08:56:16.232: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:56:16.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4475" for this suite. 03/29/23 08:56:16.241
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":269,"skipped":5050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.626 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:05.618
    Mar 29 08:56:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:56:05.618
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:05.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:05.626
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-4475 03/29/23 08:56:05.628
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[] 03/29/23 08:56:05.634
    Mar 29 08:56:05.638: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4475 03/29/23 08:56:05.638
    Mar 29 08:56:05.642: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4475" to be "running and ready"
    Mar 29 08:56:05.643: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.336822ms
    Mar 29 08:56:05.643: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:56:07.646: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319543s
    Mar 29 08:56:07.646: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 29 08:56:07.646: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod1:[80]] 03/29/23 08:56:07.647
    Mar 29 08:56:07.652: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/29/23 08:56:07.652
    Mar 29 08:56:07.652: INFO: Creating new exec pod
    Mar 29 08:56:07.655: INFO: Waiting up to 5m0s for pod "execpodl4fwm" in namespace "services-4475" to be "running"
    Mar 29 08:56:07.656: INFO: Pod "execpodl4fwm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.178675ms
    Mar 29 08:56:09.659: INFO: Pod "execpodl4fwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.003654569s
    Mar 29 08:56:09.659: INFO: Pod "execpodl4fwm" satisfied condition "running"
    Mar 29 08:56:10.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 29 08:56:10.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:10.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:56:10.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
    Mar 29 08:56:10.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:10.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-4475 03/29/23 08:56:10.84
    Mar 29 08:56:10.843: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4475" to be "running and ready"
    Mar 29 08:56:10.845: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556218ms
    Mar 29 08:56:10.845: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:56:12.847: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003981841s
    Mar 29 08:56:12.847: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 29 08:56:12.847: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod1:[80] pod2:[80]] 03/29/23 08:56:12.849
    Mar 29 08:56:12.854: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/29/23 08:56:12.854
    Mar 29 08:56:13.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 29 08:56:13.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:13.947: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:56:13.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
    Mar 29 08:56:14.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:14.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4475 03/29/23 08:56:14.033
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[pod2:[80]] 03/29/23 08:56:14.04
    Mar 29 08:56:15.048: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/29/23 08:56:15.048
    Mar 29 08:56:16.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 29 08:56:16.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:16.139: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 08:56:16.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-4475 exec execpodl4fwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.72.163 80'
    Mar 29 08:56:16.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.72.163 80\nConnection to 10.100.72.163 80 port [tcp/http] succeeded!\n"
    Mar 29 08:56:16.220: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-4475 03/29/23 08:56:16.22
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4475 to expose endpoints map[] 03/29/23 08:56:16.228
    Mar 29 08:56:16.232: INFO: successfully validated that service endpoint-test2 in namespace services-4475 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:56:16.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4475" for this suite. 03/29/23 08:56:16.241
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:16.246
Mar 29 08:56:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 08:56:16.246
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:16.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:16.254
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/29/23 08:56:16.256
Mar 29 08:56:16.256: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1641 proxy --unix-socket=/tmp/kubectl-proxy-unix1349666660/test'
STEP: retrieving proxy /api/ output 03/29/23 08:56:16.29
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 08:56:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1641" for this suite. 03/29/23 08:56:16.293
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":270,"skipped":5109,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:16.246
    Mar 29 08:56:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 08:56:16.246
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:16.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:16.254
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/29/23 08:56:16.256
    Mar 29 08:56:16.256: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-1641 proxy --unix-socket=/tmp/kubectl-proxy-unix1349666660/test'
    STEP: retrieving proxy /api/ output 03/29/23 08:56:16.29
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 08:56:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1641" for this suite. 03/29/23 08:56:16.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:16.297
Mar 29 08:56:16.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context-test 03/29/23 08:56:16.298
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:16.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:16.305
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar 29 08:56:16.310: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4" in namespace "security-context-test-3917" to be "Succeeded or Failed"
Mar 29 08:56:16.312: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348719ms
Mar 29 08:56:18.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004391081s
Mar 29 08:56:20.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004506101s
Mar 29 08:56:20.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4" satisfied condition "Succeeded or Failed"
Mar 29 08:56:20.323: INFO: Got logs for pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 08:56:20.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3917" for this suite. 03/29/23 08:56:20.325
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":5117,"failed":0}
------------------------------
â€¢ [4.031 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:16.297
    Mar 29 08:56:16.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context-test 03/29/23 08:56:16.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:16.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:16.305
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar 29 08:56:16.310: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4" in namespace "security-context-test-3917" to be "Succeeded or Failed"
    Mar 29 08:56:16.312: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348719ms
    Mar 29 08:56:18.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004391081s
    Mar 29 08:56:20.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004506101s
    Mar 29 08:56:20.315: INFO: Pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4" satisfied condition "Succeeded or Failed"
    Mar 29 08:56:20.323: INFO: Got logs for pod "busybox-privileged-false-3cefcad5-5c9c-4b33-a28e-5908180954c4": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 08:56:20.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3917" for this suite. 03/29/23 08:56:20.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:20.329
Mar 29 08:56:20.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context-test 03/29/23 08:56:20.329
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:20.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:20.336
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar 29 08:56:20.341: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0" in namespace "security-context-test-5768" to be "Succeeded or Failed"
Mar 29 08:56:20.342: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.186729ms
Mar 29 08:56:22.346: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004532982s
Mar 29 08:56:24.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003610668s
Mar 29 08:56:26.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004007986s
Mar 29 08:56:26.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 08:56:26.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5768" for this suite. 03/29/23 08:56:26.35
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":272,"skipped":5124,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.024 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:20.329
    Mar 29 08:56:20.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context-test 03/29/23 08:56:20.329
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:20.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:20.336
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar 29 08:56:20.341: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0" in namespace "security-context-test-5768" to be "Succeeded or Failed"
    Mar 29 08:56:20.342: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.186729ms
    Mar 29 08:56:22.346: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004532982s
    Mar 29 08:56:24.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003610668s
    Mar 29 08:56:26.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004007986s
    Mar 29 08:56:26.345: INFO: Pod "alpine-nnp-false-1259cde4-17d8-44e8-85c4-8c11dc2bbdc0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 08:56:26.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5768" for this suite. 03/29/23 08:56:26.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:26.353
Mar 29 08:56:26.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 08:56:26.354
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:26.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:26.362
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/29/23 08:56:26.363
Mar 29 08:56:26.363: INFO: Creating e2e-svc-a-t7gs2
Mar 29 08:56:26.367: INFO: Creating e2e-svc-b-wkmxq
Mar 29 08:56:26.372: INFO: Creating e2e-svc-c-2x852
STEP: deleting service collection 03/29/23 08:56:26.379
Mar 29 08:56:26.391: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 08:56:26.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5095" for this suite. 03/29/23 08:56:26.393
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":273,"skipped":5134,"failed":0}
------------------------------
â€¢ [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:26.353
    Mar 29 08:56:26.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 08:56:26.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:26.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:26.362
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/29/23 08:56:26.363
    Mar 29 08:56:26.363: INFO: Creating e2e-svc-a-t7gs2
    Mar 29 08:56:26.367: INFO: Creating e2e-svc-b-wkmxq
    Mar 29 08:56:26.372: INFO: Creating e2e-svc-c-2x852
    STEP: deleting service collection 03/29/23 08:56:26.379
    Mar 29 08:56:26.391: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 08:56:26.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5095" for this suite. 03/29/23 08:56:26.393
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:56:26.395
Mar 29 08:56:26.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 08:56:26.396
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:26.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:26.403
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a in namespace container-probe-3581 03/29/23 08:56:26.405
Mar 29 08:56:26.408: INFO: Waiting up to 5m0s for pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a" in namespace "container-probe-3581" to be "not pending"
Mar 29 08:56:26.409: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.287614ms
Mar 29 08:56:28.411: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00305823s
Mar 29 08:56:28.411: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a" satisfied condition "not pending"
Mar 29 08:56:28.411: INFO: Started pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a in namespace container-probe-3581
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:56:28.411
Mar 29 08:56:28.412: INFO: Initial restart count of pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a is 0
Mar 29 08:57:18.479: INFO: Restart count of pod container-probe-3581/busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a is now 1 (50.066878805s elapsed)
STEP: deleting the pod 03/29/23 08:57:18.479
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 08:57:18.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3581" for this suite. 03/29/23 08:57:18.488
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":274,"skipped":5140,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.096 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:56:26.395
    Mar 29 08:56:26.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 08:56:26.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:56:26.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:56:26.403
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a in namespace container-probe-3581 03/29/23 08:56:26.405
    Mar 29 08:56:26.408: INFO: Waiting up to 5m0s for pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a" in namespace "container-probe-3581" to be "not pending"
    Mar 29 08:56:26.409: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.287614ms
    Mar 29 08:56:28.411: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00305823s
    Mar 29 08:56:28.411: INFO: Pod "busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a" satisfied condition "not pending"
    Mar 29 08:56:28.411: INFO: Started pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a in namespace container-probe-3581
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 08:56:28.411
    Mar 29 08:56:28.412: INFO: Initial restart count of pod busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a is 0
    Mar 29 08:57:18.479: INFO: Restart count of pod container-probe-3581/busybox-14b1cc71-1373-44e6-bed9-83cfa056eb6a is now 1 (50.066878805s elapsed)
    STEP: deleting the pod 03/29/23 08:57:18.479
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 08:57:18.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3581" for this suite. 03/29/23 08:57:18.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:18.492
Mar 29 08:57:18.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:57:18.492
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:18.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:18.502
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7982-delete-me 03/29/23 08:57:18.517
STEP: Waiting for the RuntimeClass to disappear 03/29/23 08:57:18.52
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 29 08:57:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7982" for this suite. 03/29/23 08:57:18.527
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":275,"skipped":5154,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:18.492
    Mar 29 08:57:18.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename runtimeclass 03/29/23 08:57:18.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:18.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:18.502
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7982-delete-me 03/29/23 08:57:18.517
    STEP: Waiting for the RuntimeClass to disappear 03/29/23 08:57:18.52
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 29 08:57:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7982" for this suite. 03/29/23 08:57:18.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:18.531
Mar 29 08:57:18.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 08:57:18.532
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:18.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:18.539
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/29/23 08:57:18.54
STEP: Wait for the Deployment to create new ReplicaSet 03/29/23 08:57:18.542
STEP: delete the deployment 03/29/23 08:57:19.047
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/29/23 08:57:19.052
STEP: Gathering metrics 03/29/23 08:57:19.56
W0329 08:57:19.563744      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 08:57:19.563: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 08:57:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2164" for this suite. 03/29/23 08:57:19.565
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":276,"skipped":5184,"failed":0}
------------------------------
â€¢ [1.036 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:18.531
    Mar 29 08:57:18.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 08:57:18.532
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:18.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:18.539
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/29/23 08:57:18.54
    STEP: Wait for the Deployment to create new ReplicaSet 03/29/23 08:57:18.542
    STEP: delete the deployment 03/29/23 08:57:19.047
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/29/23 08:57:19.052
    STEP: Gathering metrics 03/29/23 08:57:19.56
    W0329 08:57:19.563744      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 08:57:19.563: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 08:57:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2164" for this suite. 03/29/23 08:57:19.565
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:19.568
Mar 29 08:57:19.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:57:19.568
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:19.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:19.575
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ca673782-527a-45db-b5e1-5e2bd165893a 03/29/23 08:57:19.577
STEP: Creating a pod to test consume configMaps 03/29/23 08:57:19.578
Mar 29 08:57:19.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013" in namespace "projected-9500" to be "Succeeded or Failed"
Mar 29 08:57:19.583: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200351ms
Mar 29 08:57:21.586: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004253091s
Mar 29 08:57:23.585: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.002937534s
STEP: Saw pod success 03/29/23 08:57:23.585
Mar 29 08:57:23.585: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013" satisfied condition "Succeeded or Failed"
Mar 29 08:57:23.586: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:57:23.595
Mar 29 08:57:23.601: INFO: Waiting for pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 to disappear
Mar 29 08:57:23.602: INFO: Pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:57:23.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9500" for this suite. 03/29/23 08:57:23.604
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":277,"skipped":5188,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:19.568
    Mar 29 08:57:19.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:57:19.568
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:19.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:19.575
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ca673782-527a-45db-b5e1-5e2bd165893a 03/29/23 08:57:19.577
    STEP: Creating a pod to test consume configMaps 03/29/23 08:57:19.578
    Mar 29 08:57:19.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013" in namespace "projected-9500" to be "Succeeded or Failed"
    Mar 29 08:57:19.583: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Pending", Reason="", readiness=false. Elapsed: 1.200351ms
    Mar 29 08:57:21.586: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004253091s
    Mar 29 08:57:23.585: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.002937534s
    STEP: Saw pod success 03/29/23 08:57:23.585
    Mar 29 08:57:23.585: INFO: Pod "pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013" satisfied condition "Succeeded or Failed"
    Mar 29 08:57:23.586: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:57:23.595
    Mar 29 08:57:23.601: INFO: Waiting for pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 to disappear
    Mar 29 08:57:23.602: INFO: Pod pod-projected-configmaps-d109df5d-0853-465d-a6ee-62652bb66013 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:57:23.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9500" for this suite. 03/29/23 08:57:23.604
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:23.607
Mar 29 08:57:23.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 08:57:23.607
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:23.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:23.616
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 08:57:23.623
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:57:23.868
STEP: Deploying the webhook pod 03/29/23 08:57:23.873
STEP: Wait for the deployment to be ready 03/29/23 08:57:23.878
Mar 29 08:57:23.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 08:57:25.887
STEP: Verifying the service has paired with the endpoint 03/29/23 08:57:25.893
Mar 29 08:57:26.893: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar 29 08:57:26.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7936-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 08:57:27.402
STEP: Creating a custom resource while v1 is storage version 03/29/23 08:57:27.41
STEP: Patching Custom Resource Definition to set v2 as storage 03/29/23 08:57:29.446
STEP: Patching the custom resource while v2 is storage version 03/29/23 08:57:29.457
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 08:57:29.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6545" for this suite. 03/29/23 08:57:29.999
STEP: Destroying namespace "webhook-6545-markers" for this suite. 03/29/23 08:57:30.001
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":278,"skipped":5190,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:23.607
    Mar 29 08:57:23.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 08:57:23.607
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:23.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:23.616
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 08:57:23.623
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 08:57:23.868
    STEP: Deploying the webhook pod 03/29/23 08:57:23.873
    STEP: Wait for the deployment to be ready 03/29/23 08:57:23.878
    Mar 29 08:57:23.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 08:57:25.887
    STEP: Verifying the service has paired with the endpoint 03/29/23 08:57:25.893
    Mar 29 08:57:26.893: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar 29 08:57:26.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7936-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 08:57:27.402
    STEP: Creating a custom resource while v1 is storage version 03/29/23 08:57:27.41
    STEP: Patching Custom Resource Definition to set v2 as storage 03/29/23 08:57:29.446
    STEP: Patching the custom resource while v2 is storage version 03/29/23 08:57:29.457
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 08:57:29.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6545" for this suite. 03/29/23 08:57:29.999
    STEP: Destroying namespace "webhook-6545-markers" for this suite. 03/29/23 08:57:30.001
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:30.022
Mar 29 08:57:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:57:30.022
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:30.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:30.033
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-45ff0e6d-e958-4d00-977b-43ffc5aa3204 03/29/23 08:57:30.034
STEP: Creating a pod to test consume configMaps 03/29/23 08:57:30.038
Mar 29 08:57:30.043: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf" in namespace "projected-4913" to be "Succeeded or Failed"
Mar 29 08:57:30.045: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60604ms
Mar 29 08:57:32.048: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004657554s
Mar 29 08:57:34.047: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003706526s
STEP: Saw pod success 03/29/23 08:57:34.047
Mar 29 08:57:34.047: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf" satisfied condition "Succeeded or Failed"
Mar 29 08:57:34.049: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf container agnhost-container: <nil>
STEP: delete the pod 03/29/23 08:57:34.051
Mar 29 08:57:34.056: INFO: Waiting for pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf to disappear
Mar 29 08:57:34.058: INFO: Pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 08:57:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4913" for this suite. 03/29/23 08:57:34.059
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":279,"skipped":5225,"failed":0}
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:30.022
    Mar 29 08:57:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:57:30.022
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:30.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:30.033
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-45ff0e6d-e958-4d00-977b-43ffc5aa3204 03/29/23 08:57:30.034
    STEP: Creating a pod to test consume configMaps 03/29/23 08:57:30.038
    Mar 29 08:57:30.043: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf" in namespace "projected-4913" to be "Succeeded or Failed"
    Mar 29 08:57:30.045: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60604ms
    Mar 29 08:57:32.048: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004657554s
    Mar 29 08:57:34.047: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003706526s
    STEP: Saw pod success 03/29/23 08:57:34.047
    Mar 29 08:57:34.047: INFO: Pod "pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf" satisfied condition "Succeeded or Failed"
    Mar 29 08:57:34.049: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 08:57:34.051
    Mar 29 08:57:34.056: INFO: Waiting for pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf to disappear
    Mar 29 08:57:34.058: INFO: Pod pod-projected-configmaps-8c5f90f0-6490-41a6-a807-262ec419bfdf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 08:57:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4913" for this suite. 03/29/23 08:57:34.059
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:34.063
Mar 29 08:57:34.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:57:34.063
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:34.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:34.07
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-debb9349-861b-4384-a329-75b11c8cf225 03/29/23 08:57:34.071
STEP: Creating secret with name secret-projected-all-test-volume-ffbe1895-493c-496c-9da5-57e9a49862c0 03/29/23 08:57:34.074
STEP: Creating a pod to test Check all projections for projected volume plugin 03/29/23 08:57:34.075
Mar 29 08:57:34.078: INFO: Waiting up to 5m0s for pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9" in namespace "projected-4482" to be "Succeeded or Failed"
Mar 29 08:57:34.079: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225607ms
Mar 29 08:57:36.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004373316s
Mar 29 08:57:38.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004257426s
STEP: Saw pod success 03/29/23 08:57:38.083
Mar 29 08:57:38.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9" satisfied condition "Succeeded or Failed"
Mar 29 08:57:38.084: INFO: Trying to get logs from node 10.146.0.115 pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 container projected-all-volume-test: <nil>
STEP: delete the pod 03/29/23 08:57:38.087
Mar 29 08:57:38.093: INFO: Waiting for pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 to disappear
Mar 29 08:57:38.094: INFO: Pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar 29 08:57:38.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4482" for this suite. 03/29/23 08:57:38.096
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":280,"skipped":5228,"failed":0}
------------------------------
â€¢ [4.036 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:34.063
    Mar 29 08:57:34.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:57:34.063
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:34.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:34.07
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-debb9349-861b-4384-a329-75b11c8cf225 03/29/23 08:57:34.071
    STEP: Creating secret with name secret-projected-all-test-volume-ffbe1895-493c-496c-9da5-57e9a49862c0 03/29/23 08:57:34.074
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/29/23 08:57:34.075
    Mar 29 08:57:34.078: INFO: Waiting up to 5m0s for pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9" in namespace "projected-4482" to be "Succeeded or Failed"
    Mar 29 08:57:34.079: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225607ms
    Mar 29 08:57:36.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004373316s
    Mar 29 08:57:38.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004257426s
    STEP: Saw pod success 03/29/23 08:57:38.083
    Mar 29 08:57:38.083: INFO: Pod "projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9" satisfied condition "Succeeded or Failed"
    Mar 29 08:57:38.084: INFO: Trying to get logs from node 10.146.0.115 pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 container projected-all-volume-test: <nil>
    STEP: delete the pod 03/29/23 08:57:38.087
    Mar 29 08:57:38.093: INFO: Waiting for pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 to disappear
    Mar 29 08:57:38.094: INFO: Pod projected-volume-a4efaced-68b6-4189-b376-5e32d51383d9 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar 29 08:57:38.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4482" for this suite. 03/29/23 08:57:38.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:38.099
Mar 29 08:57:38.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:57:38.1
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:38.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:38.108
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2121 03/29/23 08:57:38.109
STEP: creating a selector 03/29/23 08:57:38.109
STEP: Creating the service pods in kubernetes 03/29/23 08:57:38.109
Mar 29 08:57:38.109: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 29 08:57:38.122: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2121" to be "running and ready"
Mar 29 08:57:38.123: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601818ms
Mar 29 08:57:38.123: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 08:57:40.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004044253s
Mar 29 08:57:40.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:57:42.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004997679s
Mar 29 08:57:42.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:57:44.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003888836s
Mar 29 08:57:44.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:57:46.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004258839s
Mar 29 08:57:46.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:57:48.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004488326s
Mar 29 08:57:48.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 29 08:57:50.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004490399s
Mar 29 08:57:50.126: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 29 08:57:50.126: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 29 08:57:50.127: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2121" to be "running and ready"
Mar 29 08:57:50.129: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.202699ms
Mar 29 08:57:50.129: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 29 08:57:50.129: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 29 08:57:50.130: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2121" to be "running and ready"
Mar 29 08:57:50.131: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.189923ms
Mar 29 08:57:50.131: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 29 08:57:50.131: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/29/23 08:57:50.132
Mar 29 08:57:50.139: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2121" to be "running"
Mar 29 08:57:50.140: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.523542ms
Mar 29 08:57:52.143: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004521964s
Mar 29 08:57:52.143: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 29 08:57:52.145: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2121" to be "running"
Mar 29 08:57:52.146: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.291911ms
Mar 29 08:57:52.146: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 29 08:57:52.147: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 29 08:57:52.147: INFO: Going to poll 192.168.30.45 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:57:52.148: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.30.45:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:57:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:57:52.149: INFO: ExecWithOptions: Clientset creation
Mar 29 08:57:52.149: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.30.45%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:57:52.198: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 29 08:57:52.198: INFO: Going to poll 192.168.219.168 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:57:52.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.219.168:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:57:52.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:57:52.200: INFO: ExecWithOptions: Clientset creation
Mar 29 08:57:52.200: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.219.168%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:57:52.246: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 29 08:57:52.246: INFO: Going to poll 192.168.87.214 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 29 08:57:52.247: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.87.214:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 08:57:52.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 08:57:52.248: INFO: ExecWithOptions: Clientset creation
Mar 29 08:57:52.248: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.87.214%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 29 08:57:52.291: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 29 08:57:52.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2121" for this suite. 03/29/23 08:57:52.293
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5251,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.197 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:38.099
    Mar 29 08:57:38.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pod-network-test 03/29/23 08:57:38.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:38.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:38.108
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2121 03/29/23 08:57:38.109
    STEP: creating a selector 03/29/23 08:57:38.109
    STEP: Creating the service pods in kubernetes 03/29/23 08:57:38.109
    Mar 29 08:57:38.109: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 29 08:57:38.122: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2121" to be "running and ready"
    Mar 29 08:57:38.123: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601818ms
    Mar 29 08:57:38.123: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 08:57:40.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004044253s
    Mar 29 08:57:40.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:57:42.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004997679s
    Mar 29 08:57:42.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:57:44.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003888836s
    Mar 29 08:57:44.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:57:46.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004258839s
    Mar 29 08:57:46.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:57:48.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004488326s
    Mar 29 08:57:48.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 29 08:57:50.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004490399s
    Mar 29 08:57:50.126: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 29 08:57:50.126: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 29 08:57:50.127: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2121" to be "running and ready"
    Mar 29 08:57:50.129: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.202699ms
    Mar 29 08:57:50.129: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 29 08:57:50.129: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 29 08:57:50.130: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2121" to be "running and ready"
    Mar 29 08:57:50.131: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.189923ms
    Mar 29 08:57:50.131: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 29 08:57:50.131: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/29/23 08:57:50.132
    Mar 29 08:57:50.139: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2121" to be "running"
    Mar 29 08:57:50.140: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.523542ms
    Mar 29 08:57:52.143: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004521964s
    Mar 29 08:57:52.143: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 29 08:57:52.145: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2121" to be "running"
    Mar 29 08:57:52.146: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.291911ms
    Mar 29 08:57:52.146: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 29 08:57:52.147: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 29 08:57:52.147: INFO: Going to poll 192.168.30.45 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:57:52.148: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.30.45:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:57:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:57:52.149: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:57:52.149: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.30.45%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:57:52.198: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 29 08:57:52.198: INFO: Going to poll 192.168.219.168 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:57:52.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.219.168:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:57:52.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:57:52.200: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:57:52.200: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.219.168%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:57:52.246: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 29 08:57:52.246: INFO: Going to poll 192.168.87.214 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 29 08:57:52.247: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.87.214:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2121 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 08:57:52.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 08:57:52.248: INFO: ExecWithOptions: Clientset creation
    Mar 29 08:57:52.248: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/pod-network-test-2121/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.87.214%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 29 08:57:52.291: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 29 08:57:52.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2121" for this suite. 03/29/23 08:57:52.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:52.297
Mar 29 08:57:52.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 08:57:52.297
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:52.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:52.305
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/29/23 08:57:52.307
Mar 29 08:57:52.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808" in namespace "projected-601" to be "Succeeded or Failed"
Mar 29 08:57:52.311: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Pending", Reason="", readiness=false. Elapsed: 1.247268ms
Mar 29 08:57:54.313: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003308898s
Mar 29 08:57:56.315: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005347419s
STEP: Saw pod success 03/29/23 08:57:56.315
Mar 29 08:57:56.315: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808" satisfied condition "Succeeded or Failed"
Mar 29 08:57:56.317: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 container client-container: <nil>
STEP: delete the pod 03/29/23 08:57:56.319
Mar 29 08:57:56.326: INFO: Waiting for pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 to disappear
Mar 29 08:57:56.327: INFO: Pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 08:57:56.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-601" for this suite. 03/29/23 08:57:56.328
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":282,"skipped":5267,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:52.297
    Mar 29 08:57:52.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 08:57:52.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:52.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:52.305
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/29/23 08:57:52.307
    Mar 29 08:57:52.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808" in namespace "projected-601" to be "Succeeded or Failed"
    Mar 29 08:57:52.311: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Pending", Reason="", readiness=false. Elapsed: 1.247268ms
    Mar 29 08:57:54.313: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003308898s
    Mar 29 08:57:56.315: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005347419s
    STEP: Saw pod success 03/29/23 08:57:56.315
    Mar 29 08:57:56.315: INFO: Pod "downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808" satisfied condition "Succeeded or Failed"
    Mar 29 08:57:56.317: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 container client-container: <nil>
    STEP: delete the pod 03/29/23 08:57:56.319
    Mar 29 08:57:56.326: INFO: Waiting for pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 to disappear
    Mar 29 08:57:56.327: INFO: Pod downwardapi-volume-903f9c8f-3b0e-4da8-a9a2-3396d17db808 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 08:57:56.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-601" for this suite. 03/29/23 08:57:56.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:56.332
Mar 29 08:57:56.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename init-container 03/29/23 08:57:56.332
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:56.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:56.339
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/29/23 08:57:56.34
Mar 29 08:57:56.340: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 08:57:59.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1171" for this suite. 03/29/23 08:57:59.863
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":283,"skipped":5277,"failed":0}
------------------------------
â€¢ [3.536 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:56.332
    Mar 29 08:57:56.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename init-container 03/29/23 08:57:56.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:56.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:56.339
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/29/23 08:57:56.34
    Mar 29 08:57:56.340: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 08:57:59.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1171" for this suite. 03/29/23 08:57:59.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:57:59.868
Mar 29 08:57:59.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename cronjob 03/29/23 08:57:59.868
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:59.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:59.89
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/29/23 08:57:59.892
STEP: Ensuring a job is scheduled 03/29/23 08:57:59.896
STEP: Ensuring exactly one is scheduled 03/29/23 08:58:01.898
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/29/23 08:58:01.9
STEP: Ensuring the job is replaced with a new one 03/29/23 08:58:01.901
STEP: Removing cronjob 03/29/23 08:59:01.904
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 29 08:59:01.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7883" for this suite. 03/29/23 08:59:01.909
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":284,"skipped":5285,"failed":0}
------------------------------
â€¢ [SLOW TEST] [62.044 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:57:59.868
    Mar 29 08:57:59.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename cronjob 03/29/23 08:57:59.868
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:57:59.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:57:59.89
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/29/23 08:57:59.892
    STEP: Ensuring a job is scheduled 03/29/23 08:57:59.896
    STEP: Ensuring exactly one is scheduled 03/29/23 08:58:01.898
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/29/23 08:58:01.9
    STEP: Ensuring the job is replaced with a new one 03/29/23 08:58:01.901
    STEP: Removing cronjob 03/29/23 08:59:01.904
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 29 08:59:01.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7883" for this suite. 03/29/23 08:59:01.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 08:59:01.917
Mar 29 08:59:01.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-watch 03/29/23 08:59:01.918
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:59:01.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:59:01.925
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 29 08:59:01.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Creating first CR  03/29/23 08:59:04.454
Mar 29 08:59:04.457: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:04Z]] name:name1 resourceVersion:27087 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/29/23 08:59:14.457
Mar 29 08:59:14.461: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:14Z]] name:name2 resourceVersion:27119 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/29/23 08:59:24.461
Mar 29 08:59:24.465: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:24Z]] name:name1 resourceVersion:27133 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/29/23 08:59:34.465
Mar 29 08:59:34.470: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:34Z]] name:name2 resourceVersion:27161 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/29/23 08:59:44.471
Mar 29 08:59:44.475: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:24Z]] name:name1 resourceVersion:27177 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/29/23 08:59:54.475
Mar 29 08:59:54.479: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:34Z]] name:name2 resourceVersion:27190 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:00:04.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1580" for this suite. 03/29/23 09:00:04.989
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":285,"skipped":5360,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.075 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 08:59:01.917
    Mar 29 08:59:01.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-watch 03/29/23 08:59:01.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 08:59:01.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 08:59:01.925
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 29 08:59:01.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Creating first CR  03/29/23 08:59:04.454
    Mar 29 08:59:04.457: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:04Z]] name:name1 resourceVersion:27087 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/29/23 08:59:14.457
    Mar 29 08:59:14.461: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:14Z]] name:name2 resourceVersion:27119 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/29/23 08:59:24.461
    Mar 29 08:59:24.465: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:24Z]] name:name1 resourceVersion:27133 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/29/23 08:59:34.465
    Mar 29 08:59:34.470: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:34Z]] name:name2 resourceVersion:27161 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/29/23 08:59:44.471
    Mar 29 08:59:44.475: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:24Z]] name:name1 resourceVersion:27177 uid:80a01d83-6756-4b4e-aacf-f935b22ebfd5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/29/23 08:59:54.475
    Mar 29 08:59:54.479: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-29T08:59:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-29T08:59:34Z]] name:name2 resourceVersion:27190 uid:71f3af39-0ad6-421e-b3b9-29a8041539d0] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:00:04.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-1580" for this suite. 03/29/23 09:00:04.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:04.992
Mar 29 09:00:04.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 09:00:04.993
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:05.001
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/29/23 09:00:05.003
STEP: Ensuring job reaches completions 03/29/23 09:00:05.005
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 09:00:15.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-362" for this suite. 03/29/23 09:00:15.009
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":286,"skipped":5371,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.019 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:04.992
    Mar 29 09:00:04.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 09:00:04.993
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:05.001
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/29/23 09:00:05.003
    STEP: Ensuring job reaches completions 03/29/23 09:00:05.005
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 09:00:15.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-362" for this suite. 03/29/23 09:00:15.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:15.012
Mar 29 09:00:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 09:00:15.013
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:15.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:15.022
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/29/23 09:00:15.023
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_tcp@PTR;sleep 1; done
 03/29/23 09:00:15.03
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_tcp@PTR;sleep 1; done
 03/29/23 09:00:15.03
STEP: creating a pod to probe DNS 03/29/23 09:00:15.03
STEP: submitting the pod to kubernetes 03/29/23 09:00:15.03
Mar 29 09:00:15.036: INFO: Waiting up to 15m0s for pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c" in namespace "dns-3892" to be "running"
Mar 29 09:00:15.037: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631323ms
Mar 29 09:00:17.040: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004706632s
Mar 29 09:00:17.040: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:00:17.04
STEP: looking for the results for each expected name from probers 03/29/23 09:00:17.042
Mar 29 09:00:17.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.047: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.048: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:17.069: INFO: Lookups using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c failed for: [wheezy_udp@dns-test-service.dns-3892.svc.cluster.local wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local jessie_udp@dns-test-service.dns-3892.svc.cluster.local jessie_tcp@dns-test-service.dns-3892.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local]

Mar 29 09:00:22.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
Mar 29 09:00:22.101: INFO: Lookups using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c failed for: [wheezy_udp@dns-test-service.dns-3892.svc.cluster.local]

Mar 29 09:00:27.101: INFO: DNS probes using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c succeeded

STEP: deleting the pod 03/29/23 09:00:27.101
STEP: deleting the test service 03/29/23 09:00:27.108
STEP: deleting the test headless service 03/29/23 09:00:27.119
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 09:00:27.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3892" for this suite. 03/29/23 09:00:27.125
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":287,"skipped":5381,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.116 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:15.012
    Mar 29 09:00:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 09:00:15.013
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:15.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:15.022
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/29/23 09:00:15.023
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_tcp@PTR;sleep 1; done
     03/29/23 09:00:15.03
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3892.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3892.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3892.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.227.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.227.181_tcp@PTR;sleep 1; done
     03/29/23 09:00:15.03
    STEP: creating a pod to probe DNS 03/29/23 09:00:15.03
    STEP: submitting the pod to kubernetes 03/29/23 09:00:15.03
    Mar 29 09:00:15.036: INFO: Waiting up to 15m0s for pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c" in namespace "dns-3892" to be "running"
    Mar 29 09:00:15.037: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631323ms
    Mar 29 09:00:17.040: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004706632s
    Mar 29 09:00:17.040: INFO: Pod "dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:00:17.04
    STEP: looking for the results for each expected name from probers 03/29/23 09:00:17.042
    Mar 29 09:00:17.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.047: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.048: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:17.069: INFO: Lookups using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c failed for: [wheezy_udp@dns-test-service.dns-3892.svc.cluster.local wheezy_tcp@dns-test-service.dns-3892.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local jessie_udp@dns-test-service.dns-3892.svc.cluster.local jessie_tcp@dns-test-service.dns-3892.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3892.svc.cluster.local]

    Mar 29 09:00:22.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-3892.svc.cluster.local from pod dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c: the server could not find the requested resource (get pods dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c)
    Mar 29 09:00:22.101: INFO: Lookups using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c failed for: [wheezy_udp@dns-test-service.dns-3892.svc.cluster.local]

    Mar 29 09:00:27.101: INFO: DNS probes using dns-3892/dns-test-86ed1501-57ec-40e3-8612-f096f8d39d8c succeeded

    STEP: deleting the pod 03/29/23 09:00:27.101
    STEP: deleting the test service 03/29/23 09:00:27.108
    STEP: deleting the test headless service 03/29/23 09:00:27.119
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 09:00:27.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3892" for this suite. 03/29/23 09:00:27.125
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:27.129
Mar 29 09:00:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 09:00:27.129
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:27.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:27.137
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7914 03/29/23 09:00:27.138
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar 29 09:00:27.147: INFO: Found 0 stateful pods, waiting for 1
Mar 29 09:00:37.149: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/29/23 09:00:37.152
W0329 09:00:37.156877      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 29 09:00:37.159: INFO: Found 1 stateful pods, waiting for 2
Mar 29 09:00:47.164: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 29 09:00:47.164: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/29/23 09:00:47.167
STEP: Delete all of the StatefulSets 03/29/23 09:00:47.168
STEP: Verify that StatefulSets have been deleted 03/29/23 09:00:47.171
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 09:00:47.172: INFO: Deleting all statefulset in ns statefulset-7914
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 09:00:47.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7914" for this suite. 03/29/23 09:00:47.178
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":288,"skipped":5384,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.052 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:27.129
    Mar 29 09:00:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 09:00:27.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:27.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:27.137
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7914 03/29/23 09:00:27.138
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar 29 09:00:27.147: INFO: Found 0 stateful pods, waiting for 1
    Mar 29 09:00:37.149: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/29/23 09:00:37.152
    W0329 09:00:37.156877      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 29 09:00:37.159: INFO: Found 1 stateful pods, waiting for 2
    Mar 29 09:00:47.164: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 29 09:00:47.164: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/29/23 09:00:47.167
    STEP: Delete all of the StatefulSets 03/29/23 09:00:47.168
    STEP: Verify that StatefulSets have been deleted 03/29/23 09:00:47.171
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 09:00:47.172: INFO: Deleting all statefulset in ns statefulset-7914
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 09:00:47.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7914" for this suite. 03/29/23 09:00:47.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:47.181
Mar 29 09:00:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename ephemeral-containers-test 03/29/23 09:00:47.182
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:47.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:47.194
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/29/23 09:00:47.195
Mar 29 09:00:47.200: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2528" to be "running and ready"
Mar 29 09:00:47.202: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032537ms
Mar 29 09:00:47.202: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:00:49.204: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004192457s
Mar 29 09:00:49.204: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 29 09:00:49.204: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/29/23 09:00:49.205
Mar 29 09:00:49.213: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2528" to be "container debugger running"
Mar 29 09:00:49.214: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.313792ms
Mar 29 09:00:51.217: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004128786s
Mar 29 09:00:51.217: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/29/23 09:00:51.217
Mar 29 09:00:51.217: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2528 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:00:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:00:51.217: INFO: ExecWithOptions: Clientset creation
Mar 29 09:00:51.217: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/ephemeral-containers-test-2528/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 29 09:00:51.262: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 09:00:51.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2528" for this suite. 03/29/23 09:00:51.271
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":289,"skipped":5405,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:47.181
    Mar 29 09:00:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/29/23 09:00:47.182
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:47.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:47.194
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/29/23 09:00:47.195
    Mar 29 09:00:47.200: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2528" to be "running and ready"
    Mar 29 09:00:47.202: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032537ms
    Mar 29 09:00:47.202: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:00:49.204: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004192457s
    Mar 29 09:00:49.204: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 29 09:00:49.204: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/29/23 09:00:49.205
    Mar 29 09:00:49.213: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2528" to be "container debugger running"
    Mar 29 09:00:49.214: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.313792ms
    Mar 29 09:00:51.217: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004128786s
    Mar 29 09:00:51.217: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/29/23 09:00:51.217
    Mar 29 09:00:51.217: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2528 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:00:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:00:51.217: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:00:51.217: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/ephemeral-containers-test-2528/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 29 09:00:51.262: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 09:00:51.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2528" for this suite. 03/29/23 09:00:51.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:51.276
Mar 29 09:00:51.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:00:51.277
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:51.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:51.284
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:00:51.291
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:00:51.484
STEP: Deploying the webhook pod 03/29/23 09:00:51.488
STEP: Wait for the deployment to be ready 03/29/23 09:00:51.493
Mar 29 09:00:51.497: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:00:53.502
STEP: Verifying the service has paired with the endpoint 03/29/23 09:00:53.51
Mar 29 09:00:54.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar 29 09:00:54.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/29/23 09:00:55.018
STEP: Creating a custom resource that should be denied by the webhook 03/29/23 09:00:55.026
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/29/23 09:00:57.047
STEP: Updating the custom resource with disallowed data should be denied 03/29/23 09:00:57.052
STEP: Deleting the custom resource should be denied 03/29/23 09:00:57.056
STEP: Remove the offending key and value from the custom resource data 03/29/23 09:00:57.059
STEP: Deleting the updated custom resource should be successful 03/29/23 09:00:57.063
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:00:57.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5731" for this suite. 03/29/23 09:00:57.576
STEP: Destroying namespace "webhook-5731-markers" for this suite. 03/29/23 09:00:57.579
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":290,"skipped":5455,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.322 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:51.276
    Mar 29 09:00:51.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:00:51.277
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:51.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:51.284
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:00:51.291
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:00:51.484
    STEP: Deploying the webhook pod 03/29/23 09:00:51.488
    STEP: Wait for the deployment to be ready 03/29/23 09:00:51.493
    Mar 29 09:00:51.497: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:00:53.502
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:00:53.51
    Mar 29 09:00:54.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar 29 09:00:54.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/29/23 09:00:55.018
    STEP: Creating a custom resource that should be denied by the webhook 03/29/23 09:00:55.026
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/29/23 09:00:57.047
    STEP: Updating the custom resource with disallowed data should be denied 03/29/23 09:00:57.052
    STEP: Deleting the custom resource should be denied 03/29/23 09:00:57.056
    STEP: Remove the offending key and value from the custom resource data 03/29/23 09:00:57.059
    STEP: Deleting the updated custom resource should be successful 03/29/23 09:00:57.063
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:00:57.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5731" for this suite. 03/29/23 09:00:57.576
    STEP: Destroying namespace "webhook-5731-markers" for this suite. 03/29/23 09:00:57.579
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:00:57.599
Mar 29 09:00:57.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 09:00:57.6
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:57.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:57.613
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/29/23 09:00:57.615
STEP: Ensuring ResourceQuota status is calculated 03/29/23 09:00:57.619
STEP: Creating a ResourceQuota with not terminating scope 03/29/23 09:00:59.622
STEP: Ensuring ResourceQuota status is calculated 03/29/23 09:00:59.625
STEP: Creating a long running pod 03/29/23 09:01:01.627
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/29/23 09:01:01.634
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/29/23 09:01:03.637
STEP: Deleting the pod 03/29/23 09:01:05.64
STEP: Ensuring resource quota status released the pod usage 03/29/23 09:01:05.647
STEP: Creating a terminating pod 03/29/23 09:01:07.649
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/29/23 09:01:07.655
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/29/23 09:01:09.658
STEP: Deleting the pod 03/29/23 09:01:11.661
STEP: Ensuring resource quota status released the pod usage 03/29/23 09:01:11.666
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 09:01:13.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5127" for this suite. 03/29/23 09:01:13.671
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":291,"skipped":5482,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.075 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:00:57.599
    Mar 29 09:00:57.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 09:00:57.6
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:00:57.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:00:57.613
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/29/23 09:00:57.615
    STEP: Ensuring ResourceQuota status is calculated 03/29/23 09:00:57.619
    STEP: Creating a ResourceQuota with not terminating scope 03/29/23 09:00:59.622
    STEP: Ensuring ResourceQuota status is calculated 03/29/23 09:00:59.625
    STEP: Creating a long running pod 03/29/23 09:01:01.627
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/29/23 09:01:01.634
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/29/23 09:01:03.637
    STEP: Deleting the pod 03/29/23 09:01:05.64
    STEP: Ensuring resource quota status released the pod usage 03/29/23 09:01:05.647
    STEP: Creating a terminating pod 03/29/23 09:01:07.649
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/29/23 09:01:07.655
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/29/23 09:01:09.658
    STEP: Deleting the pod 03/29/23 09:01:11.661
    STEP: Ensuring resource quota status released the pod usage 03/29/23 09:01:11.666
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 09:01:13.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5127" for this suite. 03/29/23 09:01:13.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:13.674
Mar 29 09:01:13.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename containers 03/29/23 09:01:13.675
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:13.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:13.683
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar 29 09:01:13.687: INFO: Waiting up to 5m0s for pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3" in namespace "containers-6484" to be "running"
Mar 29 09:01:13.689: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101538ms
Mar 29 09:01:15.693: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005348798s
Mar 29 09:01:15.693: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 29 09:01:15.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6484" for this suite. 03/29/23 09:01:15.701
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":292,"skipped":5488,"failed":0}
------------------------------
â€¢ [2.029 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:13.674
    Mar 29 09:01:13.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename containers 03/29/23 09:01:13.675
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:13.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:13.683
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar 29 09:01:13.687: INFO: Waiting up to 5m0s for pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3" in namespace "containers-6484" to be "running"
    Mar 29 09:01:13.689: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101538ms
    Mar 29 09:01:15.693: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005348798s
    Mar 29 09:01:15.693: INFO: Pod "client-containers-0161f5c4-12b8-4c05-83aa-635e5274b7b3" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 29 09:01:15.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6484" for this suite. 03/29/23 09:01:15.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:15.704
Mar 29 09:01:15.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename namespaces 03/29/23 09:01:15.705
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:15.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:15.712
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/29/23 09:01:15.713
Mar 29 09:01:15.714: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/29/23 09:01:15.714
Mar 29 09:01:15.717: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/29/23 09:01:15.717
Mar 29 09:01:15.720: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 29 09:01:15.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6424" for this suite. 03/29/23 09:01:15.722
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":293,"skipped":5498,"failed":0}
------------------------------
â€¢ [0.020 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:15.704
    Mar 29 09:01:15.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename namespaces 03/29/23 09:01:15.705
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:15.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:15.712
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/29/23 09:01:15.713
    Mar 29 09:01:15.714: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/29/23 09:01:15.714
    Mar 29 09:01:15.717: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/29/23 09:01:15.717
    Mar 29 09:01:15.720: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 09:01:15.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6424" for this suite. 03/29/23 09:01:15.722
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:15.725
Mar 29 09:01:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:01:15.725
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:15.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:15.732
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-db7f9aea-23bd-410f-a89b-58393b6d4180 03/29/23 09:01:15.733
STEP: Creating a pod to test consume secrets 03/29/23 09:01:15.736
Mar 29 09:01:15.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2" in namespace "projected-5821" to be "Succeeded or Failed"
Mar 29 09:01:15.740: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.300349ms
Mar 29 09:01:17.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004500375s
Mar 29 09:01:19.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004201299s
STEP: Saw pod success 03/29/23 09:01:19.743
Mar 29 09:01:19.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2" satisfied condition "Succeeded or Failed"
Mar 29 09:01:19.745: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/29/23 09:01:19.747
Mar 29 09:01:19.753: INFO: Waiting for pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 to disappear
Mar 29 09:01:19.755: INFO: Pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 29 09:01:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5821" for this suite. 03/29/23 09:01:19.757
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":294,"skipped":5501,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:15.725
    Mar 29 09:01:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:01:15.725
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:15.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:15.732
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-db7f9aea-23bd-410f-a89b-58393b6d4180 03/29/23 09:01:15.733
    STEP: Creating a pod to test consume secrets 03/29/23 09:01:15.736
    Mar 29 09:01:15.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2" in namespace "projected-5821" to be "Succeeded or Failed"
    Mar 29 09:01:15.740: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.300349ms
    Mar 29 09:01:17.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004500375s
    Mar 29 09:01:19.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004201299s
    STEP: Saw pod success 03/29/23 09:01:19.743
    Mar 29 09:01:19.743: INFO: Pod "pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2" satisfied condition "Succeeded or Failed"
    Mar 29 09:01:19.745: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/29/23 09:01:19.747
    Mar 29 09:01:19.753: INFO: Waiting for pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 to disappear
    Mar 29 09:01:19.755: INFO: Pod pod-projected-secrets-b901a8c4-d58a-4489-82d1-8c97124939b2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 29 09:01:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5821" for this suite. 03/29/23 09:01:19.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:19.759
Mar 29 09:01:19.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename watch 03/29/23 09:01:19.76
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:19.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:19.767
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/29/23 09:01:19.768
STEP: creating a watch on configmaps with label B 03/29/23 09:01:19.769
STEP: creating a watch on configmaps with label A or B 03/29/23 09:01:19.769
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.77
Mar 29 09:01:19.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27835 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:19.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27835 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.772
Mar 29 09:01:19.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27836 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:19.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27836 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/29/23 09:01:19.775
Mar 29 09:01:19.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27837 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:19.778: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27837 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.778
Mar 29 09:01:19.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27838 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:19.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27838 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/29/23 09:01:19.78
Mar 29 09:01:19.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27839 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:19.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27839 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/29/23 09:01:29.782
Mar 29 09:01:29.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27891 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 29 09:01:29.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27891 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 29 09:01:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7449" for this suite. 03/29/23 09:01:39.79
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":295,"skipped":5506,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.033 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:19.759
    Mar 29 09:01:19.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename watch 03/29/23 09:01:19.76
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:19.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:19.767
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/29/23 09:01:19.768
    STEP: creating a watch on configmaps with label B 03/29/23 09:01:19.769
    STEP: creating a watch on configmaps with label A or B 03/29/23 09:01:19.769
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.77
    Mar 29 09:01:19.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27835 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:19.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27835 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.772
    Mar 29 09:01:19.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27836 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:19.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27836 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/29/23 09:01:19.775
    Mar 29 09:01:19.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27837 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:19.778: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27837 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/29/23 09:01:19.778
    Mar 29 09:01:19.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27838 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:19.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7449  28519593-eb49-456b-94c9-5798c7b61b4d 27838 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/29/23 09:01:19.78
    Mar 29 09:01:19.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27839 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:19.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27839 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/29/23 09:01:29.782
    Mar 29 09:01:29.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27891 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 29 09:01:29.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7449  e549a523-ae10-472a-838c-7f6b250768b6 27891 0 2023-03-29 09:01:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-29 09:01:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 29 09:01:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7449" for this suite. 03/29/23 09:01:39.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:39.793
Mar 29 09:01:39.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 09:01:39.794
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:39.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:39.803
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/29/23 09:01:39.804
STEP: Ensuring active pods == parallelism 03/29/23 09:01:39.806
STEP: Orphaning one of the Job's Pods 03/29/23 09:01:41.809
Mar 29 09:01:42.318: INFO: Successfully updated pod "adopt-release-6mvwr"
STEP: Checking that the Job readopts the Pod 03/29/23 09:01:42.318
Mar 29 09:01:42.318: INFO: Waiting up to 15m0s for pod "adopt-release-6mvwr" in namespace "job-9136" to be "adopted"
Mar 29 09:01:42.320: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 1.551593ms
Mar 29 09:01:44.322: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003718005s
Mar 29 09:01:44.322: INFO: Pod "adopt-release-6mvwr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/29/23 09:01:44.322
Mar 29 09:01:44.828: INFO: Successfully updated pod "adopt-release-6mvwr"
STEP: Checking that the Job releases the Pod 03/29/23 09:01:44.828
Mar 29 09:01:44.828: INFO: Waiting up to 15m0s for pod "adopt-release-6mvwr" in namespace "job-9136" to be "released"
Mar 29 09:01:44.830: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 1.242775ms
Mar 29 09:01:46.833: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004105429s
Mar 29 09:01:46.833: INFO: Pod "adopt-release-6mvwr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 09:01:46.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9136" for this suite. 03/29/23 09:01:46.834
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":296,"skipped":5513,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:39.793
    Mar 29 09:01:39.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 09:01:39.794
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:39.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:39.803
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/29/23 09:01:39.804
    STEP: Ensuring active pods == parallelism 03/29/23 09:01:39.806
    STEP: Orphaning one of the Job's Pods 03/29/23 09:01:41.809
    Mar 29 09:01:42.318: INFO: Successfully updated pod "adopt-release-6mvwr"
    STEP: Checking that the Job readopts the Pod 03/29/23 09:01:42.318
    Mar 29 09:01:42.318: INFO: Waiting up to 15m0s for pod "adopt-release-6mvwr" in namespace "job-9136" to be "adopted"
    Mar 29 09:01:42.320: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 1.551593ms
    Mar 29 09:01:44.322: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003718005s
    Mar 29 09:01:44.322: INFO: Pod "adopt-release-6mvwr" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/29/23 09:01:44.322
    Mar 29 09:01:44.828: INFO: Successfully updated pod "adopt-release-6mvwr"
    STEP: Checking that the Job releases the Pod 03/29/23 09:01:44.828
    Mar 29 09:01:44.828: INFO: Waiting up to 15m0s for pod "adopt-release-6mvwr" in namespace "job-9136" to be "released"
    Mar 29 09:01:44.830: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 1.242775ms
    Mar 29 09:01:46.833: INFO: Pod "adopt-release-6mvwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004105429s
    Mar 29 09:01:46.833: INFO: Pod "adopt-release-6mvwr" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 09:01:46.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9136" for this suite. 03/29/23 09:01:46.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:46.838
Mar 29 09:01:46.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:01:46.838
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:46.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:46.846
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:01:46.853
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:01:47.102
STEP: Deploying the webhook pod 03/29/23 09:01:47.105
STEP: Wait for the deployment to be ready 03/29/23 09:01:47.112
Mar 29 09:01:47.115: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:01:49.12
STEP: Verifying the service has paired with the endpoint 03/29/23 09:01:49.126
Mar 29 09:01:50.126: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/29/23 09:01:50.129
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/29/23 09:01:50.129
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/29/23 09:01:50.129
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/29/23 09:01:50.129
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/29/23 09:01:50.13
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/29/23 09:01:50.13
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/29/23 09:01:50.131
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:01:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7902" for this suite. 03/29/23 09:01:50.132
STEP: Destroying namespace "webhook-7902-markers" for this suite. 03/29/23 09:01:50.135
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":297,"skipped":5520,"failed":0}
------------------------------
â€¢ [3.322 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:46.838
    Mar 29 09:01:46.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:01:46.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:46.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:46.846
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:01:46.853
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:01:47.102
    STEP: Deploying the webhook pod 03/29/23 09:01:47.105
    STEP: Wait for the deployment to be ready 03/29/23 09:01:47.112
    Mar 29 09:01:47.115: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:01:49.12
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:01:49.126
    Mar 29 09:01:50.126: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/29/23 09:01:50.129
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/29/23 09:01:50.129
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/29/23 09:01:50.129
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/29/23 09:01:50.129
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/29/23 09:01:50.13
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/29/23 09:01:50.13
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/29/23 09:01:50.131
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:01:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7902" for this suite. 03/29/23 09:01:50.132
    STEP: Destroying namespace "webhook-7902-markers" for this suite. 03/29/23 09:01:50.135
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:50.16
Mar 29 09:01:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:01:50.16
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:50.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:50.169
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/29/23 09:01:50.17
Mar 29 09:01:50.174: INFO: Waiting up to 5m0s for pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e" in namespace "projected-520" to be "running and ready"
Mar 29 09:01:50.176: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31613ms
Mar 29 09:01:50.176: INFO: The phase of Pod annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:01:52.179: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004320944s
Mar 29 09:01:52.179: INFO: The phase of Pod annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e is Running (Ready = true)
Mar 29 09:01:52.179: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e" satisfied condition "running and ready"
Mar 29 09:01:52.691: INFO: Successfully updated pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 09:01:56.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-520" for this suite. 03/29/23 09:01:56.706
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":298,"skipped":5527,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.549 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:50.16
    Mar 29 09:01:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:01:50.16
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:50.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:50.169
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/29/23 09:01:50.17
    Mar 29 09:01:50.174: INFO: Waiting up to 5m0s for pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e" in namespace "projected-520" to be "running and ready"
    Mar 29 09:01:50.176: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31613ms
    Mar 29 09:01:50.176: INFO: The phase of Pod annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:01:52.179: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004320944s
    Mar 29 09:01:52.179: INFO: The phase of Pod annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e is Running (Ready = true)
    Mar 29 09:01:52.179: INFO: Pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e" satisfied condition "running and ready"
    Mar 29 09:01:52.691: INFO: Successfully updated pod "annotationupdate81663edc-ff1f-43c4-b9d8-40917d86386e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 09:01:56.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-520" for this suite. 03/29/23 09:01:56.706
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:01:56.709
Mar 29 09:01:56.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename job 03/29/23 09:01:56.709
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:56.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:56.716
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/29/23 09:01:56.719
STEP: Patching the Job 03/29/23 09:01:56.722
STEP: Watching for Job to be patched 03/29/23 09:01:56.731
Mar 29 09:01:56.732: INFO: Event ADDED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 29 09:01:56.732: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 29 09:01:56.732: INFO: Event MODIFIED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/29/23 09:01:56.732
STEP: Watching for Job to be updated 03/29/23 09:01:56.736
Mar 29 09:01:56.737: INFO: Event MODIFIED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:01:56.737: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/29/23 09:01:56.737
Mar 29 09:01:56.738: INFO: Job: e2e-wv8mc as labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched]
STEP: Waiting for job to complete 03/29/23 09:01:56.738
STEP: Delete a job collection with a labelselector 03/29/23 09:02:06.74
STEP: Watching for Job to be deleted 03/29/23 09:02:06.744
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 29 09:02:06.745: INFO: Event DELETED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/29/23 09:02:06.745
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 29 09:02:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5612" for this suite. 03/29/23 09:02:06.749
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":299,"skipped":5530,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.044 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:01:56.709
    Mar 29 09:01:56.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename job 03/29/23 09:01:56.709
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:01:56.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:01:56.716
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/29/23 09:01:56.719
    STEP: Patching the Job 03/29/23 09:01:56.722
    STEP: Watching for Job to be patched 03/29/23 09:01:56.731
    Mar 29 09:01:56.732: INFO: Event ADDED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 29 09:01:56.732: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 29 09:01:56.732: INFO: Event MODIFIED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/29/23 09:01:56.732
    STEP: Watching for Job to be updated 03/29/23 09:01:56.736
    Mar 29 09:01:56.737: INFO: Event MODIFIED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:01:56.737: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/29/23 09:01:56.737
    Mar 29 09:01:56.738: INFO: Job: e2e-wv8mc as labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched]
    STEP: Waiting for job to complete 03/29/23 09:01:56.738
    STEP: Delete a job collection with a labelselector 03/29/23 09:02:06.74
    STEP: Watching for Job to be deleted 03/29/23 09:02:06.744
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event MODIFIED observed for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 29 09:02:06.745: INFO: Event DELETED found for Job e2e-wv8mc in namespace job-5612 with labels: map[e2e-job-label:e2e-wv8mc e2e-wv8mc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/29/23 09:02:06.745
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 29 09:02:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5612" for this suite. 03/29/23 09:02:06.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:06.755
Mar 29 09:02:06.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 09:02:06.755
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:06.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:06.763
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/29/23 09:02:06.765
STEP: watching for the Service to be added 03/29/23 09:02:06.769
Mar 29 09:02:06.770: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 29 09:02:06.770: INFO: Service test-service-vn8bq created
STEP: Getting /status 03/29/23 09:02:06.77
Mar 29 09:02:06.772: INFO: Service test-service-vn8bq has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/29/23 09:02:06.772
STEP: watching for the Service to be patched 03/29/23 09:02:06.774
Mar 29 09:02:06.775: INFO: observed Service test-service-vn8bq in namespace services-6591 with annotations: map[] & LoadBalancer: {[]}
Mar 29 09:02:06.775: INFO: Found Service test-service-vn8bq in namespace services-6591 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 29 09:02:06.775: INFO: Service test-service-vn8bq has service status patched
STEP: updating the ServiceStatus 03/29/23 09:02:06.775
Mar 29 09:02:06.778: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/29/23 09:02:06.778
Mar 29 09:02:06.779: INFO: Observed Service test-service-vn8bq in namespace services-6591 with annotations: map[] & Conditions: {[]}
Mar 29 09:02:06.779: INFO: Observed event: &Service{ObjectMeta:{test-service-vn8bq  services-6591  71d2676e-28f5-4e9c-93fb-ccb7c02a2590 28232 0 2023-03-29 09:02:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-29 09:02:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-29 09:02:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.126.177,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.126.177],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 29 09:02:06.779: INFO: Found Service test-service-vn8bq in namespace services-6591 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 29 09:02:06.779: INFO: Service test-service-vn8bq has service status updated
STEP: patching the service 03/29/23 09:02:06.779
STEP: watching for the Service to be patched 03/29/23 09:02:06.785
Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
Mar 29 09:02:06.786: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service:patched test-service-static:true]
Mar 29 09:02:06.786: INFO: Service test-service-vn8bq patched
STEP: deleting the service 03/29/23 09:02:06.786
STEP: watching for the Service to be deleted 03/29/23 09:02:06.791
Mar 29 09:02:06.792: INFO: Observed event: ADDED
Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
Mar 29 09:02:06.792: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 29 09:02:06.792: INFO: Service test-service-vn8bq deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 09:02:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6591" for this suite. 03/29/23 09:02:06.794
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":300,"skipped":5589,"failed":0}
------------------------------
â€¢ [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:06.755
    Mar 29 09:02:06.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 09:02:06.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:06.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:06.763
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/29/23 09:02:06.765
    STEP: watching for the Service to be added 03/29/23 09:02:06.769
    Mar 29 09:02:06.770: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 29 09:02:06.770: INFO: Service test-service-vn8bq created
    STEP: Getting /status 03/29/23 09:02:06.77
    Mar 29 09:02:06.772: INFO: Service test-service-vn8bq has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/29/23 09:02:06.772
    STEP: watching for the Service to be patched 03/29/23 09:02:06.774
    Mar 29 09:02:06.775: INFO: observed Service test-service-vn8bq in namespace services-6591 with annotations: map[] & LoadBalancer: {[]}
    Mar 29 09:02:06.775: INFO: Found Service test-service-vn8bq in namespace services-6591 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 29 09:02:06.775: INFO: Service test-service-vn8bq has service status patched
    STEP: updating the ServiceStatus 03/29/23 09:02:06.775
    Mar 29 09:02:06.778: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/29/23 09:02:06.778
    Mar 29 09:02:06.779: INFO: Observed Service test-service-vn8bq in namespace services-6591 with annotations: map[] & Conditions: {[]}
    Mar 29 09:02:06.779: INFO: Observed event: &Service{ObjectMeta:{test-service-vn8bq  services-6591  71d2676e-28f5-4e9c-93fb-ccb7c02a2590 28232 0 2023-03-29 09:02:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-29 09:02:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-29 09:02:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.126.177,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.126.177],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 29 09:02:06.779: INFO: Found Service test-service-vn8bq in namespace services-6591 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 29 09:02:06.779: INFO: Service test-service-vn8bq has service status updated
    STEP: patching the service 03/29/23 09:02:06.779
    STEP: watching for the Service to be patched 03/29/23 09:02:06.785
    Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
    Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
    Mar 29 09:02:06.786: INFO: observed Service test-service-vn8bq in namespace services-6591 with labels: map[test-service-static:true]
    Mar 29 09:02:06.786: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service:patched test-service-static:true]
    Mar 29 09:02:06.786: INFO: Service test-service-vn8bq patched
    STEP: deleting the service 03/29/23 09:02:06.786
    STEP: watching for the Service to be deleted 03/29/23 09:02:06.791
    Mar 29 09:02:06.792: INFO: Observed event: ADDED
    Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
    Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
    Mar 29 09:02:06.792: INFO: Observed event: MODIFIED
    Mar 29 09:02:06.792: INFO: Found Service test-service-vn8bq in namespace services-6591 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 29 09:02:06.792: INFO: Service test-service-vn8bq deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 09:02:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6591" for this suite. 03/29/23 09:02:06.794
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:06.796
Mar 29 09:02:06.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 09:02:06.797
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:06.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:06.805
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 29 09:02:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:02:07.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3968" for this suite. 03/29/23 09:02:07.82
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":301,"skipped":5589,"failed":0}
------------------------------
â€¢ [1.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:06.796
    Mar 29 09:02:06.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename custom-resource-definition 03/29/23 09:02:06.797
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:06.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:06.805
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 29 09:02:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:02:07.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3968" for this suite. 03/29/23 09:02:07.82
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:07.823
Mar 29 09:02:07.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 09:02:07.824
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:07.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:07.831
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/29/23 09:02:07.832
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:07.834
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:07.834
STEP: creating a pod to probe DNS 03/29/23 09:02:07.834
STEP: submitting the pod to kubernetes 03/29/23 09:02:07.834
Mar 29 09:02:07.838: INFO: Waiting up to 15m0s for pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6" in namespace "dns-6694" to be "running"
Mar 29 09:02:07.840: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26924ms
Mar 29 09:02:09.841: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6": Phase="Running", Reason="", readiness=true. Elapsed: 2.002948781s
Mar 29 09:02:09.841: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:02:09.841
STEP: looking for the results for each expected name from probers 03/29/23 09:02:09.843
Mar 29 09:02:09.847: INFO: DNS probes using dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6 succeeded

STEP: deleting the pod 03/29/23 09:02:09.847
STEP: changing the externalName to bar.example.com 03/29/23 09:02:09.853
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:09.856
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:09.857
STEP: creating a second pod to probe DNS 03/29/23 09:02:09.857
STEP: submitting the pod to kubernetes 03/29/23 09:02:09.857
Mar 29 09:02:09.859: INFO: Waiting up to 15m0s for pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c" in namespace "dns-6694" to be "running"
Mar 29 09:02:09.860: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238805ms
Mar 29 09:02:11.863: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003950833s
Mar 29 09:02:11.863: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:02:11.863
STEP: looking for the results for each expected name from probers 03/29/23 09:02:11.864
Mar 29 09:02:11.866: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 09:02:11.869: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 29 09:02:11.869: INFO: Lookups using dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

Mar 29 09:02:16.873: INFO: DNS probes using dns-test-66cb871b-eade-4035-b00d-f369f19bc41c succeeded

STEP: deleting the pod 03/29/23 09:02:16.873
STEP: changing the service to type=ClusterIP 03/29/23 09:02:16.88
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:16.886
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
 03/29/23 09:02:16.886
STEP: creating a third pod to probe DNS 03/29/23 09:02:16.886
STEP: submitting the pod to kubernetes 03/29/23 09:02:16.888
Mar 29 09:02:16.891: INFO: Waiting up to 15m0s for pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229" in namespace "dns-6694" to be "running"
Mar 29 09:02:16.893: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229": Phase="Pending", Reason="", readiness=false. Elapsed: 1.284613ms
Mar 29 09:02:18.895: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229": Phase="Running", Reason="", readiness=true. Elapsed: 2.003523794s
Mar 29 09:02:18.895: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:02:18.895
STEP: looking for the results for each expected name from probers 03/29/23 09:02:18.896
Mar 29 09:02:18.898: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 contains 'bar.example.com.
' instead of '10.100.62.149'
Mar 29 09:02:18.901: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 contains 'bar.example.com.
' instead of '10.100.62.149'
Mar 29 09:02:18.901: INFO: Lookups using dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

Mar 29 09:02:23.905: INFO: DNS probes using dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 succeeded

STEP: deleting the pod 03/29/23 09:02:23.905
STEP: deleting the test externalName service 03/29/23 09:02:23.912
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 09:02:23.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6694" for this suite. 03/29/23 09:02:23.92
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":302,"skipped":5589,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.099 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:07.823
    Mar 29 09:02:07.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 09:02:07.824
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:07.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:07.831
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/29/23 09:02:07.832
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:07.834
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:07.834
    STEP: creating a pod to probe DNS 03/29/23 09:02:07.834
    STEP: submitting the pod to kubernetes 03/29/23 09:02:07.834
    Mar 29 09:02:07.838: INFO: Waiting up to 15m0s for pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6" in namespace "dns-6694" to be "running"
    Mar 29 09:02:07.840: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26924ms
    Mar 29 09:02:09.841: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6": Phase="Running", Reason="", readiness=true. Elapsed: 2.002948781s
    Mar 29 09:02:09.841: INFO: Pod "dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:02:09.841
    STEP: looking for the results for each expected name from probers 03/29/23 09:02:09.843
    Mar 29 09:02:09.847: INFO: DNS probes using dns-test-478b32b5-0f89-4bc4-ab56-2a0f6b821ed6 succeeded

    STEP: deleting the pod 03/29/23 09:02:09.847
    STEP: changing the externalName to bar.example.com 03/29/23 09:02:09.853
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:09.856
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:09.857
    STEP: creating a second pod to probe DNS 03/29/23 09:02:09.857
    STEP: submitting the pod to kubernetes 03/29/23 09:02:09.857
    Mar 29 09:02:09.859: INFO: Waiting up to 15m0s for pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c" in namespace "dns-6694" to be "running"
    Mar 29 09:02:09.860: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238805ms
    Mar 29 09:02:11.863: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003950833s
    Mar 29 09:02:11.863: INFO: Pod "dns-test-66cb871b-eade-4035-b00d-f369f19bc41c" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:02:11.863
    STEP: looking for the results for each expected name from probers 03/29/23 09:02:11.864
    Mar 29 09:02:11.866: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 29 09:02:11.869: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 29 09:02:11.869: INFO: Lookups using dns-6694/dns-test-66cb871b-eade-4035-b00d-f369f19bc41c failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

    Mar 29 09:02:16.873: INFO: DNS probes using dns-test-66cb871b-eade-4035-b00d-f369f19bc41c succeeded

    STEP: deleting the pod 03/29/23 09:02:16.873
    STEP: changing the service to type=ClusterIP 03/29/23 09:02:16.88
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:16.886
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6694.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local; sleep 1; done
     03/29/23 09:02:16.886
    STEP: creating a third pod to probe DNS 03/29/23 09:02:16.886
    STEP: submitting the pod to kubernetes 03/29/23 09:02:16.888
    Mar 29 09:02:16.891: INFO: Waiting up to 15m0s for pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229" in namespace "dns-6694" to be "running"
    Mar 29 09:02:16.893: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229": Phase="Pending", Reason="", readiness=false. Elapsed: 1.284613ms
    Mar 29 09:02:18.895: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229": Phase="Running", Reason="", readiness=true. Elapsed: 2.003523794s
    Mar 29 09:02:18.895: INFO: Pod "dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:02:18.895
    STEP: looking for the results for each expected name from probers 03/29/23 09:02:18.896
    Mar 29 09:02:18.898: INFO: File wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 contains 'bar.example.com.
    ' instead of '10.100.62.149'
    Mar 29 09:02:18.901: INFO: File jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local from pod  dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 contains 'bar.example.com.
    ' instead of '10.100.62.149'
    Mar 29 09:02:18.901: INFO: Lookups using dns-6694/dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 failed for: [wheezy_udp@dns-test-service-3.dns-6694.svc.cluster.local jessie_udp@dns-test-service-3.dns-6694.svc.cluster.local]

    Mar 29 09:02:23.905: INFO: DNS probes using dns-test-5c7f839f-38c7-4c82-8a5f-74ce2c912229 succeeded

    STEP: deleting the pod 03/29/23 09:02:23.905
    STEP: deleting the test externalName service 03/29/23 09:02:23.912
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 09:02:23.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6694" for this suite. 03/29/23 09:02:23.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:23.924
Mar 29 09:02:23.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubelet-test 03/29/23 09:02:23.924
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:23.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:23.932
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 29 09:02:23.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b" in namespace "kubelet-test-6966" to be "running and ready"
Mar 29 09:02:23.938: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29479ms
Mar 29 09:02:23.938: INFO: The phase of Pod busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:02:25.940: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003655343s
Mar 29 09:02:25.940: INFO: The phase of Pod busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b is Running (Ready = true)
Mar 29 09:02:25.940: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 29 09:02:25.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6966" for this suite. 03/29/23 09:02:25.946
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":303,"skipped":5633,"failed":0}
------------------------------
â€¢ [2.025 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:23.924
    Mar 29 09:02:23.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubelet-test 03/29/23 09:02:23.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:23.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:23.932
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 29 09:02:23.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b" in namespace "kubelet-test-6966" to be "running and ready"
    Mar 29 09:02:23.938: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29479ms
    Mar 29 09:02:23.938: INFO: The phase of Pod busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:02:25.940: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003655343s
    Mar 29 09:02:25.940: INFO: The phase of Pod busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b is Running (Ready = true)
    Mar 29 09:02:25.940: INFO: Pod "busybox-scheduling-65bb4dac-b525-49b4-80f5-794eb1959b5b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 29 09:02:25.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6966" for this suite. 03/29/23 09:02:25.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:25.949
Mar 29 09:02:25.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename daemonsets 03/29/23 09:02:25.949
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:25.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:25.958
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar 29 09:02:25.968: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/29/23 09:02:25.972
Mar 29 09:02:25.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:25.974: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/29/23 09:02:25.974
Mar 29 09:02:25.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:25.986: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 09:02:26.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 09:02:26.988: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/29/23 09:02:26.99
Mar 29 09:02:26.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 09:02:26.998: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 29 09:02:28.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:28.001: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/29/23 09:02:28.001
Mar 29 09:02:28.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:28.006: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 09:02:29.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:29.008: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 09:02:30.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:30.009: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
Mar 29 09:02:31.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 29 09:02:31.008: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/29/23 09:02:31.01
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6171, will wait for the garbage collector to delete the pods 03/29/23 09:02:31.01
Mar 29 09:02:31.064: INFO: Deleting DaemonSet.extensions daemon-set took: 2.352193ms
Mar 29 09:02:31.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.872727ms
Mar 29 09:02:33.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 29 09:02:33.667: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 29 09:02:33.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28562"},"items":null}

Mar 29 09:02:33.669: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28562"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 29 09:02:33.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6171" for this suite. 03/29/23 09:02:33.68
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":304,"skipped":5639,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.734 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:25.949
    Mar 29 09:02:25.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename daemonsets 03/29/23 09:02:25.949
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:25.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:25.958
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar 29 09:02:25.968: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/29/23 09:02:25.972
    Mar 29 09:02:25.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:25.974: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/29/23 09:02:25.974
    Mar 29 09:02:25.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:25.986: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 09:02:26.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 09:02:26.988: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/29/23 09:02:26.99
    Mar 29 09:02:26.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 09:02:26.998: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar 29 09:02:28.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:28.001: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/29/23 09:02:28.001
    Mar 29 09:02:28.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:28.006: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 09:02:29.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:29.008: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 09:02:30.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:30.009: INFO: Node 10.146.0.117 is running 0 daemon pod, expected 1
    Mar 29 09:02:31.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 29 09:02:31.008: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/29/23 09:02:31.01
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6171, will wait for the garbage collector to delete the pods 03/29/23 09:02:31.01
    Mar 29 09:02:31.064: INFO: Deleting DaemonSet.extensions daemon-set took: 2.352193ms
    Mar 29 09:02:31.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.872727ms
    Mar 29 09:02:33.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 29 09:02:33.667: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 29 09:02:33.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28562"},"items":null}

    Mar 29 09:02:33.669: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28562"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 29 09:02:33.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6171" for this suite. 03/29/23 09:02:33.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:33.683
Mar 29 09:02:33.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename svc-latency 03/29/23 09:02:33.684
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:33.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:33.69
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 29 09:02:33.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5992 03/29/23 09:02:33.692
I0329 09:02:33.695042      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5992, replica count: 1
I0329 09:02:34.746610      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 09:02:34.853: INFO: Created: latency-svc-xx48s
Mar 29 09:02:34.856: INFO: Got endpoints: latency-svc-xx48s [9.176918ms]
Mar 29 09:02:34.862: INFO: Created: latency-svc-v2lcn
Mar 29 09:02:34.865: INFO: Got endpoints: latency-svc-v2lcn [8.365045ms]
Mar 29 09:02:34.867: INFO: Created: latency-svc-ctp56
Mar 29 09:02:34.868: INFO: Got endpoints: latency-svc-ctp56 [11.581221ms]
Mar 29 09:02:34.870: INFO: Created: latency-svc-q8xl2
Mar 29 09:02:34.872: INFO: Got endpoints: latency-svc-q8xl2 [15.241532ms]
Mar 29 09:02:34.873: INFO: Created: latency-svc-nm582
Mar 29 09:02:34.876: INFO: Got endpoints: latency-svc-nm582 [19.317123ms]
Mar 29 09:02:34.877: INFO: Created: latency-svc-27ksq
Mar 29 09:02:34.881: INFO: Created: latency-svc-52vwf
Mar 29 09:02:34.882: INFO: Got endpoints: latency-svc-27ksq [25.310094ms]
Mar 29 09:02:34.883: INFO: Got endpoints: latency-svc-52vwf [26.459049ms]
Mar 29 09:02:34.885: INFO: Created: latency-svc-bwqf2
Mar 29 09:02:34.887: INFO: Got endpoints: latency-svc-bwqf2 [30.096917ms]
Mar 29 09:02:34.887: INFO: Created: latency-svc-5z6gn
Mar 29 09:02:34.890: INFO: Got endpoints: latency-svc-5z6gn [33.956577ms]
Mar 29 09:02:34.892: INFO: Created: latency-svc-tg7nf
Mar 29 09:02:34.896: INFO: Got endpoints: latency-svc-tg7nf [39.019305ms]
Mar 29 09:02:34.896: INFO: Created: latency-svc-jg4hn
Mar 29 09:02:34.899: INFO: Got endpoints: latency-svc-jg4hn [42.36403ms]
Mar 29 09:02:34.902: INFO: Created: latency-svc-l7dv8
Mar 29 09:02:34.903: INFO: Created: latency-svc-vrmvf
Mar 29 09:02:34.907: INFO: Got endpoints: latency-svc-l7dv8 [50.797622ms]
Mar 29 09:02:34.907: INFO: Got endpoints: latency-svc-vrmvf [50.927313ms]
Mar 29 09:02:34.908: INFO: Created: latency-svc-4fn6m
Mar 29 09:02:34.911: INFO: Got endpoints: latency-svc-4fn6m [54.788429ms]
Mar 29 09:02:34.912: INFO: Created: latency-svc-c5tt5
Mar 29 09:02:34.914: INFO: Got endpoints: latency-svc-c5tt5 [57.231036ms]
Mar 29 09:02:34.916: INFO: Created: latency-svc-2s6z2
Mar 29 09:02:34.918: INFO: Created: latency-svc-q25g7
Mar 29 09:02:34.918: INFO: Got endpoints: latency-svc-2s6z2 [62.003372ms]
Mar 29 09:02:34.921: INFO: Got endpoints: latency-svc-q25g7 [56.136877ms]
Mar 29 09:02:34.923: INFO: Created: latency-svc-9w6q4
Mar 29 09:02:34.926: INFO: Got endpoints: latency-svc-9w6q4 [57.616006ms]
Mar 29 09:02:34.926: INFO: Created: latency-svc-m854r
Mar 29 09:02:34.928: INFO: Got endpoints: latency-svc-m854r [56.402306ms]
Mar 29 09:02:34.930: INFO: Created: latency-svc-2nz4l
Mar 29 09:02:34.932: INFO: Got endpoints: latency-svc-2nz4l [56.021596ms]
Mar 29 09:02:34.933: INFO: Created: latency-svc-g9dll
Mar 29 09:02:34.936: INFO: Got endpoints: latency-svc-g9dll [54.226977ms]
Mar 29 09:02:34.937: INFO: Created: latency-svc-rnpk9
Mar 29 09:02:34.940: INFO: Got endpoints: latency-svc-rnpk9 [57.118128ms]
Mar 29 09:02:34.941: INFO: Created: latency-svc-dq8ng
Mar 29 09:02:34.943: INFO: Got endpoints: latency-svc-dq8ng [56.26375ms]
Mar 29 09:02:34.944: INFO: Created: latency-svc-6xdr8
Mar 29 09:02:34.946: INFO: Created: latency-svc-ndnsd
Mar 29 09:02:34.947: INFO: Got endpoints: latency-svc-6xdr8 [56.414649ms]
Mar 29 09:02:34.949: INFO: Got endpoints: latency-svc-ndnsd [53.66359ms]
Mar 29 09:02:34.950: INFO: Created: latency-svc-w6pzr
Mar 29 09:02:34.953: INFO: Got endpoints: latency-svc-w6pzr [53.585998ms]
Mar 29 09:02:34.955: INFO: Created: latency-svc-nb2sj
Mar 29 09:02:34.956: INFO: Got endpoints: latency-svc-nb2sj [48.890436ms]
Mar 29 09:02:34.958: INFO: Created: latency-svc-dbdd4
Mar 29 09:02:34.960: INFO: Created: latency-svc-prmf9
Mar 29 09:02:34.961: INFO: Got endpoints: latency-svc-dbdd4 [53.324562ms]
Mar 29 09:02:34.964: INFO: Got endpoints: latency-svc-prmf9 [52.719033ms]
Mar 29 09:02:34.964: INFO: Created: latency-svc-c6czr
Mar 29 09:02:34.968: INFO: Got endpoints: latency-svc-c6czr [54.066443ms]
Mar 29 09:02:34.969: INFO: Created: latency-svc-mxk5z
Mar 29 09:02:34.970: INFO: Got endpoints: latency-svc-mxk5z [51.774965ms]
Mar 29 09:02:34.972: INFO: Created: latency-svc-qjcm6
Mar 29 09:02:34.974: INFO: Created: latency-svc-q7tkt
Mar 29 09:02:34.975: INFO: Got endpoints: latency-svc-qjcm6 [54.039799ms]
Mar 29 09:02:34.977: INFO: Created: latency-svc-4bjcz
Mar 29 09:02:34.980: INFO: Created: latency-svc-qf97w
Mar 29 09:02:34.982: INFO: Created: latency-svc-xckf7
Mar 29 09:02:34.986: INFO: Created: latency-svc-9jdlg
Mar 29 09:02:34.990: INFO: Created: latency-svc-5qbt8
Mar 29 09:02:34.995: INFO: Created: latency-svc-85rtf
Mar 29 09:02:34.998: INFO: Created: latency-svc-r7hxf
Mar 29 09:02:35.001: INFO: Created: latency-svc-7htxj
Mar 29 09:02:35.006: INFO: Created: latency-svc-fbx85
Mar 29 09:02:35.008: INFO: Got endpoints: latency-svc-q7tkt [82.006516ms]
Mar 29 09:02:35.009: INFO: Created: latency-svc-d6xd7
Mar 29 09:02:35.012: INFO: Created: latency-svc-h27rj
Mar 29 09:02:35.014: INFO: Created: latency-svc-h92tf
Mar 29 09:02:35.016: INFO: Created: latency-svc-d56hk
Mar 29 09:02:35.019: INFO: Created: latency-svc-qk6dd
Mar 29 09:02:35.022: INFO: Created: latency-svc-wgswv
Mar 29 09:02:35.056: INFO: Got endpoints: latency-svc-4bjcz [127.661677ms]
Mar 29 09:02:35.062: INFO: Created: latency-svc-8564s
Mar 29 09:02:35.106: INFO: Got endpoints: latency-svc-qf97w [174.569974ms]
Mar 29 09:02:35.111: INFO: Created: latency-svc-pjk2n
Mar 29 09:02:35.158: INFO: Got endpoints: latency-svc-xckf7 [221.361352ms]
Mar 29 09:02:35.163: INFO: Created: latency-svc-mvhjw
Mar 29 09:02:35.208: INFO: Got endpoints: latency-svc-9jdlg [267.669102ms]
Mar 29 09:02:35.214: INFO: Created: latency-svc-xwk7v
Mar 29 09:02:35.257: INFO: Got endpoints: latency-svc-5qbt8 [313.963076ms]
Mar 29 09:02:35.263: INFO: Created: latency-svc-5zqb4
Mar 29 09:02:35.306: INFO: Got endpoints: latency-svc-85rtf [359.393991ms]
Mar 29 09:02:35.312: INFO: Created: latency-svc-gtgxq
Mar 29 09:02:35.356: INFO: Got endpoints: latency-svc-r7hxf [406.996269ms]
Mar 29 09:02:35.361: INFO: Created: latency-svc-8dgdl
Mar 29 09:02:35.407: INFO: Got endpoints: latency-svc-7htxj [454.081318ms]
Mar 29 09:02:35.413: INFO: Created: latency-svc-wk299
Mar 29 09:02:35.456: INFO: Got endpoints: latency-svc-fbx85 [499.620818ms]
Mar 29 09:02:35.461: INFO: Created: latency-svc-cl764
Mar 29 09:02:35.506: INFO: Got endpoints: latency-svc-d6xd7 [545.5194ms]
Mar 29 09:02:35.512: INFO: Created: latency-svc-qdvh4
Mar 29 09:02:35.556: INFO: Got endpoints: latency-svc-h27rj [591.89442ms]
Mar 29 09:02:35.562: INFO: Created: latency-svc-vdrlt
Mar 29 09:02:35.608: INFO: Got endpoints: latency-svc-h92tf [639.753794ms]
Mar 29 09:02:35.616: INFO: Created: latency-svc-zv5hl
Mar 29 09:02:35.657: INFO: Got endpoints: latency-svc-d56hk [686.321583ms]
Mar 29 09:02:35.662: INFO: Created: latency-svc-cl9fd
Mar 29 09:02:35.707: INFO: Got endpoints: latency-svc-qk6dd [732.002877ms]
Mar 29 09:02:35.712: INFO: Created: latency-svc-wfkqx
Mar 29 09:02:35.758: INFO: Got endpoints: latency-svc-wgswv [749.769005ms]
Mar 29 09:02:35.763: INFO: Created: latency-svc-6qb9x
Mar 29 09:02:35.806: INFO: Got endpoints: latency-svc-8564s [750.483649ms]
Mar 29 09:02:35.813: INFO: Created: latency-svc-fl8d9
Mar 29 09:02:35.856: INFO: Got endpoints: latency-svc-pjk2n [749.73586ms]
Mar 29 09:02:35.862: INFO: Created: latency-svc-5wddv
Mar 29 09:02:35.908: INFO: Got endpoints: latency-svc-mvhjw [750.189368ms]
Mar 29 09:02:35.913: INFO: Created: latency-svc-b8sfs
Mar 29 09:02:35.956: INFO: Got endpoints: latency-svc-xwk7v [747.992203ms]
Mar 29 09:02:35.961: INFO: Created: latency-svc-vcp9v
Mar 29 09:02:36.007: INFO: Got endpoints: latency-svc-5zqb4 [750.447572ms]
Mar 29 09:02:36.014: INFO: Created: latency-svc-95z9d
Mar 29 09:02:36.056: INFO: Got endpoints: latency-svc-gtgxq [749.296426ms]
Mar 29 09:02:36.062: INFO: Created: latency-svc-tjbgm
Mar 29 09:02:36.106: INFO: Got endpoints: latency-svc-8dgdl [749.685498ms]
Mar 29 09:02:36.111: INFO: Created: latency-svc-jddsj
Mar 29 09:02:36.156: INFO: Got endpoints: latency-svc-wk299 [749.70371ms]
Mar 29 09:02:36.162: INFO: Created: latency-svc-jrbn2
Mar 29 09:02:36.208: INFO: Got endpoints: latency-svc-cl764 [752.019463ms]
Mar 29 09:02:36.213: INFO: Created: latency-svc-xqshd
Mar 29 09:02:36.256: INFO: Got endpoints: latency-svc-qdvh4 [749.954308ms]
Mar 29 09:02:36.262: INFO: Created: latency-svc-cz4td
Mar 29 09:02:36.307: INFO: Got endpoints: latency-svc-vdrlt [750.464823ms]
Mar 29 09:02:36.315: INFO: Created: latency-svc-s2bxz
Mar 29 09:02:36.358: INFO: Got endpoints: latency-svc-zv5hl [749.994498ms]
Mar 29 09:02:36.363: INFO: Created: latency-svc-sh2zl
Mar 29 09:02:36.406: INFO: Got endpoints: latency-svc-cl9fd [749.219371ms]
Mar 29 09:02:36.411: INFO: Created: latency-svc-smksn
Mar 29 09:02:36.456: INFO: Got endpoints: latency-svc-wfkqx [748.542743ms]
Mar 29 09:02:36.461: INFO: Created: latency-svc-nb9vh
Mar 29 09:02:36.507: INFO: Got endpoints: latency-svc-6qb9x [749.635399ms]
Mar 29 09:02:36.513: INFO: Created: latency-svc-9z5pc
Mar 29 09:02:36.557: INFO: Got endpoints: latency-svc-fl8d9 [750.134242ms]
Mar 29 09:02:36.562: INFO: Created: latency-svc-lkt2k
Mar 29 09:02:36.607: INFO: Got endpoints: latency-svc-5wddv [750.402136ms]
Mar 29 09:02:36.612: INFO: Created: latency-svc-wz2xm
Mar 29 09:02:36.657: INFO: Got endpoints: latency-svc-b8sfs [748.766678ms]
Mar 29 09:02:36.662: INFO: Created: latency-svc-zqnhb
Mar 29 09:02:36.706: INFO: Got endpoints: latency-svc-vcp9v [750.515466ms]
Mar 29 09:02:36.713: INFO: Created: latency-svc-9n4h4
Mar 29 09:02:36.757: INFO: Got endpoints: latency-svc-95z9d [749.039757ms]
Mar 29 09:02:36.763: INFO: Created: latency-svc-9sq9p
Mar 29 09:02:36.807: INFO: Got endpoints: latency-svc-tjbgm [750.871427ms]
Mar 29 09:02:36.812: INFO: Created: latency-svc-8d9tx
Mar 29 09:02:36.858: INFO: Got endpoints: latency-svc-jddsj [751.720516ms]
Mar 29 09:02:36.863: INFO: Created: latency-svc-d8wkm
Mar 29 09:02:36.907: INFO: Got endpoints: latency-svc-jrbn2 [750.694879ms]
Mar 29 09:02:36.913: INFO: Created: latency-svc-qt9jv
Mar 29 09:02:36.957: INFO: Got endpoints: latency-svc-xqshd [748.56714ms]
Mar 29 09:02:36.963: INFO: Created: latency-svc-66g7p
Mar 29 09:02:37.006: INFO: Got endpoints: latency-svc-cz4td [749.662683ms]
Mar 29 09:02:37.011: INFO: Created: latency-svc-d9fdj
Mar 29 09:02:37.058: INFO: Got endpoints: latency-svc-s2bxz [751.00105ms]
Mar 29 09:02:37.063: INFO: Created: latency-svc-49lt5
Mar 29 09:02:37.108: INFO: Got endpoints: latency-svc-sh2zl [749.917308ms]
Mar 29 09:02:37.113: INFO: Created: latency-svc-fmh6d
Mar 29 09:02:37.156: INFO: Got endpoints: latency-svc-smksn [750.284989ms]
Mar 29 09:02:37.163: INFO: Created: latency-svc-tq7vg
Mar 29 09:02:37.206: INFO: Got endpoints: latency-svc-nb9vh [750.755865ms]
Mar 29 09:02:37.212: INFO: Created: latency-svc-hwqfq
Mar 29 09:02:37.257: INFO: Got endpoints: latency-svc-9z5pc [750.014731ms]
Mar 29 09:02:37.262: INFO: Created: latency-svc-5pxng
Mar 29 09:02:37.308: INFO: Got endpoints: latency-svc-lkt2k [751.364409ms]
Mar 29 09:02:37.313: INFO: Created: latency-svc-bbmtl
Mar 29 09:02:37.357: INFO: Got endpoints: latency-svc-wz2xm [750.204852ms]
Mar 29 09:02:37.363: INFO: Created: latency-svc-h8nw4
Mar 29 09:02:37.407: INFO: Got endpoints: latency-svc-zqnhb [750.885018ms]
Mar 29 09:02:37.413: INFO: Created: latency-svc-s6gs5
Mar 29 09:02:37.457: INFO: Got endpoints: latency-svc-9n4h4 [750.350735ms]
Mar 29 09:02:37.462: INFO: Created: latency-svc-55cgw
Mar 29 09:02:37.507: INFO: Got endpoints: latency-svc-9sq9p [750.495065ms]
Mar 29 09:02:37.513: INFO: Created: latency-svc-6tk5n
Mar 29 09:02:37.556: INFO: Got endpoints: latency-svc-8d9tx [749.697954ms]
Mar 29 09:02:37.563: INFO: Created: latency-svc-ncvz8
Mar 29 09:02:37.607: INFO: Got endpoints: latency-svc-d8wkm [749.181454ms]
Mar 29 09:02:37.612: INFO: Created: latency-svc-57czl
Mar 29 09:02:37.657: INFO: Got endpoints: latency-svc-qt9jv [749.829549ms]
Mar 29 09:02:37.662: INFO: Created: latency-svc-nv95j
Mar 29 09:02:37.707: INFO: Got endpoints: latency-svc-66g7p [750.681954ms]
Mar 29 09:02:37.712: INFO: Created: latency-svc-lrz4g
Mar 29 09:02:37.756: INFO: Got endpoints: latency-svc-d9fdj [750.438794ms]
Mar 29 09:02:37.762: INFO: Created: latency-svc-nbkcj
Mar 29 09:02:37.807: INFO: Got endpoints: latency-svc-49lt5 [749.011459ms]
Mar 29 09:02:37.812: INFO: Created: latency-svc-s2tpm
Mar 29 09:02:37.856: INFO: Got endpoints: latency-svc-fmh6d [748.736246ms]
Mar 29 09:02:37.862: INFO: Created: latency-svc-hrf5p
Mar 29 09:02:37.907: INFO: Got endpoints: latency-svc-tq7vg [750.519052ms]
Mar 29 09:02:37.912: INFO: Created: latency-svc-fk85g
Mar 29 09:02:37.957: INFO: Got endpoints: latency-svc-hwqfq [750.290183ms]
Mar 29 09:02:37.962: INFO: Created: latency-svc-7rn98
Mar 29 09:02:38.007: INFO: Got endpoints: latency-svc-5pxng [749.461807ms]
Mar 29 09:02:38.012: INFO: Created: latency-svc-vthtj
Mar 29 09:02:38.058: INFO: Got endpoints: latency-svc-bbmtl [749.823352ms]
Mar 29 09:02:38.063: INFO: Created: latency-svc-h5lqv
Mar 29 09:02:38.107: INFO: Got endpoints: latency-svc-h8nw4 [750.121871ms]
Mar 29 09:02:38.112: INFO: Created: latency-svc-bxhfg
Mar 29 09:02:38.157: INFO: Got endpoints: latency-svc-s6gs5 [749.77627ms]
Mar 29 09:02:38.163: INFO: Created: latency-svc-rn64s
Mar 29 09:02:38.207: INFO: Got endpoints: latency-svc-55cgw [749.980658ms]
Mar 29 09:02:38.213: INFO: Created: latency-svc-svz88
Mar 29 09:02:38.256: INFO: Got endpoints: latency-svc-6tk5n [749.392944ms]
Mar 29 09:02:38.262: INFO: Created: latency-svc-l9t2p
Mar 29 09:02:38.307: INFO: Got endpoints: latency-svc-ncvz8 [751.112227ms]
Mar 29 09:02:38.313: INFO: Created: latency-svc-gn7pf
Mar 29 09:02:38.357: INFO: Got endpoints: latency-svc-57czl [750.014389ms]
Mar 29 09:02:38.363: INFO: Created: latency-svc-2f7d9
Mar 29 09:02:38.406: INFO: Got endpoints: latency-svc-nv95j [749.380999ms]
Mar 29 09:02:38.413: INFO: Created: latency-svc-kfbnz
Mar 29 09:02:38.456: INFO: Got endpoints: latency-svc-lrz4g [749.175582ms]
Mar 29 09:02:38.462: INFO: Created: latency-svc-cl7zz
Mar 29 09:02:38.507: INFO: Got endpoints: latency-svc-nbkcj [750.449184ms]
Mar 29 09:02:38.512: INFO: Created: latency-svc-k4zns
Mar 29 09:02:38.556: INFO: Got endpoints: latency-svc-s2tpm [749.253105ms]
Mar 29 09:02:38.561: INFO: Created: latency-svc-dz2m2
Mar 29 09:02:38.606: INFO: Got endpoints: latency-svc-hrf5p [749.993847ms]
Mar 29 09:02:38.613: INFO: Created: latency-svc-ftvg5
Mar 29 09:02:38.656: INFO: Got endpoints: latency-svc-fk85g [749.282536ms]
Mar 29 09:02:38.662: INFO: Created: latency-svc-clw7h
Mar 29 09:02:38.707: INFO: Got endpoints: latency-svc-7rn98 [750.007667ms]
Mar 29 09:02:38.712: INFO: Created: latency-svc-rtxsw
Mar 29 09:02:38.757: INFO: Got endpoints: latency-svc-vthtj [749.846009ms]
Mar 29 09:02:38.762: INFO: Created: latency-svc-nqhs8
Mar 29 09:02:38.805: INFO: Got endpoints: latency-svc-h5lqv [747.338465ms]
Mar 29 09:02:38.811: INFO: Created: latency-svc-nzfhc
Mar 29 09:02:38.856: INFO: Got endpoints: latency-svc-bxhfg [748.739428ms]
Mar 29 09:02:38.862: INFO: Created: latency-svc-ddp8l
Mar 29 09:02:38.906: INFO: Got endpoints: latency-svc-rn64s [748.905191ms]
Mar 29 09:02:38.912: INFO: Created: latency-svc-pdzw5
Mar 29 09:02:38.956: INFO: Got endpoints: latency-svc-svz88 [749.508793ms]
Mar 29 09:02:38.962: INFO: Created: latency-svc-dmc4m
Mar 29 09:02:39.008: INFO: Got endpoints: latency-svc-l9t2p [751.086305ms]
Mar 29 09:02:39.013: INFO: Created: latency-svc-8rnn5
Mar 29 09:02:39.056: INFO: Got endpoints: latency-svc-gn7pf [748.617984ms]
Mar 29 09:02:39.062: INFO: Created: latency-svc-nz88d
Mar 29 09:02:39.106: INFO: Got endpoints: latency-svc-2f7d9 [748.660253ms]
Mar 29 09:02:39.111: INFO: Created: latency-svc-tmj8d
Mar 29 09:02:39.156: INFO: Got endpoints: latency-svc-kfbnz [749.262933ms]
Mar 29 09:02:39.161: INFO: Created: latency-svc-thcg2
Mar 29 09:02:39.206: INFO: Got endpoints: latency-svc-cl7zz [749.41376ms]
Mar 29 09:02:39.211: INFO: Created: latency-svc-ppn5s
Mar 29 09:02:39.257: INFO: Got endpoints: latency-svc-k4zns [749.65844ms]
Mar 29 09:02:39.263: INFO: Created: latency-svc-48cgq
Mar 29 09:02:39.306: INFO: Got endpoints: latency-svc-dz2m2 [750.358646ms]
Mar 29 09:02:39.312: INFO: Created: latency-svc-jvqsz
Mar 29 09:02:39.359: INFO: Got endpoints: latency-svc-ftvg5 [752.173071ms]
Mar 29 09:02:39.364: INFO: Created: latency-svc-kdthn
Mar 29 09:02:39.409: INFO: Got endpoints: latency-svc-clw7h [753.361409ms]
Mar 29 09:02:39.416: INFO: Created: latency-svc-ppkkp
Mar 29 09:02:39.458: INFO: Got endpoints: latency-svc-rtxsw [750.715226ms]
Mar 29 09:02:39.466: INFO: Created: latency-svc-xgxss
Mar 29 09:02:39.507: INFO: Got endpoints: latency-svc-nqhs8 [750.225198ms]
Mar 29 09:02:39.513: INFO: Created: latency-svc-wbjbc
Mar 29 09:02:39.559: INFO: Got endpoints: latency-svc-nzfhc [754.116614ms]
Mar 29 09:02:39.567: INFO: Created: latency-svc-dchql
Mar 29 09:02:39.607: INFO: Got endpoints: latency-svc-ddp8l [751.315938ms]
Mar 29 09:02:39.613: INFO: Created: latency-svc-9skfw
Mar 29 09:02:39.656: INFO: Got endpoints: latency-svc-pdzw5 [749.974416ms]
Mar 29 09:02:39.664: INFO: Created: latency-svc-kjqkl
Mar 29 09:02:39.708: INFO: Got endpoints: latency-svc-dmc4m [751.416278ms]
Mar 29 09:02:39.714: INFO: Created: latency-svc-gw7q7
Mar 29 09:02:39.756: INFO: Got endpoints: latency-svc-8rnn5 [748.799799ms]
Mar 29 09:02:39.762: INFO: Created: latency-svc-hw4qz
Mar 29 09:02:39.808: INFO: Got endpoints: latency-svc-nz88d [751.989391ms]
Mar 29 09:02:39.813: INFO: Created: latency-svc-7jx9x
Mar 29 09:02:39.857: INFO: Got endpoints: latency-svc-tmj8d [750.937483ms]
Mar 29 09:02:39.863: INFO: Created: latency-svc-f69vq
Mar 29 09:02:39.906: INFO: Got endpoints: latency-svc-thcg2 [750.680481ms]
Mar 29 09:02:39.912: INFO: Created: latency-svc-ntk2r
Mar 29 09:02:39.958: INFO: Got endpoints: latency-svc-ppn5s [751.62582ms]
Mar 29 09:02:39.963: INFO: Created: latency-svc-7nfk6
Mar 29 09:02:40.007: INFO: Got endpoints: latency-svc-48cgq [750.053239ms]
Mar 29 09:02:40.012: INFO: Created: latency-svc-xf4f8
Mar 29 09:02:40.056: INFO: Got endpoints: latency-svc-jvqsz [749.990174ms]
Mar 29 09:02:40.062: INFO: Created: latency-svc-9f4v2
Mar 29 09:02:40.106: INFO: Got endpoints: latency-svc-kdthn [747.860822ms]
Mar 29 09:02:40.112: INFO: Created: latency-svc-m5wk7
Mar 29 09:02:40.157: INFO: Got endpoints: latency-svc-ppkkp [748.018667ms]
Mar 29 09:02:40.163: INFO: Created: latency-svc-8skzt
Mar 29 09:02:40.206: INFO: Got endpoints: latency-svc-xgxss [748.64821ms]
Mar 29 09:02:40.211: INFO: Created: latency-svc-qw2wx
Mar 29 09:02:40.256: INFO: Got endpoints: latency-svc-wbjbc [749.277528ms]
Mar 29 09:02:40.262: INFO: Created: latency-svc-72vxx
Mar 29 09:02:40.315: INFO: Got endpoints: latency-svc-dchql [755.224473ms]
Mar 29 09:02:40.322: INFO: Created: latency-svc-fddkh
Mar 29 09:02:40.357: INFO: Got endpoints: latency-svc-9skfw [750.318517ms]
Mar 29 09:02:40.363: INFO: Created: latency-svc-ldbqt
Mar 29 09:02:40.407: INFO: Got endpoints: latency-svc-kjqkl [751.164421ms]
Mar 29 09:02:40.412: INFO: Created: latency-svc-m5v97
Mar 29 09:02:40.457: INFO: Got endpoints: latency-svc-gw7q7 [748.873598ms]
Mar 29 09:02:40.463: INFO: Created: latency-svc-n2b8t
Mar 29 09:02:40.506: INFO: Got endpoints: latency-svc-hw4qz [749.619925ms]
Mar 29 09:02:40.511: INFO: Created: latency-svc-8zb5l
Mar 29 09:02:40.557: INFO: Got endpoints: latency-svc-7jx9x [749.240366ms]
Mar 29 09:02:40.563: INFO: Created: latency-svc-md5xd
Mar 29 09:02:40.606: INFO: Got endpoints: latency-svc-f69vq [749.15418ms]
Mar 29 09:02:40.611: INFO: Created: latency-svc-6npz8
Mar 29 09:02:40.656: INFO: Got endpoints: latency-svc-ntk2r [749.315666ms]
Mar 29 09:02:40.662: INFO: Created: latency-svc-mcjvk
Mar 29 09:02:40.706: INFO: Got endpoints: latency-svc-7nfk6 [748.320045ms]
Mar 29 09:02:40.711: INFO: Created: latency-svc-tvg9x
Mar 29 09:02:40.756: INFO: Got endpoints: latency-svc-xf4f8 [749.565702ms]
Mar 29 09:02:40.761: INFO: Created: latency-svc-jbwml
Mar 29 09:02:40.808: INFO: Got endpoints: latency-svc-9f4v2 [751.240572ms]
Mar 29 09:02:40.814: INFO: Created: latency-svc-6m7ld
Mar 29 09:02:40.858: INFO: Got endpoints: latency-svc-m5wk7 [752.00236ms]
Mar 29 09:02:40.864: INFO: Created: latency-svc-7vkmk
Mar 29 09:02:40.907: INFO: Got endpoints: latency-svc-8skzt [749.545325ms]
Mar 29 09:02:40.912: INFO: Created: latency-svc-s2bnf
Mar 29 09:02:40.958: INFO: Got endpoints: latency-svc-qw2wx [751.556767ms]
Mar 29 09:02:40.963: INFO: Created: latency-svc-4ts7l
Mar 29 09:02:41.008: INFO: Got endpoints: latency-svc-72vxx [751.88407ms]
Mar 29 09:02:41.013: INFO: Created: latency-svc-lp8fc
Mar 29 09:02:41.058: INFO: Got endpoints: latency-svc-fddkh [743.002402ms]
Mar 29 09:02:41.063: INFO: Created: latency-svc-ht4f4
Mar 29 09:02:41.109: INFO: Got endpoints: latency-svc-ldbqt [751.284722ms]
Mar 29 09:02:41.114: INFO: Created: latency-svc-tzksm
Mar 29 09:02:41.156: INFO: Got endpoints: latency-svc-m5v97 [748.674166ms]
Mar 29 09:02:41.163: INFO: Created: latency-svc-n87x4
Mar 29 09:02:41.207: INFO: Got endpoints: latency-svc-n2b8t [750.054083ms]
Mar 29 09:02:41.213: INFO: Created: latency-svc-cppzr
Mar 29 09:02:41.258: INFO: Got endpoints: latency-svc-8zb5l [752.020365ms]
Mar 29 09:02:41.263: INFO: Created: latency-svc-jvw9z
Mar 29 09:02:41.306: INFO: Got endpoints: latency-svc-md5xd [748.894382ms]
Mar 29 09:02:41.313: INFO: Created: latency-svc-qmc77
Mar 29 09:02:41.356: INFO: Got endpoints: latency-svc-6npz8 [750.49028ms]
Mar 29 09:02:41.363: INFO: Created: latency-svc-7shfw
Mar 29 09:02:41.406: INFO: Got endpoints: latency-svc-mcjvk [750.172799ms]
Mar 29 09:02:41.411: INFO: Created: latency-svc-n8nqk
Mar 29 09:02:41.457: INFO: Got endpoints: latency-svc-tvg9x [750.670893ms]
Mar 29 09:02:41.462: INFO: Created: latency-svc-4wgxv
Mar 29 09:02:41.507: INFO: Got endpoints: latency-svc-jbwml [750.422605ms]
Mar 29 09:02:41.512: INFO: Created: latency-svc-fmhw9
Mar 29 09:02:41.557: INFO: Got endpoints: latency-svc-6m7ld [748.988865ms]
Mar 29 09:02:41.563: INFO: Created: latency-svc-jfgmx
Mar 29 09:02:41.606: INFO: Got endpoints: latency-svc-7vkmk [747.955301ms]
Mar 29 09:02:41.612: INFO: Created: latency-svc-fckxs
Mar 29 09:02:41.656: INFO: Got endpoints: latency-svc-s2bnf [749.164213ms]
Mar 29 09:02:41.661: INFO: Created: latency-svc-vmrfj
Mar 29 09:02:41.707: INFO: Got endpoints: latency-svc-4ts7l [748.85848ms]
Mar 29 09:02:41.712: INFO: Created: latency-svc-fw85w
Mar 29 09:02:41.758: INFO: Got endpoints: latency-svc-lp8fc [749.418912ms]
Mar 29 09:02:41.763: INFO: Created: latency-svc-5qpsb
Mar 29 09:02:41.807: INFO: Got endpoints: latency-svc-ht4f4 [749.053943ms]
Mar 29 09:02:41.812: INFO: Created: latency-svc-c66l6
Mar 29 09:02:41.858: INFO: Got endpoints: latency-svc-tzksm [749.150253ms]
Mar 29 09:02:41.863: INFO: Created: latency-svc-whbdx
Mar 29 09:02:41.906: INFO: Got endpoints: latency-svc-n87x4 [750.11053ms]
Mar 29 09:02:41.912: INFO: Created: latency-svc-b94qx
Mar 29 09:02:41.957: INFO: Got endpoints: latency-svc-cppzr [750.35402ms]
Mar 29 09:02:41.963: INFO: Created: latency-svc-wlmfg
Mar 29 09:02:42.007: INFO: Got endpoints: latency-svc-jvw9z [749.012498ms]
Mar 29 09:02:42.012: INFO: Created: latency-svc-pfvxc
Mar 29 09:02:42.058: INFO: Got endpoints: latency-svc-qmc77 [752.012099ms]
Mar 29 09:02:42.064: INFO: Created: latency-svc-cpp29
Mar 29 09:02:42.108: INFO: Got endpoints: latency-svc-7shfw [751.846385ms]
Mar 29 09:02:42.113: INFO: Created: latency-svc-5gb4f
Mar 29 09:02:42.157: INFO: Got endpoints: latency-svc-n8nqk [750.624506ms]
Mar 29 09:02:42.163: INFO: Created: latency-svc-mfkg2
Mar 29 09:02:42.206: INFO: Got endpoints: latency-svc-4wgxv [749.716269ms]
Mar 29 09:02:42.212: INFO: Created: latency-svc-n4x9j
Mar 29 09:02:42.257: INFO: Got endpoints: latency-svc-fmhw9 [750.202417ms]
Mar 29 09:02:42.263: INFO: Created: latency-svc-tklg7
Mar 29 09:02:42.306: INFO: Got endpoints: latency-svc-jfgmx [749.863805ms]
Mar 29 09:02:42.312: INFO: Created: latency-svc-7prml
Mar 29 09:02:42.356: INFO: Got endpoints: latency-svc-fckxs [749.910587ms]
Mar 29 09:02:42.363: INFO: Created: latency-svc-v2lmq
Mar 29 09:02:42.406: INFO: Got endpoints: latency-svc-vmrfj [749.70446ms]
Mar 29 09:02:42.411: INFO: Created: latency-svc-6fmbt
Mar 29 09:02:42.457: INFO: Got endpoints: latency-svc-fw85w [750.821315ms]
Mar 29 09:02:42.463: INFO: Created: latency-svc-zpcdv
Mar 29 09:02:42.506: INFO: Got endpoints: latency-svc-5qpsb [748.034816ms]
Mar 29 09:02:42.511: INFO: Created: latency-svc-bdzqx
Mar 29 09:02:42.557: INFO: Got endpoints: latency-svc-c66l6 [749.776892ms]
Mar 29 09:02:42.563: INFO: Created: latency-svc-w4h5l
Mar 29 09:02:42.607: INFO: Got endpoints: latency-svc-whbdx [748.540088ms]
Mar 29 09:02:42.612: INFO: Created: latency-svc-plkdw
Mar 29 09:02:42.657: INFO: Got endpoints: latency-svc-b94qx [750.419079ms]
Mar 29 09:02:42.662: INFO: Created: latency-svc-xdhz4
Mar 29 09:02:42.706: INFO: Got endpoints: latency-svc-wlmfg [748.595887ms]
Mar 29 09:02:42.756: INFO: Got endpoints: latency-svc-pfvxc [749.142244ms]
Mar 29 09:02:42.807: INFO: Got endpoints: latency-svc-cpp29 [749.033574ms]
Mar 29 09:02:42.857: INFO: Got endpoints: latency-svc-5gb4f [748.487369ms]
Mar 29 09:02:42.907: INFO: Got endpoints: latency-svc-mfkg2 [750.562101ms]
Mar 29 09:02:42.956: INFO: Got endpoints: latency-svc-n4x9j [749.848506ms]
Mar 29 09:02:43.007: INFO: Got endpoints: latency-svc-tklg7 [749.73578ms]
Mar 29 09:02:43.057: INFO: Got endpoints: latency-svc-7prml [750.074381ms]
Mar 29 09:02:43.106: INFO: Got endpoints: latency-svc-v2lmq [749.958501ms]
Mar 29 09:02:43.156: INFO: Got endpoints: latency-svc-6fmbt [750.111082ms]
Mar 29 09:02:43.207: INFO: Got endpoints: latency-svc-zpcdv [749.22421ms]
Mar 29 09:02:43.258: INFO: Got endpoints: latency-svc-bdzqx [752.282375ms]
Mar 29 09:02:43.306: INFO: Got endpoints: latency-svc-w4h5l [749.580864ms]
Mar 29 09:02:43.358: INFO: Got endpoints: latency-svc-plkdw [751.300689ms]
Mar 29 09:02:43.407: INFO: Got endpoints: latency-svc-xdhz4 [750.405353ms]
Mar 29 09:02:43.407: INFO: Latencies: [8.365045ms 11.581221ms 15.241532ms 19.317123ms 25.310094ms 26.459049ms 30.096917ms 33.956577ms 39.019305ms 42.36403ms 48.890436ms 50.797622ms 50.927313ms 51.774965ms 52.719033ms 53.324562ms 53.585998ms 53.66359ms 54.039799ms 54.066443ms 54.226977ms 54.788429ms 56.021596ms 56.136877ms 56.26375ms 56.402306ms 56.414649ms 57.118128ms 57.231036ms 57.616006ms 62.003372ms 82.006516ms 127.661677ms 174.569974ms 221.361352ms 267.669102ms 313.963076ms 359.393991ms 406.996269ms 454.081318ms 499.620818ms 545.5194ms 591.89442ms 639.753794ms 686.321583ms 732.002877ms 743.002402ms 747.338465ms 747.860822ms 747.955301ms 747.992203ms 748.018667ms 748.034816ms 748.320045ms 748.487369ms 748.540088ms 748.542743ms 748.56714ms 748.595887ms 748.617984ms 748.64821ms 748.660253ms 748.674166ms 748.736246ms 748.739428ms 748.766678ms 748.799799ms 748.85848ms 748.873598ms 748.894382ms 748.905191ms 748.988865ms 749.011459ms 749.012498ms 749.033574ms 749.039757ms 749.053943ms 749.142244ms 749.150253ms 749.15418ms 749.164213ms 749.175582ms 749.181454ms 749.219371ms 749.22421ms 749.240366ms 749.253105ms 749.262933ms 749.277528ms 749.282536ms 749.296426ms 749.315666ms 749.380999ms 749.392944ms 749.41376ms 749.418912ms 749.461807ms 749.508793ms 749.545325ms 749.565702ms 749.580864ms 749.619925ms 749.635399ms 749.65844ms 749.662683ms 749.685498ms 749.697954ms 749.70371ms 749.70446ms 749.716269ms 749.73578ms 749.73586ms 749.769005ms 749.77627ms 749.776892ms 749.823352ms 749.829549ms 749.846009ms 749.848506ms 749.863805ms 749.910587ms 749.917308ms 749.954308ms 749.958501ms 749.974416ms 749.980658ms 749.990174ms 749.993847ms 749.994498ms 750.007667ms 750.014389ms 750.014731ms 750.053239ms 750.054083ms 750.074381ms 750.11053ms 750.111082ms 750.121871ms 750.134242ms 750.172799ms 750.189368ms 750.202417ms 750.204852ms 750.225198ms 750.284989ms 750.290183ms 750.318517ms 750.350735ms 750.35402ms 750.358646ms 750.402136ms 750.405353ms 750.419079ms 750.422605ms 750.438794ms 750.447572ms 750.449184ms 750.464823ms 750.483649ms 750.49028ms 750.495065ms 750.515466ms 750.519052ms 750.562101ms 750.624506ms 750.670893ms 750.680481ms 750.681954ms 750.694879ms 750.715226ms 750.755865ms 750.821315ms 750.871427ms 750.885018ms 750.937483ms 751.00105ms 751.086305ms 751.112227ms 751.164421ms 751.240572ms 751.284722ms 751.300689ms 751.315938ms 751.364409ms 751.416278ms 751.556767ms 751.62582ms 751.720516ms 751.846385ms 751.88407ms 751.989391ms 752.00236ms 752.012099ms 752.019463ms 752.020365ms 752.173071ms 752.282375ms 753.361409ms 754.116614ms 755.224473ms]
Mar 29 09:02:43.407: INFO: 50 %ile: 749.580864ms
Mar 29 09:02:43.407: INFO: 90 %ile: 751.284722ms
Mar 29 09:02:43.407: INFO: 99 %ile: 754.116614ms
Mar 29 09:02:43.407: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar 29 09:02:43.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5992" for this suite. 03/29/23 09:02:43.41
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":305,"skipped":5665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.729 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:33.683
    Mar 29 09:02:33.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename svc-latency 03/29/23 09:02:33.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:33.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:33.69
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 29 09:02:33.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5992 03/29/23 09:02:33.692
    I0329 09:02:33.695042      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5992, replica count: 1
    I0329 09:02:34.746610      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 09:02:34.853: INFO: Created: latency-svc-xx48s
    Mar 29 09:02:34.856: INFO: Got endpoints: latency-svc-xx48s [9.176918ms]
    Mar 29 09:02:34.862: INFO: Created: latency-svc-v2lcn
    Mar 29 09:02:34.865: INFO: Got endpoints: latency-svc-v2lcn [8.365045ms]
    Mar 29 09:02:34.867: INFO: Created: latency-svc-ctp56
    Mar 29 09:02:34.868: INFO: Got endpoints: latency-svc-ctp56 [11.581221ms]
    Mar 29 09:02:34.870: INFO: Created: latency-svc-q8xl2
    Mar 29 09:02:34.872: INFO: Got endpoints: latency-svc-q8xl2 [15.241532ms]
    Mar 29 09:02:34.873: INFO: Created: latency-svc-nm582
    Mar 29 09:02:34.876: INFO: Got endpoints: latency-svc-nm582 [19.317123ms]
    Mar 29 09:02:34.877: INFO: Created: latency-svc-27ksq
    Mar 29 09:02:34.881: INFO: Created: latency-svc-52vwf
    Mar 29 09:02:34.882: INFO: Got endpoints: latency-svc-27ksq [25.310094ms]
    Mar 29 09:02:34.883: INFO: Got endpoints: latency-svc-52vwf [26.459049ms]
    Mar 29 09:02:34.885: INFO: Created: latency-svc-bwqf2
    Mar 29 09:02:34.887: INFO: Got endpoints: latency-svc-bwqf2 [30.096917ms]
    Mar 29 09:02:34.887: INFO: Created: latency-svc-5z6gn
    Mar 29 09:02:34.890: INFO: Got endpoints: latency-svc-5z6gn [33.956577ms]
    Mar 29 09:02:34.892: INFO: Created: latency-svc-tg7nf
    Mar 29 09:02:34.896: INFO: Got endpoints: latency-svc-tg7nf [39.019305ms]
    Mar 29 09:02:34.896: INFO: Created: latency-svc-jg4hn
    Mar 29 09:02:34.899: INFO: Got endpoints: latency-svc-jg4hn [42.36403ms]
    Mar 29 09:02:34.902: INFO: Created: latency-svc-l7dv8
    Mar 29 09:02:34.903: INFO: Created: latency-svc-vrmvf
    Mar 29 09:02:34.907: INFO: Got endpoints: latency-svc-l7dv8 [50.797622ms]
    Mar 29 09:02:34.907: INFO: Got endpoints: latency-svc-vrmvf [50.927313ms]
    Mar 29 09:02:34.908: INFO: Created: latency-svc-4fn6m
    Mar 29 09:02:34.911: INFO: Got endpoints: latency-svc-4fn6m [54.788429ms]
    Mar 29 09:02:34.912: INFO: Created: latency-svc-c5tt5
    Mar 29 09:02:34.914: INFO: Got endpoints: latency-svc-c5tt5 [57.231036ms]
    Mar 29 09:02:34.916: INFO: Created: latency-svc-2s6z2
    Mar 29 09:02:34.918: INFO: Created: latency-svc-q25g7
    Mar 29 09:02:34.918: INFO: Got endpoints: latency-svc-2s6z2 [62.003372ms]
    Mar 29 09:02:34.921: INFO: Got endpoints: latency-svc-q25g7 [56.136877ms]
    Mar 29 09:02:34.923: INFO: Created: latency-svc-9w6q4
    Mar 29 09:02:34.926: INFO: Got endpoints: latency-svc-9w6q4 [57.616006ms]
    Mar 29 09:02:34.926: INFO: Created: latency-svc-m854r
    Mar 29 09:02:34.928: INFO: Got endpoints: latency-svc-m854r [56.402306ms]
    Mar 29 09:02:34.930: INFO: Created: latency-svc-2nz4l
    Mar 29 09:02:34.932: INFO: Got endpoints: latency-svc-2nz4l [56.021596ms]
    Mar 29 09:02:34.933: INFO: Created: latency-svc-g9dll
    Mar 29 09:02:34.936: INFO: Got endpoints: latency-svc-g9dll [54.226977ms]
    Mar 29 09:02:34.937: INFO: Created: latency-svc-rnpk9
    Mar 29 09:02:34.940: INFO: Got endpoints: latency-svc-rnpk9 [57.118128ms]
    Mar 29 09:02:34.941: INFO: Created: latency-svc-dq8ng
    Mar 29 09:02:34.943: INFO: Got endpoints: latency-svc-dq8ng [56.26375ms]
    Mar 29 09:02:34.944: INFO: Created: latency-svc-6xdr8
    Mar 29 09:02:34.946: INFO: Created: latency-svc-ndnsd
    Mar 29 09:02:34.947: INFO: Got endpoints: latency-svc-6xdr8 [56.414649ms]
    Mar 29 09:02:34.949: INFO: Got endpoints: latency-svc-ndnsd [53.66359ms]
    Mar 29 09:02:34.950: INFO: Created: latency-svc-w6pzr
    Mar 29 09:02:34.953: INFO: Got endpoints: latency-svc-w6pzr [53.585998ms]
    Mar 29 09:02:34.955: INFO: Created: latency-svc-nb2sj
    Mar 29 09:02:34.956: INFO: Got endpoints: latency-svc-nb2sj [48.890436ms]
    Mar 29 09:02:34.958: INFO: Created: latency-svc-dbdd4
    Mar 29 09:02:34.960: INFO: Created: latency-svc-prmf9
    Mar 29 09:02:34.961: INFO: Got endpoints: latency-svc-dbdd4 [53.324562ms]
    Mar 29 09:02:34.964: INFO: Got endpoints: latency-svc-prmf9 [52.719033ms]
    Mar 29 09:02:34.964: INFO: Created: latency-svc-c6czr
    Mar 29 09:02:34.968: INFO: Got endpoints: latency-svc-c6czr [54.066443ms]
    Mar 29 09:02:34.969: INFO: Created: latency-svc-mxk5z
    Mar 29 09:02:34.970: INFO: Got endpoints: latency-svc-mxk5z [51.774965ms]
    Mar 29 09:02:34.972: INFO: Created: latency-svc-qjcm6
    Mar 29 09:02:34.974: INFO: Created: latency-svc-q7tkt
    Mar 29 09:02:34.975: INFO: Got endpoints: latency-svc-qjcm6 [54.039799ms]
    Mar 29 09:02:34.977: INFO: Created: latency-svc-4bjcz
    Mar 29 09:02:34.980: INFO: Created: latency-svc-qf97w
    Mar 29 09:02:34.982: INFO: Created: latency-svc-xckf7
    Mar 29 09:02:34.986: INFO: Created: latency-svc-9jdlg
    Mar 29 09:02:34.990: INFO: Created: latency-svc-5qbt8
    Mar 29 09:02:34.995: INFO: Created: latency-svc-85rtf
    Mar 29 09:02:34.998: INFO: Created: latency-svc-r7hxf
    Mar 29 09:02:35.001: INFO: Created: latency-svc-7htxj
    Mar 29 09:02:35.006: INFO: Created: latency-svc-fbx85
    Mar 29 09:02:35.008: INFO: Got endpoints: latency-svc-q7tkt [82.006516ms]
    Mar 29 09:02:35.009: INFO: Created: latency-svc-d6xd7
    Mar 29 09:02:35.012: INFO: Created: latency-svc-h27rj
    Mar 29 09:02:35.014: INFO: Created: latency-svc-h92tf
    Mar 29 09:02:35.016: INFO: Created: latency-svc-d56hk
    Mar 29 09:02:35.019: INFO: Created: latency-svc-qk6dd
    Mar 29 09:02:35.022: INFO: Created: latency-svc-wgswv
    Mar 29 09:02:35.056: INFO: Got endpoints: latency-svc-4bjcz [127.661677ms]
    Mar 29 09:02:35.062: INFO: Created: latency-svc-8564s
    Mar 29 09:02:35.106: INFO: Got endpoints: latency-svc-qf97w [174.569974ms]
    Mar 29 09:02:35.111: INFO: Created: latency-svc-pjk2n
    Mar 29 09:02:35.158: INFO: Got endpoints: latency-svc-xckf7 [221.361352ms]
    Mar 29 09:02:35.163: INFO: Created: latency-svc-mvhjw
    Mar 29 09:02:35.208: INFO: Got endpoints: latency-svc-9jdlg [267.669102ms]
    Mar 29 09:02:35.214: INFO: Created: latency-svc-xwk7v
    Mar 29 09:02:35.257: INFO: Got endpoints: latency-svc-5qbt8 [313.963076ms]
    Mar 29 09:02:35.263: INFO: Created: latency-svc-5zqb4
    Mar 29 09:02:35.306: INFO: Got endpoints: latency-svc-85rtf [359.393991ms]
    Mar 29 09:02:35.312: INFO: Created: latency-svc-gtgxq
    Mar 29 09:02:35.356: INFO: Got endpoints: latency-svc-r7hxf [406.996269ms]
    Mar 29 09:02:35.361: INFO: Created: latency-svc-8dgdl
    Mar 29 09:02:35.407: INFO: Got endpoints: latency-svc-7htxj [454.081318ms]
    Mar 29 09:02:35.413: INFO: Created: latency-svc-wk299
    Mar 29 09:02:35.456: INFO: Got endpoints: latency-svc-fbx85 [499.620818ms]
    Mar 29 09:02:35.461: INFO: Created: latency-svc-cl764
    Mar 29 09:02:35.506: INFO: Got endpoints: latency-svc-d6xd7 [545.5194ms]
    Mar 29 09:02:35.512: INFO: Created: latency-svc-qdvh4
    Mar 29 09:02:35.556: INFO: Got endpoints: latency-svc-h27rj [591.89442ms]
    Mar 29 09:02:35.562: INFO: Created: latency-svc-vdrlt
    Mar 29 09:02:35.608: INFO: Got endpoints: latency-svc-h92tf [639.753794ms]
    Mar 29 09:02:35.616: INFO: Created: latency-svc-zv5hl
    Mar 29 09:02:35.657: INFO: Got endpoints: latency-svc-d56hk [686.321583ms]
    Mar 29 09:02:35.662: INFO: Created: latency-svc-cl9fd
    Mar 29 09:02:35.707: INFO: Got endpoints: latency-svc-qk6dd [732.002877ms]
    Mar 29 09:02:35.712: INFO: Created: latency-svc-wfkqx
    Mar 29 09:02:35.758: INFO: Got endpoints: latency-svc-wgswv [749.769005ms]
    Mar 29 09:02:35.763: INFO: Created: latency-svc-6qb9x
    Mar 29 09:02:35.806: INFO: Got endpoints: latency-svc-8564s [750.483649ms]
    Mar 29 09:02:35.813: INFO: Created: latency-svc-fl8d9
    Mar 29 09:02:35.856: INFO: Got endpoints: latency-svc-pjk2n [749.73586ms]
    Mar 29 09:02:35.862: INFO: Created: latency-svc-5wddv
    Mar 29 09:02:35.908: INFO: Got endpoints: latency-svc-mvhjw [750.189368ms]
    Mar 29 09:02:35.913: INFO: Created: latency-svc-b8sfs
    Mar 29 09:02:35.956: INFO: Got endpoints: latency-svc-xwk7v [747.992203ms]
    Mar 29 09:02:35.961: INFO: Created: latency-svc-vcp9v
    Mar 29 09:02:36.007: INFO: Got endpoints: latency-svc-5zqb4 [750.447572ms]
    Mar 29 09:02:36.014: INFO: Created: latency-svc-95z9d
    Mar 29 09:02:36.056: INFO: Got endpoints: latency-svc-gtgxq [749.296426ms]
    Mar 29 09:02:36.062: INFO: Created: latency-svc-tjbgm
    Mar 29 09:02:36.106: INFO: Got endpoints: latency-svc-8dgdl [749.685498ms]
    Mar 29 09:02:36.111: INFO: Created: latency-svc-jddsj
    Mar 29 09:02:36.156: INFO: Got endpoints: latency-svc-wk299 [749.70371ms]
    Mar 29 09:02:36.162: INFO: Created: latency-svc-jrbn2
    Mar 29 09:02:36.208: INFO: Got endpoints: latency-svc-cl764 [752.019463ms]
    Mar 29 09:02:36.213: INFO: Created: latency-svc-xqshd
    Mar 29 09:02:36.256: INFO: Got endpoints: latency-svc-qdvh4 [749.954308ms]
    Mar 29 09:02:36.262: INFO: Created: latency-svc-cz4td
    Mar 29 09:02:36.307: INFO: Got endpoints: latency-svc-vdrlt [750.464823ms]
    Mar 29 09:02:36.315: INFO: Created: latency-svc-s2bxz
    Mar 29 09:02:36.358: INFO: Got endpoints: latency-svc-zv5hl [749.994498ms]
    Mar 29 09:02:36.363: INFO: Created: latency-svc-sh2zl
    Mar 29 09:02:36.406: INFO: Got endpoints: latency-svc-cl9fd [749.219371ms]
    Mar 29 09:02:36.411: INFO: Created: latency-svc-smksn
    Mar 29 09:02:36.456: INFO: Got endpoints: latency-svc-wfkqx [748.542743ms]
    Mar 29 09:02:36.461: INFO: Created: latency-svc-nb9vh
    Mar 29 09:02:36.507: INFO: Got endpoints: latency-svc-6qb9x [749.635399ms]
    Mar 29 09:02:36.513: INFO: Created: latency-svc-9z5pc
    Mar 29 09:02:36.557: INFO: Got endpoints: latency-svc-fl8d9 [750.134242ms]
    Mar 29 09:02:36.562: INFO: Created: latency-svc-lkt2k
    Mar 29 09:02:36.607: INFO: Got endpoints: latency-svc-5wddv [750.402136ms]
    Mar 29 09:02:36.612: INFO: Created: latency-svc-wz2xm
    Mar 29 09:02:36.657: INFO: Got endpoints: latency-svc-b8sfs [748.766678ms]
    Mar 29 09:02:36.662: INFO: Created: latency-svc-zqnhb
    Mar 29 09:02:36.706: INFO: Got endpoints: latency-svc-vcp9v [750.515466ms]
    Mar 29 09:02:36.713: INFO: Created: latency-svc-9n4h4
    Mar 29 09:02:36.757: INFO: Got endpoints: latency-svc-95z9d [749.039757ms]
    Mar 29 09:02:36.763: INFO: Created: latency-svc-9sq9p
    Mar 29 09:02:36.807: INFO: Got endpoints: latency-svc-tjbgm [750.871427ms]
    Mar 29 09:02:36.812: INFO: Created: latency-svc-8d9tx
    Mar 29 09:02:36.858: INFO: Got endpoints: latency-svc-jddsj [751.720516ms]
    Mar 29 09:02:36.863: INFO: Created: latency-svc-d8wkm
    Mar 29 09:02:36.907: INFO: Got endpoints: latency-svc-jrbn2 [750.694879ms]
    Mar 29 09:02:36.913: INFO: Created: latency-svc-qt9jv
    Mar 29 09:02:36.957: INFO: Got endpoints: latency-svc-xqshd [748.56714ms]
    Mar 29 09:02:36.963: INFO: Created: latency-svc-66g7p
    Mar 29 09:02:37.006: INFO: Got endpoints: latency-svc-cz4td [749.662683ms]
    Mar 29 09:02:37.011: INFO: Created: latency-svc-d9fdj
    Mar 29 09:02:37.058: INFO: Got endpoints: latency-svc-s2bxz [751.00105ms]
    Mar 29 09:02:37.063: INFO: Created: latency-svc-49lt5
    Mar 29 09:02:37.108: INFO: Got endpoints: latency-svc-sh2zl [749.917308ms]
    Mar 29 09:02:37.113: INFO: Created: latency-svc-fmh6d
    Mar 29 09:02:37.156: INFO: Got endpoints: latency-svc-smksn [750.284989ms]
    Mar 29 09:02:37.163: INFO: Created: latency-svc-tq7vg
    Mar 29 09:02:37.206: INFO: Got endpoints: latency-svc-nb9vh [750.755865ms]
    Mar 29 09:02:37.212: INFO: Created: latency-svc-hwqfq
    Mar 29 09:02:37.257: INFO: Got endpoints: latency-svc-9z5pc [750.014731ms]
    Mar 29 09:02:37.262: INFO: Created: latency-svc-5pxng
    Mar 29 09:02:37.308: INFO: Got endpoints: latency-svc-lkt2k [751.364409ms]
    Mar 29 09:02:37.313: INFO: Created: latency-svc-bbmtl
    Mar 29 09:02:37.357: INFO: Got endpoints: latency-svc-wz2xm [750.204852ms]
    Mar 29 09:02:37.363: INFO: Created: latency-svc-h8nw4
    Mar 29 09:02:37.407: INFO: Got endpoints: latency-svc-zqnhb [750.885018ms]
    Mar 29 09:02:37.413: INFO: Created: latency-svc-s6gs5
    Mar 29 09:02:37.457: INFO: Got endpoints: latency-svc-9n4h4 [750.350735ms]
    Mar 29 09:02:37.462: INFO: Created: latency-svc-55cgw
    Mar 29 09:02:37.507: INFO: Got endpoints: latency-svc-9sq9p [750.495065ms]
    Mar 29 09:02:37.513: INFO: Created: latency-svc-6tk5n
    Mar 29 09:02:37.556: INFO: Got endpoints: latency-svc-8d9tx [749.697954ms]
    Mar 29 09:02:37.563: INFO: Created: latency-svc-ncvz8
    Mar 29 09:02:37.607: INFO: Got endpoints: latency-svc-d8wkm [749.181454ms]
    Mar 29 09:02:37.612: INFO: Created: latency-svc-57czl
    Mar 29 09:02:37.657: INFO: Got endpoints: latency-svc-qt9jv [749.829549ms]
    Mar 29 09:02:37.662: INFO: Created: latency-svc-nv95j
    Mar 29 09:02:37.707: INFO: Got endpoints: latency-svc-66g7p [750.681954ms]
    Mar 29 09:02:37.712: INFO: Created: latency-svc-lrz4g
    Mar 29 09:02:37.756: INFO: Got endpoints: latency-svc-d9fdj [750.438794ms]
    Mar 29 09:02:37.762: INFO: Created: latency-svc-nbkcj
    Mar 29 09:02:37.807: INFO: Got endpoints: latency-svc-49lt5 [749.011459ms]
    Mar 29 09:02:37.812: INFO: Created: latency-svc-s2tpm
    Mar 29 09:02:37.856: INFO: Got endpoints: latency-svc-fmh6d [748.736246ms]
    Mar 29 09:02:37.862: INFO: Created: latency-svc-hrf5p
    Mar 29 09:02:37.907: INFO: Got endpoints: latency-svc-tq7vg [750.519052ms]
    Mar 29 09:02:37.912: INFO: Created: latency-svc-fk85g
    Mar 29 09:02:37.957: INFO: Got endpoints: latency-svc-hwqfq [750.290183ms]
    Mar 29 09:02:37.962: INFO: Created: latency-svc-7rn98
    Mar 29 09:02:38.007: INFO: Got endpoints: latency-svc-5pxng [749.461807ms]
    Mar 29 09:02:38.012: INFO: Created: latency-svc-vthtj
    Mar 29 09:02:38.058: INFO: Got endpoints: latency-svc-bbmtl [749.823352ms]
    Mar 29 09:02:38.063: INFO: Created: latency-svc-h5lqv
    Mar 29 09:02:38.107: INFO: Got endpoints: latency-svc-h8nw4 [750.121871ms]
    Mar 29 09:02:38.112: INFO: Created: latency-svc-bxhfg
    Mar 29 09:02:38.157: INFO: Got endpoints: latency-svc-s6gs5 [749.77627ms]
    Mar 29 09:02:38.163: INFO: Created: latency-svc-rn64s
    Mar 29 09:02:38.207: INFO: Got endpoints: latency-svc-55cgw [749.980658ms]
    Mar 29 09:02:38.213: INFO: Created: latency-svc-svz88
    Mar 29 09:02:38.256: INFO: Got endpoints: latency-svc-6tk5n [749.392944ms]
    Mar 29 09:02:38.262: INFO: Created: latency-svc-l9t2p
    Mar 29 09:02:38.307: INFO: Got endpoints: latency-svc-ncvz8 [751.112227ms]
    Mar 29 09:02:38.313: INFO: Created: latency-svc-gn7pf
    Mar 29 09:02:38.357: INFO: Got endpoints: latency-svc-57czl [750.014389ms]
    Mar 29 09:02:38.363: INFO: Created: latency-svc-2f7d9
    Mar 29 09:02:38.406: INFO: Got endpoints: latency-svc-nv95j [749.380999ms]
    Mar 29 09:02:38.413: INFO: Created: latency-svc-kfbnz
    Mar 29 09:02:38.456: INFO: Got endpoints: latency-svc-lrz4g [749.175582ms]
    Mar 29 09:02:38.462: INFO: Created: latency-svc-cl7zz
    Mar 29 09:02:38.507: INFO: Got endpoints: latency-svc-nbkcj [750.449184ms]
    Mar 29 09:02:38.512: INFO: Created: latency-svc-k4zns
    Mar 29 09:02:38.556: INFO: Got endpoints: latency-svc-s2tpm [749.253105ms]
    Mar 29 09:02:38.561: INFO: Created: latency-svc-dz2m2
    Mar 29 09:02:38.606: INFO: Got endpoints: latency-svc-hrf5p [749.993847ms]
    Mar 29 09:02:38.613: INFO: Created: latency-svc-ftvg5
    Mar 29 09:02:38.656: INFO: Got endpoints: latency-svc-fk85g [749.282536ms]
    Mar 29 09:02:38.662: INFO: Created: latency-svc-clw7h
    Mar 29 09:02:38.707: INFO: Got endpoints: latency-svc-7rn98 [750.007667ms]
    Mar 29 09:02:38.712: INFO: Created: latency-svc-rtxsw
    Mar 29 09:02:38.757: INFO: Got endpoints: latency-svc-vthtj [749.846009ms]
    Mar 29 09:02:38.762: INFO: Created: latency-svc-nqhs8
    Mar 29 09:02:38.805: INFO: Got endpoints: latency-svc-h5lqv [747.338465ms]
    Mar 29 09:02:38.811: INFO: Created: latency-svc-nzfhc
    Mar 29 09:02:38.856: INFO: Got endpoints: latency-svc-bxhfg [748.739428ms]
    Mar 29 09:02:38.862: INFO: Created: latency-svc-ddp8l
    Mar 29 09:02:38.906: INFO: Got endpoints: latency-svc-rn64s [748.905191ms]
    Mar 29 09:02:38.912: INFO: Created: latency-svc-pdzw5
    Mar 29 09:02:38.956: INFO: Got endpoints: latency-svc-svz88 [749.508793ms]
    Mar 29 09:02:38.962: INFO: Created: latency-svc-dmc4m
    Mar 29 09:02:39.008: INFO: Got endpoints: latency-svc-l9t2p [751.086305ms]
    Mar 29 09:02:39.013: INFO: Created: latency-svc-8rnn5
    Mar 29 09:02:39.056: INFO: Got endpoints: latency-svc-gn7pf [748.617984ms]
    Mar 29 09:02:39.062: INFO: Created: latency-svc-nz88d
    Mar 29 09:02:39.106: INFO: Got endpoints: latency-svc-2f7d9 [748.660253ms]
    Mar 29 09:02:39.111: INFO: Created: latency-svc-tmj8d
    Mar 29 09:02:39.156: INFO: Got endpoints: latency-svc-kfbnz [749.262933ms]
    Mar 29 09:02:39.161: INFO: Created: latency-svc-thcg2
    Mar 29 09:02:39.206: INFO: Got endpoints: latency-svc-cl7zz [749.41376ms]
    Mar 29 09:02:39.211: INFO: Created: latency-svc-ppn5s
    Mar 29 09:02:39.257: INFO: Got endpoints: latency-svc-k4zns [749.65844ms]
    Mar 29 09:02:39.263: INFO: Created: latency-svc-48cgq
    Mar 29 09:02:39.306: INFO: Got endpoints: latency-svc-dz2m2 [750.358646ms]
    Mar 29 09:02:39.312: INFO: Created: latency-svc-jvqsz
    Mar 29 09:02:39.359: INFO: Got endpoints: latency-svc-ftvg5 [752.173071ms]
    Mar 29 09:02:39.364: INFO: Created: latency-svc-kdthn
    Mar 29 09:02:39.409: INFO: Got endpoints: latency-svc-clw7h [753.361409ms]
    Mar 29 09:02:39.416: INFO: Created: latency-svc-ppkkp
    Mar 29 09:02:39.458: INFO: Got endpoints: latency-svc-rtxsw [750.715226ms]
    Mar 29 09:02:39.466: INFO: Created: latency-svc-xgxss
    Mar 29 09:02:39.507: INFO: Got endpoints: latency-svc-nqhs8 [750.225198ms]
    Mar 29 09:02:39.513: INFO: Created: latency-svc-wbjbc
    Mar 29 09:02:39.559: INFO: Got endpoints: latency-svc-nzfhc [754.116614ms]
    Mar 29 09:02:39.567: INFO: Created: latency-svc-dchql
    Mar 29 09:02:39.607: INFO: Got endpoints: latency-svc-ddp8l [751.315938ms]
    Mar 29 09:02:39.613: INFO: Created: latency-svc-9skfw
    Mar 29 09:02:39.656: INFO: Got endpoints: latency-svc-pdzw5 [749.974416ms]
    Mar 29 09:02:39.664: INFO: Created: latency-svc-kjqkl
    Mar 29 09:02:39.708: INFO: Got endpoints: latency-svc-dmc4m [751.416278ms]
    Mar 29 09:02:39.714: INFO: Created: latency-svc-gw7q7
    Mar 29 09:02:39.756: INFO: Got endpoints: latency-svc-8rnn5 [748.799799ms]
    Mar 29 09:02:39.762: INFO: Created: latency-svc-hw4qz
    Mar 29 09:02:39.808: INFO: Got endpoints: latency-svc-nz88d [751.989391ms]
    Mar 29 09:02:39.813: INFO: Created: latency-svc-7jx9x
    Mar 29 09:02:39.857: INFO: Got endpoints: latency-svc-tmj8d [750.937483ms]
    Mar 29 09:02:39.863: INFO: Created: latency-svc-f69vq
    Mar 29 09:02:39.906: INFO: Got endpoints: latency-svc-thcg2 [750.680481ms]
    Mar 29 09:02:39.912: INFO: Created: latency-svc-ntk2r
    Mar 29 09:02:39.958: INFO: Got endpoints: latency-svc-ppn5s [751.62582ms]
    Mar 29 09:02:39.963: INFO: Created: latency-svc-7nfk6
    Mar 29 09:02:40.007: INFO: Got endpoints: latency-svc-48cgq [750.053239ms]
    Mar 29 09:02:40.012: INFO: Created: latency-svc-xf4f8
    Mar 29 09:02:40.056: INFO: Got endpoints: latency-svc-jvqsz [749.990174ms]
    Mar 29 09:02:40.062: INFO: Created: latency-svc-9f4v2
    Mar 29 09:02:40.106: INFO: Got endpoints: latency-svc-kdthn [747.860822ms]
    Mar 29 09:02:40.112: INFO: Created: latency-svc-m5wk7
    Mar 29 09:02:40.157: INFO: Got endpoints: latency-svc-ppkkp [748.018667ms]
    Mar 29 09:02:40.163: INFO: Created: latency-svc-8skzt
    Mar 29 09:02:40.206: INFO: Got endpoints: latency-svc-xgxss [748.64821ms]
    Mar 29 09:02:40.211: INFO: Created: latency-svc-qw2wx
    Mar 29 09:02:40.256: INFO: Got endpoints: latency-svc-wbjbc [749.277528ms]
    Mar 29 09:02:40.262: INFO: Created: latency-svc-72vxx
    Mar 29 09:02:40.315: INFO: Got endpoints: latency-svc-dchql [755.224473ms]
    Mar 29 09:02:40.322: INFO: Created: latency-svc-fddkh
    Mar 29 09:02:40.357: INFO: Got endpoints: latency-svc-9skfw [750.318517ms]
    Mar 29 09:02:40.363: INFO: Created: latency-svc-ldbqt
    Mar 29 09:02:40.407: INFO: Got endpoints: latency-svc-kjqkl [751.164421ms]
    Mar 29 09:02:40.412: INFO: Created: latency-svc-m5v97
    Mar 29 09:02:40.457: INFO: Got endpoints: latency-svc-gw7q7 [748.873598ms]
    Mar 29 09:02:40.463: INFO: Created: latency-svc-n2b8t
    Mar 29 09:02:40.506: INFO: Got endpoints: latency-svc-hw4qz [749.619925ms]
    Mar 29 09:02:40.511: INFO: Created: latency-svc-8zb5l
    Mar 29 09:02:40.557: INFO: Got endpoints: latency-svc-7jx9x [749.240366ms]
    Mar 29 09:02:40.563: INFO: Created: latency-svc-md5xd
    Mar 29 09:02:40.606: INFO: Got endpoints: latency-svc-f69vq [749.15418ms]
    Mar 29 09:02:40.611: INFO: Created: latency-svc-6npz8
    Mar 29 09:02:40.656: INFO: Got endpoints: latency-svc-ntk2r [749.315666ms]
    Mar 29 09:02:40.662: INFO: Created: latency-svc-mcjvk
    Mar 29 09:02:40.706: INFO: Got endpoints: latency-svc-7nfk6 [748.320045ms]
    Mar 29 09:02:40.711: INFO: Created: latency-svc-tvg9x
    Mar 29 09:02:40.756: INFO: Got endpoints: latency-svc-xf4f8 [749.565702ms]
    Mar 29 09:02:40.761: INFO: Created: latency-svc-jbwml
    Mar 29 09:02:40.808: INFO: Got endpoints: latency-svc-9f4v2 [751.240572ms]
    Mar 29 09:02:40.814: INFO: Created: latency-svc-6m7ld
    Mar 29 09:02:40.858: INFO: Got endpoints: latency-svc-m5wk7 [752.00236ms]
    Mar 29 09:02:40.864: INFO: Created: latency-svc-7vkmk
    Mar 29 09:02:40.907: INFO: Got endpoints: latency-svc-8skzt [749.545325ms]
    Mar 29 09:02:40.912: INFO: Created: latency-svc-s2bnf
    Mar 29 09:02:40.958: INFO: Got endpoints: latency-svc-qw2wx [751.556767ms]
    Mar 29 09:02:40.963: INFO: Created: latency-svc-4ts7l
    Mar 29 09:02:41.008: INFO: Got endpoints: latency-svc-72vxx [751.88407ms]
    Mar 29 09:02:41.013: INFO: Created: latency-svc-lp8fc
    Mar 29 09:02:41.058: INFO: Got endpoints: latency-svc-fddkh [743.002402ms]
    Mar 29 09:02:41.063: INFO: Created: latency-svc-ht4f4
    Mar 29 09:02:41.109: INFO: Got endpoints: latency-svc-ldbqt [751.284722ms]
    Mar 29 09:02:41.114: INFO: Created: latency-svc-tzksm
    Mar 29 09:02:41.156: INFO: Got endpoints: latency-svc-m5v97 [748.674166ms]
    Mar 29 09:02:41.163: INFO: Created: latency-svc-n87x4
    Mar 29 09:02:41.207: INFO: Got endpoints: latency-svc-n2b8t [750.054083ms]
    Mar 29 09:02:41.213: INFO: Created: latency-svc-cppzr
    Mar 29 09:02:41.258: INFO: Got endpoints: latency-svc-8zb5l [752.020365ms]
    Mar 29 09:02:41.263: INFO: Created: latency-svc-jvw9z
    Mar 29 09:02:41.306: INFO: Got endpoints: latency-svc-md5xd [748.894382ms]
    Mar 29 09:02:41.313: INFO: Created: latency-svc-qmc77
    Mar 29 09:02:41.356: INFO: Got endpoints: latency-svc-6npz8 [750.49028ms]
    Mar 29 09:02:41.363: INFO: Created: latency-svc-7shfw
    Mar 29 09:02:41.406: INFO: Got endpoints: latency-svc-mcjvk [750.172799ms]
    Mar 29 09:02:41.411: INFO: Created: latency-svc-n8nqk
    Mar 29 09:02:41.457: INFO: Got endpoints: latency-svc-tvg9x [750.670893ms]
    Mar 29 09:02:41.462: INFO: Created: latency-svc-4wgxv
    Mar 29 09:02:41.507: INFO: Got endpoints: latency-svc-jbwml [750.422605ms]
    Mar 29 09:02:41.512: INFO: Created: latency-svc-fmhw9
    Mar 29 09:02:41.557: INFO: Got endpoints: latency-svc-6m7ld [748.988865ms]
    Mar 29 09:02:41.563: INFO: Created: latency-svc-jfgmx
    Mar 29 09:02:41.606: INFO: Got endpoints: latency-svc-7vkmk [747.955301ms]
    Mar 29 09:02:41.612: INFO: Created: latency-svc-fckxs
    Mar 29 09:02:41.656: INFO: Got endpoints: latency-svc-s2bnf [749.164213ms]
    Mar 29 09:02:41.661: INFO: Created: latency-svc-vmrfj
    Mar 29 09:02:41.707: INFO: Got endpoints: latency-svc-4ts7l [748.85848ms]
    Mar 29 09:02:41.712: INFO: Created: latency-svc-fw85w
    Mar 29 09:02:41.758: INFO: Got endpoints: latency-svc-lp8fc [749.418912ms]
    Mar 29 09:02:41.763: INFO: Created: latency-svc-5qpsb
    Mar 29 09:02:41.807: INFO: Got endpoints: latency-svc-ht4f4 [749.053943ms]
    Mar 29 09:02:41.812: INFO: Created: latency-svc-c66l6
    Mar 29 09:02:41.858: INFO: Got endpoints: latency-svc-tzksm [749.150253ms]
    Mar 29 09:02:41.863: INFO: Created: latency-svc-whbdx
    Mar 29 09:02:41.906: INFO: Got endpoints: latency-svc-n87x4 [750.11053ms]
    Mar 29 09:02:41.912: INFO: Created: latency-svc-b94qx
    Mar 29 09:02:41.957: INFO: Got endpoints: latency-svc-cppzr [750.35402ms]
    Mar 29 09:02:41.963: INFO: Created: latency-svc-wlmfg
    Mar 29 09:02:42.007: INFO: Got endpoints: latency-svc-jvw9z [749.012498ms]
    Mar 29 09:02:42.012: INFO: Created: latency-svc-pfvxc
    Mar 29 09:02:42.058: INFO: Got endpoints: latency-svc-qmc77 [752.012099ms]
    Mar 29 09:02:42.064: INFO: Created: latency-svc-cpp29
    Mar 29 09:02:42.108: INFO: Got endpoints: latency-svc-7shfw [751.846385ms]
    Mar 29 09:02:42.113: INFO: Created: latency-svc-5gb4f
    Mar 29 09:02:42.157: INFO: Got endpoints: latency-svc-n8nqk [750.624506ms]
    Mar 29 09:02:42.163: INFO: Created: latency-svc-mfkg2
    Mar 29 09:02:42.206: INFO: Got endpoints: latency-svc-4wgxv [749.716269ms]
    Mar 29 09:02:42.212: INFO: Created: latency-svc-n4x9j
    Mar 29 09:02:42.257: INFO: Got endpoints: latency-svc-fmhw9 [750.202417ms]
    Mar 29 09:02:42.263: INFO: Created: latency-svc-tklg7
    Mar 29 09:02:42.306: INFO: Got endpoints: latency-svc-jfgmx [749.863805ms]
    Mar 29 09:02:42.312: INFO: Created: latency-svc-7prml
    Mar 29 09:02:42.356: INFO: Got endpoints: latency-svc-fckxs [749.910587ms]
    Mar 29 09:02:42.363: INFO: Created: latency-svc-v2lmq
    Mar 29 09:02:42.406: INFO: Got endpoints: latency-svc-vmrfj [749.70446ms]
    Mar 29 09:02:42.411: INFO: Created: latency-svc-6fmbt
    Mar 29 09:02:42.457: INFO: Got endpoints: latency-svc-fw85w [750.821315ms]
    Mar 29 09:02:42.463: INFO: Created: latency-svc-zpcdv
    Mar 29 09:02:42.506: INFO: Got endpoints: latency-svc-5qpsb [748.034816ms]
    Mar 29 09:02:42.511: INFO: Created: latency-svc-bdzqx
    Mar 29 09:02:42.557: INFO: Got endpoints: latency-svc-c66l6 [749.776892ms]
    Mar 29 09:02:42.563: INFO: Created: latency-svc-w4h5l
    Mar 29 09:02:42.607: INFO: Got endpoints: latency-svc-whbdx [748.540088ms]
    Mar 29 09:02:42.612: INFO: Created: latency-svc-plkdw
    Mar 29 09:02:42.657: INFO: Got endpoints: latency-svc-b94qx [750.419079ms]
    Mar 29 09:02:42.662: INFO: Created: latency-svc-xdhz4
    Mar 29 09:02:42.706: INFO: Got endpoints: latency-svc-wlmfg [748.595887ms]
    Mar 29 09:02:42.756: INFO: Got endpoints: latency-svc-pfvxc [749.142244ms]
    Mar 29 09:02:42.807: INFO: Got endpoints: latency-svc-cpp29 [749.033574ms]
    Mar 29 09:02:42.857: INFO: Got endpoints: latency-svc-5gb4f [748.487369ms]
    Mar 29 09:02:42.907: INFO: Got endpoints: latency-svc-mfkg2 [750.562101ms]
    Mar 29 09:02:42.956: INFO: Got endpoints: latency-svc-n4x9j [749.848506ms]
    Mar 29 09:02:43.007: INFO: Got endpoints: latency-svc-tklg7 [749.73578ms]
    Mar 29 09:02:43.057: INFO: Got endpoints: latency-svc-7prml [750.074381ms]
    Mar 29 09:02:43.106: INFO: Got endpoints: latency-svc-v2lmq [749.958501ms]
    Mar 29 09:02:43.156: INFO: Got endpoints: latency-svc-6fmbt [750.111082ms]
    Mar 29 09:02:43.207: INFO: Got endpoints: latency-svc-zpcdv [749.22421ms]
    Mar 29 09:02:43.258: INFO: Got endpoints: latency-svc-bdzqx [752.282375ms]
    Mar 29 09:02:43.306: INFO: Got endpoints: latency-svc-w4h5l [749.580864ms]
    Mar 29 09:02:43.358: INFO: Got endpoints: latency-svc-plkdw [751.300689ms]
    Mar 29 09:02:43.407: INFO: Got endpoints: latency-svc-xdhz4 [750.405353ms]
    Mar 29 09:02:43.407: INFO: Latencies: [8.365045ms 11.581221ms 15.241532ms 19.317123ms 25.310094ms 26.459049ms 30.096917ms 33.956577ms 39.019305ms 42.36403ms 48.890436ms 50.797622ms 50.927313ms 51.774965ms 52.719033ms 53.324562ms 53.585998ms 53.66359ms 54.039799ms 54.066443ms 54.226977ms 54.788429ms 56.021596ms 56.136877ms 56.26375ms 56.402306ms 56.414649ms 57.118128ms 57.231036ms 57.616006ms 62.003372ms 82.006516ms 127.661677ms 174.569974ms 221.361352ms 267.669102ms 313.963076ms 359.393991ms 406.996269ms 454.081318ms 499.620818ms 545.5194ms 591.89442ms 639.753794ms 686.321583ms 732.002877ms 743.002402ms 747.338465ms 747.860822ms 747.955301ms 747.992203ms 748.018667ms 748.034816ms 748.320045ms 748.487369ms 748.540088ms 748.542743ms 748.56714ms 748.595887ms 748.617984ms 748.64821ms 748.660253ms 748.674166ms 748.736246ms 748.739428ms 748.766678ms 748.799799ms 748.85848ms 748.873598ms 748.894382ms 748.905191ms 748.988865ms 749.011459ms 749.012498ms 749.033574ms 749.039757ms 749.053943ms 749.142244ms 749.150253ms 749.15418ms 749.164213ms 749.175582ms 749.181454ms 749.219371ms 749.22421ms 749.240366ms 749.253105ms 749.262933ms 749.277528ms 749.282536ms 749.296426ms 749.315666ms 749.380999ms 749.392944ms 749.41376ms 749.418912ms 749.461807ms 749.508793ms 749.545325ms 749.565702ms 749.580864ms 749.619925ms 749.635399ms 749.65844ms 749.662683ms 749.685498ms 749.697954ms 749.70371ms 749.70446ms 749.716269ms 749.73578ms 749.73586ms 749.769005ms 749.77627ms 749.776892ms 749.823352ms 749.829549ms 749.846009ms 749.848506ms 749.863805ms 749.910587ms 749.917308ms 749.954308ms 749.958501ms 749.974416ms 749.980658ms 749.990174ms 749.993847ms 749.994498ms 750.007667ms 750.014389ms 750.014731ms 750.053239ms 750.054083ms 750.074381ms 750.11053ms 750.111082ms 750.121871ms 750.134242ms 750.172799ms 750.189368ms 750.202417ms 750.204852ms 750.225198ms 750.284989ms 750.290183ms 750.318517ms 750.350735ms 750.35402ms 750.358646ms 750.402136ms 750.405353ms 750.419079ms 750.422605ms 750.438794ms 750.447572ms 750.449184ms 750.464823ms 750.483649ms 750.49028ms 750.495065ms 750.515466ms 750.519052ms 750.562101ms 750.624506ms 750.670893ms 750.680481ms 750.681954ms 750.694879ms 750.715226ms 750.755865ms 750.821315ms 750.871427ms 750.885018ms 750.937483ms 751.00105ms 751.086305ms 751.112227ms 751.164421ms 751.240572ms 751.284722ms 751.300689ms 751.315938ms 751.364409ms 751.416278ms 751.556767ms 751.62582ms 751.720516ms 751.846385ms 751.88407ms 751.989391ms 752.00236ms 752.012099ms 752.019463ms 752.020365ms 752.173071ms 752.282375ms 753.361409ms 754.116614ms 755.224473ms]
    Mar 29 09:02:43.407: INFO: 50 %ile: 749.580864ms
    Mar 29 09:02:43.407: INFO: 90 %ile: 751.284722ms
    Mar 29 09:02:43.407: INFO: 99 %ile: 754.116614ms
    Mar 29 09:02:43.407: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar 29 09:02:43.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5992" for this suite. 03/29/23 09:02:43.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:43.414
Mar 29 09:02:43.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sysctl 03/29/23 09:02:43.414
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:43.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:43.422
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/29/23 09:02:43.424
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 09:02:43.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6781" for this suite. 03/29/23 09:02:43.428
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":306,"skipped":5697,"failed":0}
------------------------------
â€¢ [0.017 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:43.414
    Mar 29 09:02:43.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sysctl 03/29/23 09:02:43.414
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:43.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:43.422
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/29/23 09:02:43.424
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 09:02:43.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-6781" for this suite. 03/29/23 09:02:43.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:43.431
Mar 29 09:02:43.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 09:02:43.431
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:43.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:43.438
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/29/23 09:02:43.441
Mar 29 09:02:43.444: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6885" to be "running and ready"
Mar 29 09:02:43.446: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253456ms
Mar 29 09:02:43.446: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:02:45.448: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003807155s
Mar 29 09:02:45.448: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 29 09:02:45.448: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/29/23 09:02:45.45
Mar 29 09:02:45.453: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6885" to be "running and ready"
Mar 29 09:02:45.455: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319297ms
Mar 29 09:02:45.455: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:02:47.457: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003514222s
Mar 29 09:02:47.457: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 29 09:02:47.457: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/29/23 09:02:47.458
Mar 29 09:02:47.461: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 09:02:47.463: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 09:02:49.463: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 09:02:49.465: INFO: Pod pod-with-prestop-http-hook still exists
Mar 29 09:02:51.464: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 29 09:02:51.466: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/29/23 09:02:51.466
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 29 09:02:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6885" for this suite. 03/29/23 09:02:51.476
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":307,"skipped":5702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.048 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:43.431
    Mar 29 09:02:43.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/29/23 09:02:43.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:43.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:43.438
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/29/23 09:02:43.441
    Mar 29 09:02:43.444: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6885" to be "running and ready"
    Mar 29 09:02:43.446: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253456ms
    Mar 29 09:02:43.446: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:02:45.448: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003807155s
    Mar 29 09:02:45.448: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 29 09:02:45.448: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/29/23 09:02:45.45
    Mar 29 09:02:45.453: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6885" to be "running and ready"
    Mar 29 09:02:45.455: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.319297ms
    Mar 29 09:02:45.455: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:02:47.457: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003514222s
    Mar 29 09:02:47.457: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 29 09:02:47.457: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/29/23 09:02:47.458
    Mar 29 09:02:47.461: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 29 09:02:47.463: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 29 09:02:49.463: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 29 09:02:49.465: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 29 09:02:51.464: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 29 09:02:51.466: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/29/23 09:02:51.466
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 29 09:02:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6885" for this suite. 03/29/23 09:02:51.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:02:51.479
Mar 29 09:02:51.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename subpath 03/29/23 09:02:51.48
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:51.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:51.487
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/29/23 09:02:51.488
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-zsls 03/29/23 09:02:51.492
STEP: Creating a pod to test atomic-volume-subpath 03/29/23 09:02:51.492
Mar 29 09:02:51.496: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zsls" in namespace "subpath-8454" to be "Succeeded or Failed"
Mar 29 09:02:51.497: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Pending", Reason="", readiness=false. Elapsed: 1.215251ms
Mar 29 09:02:53.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 2.002992725s
Mar 29 09:02:55.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 4.003181376s
Mar 29 09:02:57.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 6.004330541s
Mar 29 09:02:59.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 8.003190545s
Mar 29 09:03:01.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 10.003194349s
Mar 29 09:03:03.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 12.004239003s
Mar 29 09:03:05.501: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 14.00533403s
Mar 29 09:03:07.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 16.004234866s
Mar 29 09:03:09.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 18.004362065s
Mar 29 09:03:11.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 20.004336568s
Mar 29 09:03:13.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=false. Elapsed: 22.003205342s
Mar 29 09:03:15.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003327376s
STEP: Saw pod success 03/29/23 09:03:15.499
Mar 29 09:03:15.499: INFO: Pod "pod-subpath-test-configmap-zsls" satisfied condition "Succeeded or Failed"
Mar 29 09:03:15.501: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-configmap-zsls container test-container-subpath-configmap-zsls: <nil>
STEP: delete the pod 03/29/23 09:03:15.509
Mar 29 09:03:15.515: INFO: Waiting for pod pod-subpath-test-configmap-zsls to disappear
Mar 29 09:03:15.517: INFO: Pod pod-subpath-test-configmap-zsls no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zsls 03/29/23 09:03:15.517
Mar 29 09:03:15.517: INFO: Deleting pod "pod-subpath-test-configmap-zsls" in namespace "subpath-8454"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 29 09:03:15.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8454" for this suite. 03/29/23 09:03:15.519
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":308,"skipped":5723,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.042 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:02:51.479
    Mar 29 09:02:51.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename subpath 03/29/23 09:02:51.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:02:51.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:02:51.487
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/29/23 09:02:51.488
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-zsls 03/29/23 09:02:51.492
    STEP: Creating a pod to test atomic-volume-subpath 03/29/23 09:02:51.492
    Mar 29 09:02:51.496: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zsls" in namespace "subpath-8454" to be "Succeeded or Failed"
    Mar 29 09:02:51.497: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Pending", Reason="", readiness=false. Elapsed: 1.215251ms
    Mar 29 09:02:53.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 2.002992725s
    Mar 29 09:02:55.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 4.003181376s
    Mar 29 09:02:57.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 6.004330541s
    Mar 29 09:02:59.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 8.003190545s
    Mar 29 09:03:01.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 10.003194349s
    Mar 29 09:03:03.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 12.004239003s
    Mar 29 09:03:05.501: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 14.00533403s
    Mar 29 09:03:07.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 16.004234866s
    Mar 29 09:03:09.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 18.004362065s
    Mar 29 09:03:11.500: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=true. Elapsed: 20.004336568s
    Mar 29 09:03:13.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Running", Reason="", readiness=false. Elapsed: 22.003205342s
    Mar 29 09:03:15.499: INFO: Pod "pod-subpath-test-configmap-zsls": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003327376s
    STEP: Saw pod success 03/29/23 09:03:15.499
    Mar 29 09:03:15.499: INFO: Pod "pod-subpath-test-configmap-zsls" satisfied condition "Succeeded or Failed"
    Mar 29 09:03:15.501: INFO: Trying to get logs from node 10.146.0.115 pod pod-subpath-test-configmap-zsls container test-container-subpath-configmap-zsls: <nil>
    STEP: delete the pod 03/29/23 09:03:15.509
    Mar 29 09:03:15.515: INFO: Waiting for pod pod-subpath-test-configmap-zsls to disappear
    Mar 29 09:03:15.517: INFO: Pod pod-subpath-test-configmap-zsls no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-zsls 03/29/23 09:03:15.517
    Mar 29 09:03:15.517: INFO: Deleting pod "pod-subpath-test-configmap-zsls" in namespace "subpath-8454"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 29 09:03:15.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8454" for this suite. 03/29/23 09:03:15.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:03:15.523
Mar 29 09:03:15.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:03:15.523
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:03:15.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:03:15.53
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:03:15.536
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:03:15.766
STEP: Deploying the webhook pod 03/29/23 09:03:15.771
STEP: Wait for the deployment to be ready 03/29/23 09:03:15.777
Mar 29 09:03:15.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:03:17.786
STEP: Verifying the service has paired with the endpoint 03/29/23 09:03:17.792
Mar 29 09:03:18.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar 29 09:03:18.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6586-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 09:03:19.301
STEP: Creating a custom resource that should be mutated by the webhook 03/29/23 09:03:19.31
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:03:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-686" for this suite. 03/29/23 09:03:21.857
STEP: Destroying namespace "webhook-686-markers" for this suite. 03/29/23 09:03:21.86
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":309,"skipped":5743,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.361 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:03:15.523
    Mar 29 09:03:15.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:03:15.523
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:03:15.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:03:15.53
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:03:15.536
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:03:15.766
    STEP: Deploying the webhook pod 03/29/23 09:03:15.771
    STEP: Wait for the deployment to be ready 03/29/23 09:03:15.777
    Mar 29 09:03:15.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:03:17.786
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:03:17.792
    Mar 29 09:03:18.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar 29 09:03:18.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6586-crds.webhook.example.com via the AdmissionRegistration API 03/29/23 09:03:19.301
    STEP: Creating a custom resource that should be mutated by the webhook 03/29/23 09:03:19.31
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:03:21.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-686" for this suite. 03/29/23 09:03:21.857
    STEP: Destroying namespace "webhook-686-markers" for this suite. 03/29/23 09:03:21.86
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:03:21.884
Mar 29 09:03:21.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename cronjob 03/29/23 09:03:21.885
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:03:21.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:03:21.894
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/29/23 09:03:21.895
STEP: Ensuring more than one job is running at a time 03/29/23 09:03:21.898
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/29/23 09:05:01.9
STEP: Removing cronjob 03/29/23 09:05:01.902
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 29 09:05:01.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5138" for this suite. 03/29/23 09:05:01.907
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":310,"skipped":5747,"failed":0}
------------------------------
â€¢ [SLOW TEST] [100.026 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:03:21.884
    Mar 29 09:03:21.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename cronjob 03/29/23 09:03:21.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:03:21.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:03:21.894
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/29/23 09:03:21.895
    STEP: Ensuring more than one job is running at a time 03/29/23 09:03:21.898
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/29/23 09:05:01.9
    STEP: Removing cronjob 03/29/23 09:05:01.902
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 29 09:05:01.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5138" for this suite. 03/29/23 09:05:01.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:05:01.911
Mar 29 09:05:01.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 09:05:01.912
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:01.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:01.923
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-e9024ad4-2c3d-4aed-9f2b-273859046c00 03/29/23 09:05:01.925
STEP: Creating a pod to test consume configMaps 03/29/23 09:05:01.927
Mar 29 09:05:01.933: INFO: Waiting up to 5m0s for pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8" in namespace "configmap-2066" to be "Succeeded or Failed"
Mar 29 09:05:01.934: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.450621ms
Mar 29 09:05:03.937: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004448651s
Mar 29 09:05:05.938: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005308327s
STEP: Saw pod success 03/29/23 09:05:05.938
Mar 29 09:05:05.938: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8" satisfied condition "Succeeded or Failed"
Mar 29 09:05:05.940: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 09:05:05.947
Mar 29 09:05:05.952: INFO: Waiting for pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 to disappear
Mar 29 09:05:05.953: INFO: Pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 09:05:05.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2066" for this suite. 03/29/23 09:05:05.955
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":311,"skipped":5773,"failed":0}
------------------------------
â€¢ [4.047 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:05:01.911
    Mar 29 09:05:01.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 09:05:01.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:01.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:01.923
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-e9024ad4-2c3d-4aed-9f2b-273859046c00 03/29/23 09:05:01.925
    STEP: Creating a pod to test consume configMaps 03/29/23 09:05:01.927
    Mar 29 09:05:01.933: INFO: Waiting up to 5m0s for pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8" in namespace "configmap-2066" to be "Succeeded or Failed"
    Mar 29 09:05:01.934: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.450621ms
    Mar 29 09:05:03.937: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004448651s
    Mar 29 09:05:05.938: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005308327s
    STEP: Saw pod success 03/29/23 09:05:05.938
    Mar 29 09:05:05.938: INFO: Pod "pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8" satisfied condition "Succeeded or Failed"
    Mar 29 09:05:05.940: INFO: Trying to get logs from node 10.146.0.116 pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 09:05:05.947
    Mar 29 09:05:05.952: INFO: Waiting for pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 to disappear
    Mar 29 09:05:05.953: INFO: Pod pod-configmaps-250a52ae-d783-4d00-85bb-99cab91171d8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 09:05:05.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2066" for this suite. 03/29/23 09:05:05.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:05:05.958
Mar 29 09:05:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 09:05:05.959
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:05.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:05.965
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/29/23 09:05:05.967
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +notcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_tcp@PTR;sleep 1; done
 03/29/23 09:05:05.977
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +notcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_tcp@PTR;sleep 1; done
 03/29/23 09:05:05.977
STEP: creating a pod to probe DNS 03/29/23 09:05:05.977
STEP: submitting the pod to kubernetes 03/29/23 09:05:05.977
Mar 29 09:05:05.984: INFO: Waiting up to 15m0s for pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d" in namespace "dns-2889" to be "running"
Mar 29 09:05:05.986: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782206ms
Mar 29 09:05:07.989: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004856908s
Mar 29 09:05:07.989: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:05:07.989
STEP: looking for the results for each expected name from probers 03/29/23 09:05:07.991
Mar 29 09:05:07.993: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:07.994: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:07.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:07.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.004: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.006: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.014: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.016: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.019: INFO: Unable to read jessie_udp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.023: INFO: Unable to read jessie_udp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.028: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.030: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:08.037: INFO: Lookups using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2889 wheezy_tcp@dns-test-service.dns-2889 wheezy_udp@dns-test-service.dns-2889.svc wheezy_tcp@dns-test-service.dns-2889.svc wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2889 jessie_tcp@dns-test-service.dns-2889 jessie_udp@dns-test-service.dns-2889.svc jessie_tcp@dns-test-service.dns-2889.svc jessie_udp@_http._tcp.dns-test-service.dns-2889.svc jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc]

Mar 29 09:05:13.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
Mar 29 09:05:13.086: INFO: Lookups using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d failed for: [jessie_udp@_http._tcp.dns-test-service.dns-2889.svc]

Mar 29 09:05:18.078: INFO: DNS probes using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d succeeded

STEP: deleting the pod 03/29/23 09:05:18.078
STEP: deleting the test service 03/29/23 09:05:18.085
STEP: deleting the test headless service 03/29/23 09:05:18.095
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 09:05:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2889" for this suite. 03/29/23 09:05:18.102
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":312,"skipped":5778,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.146 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:05:05.958
    Mar 29 09:05:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 09:05:05.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:05.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:05.965
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/29/23 09:05:05.967
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +notcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_tcp@PTR;sleep 1; done
     03/29/23 09:05:05.977
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2889.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2889.svc;check="$$(dig +notcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.58.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.58.81_tcp@PTR;sleep 1; done
     03/29/23 09:05:05.977
    STEP: creating a pod to probe DNS 03/29/23 09:05:05.977
    STEP: submitting the pod to kubernetes 03/29/23 09:05:05.977
    Mar 29 09:05:05.984: INFO: Waiting up to 15m0s for pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d" in namespace "dns-2889" to be "running"
    Mar 29 09:05:05.986: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782206ms
    Mar 29 09:05:07.989: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004856908s
    Mar 29 09:05:07.989: INFO: Pod "dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:05:07.989
    STEP: looking for the results for each expected name from probers 03/29/23 09:05:07.991
    Mar 29 09:05:07.993: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:07.994: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:07.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:07.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.004: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.006: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.014: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.016: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.019: INFO: Unable to read jessie_udp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-2889 from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.023: INFO: Unable to read jessie_udp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.028: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.030: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:08.037: INFO: Lookups using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2889 wheezy_tcp@dns-test-service.dns-2889 wheezy_udp@dns-test-service.dns-2889.svc wheezy_tcp@dns-test-service.dns-2889.svc wheezy_udp@_http._tcp.dns-test-service.dns-2889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2889 jessie_tcp@dns-test-service.dns-2889 jessie_udp@dns-test-service.dns-2889.svc jessie_tcp@dns-test-service.dns-2889.svc jessie_udp@_http._tcp.dns-test-service.dns-2889.svc jessie_tcp@_http._tcp.dns-test-service.dns-2889.svc]

    Mar 29 09:05:13.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2889.svc from pod dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d: the server could not find the requested resource (get pods dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d)
    Mar 29 09:05:13.086: INFO: Lookups using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d failed for: [jessie_udp@_http._tcp.dns-test-service.dns-2889.svc]

    Mar 29 09:05:18.078: INFO: DNS probes using dns-2889/dns-test-3c41ff89-93ad-476b-be59-3c4705d48f3d succeeded

    STEP: deleting the pod 03/29/23 09:05:18.078
    STEP: deleting the test service 03/29/23 09:05:18.085
    STEP: deleting the test headless service 03/29/23 09:05:18.095
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 09:05:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2889" for this suite. 03/29/23 09:05:18.102
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:05:18.105
Mar 29 09:05:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename init-container 03/29/23 09:05:18.105
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:18.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:18.114
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/29/23 09:05:18.115
Mar 29 09:05:18.115: INFO: PodSpec: initContainers in spec.initContainers
Mar 29 09:06:00.870: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dcd9658f-d803-45f8-a96e-c76b04a6ed1a", GenerateName:"", Namespace:"init-container-8732", SelfLink:"", UID:"8b7a9358-c4e4-4f88-82e5-eb2fa6727b1e", ResourceVersion:"30941", Generation:0, CreationTimestamp:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"115723983"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"edb302c50f2e53ce96678f513d99f155896df464c6ba41a79efd49a7c54c9ee0", "cni.projectcalico.org/podIP":"192.168.30.3/32", "cni.projectcalico.org/podIPs":"192.168.30.3/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c078), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c0a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 6, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c0d8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-l75nv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008578000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00416c130), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.146.0.115", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032c8000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416c1c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416c1e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00416c1e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00416c1ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000dbc020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.146.0.115", PodIP:"192.168.30.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.30.3"}}, StartTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032c80e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032c8150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://fbac22cefcba6bf67c9b91ff8cf6859f2a13714c81df8c23d3b3e8ff58066a66", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008578080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008578060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00416c26f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 09:06:00.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8732" for this suite. 03/29/23 09:06:00.873
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":313,"skipped":5779,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.771 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:05:18.105
    Mar 29 09:05:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename init-container 03/29/23 09:05:18.105
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:05:18.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:05:18.114
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/29/23 09:05:18.115
    Mar 29 09:05:18.115: INFO: PodSpec: initContainers in spec.initContainers
    Mar 29 09:06:00.870: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dcd9658f-d803-45f8-a96e-c76b04a6ed1a", GenerateName:"", Namespace:"init-container-8732", SelfLink:"", UID:"8b7a9358-c4e4-4f88-82e5-eb2fa6727b1e", ResourceVersion:"30941", Generation:0, CreationTimestamp:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"115723983"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"edb302c50f2e53ce96678f513d99f155896df464c6ba41a79efd49a7c54c9ee0", "cni.projectcalico.org/podIP":"192.168.30.3/32", "cni.projectcalico.org/podIPs":"192.168.30.3/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c078), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c0a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 29, 9, 6, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e7c0d8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-l75nv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008578000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l75nv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00416c130), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.146.0.115", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032c8000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416c1c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416c1e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00416c1e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00416c1ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000dbc020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.146.0.115", PodIP:"192.168.30.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.30.3"}}, StartTime:time.Date(2023, time.March, 29, 9, 5, 18, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032c80e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032c8150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://fbac22cefcba6bf67c9b91ff8cf6859f2a13714c81df8c23d3b3e8ff58066a66", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008578080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008578060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00416c26f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 09:06:00.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8732" for this suite. 03/29/23 09:06:00.873
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:00.875
Mar 29 09:06:00.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:06:00.876
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:00.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:00.886
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/29/23 09:06:00.888
Mar 29 09:06:00.891: INFO: Waiting up to 5m0s for pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81" in namespace "projected-9903" to be "running and ready"
Mar 29 09:06:00.892: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25031ms
Mar 29 09:06:00.892: INFO: The phase of Pod labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:06:02.894: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81": Phase="Running", Reason="", readiness=true. Elapsed: 2.00325479s
Mar 29 09:06:02.894: INFO: The phase of Pod labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81 is Running (Ready = true)
Mar 29 09:06:02.894: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81" satisfied condition "running and ready"
Mar 29 09:06:03.405: INFO: Successfully updated pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 09:06:07.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9903" for this suite. 03/29/23 09:06:07.418
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":314,"skipped":5779,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.546 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:00.875
    Mar 29 09:06:00.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:06:00.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:00.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:00.886
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/29/23 09:06:00.888
    Mar 29 09:06:00.891: INFO: Waiting up to 5m0s for pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81" in namespace "projected-9903" to be "running and ready"
    Mar 29 09:06:00.892: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25031ms
    Mar 29 09:06:00.892: INFO: The phase of Pod labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:06:02.894: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81": Phase="Running", Reason="", readiness=true. Elapsed: 2.00325479s
    Mar 29 09:06:02.894: INFO: The phase of Pod labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81 is Running (Ready = true)
    Mar 29 09:06:02.894: INFO: Pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81" satisfied condition "running and ready"
    Mar 29 09:06:03.405: INFO: Successfully updated pod "labelsupdate5cd417cb-359e-40d0-8d42-d162eebb4e81"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 09:06:07.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9903" for this suite. 03/29/23 09:06:07.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:07.421
Mar 29 09:06:07.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 09:06:07.422
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:07.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:07.429
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar 29 09:06:07.433: INFO: Waiting up to 2m0s for pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" in namespace "var-expansion-3096" to be "container 0 failed with reason CreateContainerConfigError"
Mar 29 09:06:07.434: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.166264ms
Mar 29 09:06:09.436: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003078616s
Mar 29 09:06:09.436: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 29 09:06:09.436: INFO: Deleting pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" in namespace "var-expansion-3096"
Mar 29 09:06:09.440: INFO: Wait up to 5m0s for pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 09:06:13.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3096" for this suite. 03/29/23 09:06:13.446
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":315,"skipped":5794,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.027 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:07.421
    Mar 29 09:06:07.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 09:06:07.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:07.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:07.429
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar 29 09:06:07.433: INFO: Waiting up to 2m0s for pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" in namespace "var-expansion-3096" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 29 09:06:07.434: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.166264ms
    Mar 29 09:06:09.436: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003078616s
    Mar 29 09:06:09.436: INFO: Pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 29 09:06:09.436: INFO: Deleting pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" in namespace "var-expansion-3096"
    Mar 29 09:06:09.440: INFO: Wait up to 5m0s for pod "var-expansion-609e023b-9351-4f31-8413-933d76e2cb54" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 09:06:13.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3096" for this suite. 03/29/23 09:06:13.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:13.449
Mar 29 09:06:13.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 09:06:13.45
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:13.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:13.457
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/29/23 09:06:13.459
Mar 29 09:06:13.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64" in namespace "downward-api-8716" to be "Succeeded or Failed"
Mar 29 09:06:13.464: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Pending", Reason="", readiness=false. Elapsed: 1.262528ms
Mar 29 09:06:15.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003881025s
Mar 29 09:06:17.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004265335s
STEP: Saw pod success 03/29/23 09:06:17.467
Mar 29 09:06:17.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64" satisfied condition "Succeeded or Failed"
Mar 29 09:06:17.469: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 container client-container: <nil>
STEP: delete the pod 03/29/23 09:06:17.475
Mar 29 09:06:17.482: INFO: Waiting for pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 to disappear
Mar 29 09:06:17.483: INFO: Pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 09:06:17.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8716" for this suite. 03/29/23 09:06:17.484
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":316,"skipped":5799,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:13.449
    Mar 29 09:06:13.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 09:06:13.45
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:13.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:13.457
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/29/23 09:06:13.459
    Mar 29 09:06:13.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64" in namespace "downward-api-8716" to be "Succeeded or Failed"
    Mar 29 09:06:13.464: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Pending", Reason="", readiness=false. Elapsed: 1.262528ms
    Mar 29 09:06:15.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003881025s
    Mar 29 09:06:17.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004265335s
    STEP: Saw pod success 03/29/23 09:06:17.467
    Mar 29 09:06:17.467: INFO: Pod "downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64" satisfied condition "Succeeded or Failed"
    Mar 29 09:06:17.469: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 container client-container: <nil>
    STEP: delete the pod 03/29/23 09:06:17.475
    Mar 29 09:06:17.482: INFO: Waiting for pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 to disappear
    Mar 29 09:06:17.483: INFO: Pod downwardapi-volume-bb124a1a-23bf-4970-925c-39ddc4d0aa64 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 09:06:17.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8716" for this suite. 03/29/23 09:06:17.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:17.487
Mar 29 09:06:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename statefulset 03/29/23 09:06:17.487
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:17.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:17.493
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1216 03/29/23 09:06:17.495
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1216 03/29/23 09:06:17.497
Mar 29 09:06:17.501: INFO: Found 0 stateful pods, waiting for 1
Mar 29 09:06:27.504: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/29/23 09:06:27.506
STEP: updating a scale subresource 03/29/23 09:06:27.508
STEP: verifying the statefulset Spec.Replicas was modified 03/29/23 09:06:27.511
STEP: Patch a scale subresource 03/29/23 09:06:27.512
STEP: verifying the statefulset Spec.Replicas was modified 03/29/23 09:06:27.516
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 29 09:06:27.521: INFO: Deleting all statefulset in ns statefulset-1216
Mar 29 09:06:27.522: INFO: Scaling statefulset ss to 0
Mar 29 09:06:37.534: INFO: Waiting for statefulset status.replicas updated to 0
Mar 29 09:06:37.535: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 29 09:06:37.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1216" for this suite. 03/29/23 09:06:37.542
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":317,"skipped":5801,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.058 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:17.487
    Mar 29 09:06:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename statefulset 03/29/23 09:06:17.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:17.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:17.493
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1216 03/29/23 09:06:17.495
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1216 03/29/23 09:06:17.497
    Mar 29 09:06:17.501: INFO: Found 0 stateful pods, waiting for 1
    Mar 29 09:06:27.504: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/29/23 09:06:27.506
    STEP: updating a scale subresource 03/29/23 09:06:27.508
    STEP: verifying the statefulset Spec.Replicas was modified 03/29/23 09:06:27.511
    STEP: Patch a scale subresource 03/29/23 09:06:27.512
    STEP: verifying the statefulset Spec.Replicas was modified 03/29/23 09:06:27.516
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 29 09:06:27.521: INFO: Deleting all statefulset in ns statefulset-1216
    Mar 29 09:06:27.522: INFO: Scaling statefulset ss to 0
    Mar 29 09:06:37.534: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 29 09:06:37.535: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 29 09:06:37.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1216" for this suite. 03/29/23 09:06:37.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:37.545
Mar 29 09:06:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:06:37.546
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:37.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:37.554
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:06:37.56
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:06:37.954
STEP: Deploying the webhook pod 03/29/23 09:06:37.958
STEP: Wait for the deployment to be ready 03/29/23 09:06:37.964
Mar 29 09:06:37.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:06:39.973
STEP: Verifying the service has paired with the endpoint 03/29/23 09:06:39.978
Mar 29 09:06:40.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/29/23 09:06:40.98
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/29/23 09:06:40.99
STEP: Creating a dummy validating-webhook-configuration object 03/29/23 09:06:40.997
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/29/23 09:06:41.001
STEP: Creating a dummy mutating-webhook-configuration object 03/29/23 09:06:41.003
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/29/23 09:06:41.007
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:06:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1112" for this suite. 03/29/23 09:06:41.019
STEP: Destroying namespace "webhook-1112-markers" for this suite. 03/29/23 09:06:41.022
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":318,"skipped":5810,"failed":0}
------------------------------
â€¢ [3.494 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:37.545
    Mar 29 09:06:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:06:37.546
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:37.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:37.554
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:06:37.56
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:06:37.954
    STEP: Deploying the webhook pod 03/29/23 09:06:37.958
    STEP: Wait for the deployment to be ready 03/29/23 09:06:37.964
    Mar 29 09:06:37.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:06:39.973
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:06:39.978
    Mar 29 09:06:40.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/29/23 09:06:40.98
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/29/23 09:06:40.99
    STEP: Creating a dummy validating-webhook-configuration object 03/29/23 09:06:40.997
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/29/23 09:06:41.001
    STEP: Creating a dummy mutating-webhook-configuration object 03/29/23 09:06:41.003
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/29/23 09:06:41.007
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:06:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1112" for this suite. 03/29/23 09:06:41.019
    STEP: Destroying namespace "webhook-1112-markers" for this suite. 03/29/23 09:06:41.022
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:41.04
Mar 29 09:06:41.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:06:41.04
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:41.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:41.049
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-aec195c9-64d6-4c72-92ff-fd52c40f6d9b 03/29/23 09:06:41.05
STEP: Creating a pod to test consume configMaps 03/29/23 09:06:41.052
Mar 29 09:06:41.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507" in namespace "projected-7007" to be "Succeeded or Failed"
Mar 29 09:06:41.059: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565141ms
Mar 29 09:06:43.062: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005491853s
Mar 29 09:06:45.061: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004495499s
STEP: Saw pod success 03/29/23 09:06:45.061
Mar 29 09:06:45.061: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507" satisfied condition "Succeeded or Failed"
Mar 29 09:06:45.063: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 09:06:45.065
Mar 29 09:06:45.072: INFO: Waiting for pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 to disappear
Mar 29 09:06:45.074: INFO: Pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 09:06:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7007" for this suite. 03/29/23 09:06:45.075
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":319,"skipped":5828,"failed":0}
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:41.04
    Mar 29 09:06:41.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:06:41.04
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:41.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:41.049
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-aec195c9-64d6-4c72-92ff-fd52c40f6d9b 03/29/23 09:06:41.05
    STEP: Creating a pod to test consume configMaps 03/29/23 09:06:41.052
    Mar 29 09:06:41.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507" in namespace "projected-7007" to be "Succeeded or Failed"
    Mar 29 09:06:41.059: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565141ms
    Mar 29 09:06:43.062: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005491853s
    Mar 29 09:06:45.061: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004495499s
    STEP: Saw pod success 03/29/23 09:06:45.061
    Mar 29 09:06:45.061: INFO: Pod "pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507" satisfied condition "Succeeded or Failed"
    Mar 29 09:06:45.063: INFO: Trying to get logs from node 10.146.0.116 pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 09:06:45.065
    Mar 29 09:06:45.072: INFO: Waiting for pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 to disappear
    Mar 29 09:06:45.074: INFO: Pod pod-projected-configmaps-4ec9a7d0-d5b9-4482-9d33-95884931b507 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 09:06:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7007" for this suite. 03/29/23 09:06:45.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:45.078
Mar 29 09:06:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/29/23 09:06:45.079
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:45.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:45.085
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/29/23 09:06:45.087
STEP: Creating hostNetwork=false pod 03/29/23 09:06:45.087
Mar 29 09:06:45.090: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1552" to be "running and ready"
Mar 29 09:06:45.091: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206777ms
Mar 29 09:06:45.091: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:06:47.093: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003246253s
Mar 29 09:06:47.093: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 29 09:06:47.093: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/29/23 09:06:47.095
Mar 29 09:06:47.099: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1552" to be "running and ready"
Mar 29 09:06:47.100: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346501ms
Mar 29 09:06:47.100: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:06:49.102: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003419497s
Mar 29 09:06:49.102: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 29 09:06:49.102: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/29/23 09:06:49.103
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/29/23 09:06:49.103
Mar 29 09:06:49.104: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.104: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.104: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 29 09:06:49.151: INFO: Exec stderr: ""
Mar 29 09:06:49.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.151: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.151: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 29 09:06:49.194: INFO: Exec stderr: ""
Mar 29 09:06:49.194: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.194: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.194: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 29 09:06:49.235: INFO: Exec stderr: ""
Mar 29 09:06:49.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.235: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.235: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 29 09:06:49.278: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/29/23 09:06:49.278
Mar 29 09:06:49.278: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.278: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.278: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 29 09:06:49.318: INFO: Exec stderr: ""
Mar 29 09:06:49.318: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.318: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.318: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 29 09:06:49.358: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/29/23 09:06:49.358
Mar 29 09:06:49.359: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.359: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.359: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 29 09:06:49.406: INFO: Exec stderr: ""
Mar 29 09:06:49.406: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.407: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.407: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 29 09:06:49.447: INFO: Exec stderr: ""
Mar 29 09:06:49.447: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.447: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.447: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 29 09:06:49.490: INFO: Exec stderr: ""
Mar 29 09:06:49.490: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 29 09:06:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:06:49.490: INFO: ExecWithOptions: Clientset creation
Mar 29 09:06:49.490: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 29 09:06:49.532: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar 29 09:06:49.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1552" for this suite. 03/29/23 09:06:49.535
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":320,"skipped":5839,"failed":0}
------------------------------
â€¢ [4.459 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:45.078
    Mar 29 09:06:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/29/23 09:06:45.079
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:45.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:45.085
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/29/23 09:06:45.087
    STEP: Creating hostNetwork=false pod 03/29/23 09:06:45.087
    Mar 29 09:06:45.090: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1552" to be "running and ready"
    Mar 29 09:06:45.091: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.206777ms
    Mar 29 09:06:45.091: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:06:47.093: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003246253s
    Mar 29 09:06:47.093: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 29 09:06:47.093: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/29/23 09:06:47.095
    Mar 29 09:06:47.099: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1552" to be "running and ready"
    Mar 29 09:06:47.100: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346501ms
    Mar 29 09:06:47.100: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:06:49.102: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003419497s
    Mar 29 09:06:49.102: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 29 09:06:49.102: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/29/23 09:06:49.103
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/29/23 09:06:49.103
    Mar 29 09:06:49.104: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.104: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.104: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 29 09:06:49.151: INFO: Exec stderr: ""
    Mar 29 09:06:49.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.151: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.151: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 29 09:06:49.194: INFO: Exec stderr: ""
    Mar 29 09:06:49.194: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.194: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.194: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 29 09:06:49.235: INFO: Exec stderr: ""
    Mar 29 09:06:49.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.235: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.235: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 29 09:06:49.278: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/29/23 09:06:49.278
    Mar 29 09:06:49.278: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.278: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.278: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 29 09:06:49.318: INFO: Exec stderr: ""
    Mar 29 09:06:49.318: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.318: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.318: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 29 09:06:49.358: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/29/23 09:06:49.358
    Mar 29 09:06:49.359: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.359: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.359: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 29 09:06:49.406: INFO: Exec stderr: ""
    Mar 29 09:06:49.406: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.407: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.407: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 29 09:06:49.447: INFO: Exec stderr: ""
    Mar 29 09:06:49.447: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.447: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.447: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 29 09:06:49.490: INFO: Exec stderr: ""
    Mar 29 09:06:49.490: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1552 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 29 09:06:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:06:49.490: INFO: ExecWithOptions: Clientset creation
    Mar 29 09:06:49.490: INFO: ExecWithOptions: execute(POST https://10.100.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1552/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 29 09:06:49.532: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar 29 09:06:49.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1552" for this suite. 03/29/23 09:06:49.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:49.538
Mar 29 09:06:49.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename downward-api 03/29/23 09:06:49.539
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:49.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:49.546
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/29/23 09:06:49.548
Mar 29 09:06:49.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5" in namespace "downward-api-1353" to be "Succeeded or Failed"
Mar 29 09:06:49.552: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.227797ms
Mar 29 09:06:51.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004026544s
Mar 29 09:06:53.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004000346s
STEP: Saw pod success 03/29/23 09:06:53.555
Mar 29 09:06:53.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5" satisfied condition "Succeeded or Failed"
Mar 29 09:06:53.557: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 container client-container: <nil>
STEP: delete the pod 03/29/23 09:06:53.559
Mar 29 09:06:53.565: INFO: Waiting for pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 to disappear
Mar 29 09:06:53.566: INFO: Pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 29 09:06:53.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1353" for this suite. 03/29/23 09:06:53.568
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":321,"skipped":5851,"failed":0}
------------------------------
â€¢ [4.032 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:49.538
    Mar 29 09:06:49.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename downward-api 03/29/23 09:06:49.539
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:49.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:49.546
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/29/23 09:06:49.548
    Mar 29 09:06:49.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5" in namespace "downward-api-1353" to be "Succeeded or Failed"
    Mar 29 09:06:49.552: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.227797ms
    Mar 29 09:06:51.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004026544s
    Mar 29 09:06:53.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004000346s
    STEP: Saw pod success 03/29/23 09:06:53.555
    Mar 29 09:06:53.555: INFO: Pod "downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5" satisfied condition "Succeeded or Failed"
    Mar 29 09:06:53.557: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 container client-container: <nil>
    STEP: delete the pod 03/29/23 09:06:53.559
    Mar 29 09:06:53.565: INFO: Waiting for pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 to disappear
    Mar 29 09:06:53.566: INFO: Pod downwardapi-volume-3a964b3f-41e7-4dff-9ada-4ff5b4945dd5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 29 09:06:53.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1353" for this suite. 03/29/23 09:06:53.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:06:53.571
Mar 29 09:06:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 09:06:53.572
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:53.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:53.58
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/29/23 09:06:53.581
STEP: submitting the pod to kubernetes 03/29/23 09:06:53.581
Mar 29 09:06:53.584: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" in namespace "pods-4621" to be "running and ready"
Mar 29 09:06:53.586: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268611ms
Mar 29 09:06:53.586: INFO: The phase of Pod pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:06:55.588: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003482565s
Mar 29 09:06:55.588: INFO: The phase of Pod pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b is Running (Ready = true)
Mar 29 09:06:55.588: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/29/23 09:06:55.589
STEP: updating the pod 03/29/23 09:06:55.591
Mar 29 09:06:56.098: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b"
Mar 29 09:06:56.098: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" in namespace "pods-4621" to be "terminated with reason DeadlineExceeded"
Mar 29 09:06:56.100: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 1.406668ms
Mar 29 09:06:58.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003774264s
Mar 29 09:07:00.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.003790982s
Mar 29 09:07:00.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 09:07:00.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4621" for this suite. 03/29/23 09:07:00.104
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":322,"skipped":5878,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.536 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:06:53.571
    Mar 29 09:06:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 09:06:53.572
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:06:53.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:06:53.58
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/29/23 09:06:53.581
    STEP: submitting the pod to kubernetes 03/29/23 09:06:53.581
    Mar 29 09:06:53.584: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" in namespace "pods-4621" to be "running and ready"
    Mar 29 09:06:53.586: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268611ms
    Mar 29 09:06:53.586: INFO: The phase of Pod pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:06:55.588: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003482565s
    Mar 29 09:06:55.588: INFO: The phase of Pod pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b is Running (Ready = true)
    Mar 29 09:06:55.588: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/29/23 09:06:55.589
    STEP: updating the pod 03/29/23 09:06:55.591
    Mar 29 09:06:56.098: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b"
    Mar 29 09:06:56.098: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" in namespace "pods-4621" to be "terminated with reason DeadlineExceeded"
    Mar 29 09:06:56.100: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 1.406668ms
    Mar 29 09:06:58.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Running", Reason="", readiness=true. Elapsed: 2.003774264s
    Mar 29 09:07:00.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.003790982s
    Mar 29 09:07:00.102: INFO: Pod "pod-update-activedeadlineseconds-ac9a8116-e986-4b02-87f5-73b5e8b9107b" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 09:07:00.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4621" for this suite. 03/29/23 09:07:00.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:07:00.108
Mar 29 09:07:00.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 09:07:00.109
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:07:00.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:07:00.117
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-a853c2d6-09b5-4954-a18a-dde148944466 03/29/23 09:07:00.12
STEP: Creating configMap with name cm-test-opt-upd-fb06f461-23c7-4c99-94df-1c9599a896e3 03/29/23 09:07:00.122
STEP: Creating the pod 03/29/23 09:07:00.124
Mar 29 09:07:00.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69" in namespace "configmap-2238" to be "running and ready"
Mar 29 09:07:00.130: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3574ms
Mar 29 09:07:00.130: INFO: The phase of Pod pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:07:02.132: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69": Phase="Running", Reason="", readiness=true. Elapsed: 2.003544741s
Mar 29 09:07:02.132: INFO: The phase of Pod pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69 is Running (Ready = true)
Mar 29 09:07:02.132: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a853c2d6-09b5-4954-a18a-dde148944466 03/29/23 09:07:02.14
STEP: Updating configmap cm-test-opt-upd-fb06f461-23c7-4c99-94df-1c9599a896e3 03/29/23 09:07:02.143
STEP: Creating configMap with name cm-test-opt-create-31da1182-4f2e-4d82-ba9d-782f05b538da 03/29/23 09:07:02.146
STEP: waiting to observe update in volume 03/29/23 09:07:02.148
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 09:08:18.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2238" for this suite. 03/29/23 09:08:18.331
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":323,"skipped":5918,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.225 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:07:00.108
    Mar 29 09:07:00.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 09:07:00.109
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:07:00.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:07:00.117
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-a853c2d6-09b5-4954-a18a-dde148944466 03/29/23 09:07:00.12
    STEP: Creating configMap with name cm-test-opt-upd-fb06f461-23c7-4c99-94df-1c9599a896e3 03/29/23 09:07:00.122
    STEP: Creating the pod 03/29/23 09:07:00.124
    Mar 29 09:07:00.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69" in namespace "configmap-2238" to be "running and ready"
    Mar 29 09:07:00.130: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3574ms
    Mar 29 09:07:00.130: INFO: The phase of Pod pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:07:02.132: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69": Phase="Running", Reason="", readiness=true. Elapsed: 2.003544741s
    Mar 29 09:07:02.132: INFO: The phase of Pod pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69 is Running (Ready = true)
    Mar 29 09:07:02.132: INFO: Pod "pod-configmaps-4a313980-0fb5-4800-9c7b-88b92a65db69" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a853c2d6-09b5-4954-a18a-dde148944466 03/29/23 09:07:02.14
    STEP: Updating configmap cm-test-opt-upd-fb06f461-23c7-4c99-94df-1c9599a896e3 03/29/23 09:07:02.143
    STEP: Creating configMap with name cm-test-opt-create-31da1182-4f2e-4d82-ba9d-782f05b538da 03/29/23 09:07:02.146
    STEP: waiting to observe update in volume 03/29/23 09:07:02.148
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 09:08:18.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2238" for this suite. 03/29/23 09:08:18.331
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:08:18.334
Mar 29 09:08:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename security-context-test 03/29/23 09:08:18.334
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:08:18.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:08:18.344
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar 29 09:08:18.348: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903" in namespace "security-context-test-1199" to be "Succeeded or Failed"
Mar 29 09:08:18.350: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Pending", Reason="", readiness=false. Elapsed: 1.116061ms
Mar 29 09:08:20.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004512312s
Mar 29 09:08:22.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004599845s
Mar 29 09:08:22.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 29 09:08:22.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1199" for this suite. 03/29/23 09:08:22.355
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":324,"skipped":5919,"failed":0}
------------------------------
â€¢ [4.025 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:08:18.334
    Mar 29 09:08:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename security-context-test 03/29/23 09:08:18.334
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:08:18.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:08:18.344
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar 29 09:08:18.348: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903" in namespace "security-context-test-1199" to be "Succeeded or Failed"
    Mar 29 09:08:18.350: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Pending", Reason="", readiness=false. Elapsed: 1.116061ms
    Mar 29 09:08:20.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004512312s
    Mar 29 09:08:22.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004599845s
    Mar 29 09:08:22.353: INFO: Pod "busybox-readonly-false-2c35e251-345d-41fb-9195-820d23462903" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 29 09:08:22.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1199" for this suite. 03/29/23 09:08:22.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:08:22.359
Mar 29 09:08:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename cronjob 03/29/23 09:08:22.36
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:08:22.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:08:22.369
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/29/23 09:08:22.37
STEP: Ensuring a job is scheduled 03/29/23 09:08:22.372
STEP: Ensuring exactly one is scheduled 03/29/23 09:09:00.376
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/29/23 09:09:00.377
STEP: Ensuring no more jobs are scheduled 03/29/23 09:09:00.378
STEP: Removing cronjob 03/29/23 09:14:00.382
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 29 09:14:00.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4354" for this suite. 03/29/23 09:14:00.387
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":325,"skipped":5951,"failed":0}
------------------------------
â€¢ [SLOW TEST] [338.031 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:08:22.359
    Mar 29 09:08:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename cronjob 03/29/23 09:08:22.36
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:08:22.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:08:22.369
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/29/23 09:08:22.37
    STEP: Ensuring a job is scheduled 03/29/23 09:08:22.372
    STEP: Ensuring exactly one is scheduled 03/29/23 09:09:00.376
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/29/23 09:09:00.377
    STEP: Ensuring no more jobs are scheduled 03/29/23 09:09:00.378
    STEP: Removing cronjob 03/29/23 09:14:00.382
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 29 09:14:00.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4354" for this suite. 03/29/23 09:14:00.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:00.392
Mar 29 09:14:00.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename limitrange 03/29/23 09:14:00.393
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:00.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:00.404
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/29/23 09:14:00.405
STEP: Setting up watch 03/29/23 09:14:00.405
STEP: Submitting a LimitRange 03/29/23 09:14:00.506
STEP: Verifying LimitRange creation was observed 03/29/23 09:14:00.509
STEP: Fetching the LimitRange to ensure it has proper values 03/29/23 09:14:00.509
Mar 29 09:14:00.510: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 29 09:14:00.510: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/29/23 09:14:00.51
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/29/23 09:14:00.513
Mar 29 09:14:00.515: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 29 09:14:00.515: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/29/23 09:14:00.515
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/29/23 09:14:00.518
Mar 29 09:14:00.519: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 29 09:14:00.519: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/29/23 09:14:00.519
STEP: Failing to create a Pod with more than max resources 03/29/23 09:14:00.52
STEP: Updating a LimitRange 03/29/23 09:14:00.521
STEP: Verifying LimitRange updating is effective 03/29/23 09:14:00.523
STEP: Creating a Pod with less than former min resources 03/29/23 09:14:02.526
STEP: Failing to create a Pod with more than max resources 03/29/23 09:14:02.529
STEP: Deleting a LimitRange 03/29/23 09:14:02.531
STEP: Verifying the LimitRange was deleted 03/29/23 09:14:02.534
Mar 29 09:14:07.539: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/29/23 09:14:07.539
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar 29 09:14:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5390" for this suite. 03/29/23 09:14:07.545
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":326,"skipped":5970,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.157 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:00.392
    Mar 29 09:14:00.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename limitrange 03/29/23 09:14:00.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:00.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:00.404
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/29/23 09:14:00.405
    STEP: Setting up watch 03/29/23 09:14:00.405
    STEP: Submitting a LimitRange 03/29/23 09:14:00.506
    STEP: Verifying LimitRange creation was observed 03/29/23 09:14:00.509
    STEP: Fetching the LimitRange to ensure it has proper values 03/29/23 09:14:00.509
    Mar 29 09:14:00.510: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 29 09:14:00.510: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/29/23 09:14:00.51
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/29/23 09:14:00.513
    Mar 29 09:14:00.515: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 29 09:14:00.515: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/29/23 09:14:00.515
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/29/23 09:14:00.518
    Mar 29 09:14:00.519: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 29 09:14:00.519: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/29/23 09:14:00.519
    STEP: Failing to create a Pod with more than max resources 03/29/23 09:14:00.52
    STEP: Updating a LimitRange 03/29/23 09:14:00.521
    STEP: Verifying LimitRange updating is effective 03/29/23 09:14:00.523
    STEP: Creating a Pod with less than former min resources 03/29/23 09:14:02.526
    STEP: Failing to create a Pod with more than max resources 03/29/23 09:14:02.529
    STEP: Deleting a LimitRange 03/29/23 09:14:02.531
    STEP: Verifying the LimitRange was deleted 03/29/23 09:14:02.534
    Mar 29 09:14:07.539: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/29/23 09:14:07.539
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar 29 09:14:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-5390" for this suite. 03/29/23 09:14:07.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:07.549
Mar 29 09:14:07.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:14:07.55
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:07.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:07.556
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:14:07.563
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:14:08.257
STEP: Deploying the webhook pod 03/29/23 09:14:08.262
STEP: Wait for the deployment to be ready 03/29/23 09:14:08.267
Mar 29 09:14:08.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:14:10.276
STEP: Verifying the service has paired with the endpoint 03/29/23 09:14:10.282
Mar 29 09:14:11.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/29/23 09:14:11.312
STEP: Creating a configMap that should be mutated 03/29/23 09:14:11.319
STEP: Deleting the collection of validation webhooks 03/29/23 09:14:11.331
STEP: Creating a configMap that should not be mutated 03/29/23 09:14:11.348
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:14:11.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9941" for this suite. 03/29/23 09:14:11.355
STEP: Destroying namespace "webhook-9941-markers" for this suite. 03/29/23 09:14:11.357
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":327,"skipped":5978,"failed":0}
------------------------------
â€¢ [3.824 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:07.549
    Mar 29 09:14:07.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:14:07.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:07.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:07.556
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:14:07.563
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:14:08.257
    STEP: Deploying the webhook pod 03/29/23 09:14:08.262
    STEP: Wait for the deployment to be ready 03/29/23 09:14:08.267
    Mar 29 09:14:08.271: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:14:10.276
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:14:10.282
    Mar 29 09:14:11.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/29/23 09:14:11.312
    STEP: Creating a configMap that should be mutated 03/29/23 09:14:11.319
    STEP: Deleting the collection of validation webhooks 03/29/23 09:14:11.331
    STEP: Creating a configMap that should not be mutated 03/29/23 09:14:11.348
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:14:11.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9941" for this suite. 03/29/23 09:14:11.355
    STEP: Destroying namespace "webhook-9941-markers" for this suite. 03/29/23 09:14:11.357
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:11.374
Mar 29 09:14:11.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename csistoragecapacity 03/29/23 09:14:11.375
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:11.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:11.384
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/29/23 09:14:11.386
STEP: getting /apis/storage.k8s.io 03/29/23 09:14:11.387
STEP: getting /apis/storage.k8s.io/v1 03/29/23 09:14:11.388
STEP: creating 03/29/23 09:14:11.388
STEP: watching 03/29/23 09:14:11.396
Mar 29 09:14:11.396: INFO: starting watch
STEP: getting 03/29/23 09:14:11.4
STEP: listing in namespace 03/29/23 09:14:11.402
STEP: listing across namespaces 03/29/23 09:14:11.403
STEP: patching 03/29/23 09:14:11.404
STEP: updating 03/29/23 09:14:11.407
Mar 29 09:14:11.409: INFO: waiting for watch events with expected annotations in namespace
Mar 29 09:14:11.409: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/29/23 09:14:11.409
STEP: deleting a collection 03/29/23 09:14:11.414
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar 29 09:14:11.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5534" for this suite. 03/29/23 09:14:11.422
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":328,"skipped":5988,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:11.374
    Mar 29 09:14:11.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename csistoragecapacity 03/29/23 09:14:11.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:11.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:11.384
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/29/23 09:14:11.386
    STEP: getting /apis/storage.k8s.io 03/29/23 09:14:11.387
    STEP: getting /apis/storage.k8s.io/v1 03/29/23 09:14:11.388
    STEP: creating 03/29/23 09:14:11.388
    STEP: watching 03/29/23 09:14:11.396
    Mar 29 09:14:11.396: INFO: starting watch
    STEP: getting 03/29/23 09:14:11.4
    STEP: listing in namespace 03/29/23 09:14:11.402
    STEP: listing across namespaces 03/29/23 09:14:11.403
    STEP: patching 03/29/23 09:14:11.404
    STEP: updating 03/29/23 09:14:11.407
    Mar 29 09:14:11.409: INFO: waiting for watch events with expected annotations in namespace
    Mar 29 09:14:11.409: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/29/23 09:14:11.409
    STEP: deleting a collection 03/29/23 09:14:11.414
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar 29 09:14:11.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-5534" for this suite. 03/29/23 09:14:11.422
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:11.425
Mar 29 09:14:11.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 09:14:11.425
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:11.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:11.432
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-6938 03/29/23 09:14:11.433
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[] 03/29/23 09:14:11.438
Mar 29 09:14:11.442: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6938 03/29/23 09:14:11.442
Mar 29 09:14:11.445: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6938" to be "running and ready"
Mar 29 09:14:11.446: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.304358ms
Mar 29 09:14:11.446: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:14:13.448: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003224612s
Mar 29 09:14:13.448: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 29 09:14:13.448: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod1:[100]] 03/29/23 09:14:13.45
Mar 29 09:14:13.454: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6938 03/29/23 09:14:13.454
Mar 29 09:14:13.458: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6938" to be "running and ready"
Mar 29 09:14:13.459: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2718ms
Mar 29 09:14:13.459: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:14:15.462: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004345974s
Mar 29 09:14:15.462: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 29 09:14:15.462: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod1:[100] pod2:[101]] 03/29/23 09:14:15.463
Mar 29 09:14:15.469: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/29/23 09:14:15.469
Mar 29 09:14:15.469: INFO: Creating new exec pod
Mar 29 09:14:15.471: INFO: Waiting up to 5m0s for pod "execpod89mkd" in namespace "services-6938" to be "running"
Mar 29 09:14:15.472: INFO: Pod "execpod89mkd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.257582ms
Mar 29 09:14:17.475: INFO: Pod "execpod89mkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00407834s
Mar 29 09:14:17.475: INFO: Pod "execpod89mkd" satisfied condition "running"
Mar 29 09:14:18.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar 29 09:14:18.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 29 09:14:18.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:14:18.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.119.94 80'
Mar 29 09:14:18.651: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.119.94 80\nConnection to 10.100.119.94 80 port [tcp/http] succeeded!\n"
Mar 29 09:14:18.651: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:14:18.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar 29 09:14:18.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 29 09:14:18.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:14:18.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.119.94 81'
Mar 29 09:14:18.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.119.94 81\nConnection to 10.100.119.94 81 port [tcp/*] succeeded!\n"
Mar 29 09:14:18.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6938 03/29/23 09:14:18.811
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod2:[101]] 03/29/23 09:14:18.822
Mar 29 09:14:18.833: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6938 03/29/23 09:14:18.833
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[] 03/29/23 09:14:18.84
Mar 29 09:14:19.848: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 09:14:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6938" for this suite. 03/29/23 09:14:19.86
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":329,"skipped":5990,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.438 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:11.425
    Mar 29 09:14:11.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 09:14:11.425
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:11.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:11.432
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-6938 03/29/23 09:14:11.433
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[] 03/29/23 09:14:11.438
    Mar 29 09:14:11.442: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6938 03/29/23 09:14:11.442
    Mar 29 09:14:11.445: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6938" to be "running and ready"
    Mar 29 09:14:11.446: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.304358ms
    Mar 29 09:14:11.446: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:14:13.448: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003224612s
    Mar 29 09:14:13.448: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 29 09:14:13.448: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod1:[100]] 03/29/23 09:14:13.45
    Mar 29 09:14:13.454: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-6938 03/29/23 09:14:13.454
    Mar 29 09:14:13.458: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6938" to be "running and ready"
    Mar 29 09:14:13.459: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2718ms
    Mar 29 09:14:13.459: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:14:15.462: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004345974s
    Mar 29 09:14:15.462: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 29 09:14:15.462: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod1:[100] pod2:[101]] 03/29/23 09:14:15.463
    Mar 29 09:14:15.469: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/29/23 09:14:15.469
    Mar 29 09:14:15.469: INFO: Creating new exec pod
    Mar 29 09:14:15.471: INFO: Waiting up to 5m0s for pod "execpod89mkd" in namespace "services-6938" to be "running"
    Mar 29 09:14:15.472: INFO: Pod "execpod89mkd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.257582ms
    Mar 29 09:14:17.475: INFO: Pod "execpod89mkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00407834s
    Mar 29 09:14:17.475: INFO: Pod "execpod89mkd" satisfied condition "running"
    Mar 29 09:14:18.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar 29 09:14:18.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 29 09:14:18.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:14:18.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.119.94 80'
    Mar 29 09:14:18.651: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.119.94 80\nConnection to 10.100.119.94 80 port [tcp/http] succeeded!\n"
    Mar 29 09:14:18.651: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:14:18.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar 29 09:14:18.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 29 09:14:18.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:14:18.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6938 exec execpod89mkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.119.94 81'
    Mar 29 09:14:18.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.119.94 81\nConnection to 10.100.119.94 81 port [tcp/*] succeeded!\n"
    Mar 29 09:14:18.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-6938 03/29/23 09:14:18.811
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[pod2:[101]] 03/29/23 09:14:18.822
    Mar 29 09:14:18.833: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-6938 03/29/23 09:14:18.833
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6938 to expose endpoints map[] 03/29/23 09:14:18.84
    Mar 29 09:14:19.848: INFO: successfully validated that service multi-endpoint-test in namespace services-6938 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 09:14:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6938" for this suite. 03/29/23 09:14:19.86
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:19.863
Mar 29 09:14:19.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename var-expansion 03/29/23 09:14:19.863
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:19.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:19.872
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/29/23 09:14:19.873
Mar 29 09:14:19.877: INFO: Waiting up to 5m0s for pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de" in namespace "var-expansion-5375" to be "Succeeded or Failed"
Mar 29 09:14:19.879: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361315ms
Mar 29 09:14:21.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00350538s
Mar 29 09:14:23.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003345835s
STEP: Saw pod success 03/29/23 09:14:23.881
Mar 29 09:14:23.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de" satisfied condition "Succeeded or Failed"
Mar 29 09:14:23.882: INFO: Trying to get logs from node 10.146.0.115 pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de container dapi-container: <nil>
STEP: delete the pod 03/29/23 09:14:23.89
Mar 29 09:14:23.896: INFO: Waiting for pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de to disappear
Mar 29 09:14:23.897: INFO: Pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 29 09:14:23.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5375" for this suite. 03/29/23 09:14:23.899
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":330,"skipped":6002,"failed":0}
------------------------------
â€¢ [4.039 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:19.863
    Mar 29 09:14:19.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename var-expansion 03/29/23 09:14:19.863
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:19.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:19.872
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/29/23 09:14:19.873
    Mar 29 09:14:19.877: INFO: Waiting up to 5m0s for pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de" in namespace "var-expansion-5375" to be "Succeeded or Failed"
    Mar 29 09:14:19.879: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361315ms
    Mar 29 09:14:21.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00350538s
    Mar 29 09:14:23.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003345835s
    STEP: Saw pod success 03/29/23 09:14:23.881
    Mar 29 09:14:23.881: INFO: Pod "var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de" satisfied condition "Succeeded or Failed"
    Mar 29 09:14:23.882: INFO: Trying to get logs from node 10.146.0.115 pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de container dapi-container: <nil>
    STEP: delete the pod 03/29/23 09:14:23.89
    Mar 29 09:14:23.896: INFO: Waiting for pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de to disappear
    Mar 29 09:14:23.897: INFO: Pod var-expansion-02cb9d87-787d-449f-8205-0b06ceeb21de no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 29 09:14:23.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5375" for this suite. 03/29/23 09:14:23.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:23.902
Mar 29 09:14:23.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:14:23.902
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:23.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:23.909
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:14:23.915
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:14:24.076
STEP: Deploying the webhook pod 03/29/23 09:14:24.079
STEP: Wait for the deployment to be ready 03/29/23 09:14:24.085
Mar 29 09:14:24.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:14:26.094
STEP: Verifying the service has paired with the endpoint 03/29/23 09:14:26.1
Mar 29 09:14:27.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/29/23 09:14:27.103
STEP: create a pod that should be updated by the webhook 03/29/23 09:14:27.111
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:14:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9674" for this suite. 03/29/23 09:14:27.124
STEP: Destroying namespace "webhook-9674-markers" for this suite. 03/29/23 09:14:27.128
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":331,"skipped":6009,"failed":0}
------------------------------
â€¢ [3.245 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:23.902
    Mar 29 09:14:23.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:14:23.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:23.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:23.909
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:14:23.915
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:14:24.076
    STEP: Deploying the webhook pod 03/29/23 09:14:24.079
    STEP: Wait for the deployment to be ready 03/29/23 09:14:24.085
    Mar 29 09:14:24.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:14:26.094
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:14:26.1
    Mar 29 09:14:27.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/29/23 09:14:27.103
    STEP: create a pod that should be updated by the webhook 03/29/23 09:14:27.111
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:14:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9674" for this suite. 03/29/23 09:14:27.124
    STEP: Destroying namespace "webhook-9674-markers" for this suite. 03/29/23 09:14:27.128
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:27.148
Mar 29 09:14:27.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 09:14:27.148
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:27.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:27.159
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar 29 09:14:27.165: INFO: Waiting up to 5m0s for pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f" in namespace "container-probe-2716" to be "running and ready"
Mar 29 09:14:27.167: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645137ms
Mar 29 09:14:27.167: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:14:29.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 2.003510728s
Mar 29 09:14:29.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:31.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004367934s
Mar 29 09:14:31.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:33.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 6.004108764s
Mar 29 09:14:33.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:35.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 8.004860127s
Mar 29 09:14:35.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:37.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 10.003622676s
Mar 29 09:14:37.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:39.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 12.003880203s
Mar 29 09:14:39.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:41.171: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 14.005604268s
Mar 29 09:14:41.171: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:43.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 16.00461851s
Mar 29 09:14:43.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:45.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 18.004617142s
Mar 29 09:14:45.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:47.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 20.005348738s
Mar 29 09:14:47.171: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
Mar 29 09:14:49.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=true. Elapsed: 22.00454591s
Mar 29 09:14:49.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = true)
Mar 29 09:14:49.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f" satisfied condition "running and ready"
Mar 29 09:14:49.171: INFO: Container started at 2023-03-29 09:14:27 +0000 UTC, pod became ready at 2023-03-29 09:14:47 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 09:14:49.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2716" for this suite. 03/29/23 09:14:49.173
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":332,"skipped":6022,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.028 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:27.148
    Mar 29 09:14:27.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 09:14:27.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:27.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:27.159
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar 29 09:14:27.165: INFO: Waiting up to 5m0s for pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f" in namespace "container-probe-2716" to be "running and ready"
    Mar 29 09:14:27.167: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645137ms
    Mar 29 09:14:27.167: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:14:29.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 2.003510728s
    Mar 29 09:14:29.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:31.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004367934s
    Mar 29 09:14:31.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:33.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 6.004108764s
    Mar 29 09:14:33.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:35.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 8.004860127s
    Mar 29 09:14:35.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:37.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 10.003622676s
    Mar 29 09:14:37.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:39.169: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 12.003880203s
    Mar 29 09:14:39.169: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:41.171: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 14.005604268s
    Mar 29 09:14:41.171: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:43.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 16.00461851s
    Mar 29 09:14:43.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:45.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 18.004617142s
    Mar 29 09:14:45.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:47.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=false. Elapsed: 20.005348738s
    Mar 29 09:14:47.171: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = false)
    Mar 29 09:14:49.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f": Phase="Running", Reason="", readiness=true. Elapsed: 22.00454591s
    Mar 29 09:14:49.170: INFO: The phase of Pod test-webserver-68d04677-2769-4115-9266-b59f28ee695f is Running (Ready = true)
    Mar 29 09:14:49.170: INFO: Pod "test-webserver-68d04677-2769-4115-9266-b59f28ee695f" satisfied condition "running and ready"
    Mar 29 09:14:49.171: INFO: Container started at 2023-03-29 09:14:27 +0000 UTC, pod became ready at 2023-03-29 09:14:47 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 09:14:49.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2716" for this suite. 03/29/23 09:14:49.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:14:49.176
Mar 29 09:14:49.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename deployment 03/29/23 09:14:49.177
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:49.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:49.184
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 29 09:14:49.189: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 29 09:14:54.191: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/29/23 09:14:54.191
Mar 29 09:14:54.191: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 29 09:14:56.193: INFO: Creating deployment "test-rollover-deployment"
Mar 29 09:14:56.197: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 29 09:14:58.200: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 29 09:14:58.203: INFO: Ensure that both replica sets have 1 created replica
Mar 29 09:14:58.206: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 29 09:14:58.210: INFO: Updating deployment test-rollover-deployment
Mar 29 09:14:58.210: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 29 09:15:00.213: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 29 09:15:00.216: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 29 09:15:00.218: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 09:15:00.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 09:15:02.223: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 09:15:02.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 09:15:04.222: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 09:15:04.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 09:15:06.224: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 09:15:06.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 09:15:08.224: INFO: all replica sets need to contain the pod-template-hash label
Mar 29 09:15:08.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 29 09:15:10.222: INFO: 
Mar 29 09:15:10.222: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 29 09:15:10.226: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5874  7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 32721 2 2023-03-29 09:14:56 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb1b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 09:14:56 +0000 UTC,LastTransitionTime:2023-03-29 09:14:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-29 09:15:09 +0000 UTC,LastTransitionTime:2023-03-29 09:14:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 29 09:15:10.227: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5874  a0c98b11-d6b6-4966-8c83-497c57f960e9 32711 2 2023-03-29 09:14:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef3d7 0xc0040ef3d8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ef488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 29 09:15:10.227: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 29 09:15:10.227: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5874  8196c4e1-2827-4689-888d-220a523ff073 32720 2 2023-03-29 09:14:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef187 0xc0040ef188}] [] [{e2e.test Update apps/v1 2023-03-29 09:14:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040ef248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 09:15:10.227: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5874  8b7ba661-6d45-4dbc-823d-7ed1e3c743f3 32669 2 2023-03-29 09:14:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef2b7 0xc0040ef2b8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ef368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 29 09:15:10.229: INFO: Pod "test-rollover-deployment-6d45fd857b-xvbvd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-xvbvd test-rollover-deployment-6d45fd857b- deployment-5874  deea951a-b5c6-4c25-85b8-3d47b3c2b347 32691 0 2023-03-29 09:14:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:d6dd7aba225ecabeacc565bce0ca49f9223dad1e71e28e68b93ba7936109690b cni.projectcalico.org/podIP:192.168.30.21/32 cni.projectcalico.org/podIPs:192.168.30.21/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a0c98b11-d6b6-4966-8c83-497c57f960e9 0xc003eb1ef7 0xc003eb1ef8}] [] [{calico Update v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0c98b11-d6b6-4966-8c83-497c57f960e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 09:14:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvtnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvtnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.21,StartTime:2023-03-29 09:14:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 09:14:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2e9d3a553fb8a9b38bd69e0bfe4e752c5e928a324398131abdefd2a4b4bb0c9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 29 09:15:10.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5874" for this suite. 03/29/23 09:15:10.23
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":333,"skipped":6033,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.057 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:14:49.176
    Mar 29 09:14:49.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename deployment 03/29/23 09:14:49.177
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:14:49.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:14:49.184
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 29 09:14:49.189: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 29 09:14:54.191: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/29/23 09:14:54.191
    Mar 29 09:14:54.191: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 29 09:14:56.193: INFO: Creating deployment "test-rollover-deployment"
    Mar 29 09:14:56.197: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 29 09:14:58.200: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 29 09:14:58.203: INFO: Ensure that both replica sets have 1 created replica
    Mar 29 09:14:58.206: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 29 09:14:58.210: INFO: Updating deployment test-rollover-deployment
    Mar 29 09:14:58.210: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 29 09:15:00.213: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 29 09:15:00.216: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 29 09:15:00.218: INFO: all replica sets need to contain the pod-template-hash label
    Mar 29 09:15:00.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 09:15:02.223: INFO: all replica sets need to contain the pod-template-hash label
    Mar 29 09:15:02.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 09:15:04.222: INFO: all replica sets need to contain the pod-template-hash label
    Mar 29 09:15:04.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 09:15:06.224: INFO: all replica sets need to contain the pod-template-hash label
    Mar 29 09:15:06.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 09:15:08.224: INFO: all replica sets need to contain the pod-template-hash label
    Mar 29 09:15:08.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 29, 9, 14, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 29, 9, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 29 09:15:10.222: INFO: 
    Mar 29 09:15:10.222: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 29 09:15:10.226: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5874  7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 32721 2 2023-03-29 09:14:56 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb1b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-29 09:14:56 +0000 UTC,LastTransitionTime:2023-03-29 09:14:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-29 09:15:09 +0000 UTC,LastTransitionTime:2023-03-29 09:14:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 29 09:15:10.227: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5874  a0c98b11-d6b6-4966-8c83-497c57f960e9 32711 2 2023-03-29 09:14:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef3d7 0xc0040ef3d8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ef488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 09:15:10.227: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 29 09:15:10.227: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5874  8196c4e1-2827-4689-888d-220a523ff073 32720 2 2023-03-29 09:14:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef187 0xc0040ef188}] [] [{e2e.test Update apps/v1 2023-03-29 09:14:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:15:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040ef248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 09:15:10.227: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5874  8b7ba661-6d45-4dbc-823d-7ed1e3c743f3 32669 2 2023-03-29 09:14:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e 0xc0040ef2b7 0xc0040ef2b8}] [] [{kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f7907d6-37c8-4d68-abce-a8b7f8eb5f3e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ef368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 29 09:15:10.229: INFO: Pod "test-rollover-deployment-6d45fd857b-xvbvd" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-xvbvd test-rollover-deployment-6d45fd857b- deployment-5874  deea951a-b5c6-4c25-85b8-3d47b3c2b347 32691 0 2023-03-29 09:14:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:d6dd7aba225ecabeacc565bce0ca49f9223dad1e71e28e68b93ba7936109690b cni.projectcalico.org/podIP:192.168.30.21/32 cni.projectcalico.org/podIPs:192.168.30.21/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a0c98b11-d6b6-4966-8c83-497c57f960e9 0xc003eb1ef7 0xc003eb1ef8}] [] [{calico Update v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-29 09:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0c98b11-d6b6-4966-8c83-497c57f960e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-29 09:14:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.30.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvtnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvtnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.146.0.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-29 09:14:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.146.0.115,PodIP:192.168.30.21,StartTime:2023-03-29 09:14:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-29 09:14:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2e9d3a553fb8a9b38bd69e0bfe4e752c5e928a324398131abdefd2a4b4bb0c9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.30.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 29 09:15:10.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5874" for this suite. 03/29/23 09:15:10.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:10.233
Mar 29 09:15:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename emptydir 03/29/23 09:15:10.234
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:10.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:10.243
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/29/23 09:15:10.244
Mar 29 09:15:10.247: INFO: Waiting up to 5m0s for pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec" in namespace "emptydir-28" to be "Succeeded or Failed"
Mar 29 09:15:10.248: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.257057ms
Mar 29 09:15:12.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004208111s
Mar 29 09:15:14.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004183795s
STEP: Saw pod success 03/29/23 09:15:14.251
Mar 29 09:15:14.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec" satisfied condition "Succeeded or Failed"
Mar 29 09:15:14.253: INFO: Trying to get logs from node 10.146.0.115 pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec container test-container: <nil>
STEP: delete the pod 03/29/23 09:15:14.255
Mar 29 09:15:14.261: INFO: Waiting for pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec to disappear
Mar 29 09:15:14.263: INFO: Pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 29 09:15:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-28" for this suite. 03/29/23 09:15:14.264
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6044,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:10.233
    Mar 29 09:15:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename emptydir 03/29/23 09:15:10.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:10.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:10.243
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/29/23 09:15:10.244
    Mar 29 09:15:10.247: INFO: Waiting up to 5m0s for pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec" in namespace "emptydir-28" to be "Succeeded or Failed"
    Mar 29 09:15:10.248: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.257057ms
    Mar 29 09:15:12.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004208111s
    Mar 29 09:15:14.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004183795s
    STEP: Saw pod success 03/29/23 09:15:14.251
    Mar 29 09:15:14.251: INFO: Pod "pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec" satisfied condition "Succeeded or Failed"
    Mar 29 09:15:14.253: INFO: Trying to get logs from node 10.146.0.115 pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec container test-container: <nil>
    STEP: delete the pod 03/29/23 09:15:14.255
    Mar 29 09:15:14.261: INFO: Waiting for pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec to disappear
    Mar 29 09:15:14.263: INFO: Pod pod-1d6e7336-8ca7-4293-86e2-c0cf7903c7ec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 29 09:15:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-28" for this suite. 03/29/23 09:15:14.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:14.268
Mar 29 09:15:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 09:15:14.268
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:14.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:14.275
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/29/23 09:15:14.276
Mar 29 09:15:14.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 create -f -'
Mar 29 09:15:14.884: INFO: stderr: ""
Mar 29 09:15:14.884: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/29/23 09:15:14.884
Mar 29 09:15:14.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 diff -f -'
Mar 29 09:15:15.010: INFO: rc: 1
Mar 29 09:15:15.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 delete -f -'
Mar 29 09:15:15.052: INFO: stderr: ""
Mar 29 09:15:15.052: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 09:15:15.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-349" for this suite. 03/29/23 09:15:15.055
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":335,"skipped":6073,"failed":0}
------------------------------
â€¢ [0.789 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:14.268
    Mar 29 09:15:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 09:15:14.268
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:14.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:14.275
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/29/23 09:15:14.276
    Mar 29 09:15:14.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 create -f -'
    Mar 29 09:15:14.884: INFO: stderr: ""
    Mar 29 09:15:14.884: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/29/23 09:15:14.884
    Mar 29 09:15:14.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 diff -f -'
    Mar 29 09:15:15.010: INFO: rc: 1
    Mar 29 09:15:15.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-349 delete -f -'
    Mar 29 09:15:15.052: INFO: stderr: ""
    Mar 29 09:15:15.052: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 09:15:15.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-349" for this suite. 03/29/23 09:15:15.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:15.058
Mar 29 09:15:15.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename gc 03/29/23 09:15:15.058
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:15.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:15.067
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/29/23 09:15:15.07
STEP: create the rc2 03/29/23 09:15:15.072
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/29/23 09:15:20.078
STEP: delete the rc simpletest-rc-to-be-deleted 03/29/23 09:15:20.297
STEP: wait for the rc to be deleted 03/29/23 09:15:20.3
Mar 29 09:15:25.308: INFO: 73 pods remaining
Mar 29 09:15:25.308: INFO: 73 pods has nil DeletionTimestamp
Mar 29 09:15:25.308: INFO: 
STEP: Gathering metrics 03/29/23 09:15:30.308
W0329 09:15:30.311628      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 29 09:15:30.311: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 29 09:15:30.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gmpf" in namespace "gc-7793"
Mar 29 09:15:30.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ts7t" in namespace "gc-7793"
Mar 29 09:15:30.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vghs" in namespace "gc-7793"
Mar 29 09:15:30.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vk9z" in namespace "gc-7793"
Mar 29 09:15:30.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-44z2q" in namespace "gc-7793"
Mar 29 09:15:30.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b7vv" in namespace "gc-7793"
Mar 29 09:15:30.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c8h4" in namespace "gc-7793"
Mar 29 09:15:30.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t79j" in namespace "gc-7793"
Mar 29 09:15:30.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-59wbl" in namespace "gc-7793"
Mar 29 09:15:30.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cvll" in namespace "gc-7793"
Mar 29 09:15:30.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wzk5" in namespace "gc-7793"
Mar 29 09:15:30.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-6248p" in namespace "gc-7793"
Mar 29 09:15:30.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-688tm" in namespace "gc-7793"
Mar 29 09:15:30.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-69wfl" in namespace "gc-7793"
Mar 29 09:15:30.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h2gx" in namespace "gc-7793"
Mar 29 09:15:30.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwjq" in namespace "gc-7793"
Mar 29 09:15:30.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s4xw" in namespace "gc-7793"
Mar 29 09:15:30.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s882" in namespace "gc-7793"
Mar 29 09:15:30.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-74lg8" in namespace "gc-7793"
Mar 29 09:15:30.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-795qq" in namespace "gc-7793"
Mar 29 09:15:30.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b8h5" in namespace "gc-7793"
Mar 29 09:15:30.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-85nxb" in namespace "gc-7793"
Mar 29 09:15:30.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-882zt" in namespace "gc-7793"
Mar 29 09:15:30.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ctpg" in namespace "gc-7793"
Mar 29 09:15:30.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mhxv" in namespace "gc-7793"
Mar 29 09:15:30.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-92swk" in namespace "gc-7793"
Mar 29 09:15:30.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-97d2b" in namespace "gc-7793"
Mar 29 09:15:30.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-98tpw" in namespace "gc-7793"
Mar 29 09:15:30.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-9grvb" in namespace "gc-7793"
Mar 29 09:15:30.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ms6p" in namespace "gc-7793"
Mar 29 09:15:30.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rknv" in namespace "gc-7793"
Mar 29 09:15:30.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2vh6" in namespace "gc-7793"
Mar 29 09:15:30.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgznv" in namespace "gc-7793"
Mar 29 09:15:30.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqb4" in namespace "gc-7793"
Mar 29 09:15:30.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpbgh" in namespace "gc-7793"
Mar 29 09:15:30.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccqh9" in namespace "gc-7793"
Mar 29 09:15:30.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl7zg" in namespace "gc-7793"
Mar 29 09:15:30.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvtbb" in namespace "gc-7793"
Mar 29 09:15:30.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddx8n" in namespace "gc-7793"
Mar 29 09:15:30.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkcdz" in namespace "gc-7793"
Mar 29 09:15:30.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmdvn" in namespace "gc-7793"
Mar 29 09:15:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpmn4" in namespace "gc-7793"
Mar 29 09:15:30.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfbh" in namespace "gc-7793"
Mar 29 09:15:30.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzgcp" in namespace "gc-7793"
Mar 29 09:15:30.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7gg6" in namespace "gc-7793"
Mar 29 09:15:30.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8csv" in namespace "gc-7793"
Mar 29 09:15:30.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9c7l" in namespace "gc-7793"
Mar 29 09:15:30.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdj9h" in namespace "gc-7793"
Mar 29 09:15:30.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhlpc" in namespace "gc-7793"
Mar 29 09:15:30.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj26l" in namespace "gc-7793"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 29 09:15:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7793" for this suite. 03/29/23 09:15:30.981
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":336,"skipped":6083,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.930 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:15.058
    Mar 29 09:15:15.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename gc 03/29/23 09:15:15.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:15.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:15.067
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/29/23 09:15:15.07
    STEP: create the rc2 03/29/23 09:15:15.072
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/29/23 09:15:20.078
    STEP: delete the rc simpletest-rc-to-be-deleted 03/29/23 09:15:20.297
    STEP: wait for the rc to be deleted 03/29/23 09:15:20.3
    Mar 29 09:15:25.308: INFO: 73 pods remaining
    Mar 29 09:15:25.308: INFO: 73 pods has nil DeletionTimestamp
    Mar 29 09:15:25.308: INFO: 
    STEP: Gathering metrics 03/29/23 09:15:30.308
    W0329 09:15:30.311628      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 29 09:15:30.311: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 29 09:15:30.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gmpf" in namespace "gc-7793"
    Mar 29 09:15:30.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ts7t" in namespace "gc-7793"
    Mar 29 09:15:30.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vghs" in namespace "gc-7793"
    Mar 29 09:15:30.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vk9z" in namespace "gc-7793"
    Mar 29 09:15:30.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-44z2q" in namespace "gc-7793"
    Mar 29 09:15:30.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b7vv" in namespace "gc-7793"
    Mar 29 09:15:30.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c8h4" in namespace "gc-7793"
    Mar 29 09:15:30.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t79j" in namespace "gc-7793"
    Mar 29 09:15:30.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-59wbl" in namespace "gc-7793"
    Mar 29 09:15:30.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cvll" in namespace "gc-7793"
    Mar 29 09:15:30.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wzk5" in namespace "gc-7793"
    Mar 29 09:15:30.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-6248p" in namespace "gc-7793"
    Mar 29 09:15:30.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-688tm" in namespace "gc-7793"
    Mar 29 09:15:30.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-69wfl" in namespace "gc-7793"
    Mar 29 09:15:30.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h2gx" in namespace "gc-7793"
    Mar 29 09:15:30.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwjq" in namespace "gc-7793"
    Mar 29 09:15:30.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s4xw" in namespace "gc-7793"
    Mar 29 09:15:30.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s882" in namespace "gc-7793"
    Mar 29 09:15:30.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-74lg8" in namespace "gc-7793"
    Mar 29 09:15:30.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-795qq" in namespace "gc-7793"
    Mar 29 09:15:30.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b8h5" in namespace "gc-7793"
    Mar 29 09:15:30.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-85nxb" in namespace "gc-7793"
    Mar 29 09:15:30.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-882zt" in namespace "gc-7793"
    Mar 29 09:15:30.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ctpg" in namespace "gc-7793"
    Mar 29 09:15:30.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mhxv" in namespace "gc-7793"
    Mar 29 09:15:30.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-92swk" in namespace "gc-7793"
    Mar 29 09:15:30.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-97d2b" in namespace "gc-7793"
    Mar 29 09:15:30.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-98tpw" in namespace "gc-7793"
    Mar 29 09:15:30.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-9grvb" in namespace "gc-7793"
    Mar 29 09:15:30.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ms6p" in namespace "gc-7793"
    Mar 29 09:15:30.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rknv" in namespace "gc-7793"
    Mar 29 09:15:30.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2vh6" in namespace "gc-7793"
    Mar 29 09:15:30.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgznv" in namespace "gc-7793"
    Mar 29 09:15:30.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqb4" in namespace "gc-7793"
    Mar 29 09:15:30.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpbgh" in namespace "gc-7793"
    Mar 29 09:15:30.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccqh9" in namespace "gc-7793"
    Mar 29 09:15:30.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl7zg" in namespace "gc-7793"
    Mar 29 09:15:30.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvtbb" in namespace "gc-7793"
    Mar 29 09:15:30.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddx8n" in namespace "gc-7793"
    Mar 29 09:15:30.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkcdz" in namespace "gc-7793"
    Mar 29 09:15:30.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmdvn" in namespace "gc-7793"
    Mar 29 09:15:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpmn4" in namespace "gc-7793"
    Mar 29 09:15:30.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtfbh" in namespace "gc-7793"
    Mar 29 09:15:30.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzgcp" in namespace "gc-7793"
    Mar 29 09:15:30.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7gg6" in namespace "gc-7793"
    Mar 29 09:15:30.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8csv" in namespace "gc-7793"
    Mar 29 09:15:30.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9c7l" in namespace "gc-7793"
    Mar 29 09:15:30.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdj9h" in namespace "gc-7793"
    Mar 29 09:15:30.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhlpc" in namespace "gc-7793"
    Mar 29 09:15:30.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj26l" in namespace "gc-7793"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 29 09:15:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7793" for this suite. 03/29/23 09:15:30.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:30.99
Mar 29 09:15:30.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:15:30.991
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:31.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:31.008
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/29/23 09:15:31.01
Mar 29 09:15:31.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf" in namespace "projected-5988" to be "Succeeded or Failed"
Mar 29 09:15:31.022: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676463ms
Mar 29 09:15:33.024: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004946347s
Mar 29 09:15:35.025: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005997326s
STEP: Saw pod success 03/29/23 09:15:35.025
Mar 29 09:15:35.025: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf" satisfied condition "Succeeded or Failed"
Mar 29 09:15:35.027: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf container client-container: <nil>
STEP: delete the pod 03/29/23 09:15:35.029
Mar 29 09:15:35.035: INFO: Waiting for pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf to disappear
Mar 29 09:15:35.037: INFO: Pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 29 09:15:35.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5988" for this suite. 03/29/23 09:15:35.038
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":337,"skipped":6103,"failed":0}
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:30.99
    Mar 29 09:15:30.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:15:30.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:31.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:31.008
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/29/23 09:15:31.01
    Mar 29 09:15:31.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf" in namespace "projected-5988" to be "Succeeded or Failed"
    Mar 29 09:15:31.022: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.676463ms
    Mar 29 09:15:33.024: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004946347s
    Mar 29 09:15:35.025: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005997326s
    STEP: Saw pod success 03/29/23 09:15:35.025
    Mar 29 09:15:35.025: INFO: Pod "downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf" satisfied condition "Succeeded or Failed"
    Mar 29 09:15:35.027: INFO: Trying to get logs from node 10.146.0.115 pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf container client-container: <nil>
    STEP: delete the pod 03/29/23 09:15:35.029
    Mar 29 09:15:35.035: INFO: Waiting for pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf to disappear
    Mar 29 09:15:35.037: INFO: Pod downwardapi-volume-fc4611ca-9d00-4615-bc8c-9863b514bbcf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 29 09:15:35.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5988" for this suite. 03/29/23 09:15:35.038
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:35.041
Mar 29 09:15:35.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:15:35.041
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:35.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:35.048
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-0db051e8-c677-4c9f-b854-a2829e92ac47 03/29/23 09:15:35.05
STEP: Creating a pod to test consume configMaps 03/29/23 09:15:35.051
Mar 29 09:15:35.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469" in namespace "projected-6799" to be "Succeeded or Failed"
Mar 29 09:15:35.057: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Pending", Reason="", readiness=false. Elapsed: 1.154872ms
Mar 29 09:15:37.059: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00398119s
Mar 29 09:15:39.058: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00299275s
STEP: Saw pod success 03/29/23 09:15:39.058
Mar 29 09:15:39.059: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469" satisfied condition "Succeeded or Failed"
Mar 29 09:15:39.060: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 container agnhost-container: <nil>
STEP: delete the pod 03/29/23 09:15:39.062
Mar 29 09:15:39.070: INFO: Waiting for pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 to disappear
Mar 29 09:15:39.071: INFO: Pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 09:15:39.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6799" for this suite. 03/29/23 09:15:39.072
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":338,"skipped":6104,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:35.041
    Mar 29 09:15:35.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:15:35.041
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:35.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:35.048
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-0db051e8-c677-4c9f-b854-a2829e92ac47 03/29/23 09:15:35.05
    STEP: Creating a pod to test consume configMaps 03/29/23 09:15:35.051
    Mar 29 09:15:35.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469" in namespace "projected-6799" to be "Succeeded or Failed"
    Mar 29 09:15:35.057: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Pending", Reason="", readiness=false. Elapsed: 1.154872ms
    Mar 29 09:15:37.059: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00398119s
    Mar 29 09:15:39.058: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00299275s
    STEP: Saw pod success 03/29/23 09:15:39.058
    Mar 29 09:15:39.059: INFO: Pod "pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469" satisfied condition "Succeeded or Failed"
    Mar 29 09:15:39.060: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 09:15:39.062
    Mar 29 09:15:39.070: INFO: Waiting for pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 to disappear
    Mar 29 09:15:39.071: INFO: Pod pod-projected-configmaps-2236218b-49e3-4d57-927c-9525aaaca469 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 09:15:39.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6799" for this suite. 03/29/23 09:15:39.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:39.075
Mar 29 09:15:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:15:39.076
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:39.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:39.084
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:15:39.09
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:39.307
STEP: Deploying the webhook pod 03/29/23 09:15:39.31
STEP: Wait for the deployment to be ready 03/29/23 09:15:39.316
Mar 29 09:15:39.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/29/23 09:15:41.322
STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:41.328
Mar 29 09:15:42.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/29/23 09:15:42.33
STEP: Creating a custom resource definition that should be denied by the webhook 03/29/23 09:15:42.341
Mar 29 09:15:42.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:15:42.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4931" for this suite. 03/29/23 09:15:42.352
STEP: Destroying namespace "webhook-4931-markers" for this suite. 03/29/23 09:15:42.356
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":339,"skipped":6123,"failed":0}
------------------------------
â€¢ [3.301 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:39.075
    Mar 29 09:15:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:15:39.076
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:39.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:39.084
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:15:39.09
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:39.307
    STEP: Deploying the webhook pod 03/29/23 09:15:39.31
    STEP: Wait for the deployment to be ready 03/29/23 09:15:39.316
    Mar 29 09:15:39.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/29/23 09:15:41.322
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:41.328
    Mar 29 09:15:42.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/29/23 09:15:42.33
    STEP: Creating a custom resource definition that should be denied by the webhook 03/29/23 09:15:42.341
    Mar 29 09:15:42.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:15:42.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4931" for this suite. 03/29/23 09:15:42.352
    STEP: Destroying namespace "webhook-4931-markers" for this suite. 03/29/23 09:15:42.356
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:42.377
Mar 29 09:15:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:15:42.377
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:42.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:42.387
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:15:42.396
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:42.908
STEP: Deploying the webhook pod 03/29/23 09:15:42.911
STEP: Wait for the deployment to be ready 03/29/23 09:15:42.92
Mar 29 09:15:42.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:15:44.929
STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:44.935
Mar 29 09:15:45.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/29/23 09:15:45.937
STEP: create a namespace for the webhook 03/29/23 09:15:45.944
STEP: create a configmap should be unconditionally rejected by the webhook 03/29/23 09:15:45.949
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:15:45.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9361" for this suite. 03/29/23 09:15:45.973
STEP: Destroying namespace "webhook-9361-markers" for this suite. 03/29/23 09:15:45.976
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":340,"skipped":6150,"failed":0}
------------------------------
â€¢ [3.618 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:42.377
    Mar 29 09:15:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:15:42.377
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:42.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:42.387
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:15:42.396
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:42.908
    STEP: Deploying the webhook pod 03/29/23 09:15:42.911
    STEP: Wait for the deployment to be ready 03/29/23 09:15:42.92
    Mar 29 09:15:42.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:15:44.929
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:44.935
    Mar 29 09:15:45.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/29/23 09:15:45.937
    STEP: create a namespace for the webhook 03/29/23 09:15:45.944
    STEP: create a configmap should be unconditionally rejected by the webhook 03/29/23 09:15:45.949
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:15:45.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9361" for this suite. 03/29/23 09:15:45.973
    STEP: Destroying namespace "webhook-9361-markers" for this suite. 03/29/23 09:15:45.976
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:45.998
Mar 29 09:15:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename projected 03/29/23 09:15:45.998
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:46.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:46.008
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-288cf3e3-22c5-4ec2-bb3b-87835ab42035 03/29/23 09:15:46.01
STEP: Creating a pod to test consume configMaps 03/29/23 09:15:46.012
Mar 29 09:15:46.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f" in namespace "projected-4958" to be "Succeeded or Failed"
Mar 29 09:15:46.020: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.901424ms
Mar 29 09:15:48.023: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004909774s
Mar 29 09:15:50.022: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004066276s
STEP: Saw pod success 03/29/23 09:15:50.022
Mar 29 09:15:50.022: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f" satisfied condition "Succeeded or Failed"
Mar 29 09:15:50.024: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/29/23 09:15:50.026
Mar 29 09:15:50.033: INFO: Waiting for pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f to disappear
Mar 29 09:15:50.034: INFO: Pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 29 09:15:50.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4958" for this suite. 03/29/23 09:15:50.037
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":341,"skipped":6234,"failed":0}
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:45.998
    Mar 29 09:15:45.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename projected 03/29/23 09:15:45.998
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:46.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:46.008
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-288cf3e3-22c5-4ec2-bb3b-87835ab42035 03/29/23 09:15:46.01
    STEP: Creating a pod to test consume configMaps 03/29/23 09:15:46.012
    Mar 29 09:15:46.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f" in namespace "projected-4958" to be "Succeeded or Failed"
    Mar 29 09:15:46.020: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.901424ms
    Mar 29 09:15:48.023: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004909774s
    Mar 29 09:15:50.022: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004066276s
    STEP: Saw pod success 03/29/23 09:15:50.022
    Mar 29 09:15:50.022: INFO: Pod "pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f" satisfied condition "Succeeded or Failed"
    Mar 29 09:15:50.024: INFO: Trying to get logs from node 10.146.0.115 pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/29/23 09:15:50.026
    Mar 29 09:15:50.033: INFO: Waiting for pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f to disappear
    Mar 29 09:15:50.034: INFO: Pod pod-projected-configmaps-80102f83-8dcd-4aee-be54-19a3fce3161f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 29 09:15:50.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4958" for this suite. 03/29/23 09:15:50.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:50.045
Mar 29 09:15:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename kubectl 03/29/23 09:15:50.046
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:50.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:50.059
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar 29 09:15:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5807 version'
Mar 29 09:15:50.099: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 29 09:15:50.099: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"archive\", BuildDate:\"2023-02-06T09:05:12Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 29 09:15:50.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5807" for this suite. 03/29/23 09:15:50.101
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":342,"skipped":6240,"failed":0}
------------------------------
â€¢ [0.058 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:50.045
    Mar 29 09:15:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename kubectl 03/29/23 09:15:50.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:50.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:50.059
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar 29 09:15:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=kubectl-5807 version'
    Mar 29 09:15:50.099: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 29 09:15:50.099: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"archive\", BuildDate:\"2023-02-06T09:05:12Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 29 09:15:50.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5807" for this suite. 03/29/23 09:15:50.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:50.104
Mar 29 09:15:50.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:15:50.104
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:50.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:50.111
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:15:50.119
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:50.506
STEP: Deploying the webhook pod 03/29/23 09:15:50.509
STEP: Wait for the deployment to be ready 03/29/23 09:15:50.515
Mar 29 09:15:50.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:15:52.525
STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:52.53
Mar 29 09:15:53.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/29/23 09:15:53.561
STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 09:15:53.582
STEP: Deleting the collection of validation webhooks 03/29/23 09:15:53.6
STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 09:15:53.619
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:15:53.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2776" for this suite. 03/29/23 09:15:53.624
STEP: Destroying namespace "webhook-2776-markers" for this suite. 03/29/23 09:15:53.626
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":343,"skipped":6246,"failed":0}
------------------------------
â€¢ [3.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:50.104
    Mar 29 09:15:50.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:15:50.104
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:50.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:50.111
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:15:50.119
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:15:50.506
    STEP: Deploying the webhook pod 03/29/23 09:15:50.509
    STEP: Wait for the deployment to be ready 03/29/23 09:15:50.515
    Mar 29 09:15:50.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:15:52.525
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:15:52.53
    Mar 29 09:15:53.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/29/23 09:15:53.561
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 09:15:53.582
    STEP: Deleting the collection of validation webhooks 03/29/23 09:15:53.6
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/29/23 09:15:53.619
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:15:53.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2776" for this suite. 03/29/23 09:15:53.624
    STEP: Destroying namespace "webhook-2776-markers" for this suite. 03/29/23 09:15:53.626
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:53.646
Mar 29 09:15:53.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-runtime 03/29/23 09:15:53.647
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:53.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:53.654
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/29/23 09:15:53.656
STEP: wait for the container to reach Succeeded 03/29/23 09:15:53.659
STEP: get the container status 03/29/23 09:15:57.67
STEP: the container should be terminated 03/29/23 09:15:57.671
STEP: the termination message should be set 03/29/23 09:15:57.671
Mar 29 09:15:57.671: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/29/23 09:15:57.671
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 29 09:15:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5009" for this suite. 03/29/23 09:15:57.68
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":344,"skipped":6263,"failed":0}
------------------------------
â€¢ [4.037 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:53.646
    Mar 29 09:15:53.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-runtime 03/29/23 09:15:53.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:53.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:53.654
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/29/23 09:15:53.656
    STEP: wait for the container to reach Succeeded 03/29/23 09:15:53.659
    STEP: get the container status 03/29/23 09:15:57.67
    STEP: the container should be terminated 03/29/23 09:15:57.671
    STEP: the termination message should be set 03/29/23 09:15:57.671
    Mar 29 09:15:57.671: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/29/23 09:15:57.671
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 29 09:15:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5009" for this suite. 03/29/23 09:15:57.68
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:57.683
Mar 29 09:15:57.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 09:15:57.684
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:57.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:57.69
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-6b454384-0a6a-4f11-b8af-0933ed18b19a 03/29/23 09:15:57.693
STEP: Creating the pod 03/29/23 09:15:57.696
Mar 29 09:15:57.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240" in namespace "configmap-195" to be "running"
Mar 29 09:15:57.701: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19161ms
Mar 29 09:15:59.703: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240": Phase="Running", Reason="", readiness=false. Elapsed: 2.003642924s
Mar 29 09:15:59.703: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240" satisfied condition "running"
STEP: Waiting for pod with text data 03/29/23 09:15:59.703
STEP: Waiting for pod with binary data 03/29/23 09:15:59.711
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 09:15:59.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-195" for this suite. 03/29/23 09:15:59.715
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":345,"skipped":6264,"failed":0}
------------------------------
â€¢ [2.035 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:57.683
    Mar 29 09:15:57.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 09:15:57.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:57.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:57.69
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-6b454384-0a6a-4f11-b8af-0933ed18b19a 03/29/23 09:15:57.693
    STEP: Creating the pod 03/29/23 09:15:57.696
    Mar 29 09:15:57.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240" in namespace "configmap-195" to be "running"
    Mar 29 09:15:57.701: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19161ms
    Mar 29 09:15:59.703: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240": Phase="Running", Reason="", readiness=false. Elapsed: 2.003642924s
    Mar 29 09:15:59.703: INFO: Pod "pod-configmaps-f57184e0-e692-4f4a-bba6-2dbf1443d240" satisfied condition "running"
    STEP: Waiting for pod with text data 03/29/23 09:15:59.703
    STEP: Waiting for pod with binary data 03/29/23 09:15:59.711
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 09:15:59.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-195" for this suite. 03/29/23 09:15:59.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:15:59.721
Mar 29 09:15:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:15:59.721
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:59.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:59.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:15:59.734
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:16:00.036
STEP: Deploying the webhook pod 03/29/23 09:16:00.039
STEP: Wait for the deployment to be ready 03/29/23 09:16:00.045
Mar 29 09:16:00.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:16:02.054
STEP: Verifying the service has paired with the endpoint 03/29/23 09:16:02.06
Mar 29 09:16:03.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/29/23 09:16:03.062
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/29/23 09:16:03.072
STEP: Creating a configMap that should not be mutated 03/29/23 09:16:03.075
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/29/23 09:16:03.079
STEP: Creating a configMap that should be mutated 03/29/23 09:16:03.083
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:16:03.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5952" for this suite. 03/29/23 09:16:03.095
STEP: Destroying namespace "webhook-5952-markers" for this suite. 03/29/23 09:16:03.098
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":346,"skipped":6360,"failed":0}
------------------------------
â€¢ [3.394 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:15:59.721
    Mar 29 09:15:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:15:59.721
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:15:59.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:15:59.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:15:59.734
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:16:00.036
    STEP: Deploying the webhook pod 03/29/23 09:16:00.039
    STEP: Wait for the deployment to be ready 03/29/23 09:16:00.045
    Mar 29 09:16:00.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:16:02.054
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:16:02.06
    Mar 29 09:16:03.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/29/23 09:16:03.062
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/29/23 09:16:03.072
    STEP: Creating a configMap that should not be mutated 03/29/23 09:16:03.075
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/29/23 09:16:03.079
    STEP: Creating a configMap that should be mutated 03/29/23 09:16:03.083
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:16:03.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5952" for this suite. 03/29/23 09:16:03.095
    STEP: Destroying namespace "webhook-5952-markers" for this suite. 03/29/23 09:16:03.098
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:16:03.115
Mar 29 09:16:03.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 09:16:03.116
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:03.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:03.127
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar 29 09:16:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: creating the pod 03/29/23 09:16:03.129
STEP: submitting the pod to kubernetes 03/29/23 09:16:03.129
Mar 29 09:16:03.134: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b" in namespace "pods-2902" to be "running and ready"
Mar 29 09:16:03.136: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605985ms
Mar 29 09:16:03.136: INFO: The phase of Pod pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:16:05.139: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004550278s
Mar 29 09:16:05.139: INFO: The phase of Pod pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b is Running (Ready = true)
Mar 29 09:16:05.139: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 09:16:05.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2902" for this suite. 03/29/23 09:16:05.147
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":347,"skipped":6376,"failed":0}
------------------------------
â€¢ [2.035 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:16:03.115
    Mar 29 09:16:03.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 09:16:03.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:03.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:03.127
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar 29 09:16:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: creating the pod 03/29/23 09:16:03.129
    STEP: submitting the pod to kubernetes 03/29/23 09:16:03.129
    Mar 29 09:16:03.134: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b" in namespace "pods-2902" to be "running and ready"
    Mar 29 09:16:03.136: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605985ms
    Mar 29 09:16:03.136: INFO: The phase of Pod pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:16:05.139: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004550278s
    Mar 29 09:16:05.139: INFO: The phase of Pod pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b is Running (Ready = true)
    Mar 29 09:16:05.139: INFO: Pod "pod-logs-websocket-b4c5163f-7e86-4835-af1a-8d20424e2c3b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 09:16:05.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2902" for this suite. 03/29/23 09:16:05.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:16:05.151
Mar 29 09:16:05.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename containers 03/29/23 09:16:05.152
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:05.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:05.16
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/29/23 09:16:05.161
Mar 29 09:16:05.165: INFO: Waiting up to 5m0s for pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b" in namespace "containers-2336" to be "Succeeded or Failed"
Mar 29 09:16:05.166: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310694ms
Mar 29 09:16:07.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004190659s
Mar 29 09:16:09.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003436734s
STEP: Saw pod success 03/29/23 09:16:09.169
Mar 29 09:16:09.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b" satisfied condition "Succeeded or Failed"
Mar 29 09:16:09.170: INFO: Trying to get logs from node 10.146.0.116 pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b container agnhost-container: <nil>
STEP: delete the pod 03/29/23 09:16:09.173
Mar 29 09:16:09.179: INFO: Waiting for pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b to disappear
Mar 29 09:16:09.180: INFO: Pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 29 09:16:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2336" for this suite. 03/29/23 09:16:09.182
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":348,"skipped":6419,"failed":0}
------------------------------
â€¢ [4.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:16:05.151
    Mar 29 09:16:05.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename containers 03/29/23 09:16:05.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:05.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:05.16
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/29/23 09:16:05.161
    Mar 29 09:16:05.165: INFO: Waiting up to 5m0s for pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b" in namespace "containers-2336" to be "Succeeded or Failed"
    Mar 29 09:16:05.166: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310694ms
    Mar 29 09:16:07.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004190659s
    Mar 29 09:16:09.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003436734s
    STEP: Saw pod success 03/29/23 09:16:09.169
    Mar 29 09:16:09.169: INFO: Pod "client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b" satisfied condition "Succeeded or Failed"
    Mar 29 09:16:09.170: INFO: Trying to get logs from node 10.146.0.116 pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b container agnhost-container: <nil>
    STEP: delete the pod 03/29/23 09:16:09.173
    Mar 29 09:16:09.179: INFO: Waiting for pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b to disappear
    Mar 29 09:16:09.180: INFO: Pod client-containers-5bdb0573-5518-42e2-a33f-bd322a056f9b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 29 09:16:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2336" for this suite. 03/29/23 09:16:09.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:16:09.186
Mar 29 09:16:09.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 09:16:09.186
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:09.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:09.194
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/29/23 09:16:09.195
Mar 29 09:16:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: rename a version 03/29/23 09:16:14.497
STEP: check the new version name is served 03/29/23 09:16:14.506
STEP: check the old version name is removed 03/29/23 09:16:15.83
STEP: check the other version is not changed 03/29/23 09:16:16.973
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:16:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7728" for this suite. 03/29/23 09:16:21.085
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":349,"skipped":6437,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.903 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:16:09.186
    Mar 29 09:16:09.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 09:16:09.186
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:09.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:09.194
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/29/23 09:16:09.195
    Mar 29 09:16:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: rename a version 03/29/23 09:16:14.497
    STEP: check the new version name is served 03/29/23 09:16:14.506
    STEP: check the old version name is removed 03/29/23 09:16:15.83
    STEP: check the other version is not changed 03/29/23 09:16:16.973
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:16:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7728" for this suite. 03/29/23 09:16:21.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:16:21.089
Mar 29 09:16:21.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename container-probe 03/29/23 09:16:21.089
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:21.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:21.098
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 in namespace container-probe-5228 03/29/23 09:16:21.099
Mar 29 09:16:21.103: INFO: Waiting up to 5m0s for pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0" in namespace "container-probe-5228" to be "not pending"
Mar 29 09:16:21.104: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.119109ms
Mar 29 09:16:23.107: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004076412s
Mar 29 09:16:23.107: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0" satisfied condition "not pending"
Mar 29 09:16:23.107: INFO: Started pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 in namespace container-probe-5228
STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 09:16:23.107
Mar 29 09:16:23.109: INFO: Initial restart count of pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is 0
Mar 29 09:16:43.138: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 1 (20.029209336s elapsed)
Mar 29 09:17:03.168: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 2 (40.059067331s elapsed)
Mar 29 09:17:23.197: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 3 (1m0.0878365s elapsed)
Mar 29 09:17:43.226: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 4 (1m20.117222059s elapsed)
Mar 29 09:18:55.318: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 5 (2m32.208838924s elapsed)
STEP: deleting the pod 03/29/23 09:18:55.318
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 29 09:18:55.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5228" for this suite. 03/29/23 09:18:55.326
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":350,"skipped":6451,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.239 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:16:21.089
    Mar 29 09:16:21.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename container-probe 03/29/23 09:16:21.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:16:21.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:16:21.098
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 in namespace container-probe-5228 03/29/23 09:16:21.099
    Mar 29 09:16:21.103: INFO: Waiting up to 5m0s for pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0" in namespace "container-probe-5228" to be "not pending"
    Mar 29 09:16:21.104: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.119109ms
    Mar 29 09:16:23.107: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004076412s
    Mar 29 09:16:23.107: INFO: Pod "liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0" satisfied condition "not pending"
    Mar 29 09:16:23.107: INFO: Started pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 in namespace container-probe-5228
    STEP: checking the pod's current state and verifying that restartCount is present 03/29/23 09:16:23.107
    Mar 29 09:16:23.109: INFO: Initial restart count of pod liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is 0
    Mar 29 09:16:43.138: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 1 (20.029209336s elapsed)
    Mar 29 09:17:03.168: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 2 (40.059067331s elapsed)
    Mar 29 09:17:23.197: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 3 (1m0.0878365s elapsed)
    Mar 29 09:17:43.226: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 4 (1m20.117222059s elapsed)
    Mar 29 09:18:55.318: INFO: Restart count of pod container-probe-5228/liveness-e4626c7f-cdb0-4cff-a04f-132e02b680c0 is now 5 (2m32.208838924s elapsed)
    STEP: deleting the pod 03/29/23 09:18:55.318
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 29 09:18:55.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5228" for this suite. 03/29/23 09:18:55.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:18:55.329
Mar 29 09:18:55.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename conformance-tests 03/29/23 09:18:55.329
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:18:55.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:18:55.337
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/29/23 09:18:55.338
Mar 29 09:18:55.338: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar 29 09:18:55.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-5482" for this suite. 03/29/23 09:18:55.342
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":351,"skipped":6464,"failed":0}
------------------------------
â€¢ [0.016 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:18:55.329
    Mar 29 09:18:55.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename conformance-tests 03/29/23 09:18:55.329
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:18:55.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:18:55.337
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/29/23 09:18:55.338
    Mar 29 09:18:55.338: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar 29 09:18:55.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-5482" for this suite. 03/29/23 09:18:55.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:18:55.345
Mar 29 09:18:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename configmap 03/29/23 09:18:55.345
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:18:55.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:18:55.353
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-9710a2b5-07b9-46ad-80e7-e541a7aacc01 03/29/23 09:18:55.355
STEP: Creating the pod 03/29/23 09:18:55.357
Mar 29 09:18:55.361: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c" in namespace "configmap-755" to be "running and ready"
Mar 29 09:18:55.363: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253886ms
Mar 29 09:18:55.363: INFO: The phase of Pod pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c is Pending, waiting for it to be Running (with Ready = true)
Mar 29 09:18:57.365: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078464s
Mar 29 09:18:57.365: INFO: The phase of Pod pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c is Running (Ready = true)
Mar 29 09:18:57.365: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-9710a2b5-07b9-46ad-80e7-e541a7aacc01 03/29/23 09:18:57.374
STEP: waiting to observe update in volume 03/29/23 09:18:57.377
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 29 09:19:01.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-755" for this suite. 03/29/23 09:19:01.389
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":352,"skipped":6487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.047 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:18:55.345
    Mar 29 09:18:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename configmap 03/29/23 09:18:55.345
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:18:55.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:18:55.353
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-9710a2b5-07b9-46ad-80e7-e541a7aacc01 03/29/23 09:18:55.355
    STEP: Creating the pod 03/29/23 09:18:55.357
    Mar 29 09:18:55.361: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c" in namespace "configmap-755" to be "running and ready"
    Mar 29 09:18:55.363: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.253886ms
    Mar 29 09:18:55.363: INFO: The phase of Pod pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c is Pending, waiting for it to be Running (with Ready = true)
    Mar 29 09:18:57.365: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078464s
    Mar 29 09:18:57.365: INFO: The phase of Pod pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c is Running (Ready = true)
    Mar 29 09:18:57.365: INFO: Pod "pod-configmaps-26e732fd-0173-4ad0-abc1-c01991e1407c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-9710a2b5-07b9-46ad-80e7-e541a7aacc01 03/29/23 09:18:57.374
    STEP: waiting to observe update in volume 03/29/23 09:18:57.377
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 29 09:19:01.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-755" for this suite. 03/29/23 09:19:01.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:01.392
Mar 29 09:19:01.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename proxy 03/29/23 09:19:01.393
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:01.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:01.4
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/29/23 09:19:01.405
STEP: creating replication controller proxy-service-4kwqj in namespace proxy-2101 03/29/23 09:19:01.405
I0329 09:19:01.409620      22 runners.go:193] Created replication controller with name: proxy-service-4kwqj, namespace: proxy-2101, replica count: 1
I0329 09:19:02.460246      22 runners.go:193] proxy-service-4kwqj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 09:19:02.461: INFO: setup took 1.060493478s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/29/23 09:19:02.461
Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.728602ms)
Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.722763ms)
Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.78754ms)
Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.832972ms)
Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 4.662338ms)
Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 4.764487ms)
Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 4.701438ms)
Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 4.917306ms)
Mar 29 09:19:02.467: INFO: (0) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 5.067145ms)
Mar 29 09:19:02.467: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 5.05922ms)
Mar 29 09:19:02.468: INFO: (0) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 6.149836ms)
Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 7.188406ms)
Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 7.870659ms)
Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 7.914161ms)
Mar 29 09:19:02.471: INFO: (0) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 9.107378ms)
Mar 29 09:19:02.471: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 9.118845ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.186951ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.20564ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.215688ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.321977ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.29629ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.620187ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.625565ms)
Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.616189ms)
Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.005468ms)
Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.025069ms)
Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.146491ms)
Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.13136ms)
Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.156771ms)
Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.657633ms)
Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.679273ms)
Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.691959ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.099826ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.051939ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.298463ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.308897ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.335114ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.353242ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.390947ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.389073ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.650586ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.614057ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.682457ms)
Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.765966ms)
Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.333592ms)
Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.377349ms)
Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.325711ms)
Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.3672ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.802263ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.965099ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.959534ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.940468ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.999134ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.021236ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.946124ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.099465ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.116355ms)
Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.189512ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.587119ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.655248ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.739145ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.711055ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.800423ms)
Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.686552ms)
Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.982778ms)
Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.517673ms)
Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.485226ms)
Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.552641ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.688551ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.637894ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.589664ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.889636ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.849662ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 2.945772ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.883306ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.90933ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.302869ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.259712ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.326393ms)
Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.301871ms)
Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 1.871511ms)
Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.94363ms)
Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.056221ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.149115ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.221744ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.167673ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.251159ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.206455ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.212539ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.429576ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.10981ms)
Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.143361ms)
Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.158999ms)
Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.237624ms)
Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.208864ms)
Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.199588ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.053809ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.119304ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.132865ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.593877ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.617382ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.626045ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.679945ms)
Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.695352ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.931314ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.973016ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.173521ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.133833ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.207548ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.194489ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.139619ms)
Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.242614ms)
Mar 29 09:19:02.493: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.535884ms)
Mar 29 09:19:02.494: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.289955ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.582787ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.609528ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.67166ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.707475ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.738392ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.780167ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.862733ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.821539ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.893778ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 2.992813ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 2.962829ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.236454ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.308543ms)
Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.270529ms)
Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 1.549312ms)
Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 1.644028ms)
Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.704551ms)
Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.714776ms)
Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.776098ms)
Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.825921ms)
Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.994779ms)
Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.070185ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.205947ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.236378ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.23483ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.31153ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.29187ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.307822ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.359834ms)
Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.391061ms)
Mar 29 09:19:02.500: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 1.400191ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.258044ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.266347ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.236111ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.392468ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.469908ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.516683ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.530128ms)
Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.581938ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.865699ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.207144ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.187988ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.201403ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.541368ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.484431ms)
Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.467434ms)
Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.99334ms)
Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.969364ms)
Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.006199ms)
Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.013188ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.388074ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.386975ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.471803ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.023431ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.006458ms)
Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.97057ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.070667ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.102505ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.104427ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.139581ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.759821ms)
Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.776931ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 2.87514ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.877384ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.941273ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.987477ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.928889ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.080526ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.058635ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.125525ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.195679ms)
Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.264121ms)
Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.307061ms)
Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.332188ms)
Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.350747ms)
Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.408427ms)
Mar 29 09:19:02.511: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 4.444763ms)
Mar 29 09:19:02.511: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 4.439762ms)
Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.226206ms)
Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.248213ms)
Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.373679ms)
Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.427661ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.776506ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.841267ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.86111ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.866483ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.038939ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.017233ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.130868ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.166732ms)
Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.686297ms)
Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.761374ms)
Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.799359ms)
Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.745644ms)
Mar 29 09:19:02.517: INFO: (13) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.806027ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.12203ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.235033ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.211604ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.219784ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.304682ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.302994ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.318448ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.384063ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.347198ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.562586ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.587269ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.523424ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.59172ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.582672ms)
Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.6809ms)
Mar 29 09:19:02.520: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 1.495894ms)
Mar 29 09:19:02.520: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.50514ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.516552ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.574091ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.58562ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 2.645399ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.648723ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.71094ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.838889ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.888803ms)
Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.14483ms)
Mar 29 09:19:02.522: INFO: (14) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.614016ms)
Mar 29 09:19:02.522: INFO: (14) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.633184ms)
Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 4.377772ms)
Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.414059ms)
Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 4.459495ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 1.701022ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.067763ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.104676ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.121448ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.169917ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.25817ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.304365ms)
Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.299675ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.990811ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 2.983235ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.011623ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.091655ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.665951ms)
Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.606769ms)
Mar 29 09:19:02.527: INFO: (15) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.62673ms)
Mar 29 09:19:02.527: INFO: (15) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.644064ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.007798ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.232586ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.251184ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.281385ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.566917ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.686007ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.826607ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.846987ms)
Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.821979ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.064526ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.036486ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.088268ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.071106ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.583353ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.622781ms)
Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.627637ms)
Mar 29 09:19:02.532: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.372099ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.188467ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.209139ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.219397ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.212926ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.204615ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.237936ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.254856ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.307937ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.355942ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.550031ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.533744ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.651854ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.667946ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.633371ms)
Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.720457ms)
Mar 29 09:19:02.536: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.382731ms)
Mar 29 09:19:02.536: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.475093ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.484774ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.246964ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.190621ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.236656ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.225472ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.221007ms)
Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.296292ms)
Mar 29 09:19:02.538: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 4.260345ms)
Mar 29 09:19:02.538: INFO: (18) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 4.322143ms)
Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.583664ms)
Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 4.655676ms)
Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 4.680619ms)
Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 4.71307ms)
Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 4.787836ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.420049ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.348589ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.446433ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.526532ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.514436ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.606952ms)
Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.527218ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.584186ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.613743ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.778422ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.190926ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.219971ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.210993ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.292169ms)
Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.259709ms)
Mar 29 09:19:02.543: INFO: (19) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.312802ms)
STEP: deleting ReplicationController proxy-service-4kwqj in namespace proxy-2101, will wait for the garbage collector to delete the pods 03/29/23 09:19:02.543
Mar 29 09:19:02.598: INFO: Deleting ReplicationController proxy-service-4kwqj took: 2.856932ms
Mar 29 09:19:02.698: INFO: Terminating ReplicationController proxy-service-4kwqj pods took: 100.220038ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 29 09:19:05.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2101" for this suite. 03/29/23 09:19:05.301
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":353,"skipped":6519,"failed":0}
------------------------------
â€¢ [3.911 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:01.392
    Mar 29 09:19:01.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename proxy 03/29/23 09:19:01.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:01.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:01.4
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/29/23 09:19:01.405
    STEP: creating replication controller proxy-service-4kwqj in namespace proxy-2101 03/29/23 09:19:01.405
    I0329 09:19:01.409620      22 runners.go:193] Created replication controller with name: proxy-service-4kwqj, namespace: proxy-2101, replica count: 1
    I0329 09:19:02.460246      22 runners.go:193] proxy-service-4kwqj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 09:19:02.461: INFO: setup took 1.060493478s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/29/23 09:19:02.461
    Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.728602ms)
    Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.722763ms)
    Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.78754ms)
    Mar 29 09:19:02.464: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.832972ms)
    Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 4.662338ms)
    Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 4.764487ms)
    Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 4.701438ms)
    Mar 29 09:19:02.466: INFO: (0) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 4.917306ms)
    Mar 29 09:19:02.467: INFO: (0) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 5.067145ms)
    Mar 29 09:19:02.467: INFO: (0) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 5.05922ms)
    Mar 29 09:19:02.468: INFO: (0) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 6.149836ms)
    Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 7.188406ms)
    Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 7.870659ms)
    Mar 29 09:19:02.469: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 7.914161ms)
    Mar 29 09:19:02.471: INFO: (0) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 9.107378ms)
    Mar 29 09:19:02.471: INFO: (0) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 9.118845ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.186951ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.20564ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.215688ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.321977ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.29629ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.620187ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.625565ms)
    Mar 29 09:19:02.473: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.616189ms)
    Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.005468ms)
    Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.025069ms)
    Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.146491ms)
    Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.13136ms)
    Mar 29 09:19:02.474: INFO: (1) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.156771ms)
    Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.657633ms)
    Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.679273ms)
    Mar 29 09:19:02.475: INFO: (1) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.691959ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.099826ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.051939ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.298463ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.308897ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.335114ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.353242ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.390947ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.389073ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.650586ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.614057ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.682457ms)
    Mar 29 09:19:02.477: INFO: (2) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.765966ms)
    Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.333592ms)
    Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.377349ms)
    Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.325711ms)
    Mar 29 09:19:02.478: INFO: (2) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.3672ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.802263ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.965099ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.959534ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.940468ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.999134ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.021236ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.946124ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.099465ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.116355ms)
    Mar 29 09:19:02.481: INFO: (3) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.189512ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.587119ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.655248ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.739145ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.711055ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.800423ms)
    Mar 29 09:19:02.482: INFO: (3) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.686552ms)
    Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.982778ms)
    Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.517673ms)
    Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.485226ms)
    Mar 29 09:19:02.484: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.552641ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.688551ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.637894ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.589664ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.889636ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.849662ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 2.945772ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.883306ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.90933ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.302869ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.259712ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.326393ms)
    Mar 29 09:19:02.485: INFO: (4) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.301871ms)
    Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 1.871511ms)
    Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.94363ms)
    Mar 29 09:19:02.487: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.056221ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.149115ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.221744ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.167673ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.251159ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.206455ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.212539ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.429576ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.10981ms)
    Mar 29 09:19:02.488: INFO: (5) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.143361ms)
    Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.158999ms)
    Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.237624ms)
    Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.208864ms)
    Mar 29 09:19:02.489: INFO: (5) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.199588ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.053809ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.119304ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.132865ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.593877ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.617382ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.626045ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.679945ms)
    Mar 29 09:19:02.491: INFO: (6) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.695352ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.931314ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.973016ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.173521ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.133833ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.207548ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.194489ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.139619ms)
    Mar 29 09:19:02.492: INFO: (6) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.242614ms)
    Mar 29 09:19:02.493: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.535884ms)
    Mar 29 09:19:02.494: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.289955ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.582787ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.609528ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.67166ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.707475ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.738392ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.780167ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.862733ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.821539ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.893778ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 2.992813ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 2.962829ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.236454ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.308543ms)
    Mar 29 09:19:02.495: INFO: (7) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.270529ms)
    Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 1.549312ms)
    Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 1.644028ms)
    Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.704551ms)
    Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.714776ms)
    Mar 29 09:19:02.497: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.776098ms)
    Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.825921ms)
    Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.994779ms)
    Mar 29 09:19:02.498: INFO: (8) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.070185ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.205947ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.236378ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.23483ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.31153ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.29187ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.307822ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.359834ms)
    Mar 29 09:19:02.499: INFO: (8) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.391061ms)
    Mar 29 09:19:02.500: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 1.400191ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.258044ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.266347ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.236111ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.392468ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.469908ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.516683ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.530128ms)
    Mar 29 09:19:02.501: INFO: (9) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.581938ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.865699ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.207144ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.187988ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.201403ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.541368ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.484431ms)
    Mar 29 09:19:02.502: INFO: (9) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.467434ms)
    Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.99334ms)
    Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 1.969364ms)
    Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.006199ms)
    Mar 29 09:19:02.504: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.013188ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.388074ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.386975ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.471803ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.023431ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.006458ms)
    Mar 29 09:19:02.505: INFO: (10) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.97057ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.070667ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.102505ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.104427ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.139581ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.759821ms)
    Mar 29 09:19:02.506: INFO: (10) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.776931ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 2.87514ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.877384ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.941273ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.987477ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 2.928889ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.080526ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.058635ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.125525ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.195679ms)
    Mar 29 09:19:02.509: INFO: (11) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.264121ms)
    Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.307061ms)
    Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.332188ms)
    Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.350747ms)
    Mar 29 09:19:02.510: INFO: (11) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.408427ms)
    Mar 29 09:19:02.511: INFO: (11) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 4.444763ms)
    Mar 29 09:19:02.511: INFO: (11) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 4.439762ms)
    Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.226206ms)
    Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.248213ms)
    Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.373679ms)
    Mar 29 09:19:02.513: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.427661ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.776506ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.841267ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.86111ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.866483ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.038939ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.017233ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.130868ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.166732ms)
    Mar 29 09:19:02.514: INFO: (12) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.686297ms)
    Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.761374ms)
    Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.799359ms)
    Mar 29 09:19:02.515: INFO: (12) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.745644ms)
    Mar 29 09:19:02.517: INFO: (13) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 2.806027ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.12203ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.235033ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.211604ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.219784ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.304682ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.302994ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.318448ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.384063ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.347198ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.562586ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.587269ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.523424ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.59172ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.582672ms)
    Mar 29 09:19:02.518: INFO: (13) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.6809ms)
    Mar 29 09:19:02.520: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 1.495894ms)
    Mar 29 09:19:02.520: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.50514ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.516552ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.574091ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.58562ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 2.645399ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.648723ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.71094ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.838889ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.888803ms)
    Mar 29 09:19:02.521: INFO: (14) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.14483ms)
    Mar 29 09:19:02.522: INFO: (14) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.614016ms)
    Mar 29 09:19:02.522: INFO: (14) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.633184ms)
    Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 4.377772ms)
    Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.414059ms)
    Mar 29 09:19:02.523: INFO: (14) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 4.459495ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 1.701022ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.067763ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.104676ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.121448ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.169917ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.25817ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.304365ms)
    Mar 29 09:19:02.525: INFO: (15) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.299675ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.990811ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 2.983235ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.011623ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.091655ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.665951ms)
    Mar 29 09:19:02.526: INFO: (15) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.606769ms)
    Mar 29 09:19:02.527: INFO: (15) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.62673ms)
    Mar 29 09:19:02.527: INFO: (15) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.644064ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.007798ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.232586ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.251184ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.281385ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.566917ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.686007ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.826607ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.846987ms)
    Mar 29 09:19:02.529: INFO: (16) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.821979ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.064526ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.036486ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.088268ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.071106ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.583353ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.622781ms)
    Mar 29 09:19:02.530: INFO: (16) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.627637ms)
    Mar 29 09:19:02.532: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 1.372099ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.188467ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.209139ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 3.219397ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.212926ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.204615ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.237936ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.254856ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 3.307937ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.355942ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.550031ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.533744ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.651854ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 3.667946ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.633371ms)
    Mar 29 09:19:02.534: INFO: (17) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.720457ms)
    Mar 29 09:19:02.536: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.382731ms)
    Mar 29 09:19:02.536: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.475093ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.484774ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 3.246964ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 3.190621ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 3.236656ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 3.225472ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 3.221007ms)
    Mar 29 09:19:02.537: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 3.296292ms)
    Mar 29 09:19:02.538: INFO: (18) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 4.260345ms)
    Mar 29 09:19:02.538: INFO: (18) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 4.322143ms)
    Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.583664ms)
    Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 4.655676ms)
    Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 4.680619ms)
    Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 4.71307ms)
    Mar 29 09:19:02.539: INFO: (18) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 4.787836ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.420049ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.348589ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:443/proxy/tlsrewritem... (200; 2.446433ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">test<... (200; 2.526532ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/http:proxy-service-4kwqj-sxl7c:1080/proxy/rewriteme">... (200; 2.514436ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:462/proxy/: tls qux (200; 2.606952ms)
    Mar 29 09:19:02.541: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:162/proxy/: bar (200; 2.527218ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c:160/proxy/: foo (200; 2.584186ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/https:proxy-service-4kwqj-sxl7c:460/proxy/: tls baz (200; 2.613743ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/: <a href="/api/v1/namespaces/proxy-2101/pods/proxy-service-4kwqj-sxl7c/proxy/rewriteme">test</a> (200; 2.778422ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname1/proxy/: foo (200; 3.190926ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname1/proxy/: tls baz (200; 3.219971ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/https:proxy-service-4kwqj:tlsportname2/proxy/: tls qux (200; 3.210993ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname2/proxy/: bar (200; 3.292169ms)
    Mar 29 09:19:02.542: INFO: (19) /api/v1/namespaces/proxy-2101/services/proxy-service-4kwqj:portname1/proxy/: foo (200; 3.259709ms)
    Mar 29 09:19:02.543: INFO: (19) /api/v1/namespaces/proxy-2101/services/http:proxy-service-4kwqj:portname2/proxy/: bar (200; 4.312802ms)
    STEP: deleting ReplicationController proxy-service-4kwqj in namespace proxy-2101, will wait for the garbage collector to delete the pods 03/29/23 09:19:02.543
    Mar 29 09:19:02.598: INFO: Deleting ReplicationController proxy-service-4kwqj took: 2.856932ms
    Mar 29 09:19:02.698: INFO: Terminating ReplicationController proxy-service-4kwqj pods took: 100.220038ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 29 09:19:05.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2101" for this suite. 03/29/23 09:19:05.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:05.305
Mar 29 09:19:05.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename services 03/29/23 09:19:05.306
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:05.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:05.316
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6133 03/29/23 09:19:05.318
STEP: creating service affinity-nodeport-transition in namespace services-6133 03/29/23 09:19:05.318
STEP: creating replication controller affinity-nodeport-transition in namespace services-6133 03/29/23 09:19:05.324
I0329 09:19:05.326896      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6133, replica count: 3
I0329 09:19:08.378150      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 29 09:19:08.382: INFO: Creating new exec pod
Mar 29 09:19:08.386: INFO: Waiting up to 5m0s for pod "execpod-affinityhw7kk" in namespace "services-6133" to be "running"
Mar 29 09:19:08.387: INFO: Pod "execpod-affinityhw7kk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245906ms
Mar 29 09:19:10.390: INFO: Pod "execpod-affinityhw7kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.003499777s
Mar 29 09:19:10.390: INFO: Pod "execpod-affinityhw7kk" satisfied condition "running"
Mar 29 09:19:11.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 29 09:19:11.482: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 29 09:19:11.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:19:11.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.73.163 80'
Mar 29 09:19:11.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.73.163 80\nConnection to 10.100.73.163 80 port [tcp/http] succeeded!\n"
Mar 29 09:19:11.565: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:19:11.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 31914'
Mar 29 09:19:11.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 31914\nConnection to 10.146.0.116 31914 port [tcp/*] succeeded!\n"
Mar 29 09:19:11.652: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:19:11.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 31914'
Mar 29 09:19:11.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 31914\nConnection to 10.146.0.117 31914 port [tcp/*] succeeded!\n"
Mar 29 09:19:11.735: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 29 09:19:11.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:31914/ ; done'
Mar 29 09:19:11.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n"
Mar 29 09:19:11.869: INFO: stdout: "\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl"
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
Mar 29 09:19:11.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:31914/ ; done'
Mar 29 09:19:12.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n"
Mar 29 09:19:12.009: INFO: stdout: "\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll"
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
Mar 29 09:19:12.009: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6133, will wait for the garbage collector to delete the pods 03/29/23 09:19:12.016
Mar 29 09:19:12.070: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.719893ms
Mar 29 09:19:12.171: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.883201ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 29 09:19:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6133" for this suite. 03/29/23 09:19:14.283
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":354,"skipped":6559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.980 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:05.305
    Mar 29 09:19:05.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename services 03/29/23 09:19:05.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:05.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:05.316
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6133 03/29/23 09:19:05.318
    STEP: creating service affinity-nodeport-transition in namespace services-6133 03/29/23 09:19:05.318
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6133 03/29/23 09:19:05.324
    I0329 09:19:05.326896      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6133, replica count: 3
    I0329 09:19:08.378150      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 29 09:19:08.382: INFO: Creating new exec pod
    Mar 29 09:19:08.386: INFO: Waiting up to 5m0s for pod "execpod-affinityhw7kk" in namespace "services-6133" to be "running"
    Mar 29 09:19:08.387: INFO: Pod "execpod-affinityhw7kk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.245906ms
    Mar 29 09:19:10.390: INFO: Pod "execpod-affinityhw7kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.003499777s
    Mar 29 09:19:10.390: INFO: Pod "execpod-affinityhw7kk" satisfied condition "running"
    Mar 29 09:19:11.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar 29 09:19:11.482: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 29 09:19:11.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:19:11.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.73.163 80'
    Mar 29 09:19:11.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.73.163 80\nConnection to 10.100.73.163 80 port [tcp/http] succeeded!\n"
    Mar 29 09:19:11.565: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:19:11.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.116 31914'
    Mar 29 09:19:11.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.116 31914\nConnection to 10.146.0.116 31914 port [tcp/*] succeeded!\n"
    Mar 29 09:19:11.652: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:19:11.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.146.0.117 31914'
    Mar 29 09:19:11.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.146.0.117 31914\nConnection to 10.146.0.117 31914 port [tcp/*] succeeded!\n"
    Mar 29 09:19:11.735: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 29 09:19:11.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:31914/ ; done'
    Mar 29 09:19:11.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n"
    Mar 29 09:19:11.869: INFO: stdout: "\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-mr65d\naffinity-nodeport-transition-7l8vl"
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-mr65d
    Mar 29 09:19:11.869: INFO: Received response from host: affinity-nodeport-transition-7l8vl
    Mar 29 09:19:11.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3218831692 --namespace=services-6133 exec execpod-affinityhw7kk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.146.0.115:31914/ ; done'
    Mar 29 09:19:12.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.146.0.115:31914/\n"
    Mar 29 09:19:12.009: INFO: stdout: "\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll\naffinity-nodeport-transition-8j2ll"
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Received response from host: affinity-nodeport-transition-8j2ll
    Mar 29 09:19:12.009: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6133, will wait for the garbage collector to delete the pods 03/29/23 09:19:12.016
    Mar 29 09:19:12.070: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.719893ms
    Mar 29 09:19:12.171: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.883201ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 29 09:19:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6133" for this suite. 03/29/23 09:19:14.283
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:14.286
Mar 29 09:19:14.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 09:19:14.287
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:14.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:14.295
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/29/23 09:19:14.297
Mar 29 09:19:14.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
Mar 29 09:19:17.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:19:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-812" for this suite. 03/29/23 09:19:24.879
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":355,"skipped":6571,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.595 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:14.286
    Mar 29 09:19:14.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename crd-publish-openapi 03/29/23 09:19:14.287
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:14.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:14.295
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/29/23 09:19:14.297
    Mar 29 09:19:14.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    Mar 29 09:19:17.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:19:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-812" for this suite. 03/29/23 09:19:24.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:24.882
Mar 29 09:19:24.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename pods 03/29/23 09:19:24.883
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:24.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:24.891
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/29/23 09:19:24.892
STEP: setting up watch 03/29/23 09:19:24.892
STEP: submitting the pod to kubernetes 03/29/23 09:19:24.994
STEP: verifying the pod is in kubernetes 03/29/23 09:19:24.998
STEP: verifying pod creation was observed 03/29/23 09:19:25
Mar 29 09:19:25.000: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c" in namespace "pods-6358" to be "running"
Mar 29 09:19:25.001: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204836ms
Mar 29 09:19:27.003: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0034894s
Mar 29 09:19:27.003: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c" satisfied condition "running"
STEP: deleting the pod gracefully 03/29/23 09:19:27.004
STEP: verifying pod deletion was observed 03/29/23 09:19:27.007
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 29 09:19:29.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6358" for this suite. 03/29/23 09:19:29.284
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":356,"skipped":6594,"failed":0}
------------------------------
â€¢ [4.405 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:24.882
    Mar 29 09:19:24.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename pods 03/29/23 09:19:24.883
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:24.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:24.891
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/29/23 09:19:24.892
    STEP: setting up watch 03/29/23 09:19:24.892
    STEP: submitting the pod to kubernetes 03/29/23 09:19:24.994
    STEP: verifying the pod is in kubernetes 03/29/23 09:19:24.998
    STEP: verifying pod creation was observed 03/29/23 09:19:25
    Mar 29 09:19:25.000: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c" in namespace "pods-6358" to be "running"
    Mar 29 09:19:25.001: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.204836ms
    Mar 29 09:19:27.003: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0034894s
    Mar 29 09:19:27.003: INFO: Pod "pod-submit-remove-1c2ad115-c2e3-4caf-a3a9-d4d3e9570f9c" satisfied condition "running"
    STEP: deleting the pod gracefully 03/29/23 09:19:27.004
    STEP: verifying pod deletion was observed 03/29/23 09:19:27.007
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 29 09:19:29.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6358" for this suite. 03/29/23 09:19:29.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:29.288
Mar 29 09:19:29.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 09:19:29.288
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:29.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:29.296
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/29/23 09:19:29.297
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/29/23 09:19:29.299
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/29/23 09:19:29.299
STEP: creating a pod to probe DNS 03/29/23 09:19:29.299
STEP: submitting the pod to kubernetes 03/29/23 09:19:29.3
Mar 29 09:19:29.305: INFO: Waiting up to 15m0s for pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c" in namespace "dns-9074" to be "running"
Mar 29 09:19:29.306: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279441ms
Mar 29 09:19:31.309: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004178922s
Mar 29 09:19:31.309: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:19:31.309
STEP: looking for the results for each expected name from probers 03/29/23 09:19:31.311
Mar 29 09:19:31.318: INFO: DNS probes using dns-9074/dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c succeeded

STEP: deleting the pod 03/29/23 09:19:31.318
STEP: deleting the test headless service 03/29/23 09:19:31.324
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 09:19:31.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9074" for this suite. 03/29/23 09:19:31.332
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":357,"skipped":6603,"failed":0}
------------------------------
â€¢ [2.047 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:29.288
    Mar 29 09:19:29.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 09:19:29.288
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:29.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:29.296
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/29/23 09:19:29.297
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/29/23 09:19:29.299
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/29/23 09:19:29.299
    STEP: creating a pod to probe DNS 03/29/23 09:19:29.299
    STEP: submitting the pod to kubernetes 03/29/23 09:19:29.3
    Mar 29 09:19:29.305: INFO: Waiting up to 15m0s for pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c" in namespace "dns-9074" to be "running"
    Mar 29 09:19:29.306: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.279441ms
    Mar 29 09:19:31.309: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004178922s
    Mar 29 09:19:31.309: INFO: Pod "dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:19:31.309
    STEP: looking for the results for each expected name from probers 03/29/23 09:19:31.311
    Mar 29 09:19:31.318: INFO: DNS probes using dns-9074/dns-test-ef7f22e6-bfdf-43a4-aa33-b12f15bb092c succeeded

    STEP: deleting the pod 03/29/23 09:19:31.318
    STEP: deleting the test headless service 03/29/23 09:19:31.324
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 09:19:31.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9074" for this suite. 03/29/23 09:19:31.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:31.335
Mar 29 09:19:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename sysctl 03/29/23 09:19:31.335
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:31.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:31.342
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/29/23 09:19:31.343
STEP: Watching for error events or started pod 03/29/23 09:19:31.347
STEP: Waiting for pod completion 03/29/23 09:19:33.35
Mar 29 09:19:33.350: INFO: Waiting up to 3m0s for pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898" in namespace "sysctl-4458" to be "completed"
Mar 29 09:19:33.352: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422548ms
Mar 29 09:19:35.354: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003486754s
Mar 29 09:19:35.354: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/29/23 09:19:35.355
STEP: Getting logs from the pod 03/29/23 09:19:35.355
STEP: Checking that the sysctl is actually updated 03/29/23 09:19:35.363
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 29 09:19:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4458" for this suite. 03/29/23 09:19:35.365
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":358,"skipped":6615,"failed":0}
------------------------------
â€¢ [4.033 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:31.335
    Mar 29 09:19:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename sysctl 03/29/23 09:19:31.335
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:31.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:31.342
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/29/23 09:19:31.343
    STEP: Watching for error events or started pod 03/29/23 09:19:31.347
    STEP: Waiting for pod completion 03/29/23 09:19:33.35
    Mar 29 09:19:33.350: INFO: Waiting up to 3m0s for pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898" in namespace "sysctl-4458" to be "completed"
    Mar 29 09:19:33.352: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898": Phase="Pending", Reason="", readiness=false. Elapsed: 1.422548ms
    Mar 29 09:19:35.354: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003486754s
    Mar 29 09:19:35.354: INFO: Pod "sysctl-c7b86f0e-e346-4fe2-92a1-27a3371dc898" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/29/23 09:19:35.355
    STEP: Getting logs from the pod 03/29/23 09:19:35.355
    STEP: Checking that the sysctl is actually updated 03/29/23 09:19:35.363
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 29 09:19:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-4458" for this suite. 03/29/23 09:19:35.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:35.368
Mar 29 09:19:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename cronjob 03/29/23 09:19:35.369
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:35.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:35.376
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/29/23 09:19:35.377
STEP: creating 03/29/23 09:19:35.378
STEP: getting 03/29/23 09:19:35.381
STEP: listing 03/29/23 09:19:35.382
STEP: watching 03/29/23 09:19:35.383
Mar 29 09:19:35.383: INFO: starting watch
STEP: cluster-wide listing 03/29/23 09:19:35.383
STEP: cluster-wide watching 03/29/23 09:19:35.385
Mar 29 09:19:35.385: INFO: starting watch
STEP: patching 03/29/23 09:19:35.385
STEP: updating 03/29/23 09:19:35.388
Mar 29 09:19:35.392: INFO: waiting for watch events with expected annotations
Mar 29 09:19:35.392: INFO: saw patched and updated annotations
STEP: patching /status 03/29/23 09:19:35.392
STEP: updating /status 03/29/23 09:19:35.395
STEP: get /status 03/29/23 09:19:35.398
STEP: deleting 03/29/23 09:19:35.399
STEP: deleting a collection 03/29/23 09:19:35.405
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 29 09:19:35.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2612" for this suite. 03/29/23 09:19:35.41
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":359,"skipped":6630,"failed":0}
------------------------------
â€¢ [0.044 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:35.368
    Mar 29 09:19:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename cronjob 03/29/23 09:19:35.369
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:35.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:35.376
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/29/23 09:19:35.377
    STEP: creating 03/29/23 09:19:35.378
    STEP: getting 03/29/23 09:19:35.381
    STEP: listing 03/29/23 09:19:35.382
    STEP: watching 03/29/23 09:19:35.383
    Mar 29 09:19:35.383: INFO: starting watch
    STEP: cluster-wide listing 03/29/23 09:19:35.383
    STEP: cluster-wide watching 03/29/23 09:19:35.385
    Mar 29 09:19:35.385: INFO: starting watch
    STEP: patching 03/29/23 09:19:35.385
    STEP: updating 03/29/23 09:19:35.388
    Mar 29 09:19:35.392: INFO: waiting for watch events with expected annotations
    Mar 29 09:19:35.392: INFO: saw patched and updated annotations
    STEP: patching /status 03/29/23 09:19:35.392
    STEP: updating /status 03/29/23 09:19:35.395
    STEP: get /status 03/29/23 09:19:35.398
    STEP: deleting 03/29/23 09:19:35.399
    STEP: deleting a collection 03/29/23 09:19:35.405
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 29 09:19:35.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2612" for this suite. 03/29/23 09:19:35.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:35.412
Mar 29 09:19:35.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename dns 03/29/23 09:19:35.413
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:35.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:35.42
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/29/23 09:19:35.421
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local;sleep 1; done
 03/29/23 09:19:35.423
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local;sleep 1; done
 03/29/23 09:19:35.423
STEP: creating a pod to probe DNS 03/29/23 09:19:35.423
STEP: submitting the pod to kubernetes 03/29/23 09:19:35.423
Mar 29 09:19:35.428: INFO: Waiting up to 15m0s for pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e" in namespace "dns-9160" to be "running"
Mar 29 09:19:35.429: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403245ms
Mar 29 09:19:37.431: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003272459s
Mar 29 09:19:37.431: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e" satisfied condition "running"
STEP: retrieving the pod 03/29/23 09:19:37.431
STEP: looking for the results for each expected name from probers 03/29/23 09:19:37.432
Mar 29 09:19:37.434: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.436: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.438: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.439: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.441: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.442: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.443: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.444: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
Mar 29 09:19:37.444: INFO: Lookups using dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local]

Mar 29 09:19:42.460: INFO: DNS probes using dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e succeeded

STEP: deleting the pod 03/29/23 09:19:42.46
STEP: deleting the test headless service 03/29/23 09:19:42.468
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 29 09:19:42.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9160" for this suite. 03/29/23 09:19:42.475
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":360,"skipped":6643,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.066 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:35.412
    Mar 29 09:19:35.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename dns 03/29/23 09:19:35.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:35.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:35.42
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/29/23 09:19:35.421
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local;sleep 1; done
     03/29/23 09:19:35.423
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9160.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local;sleep 1; done
     03/29/23 09:19:35.423
    STEP: creating a pod to probe DNS 03/29/23 09:19:35.423
    STEP: submitting the pod to kubernetes 03/29/23 09:19:35.423
    Mar 29 09:19:35.428: INFO: Waiting up to 15m0s for pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e" in namespace "dns-9160" to be "running"
    Mar 29 09:19:35.429: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403245ms
    Mar 29 09:19:37.431: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003272459s
    Mar 29 09:19:37.431: INFO: Pod "dns-test-31327db0-5746-4ca5-afad-5ce9445a045e" satisfied condition "running"
    STEP: retrieving the pod 03/29/23 09:19:37.431
    STEP: looking for the results for each expected name from probers 03/29/23 09:19:37.432
    Mar 29 09:19:37.434: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.436: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.438: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.439: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.441: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.442: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.443: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.444: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local from pod dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e: the server could not find the requested resource (get pods dns-test-31327db0-5746-4ca5-afad-5ce9445a045e)
    Mar 29 09:19:37.444: INFO: Lookups using dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9160.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9160.svc.cluster.local jessie_udp@dns-test-service-2.dns-9160.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9160.svc.cluster.local]

    Mar 29 09:19:42.460: INFO: DNS probes using dns-9160/dns-test-31327db0-5746-4ca5-afad-5ce9445a045e succeeded

    STEP: deleting the pod 03/29/23 09:19:42.46
    STEP: deleting the test headless service 03/29/23 09:19:42.468
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 29 09:19:42.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9160" for this suite. 03/29/23 09:19:42.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:42.479
Mar 29 09:19:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename resourcequota 03/29/23 09:19:42.48
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:42.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:42.488
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/29/23 09:19:42.489
STEP: Creating a ResourceQuota 03/29/23 09:19:47.491
STEP: Ensuring resource quota status is calculated 03/29/23 09:19:47.496
STEP: Creating a Pod that fits quota 03/29/23 09:19:49.499
STEP: Ensuring ResourceQuota status captures the pod usage 03/29/23 09:19:49.506
STEP: Not allowing a pod to be created that exceeds remaining quota 03/29/23 09:19:51.509
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/29/23 09:19:51.511
STEP: Ensuring a pod cannot update its resource requirements 03/29/23 09:19:51.512
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/29/23 09:19:51.514
STEP: Deleting the pod 03/29/23 09:19:53.516
STEP: Ensuring resource quota status released the pod usage 03/29/23 09:19:53.52
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 29 09:19:55.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-245" for this suite. 03/29/23 09:19:55.524
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":361,"skipped":6671,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:42.479
    Mar 29 09:19:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename resourcequota 03/29/23 09:19:42.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:42.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:42.488
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/29/23 09:19:42.489
    STEP: Creating a ResourceQuota 03/29/23 09:19:47.491
    STEP: Ensuring resource quota status is calculated 03/29/23 09:19:47.496
    STEP: Creating a Pod that fits quota 03/29/23 09:19:49.499
    STEP: Ensuring ResourceQuota status captures the pod usage 03/29/23 09:19:49.506
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/29/23 09:19:51.509
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/29/23 09:19:51.511
    STEP: Ensuring a pod cannot update its resource requirements 03/29/23 09:19:51.512
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/29/23 09:19:51.514
    STEP: Deleting the pod 03/29/23 09:19:53.516
    STEP: Ensuring resource quota status released the pod usage 03/29/23 09:19:53.52
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 29 09:19:55.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-245" for this suite. 03/29/23 09:19:55.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/29/23 09:19:55.528
Mar 29 09:19:55.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
STEP: Building a namespace api object, basename webhook 03/29/23 09:19:55.528
STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:55.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:55.536
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/29/23 09:19:55.543
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:19:56.06
STEP: Deploying the webhook pod 03/29/23 09:19:56.064
STEP: Wait for the deployment to be ready 03/29/23 09:19:56.07
Mar 29 09:19:56.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/29/23 09:19:58.08
STEP: Verifying the service has paired with the endpoint 03/29/23 09:19:58.085
Mar 29 09:19:59.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/29/23 09:19:59.087
STEP: create a pod that should be denied by the webhook 03/29/23 09:19:59.095
STEP: create a pod that causes the webhook to hang 03/29/23 09:19:59.101
STEP: create a configmap that should be denied by the webhook 03/29/23 09:20:09.105
STEP: create a configmap that should be admitted by the webhook 03/29/23 09:20:09.109
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/29/23 09:20:09.114
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/29/23 09:20:09.117
STEP: create a namespace that bypass the webhook 03/29/23 09:20:09.119
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/29/23 09:20:09.122
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 29 09:20:09.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6888" for this suite. 03/29/23 09:20:09.134
STEP: Destroying namespace "webhook-6888-markers" for this suite. 03/29/23 09:20:09.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":362,"skipped":6702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.629 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/29/23 09:19:55.528
    Mar 29 09:19:55.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3218831692
    STEP: Building a namespace api object, basename webhook 03/29/23 09:19:55.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/29/23 09:19:55.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/29/23 09:19:55.536
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/29/23 09:19:55.543
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/29/23 09:19:56.06
    STEP: Deploying the webhook pod 03/29/23 09:19:56.064
    STEP: Wait for the deployment to be ready 03/29/23 09:19:56.07
    Mar 29 09:19:56.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/29/23 09:19:58.08
    STEP: Verifying the service has paired with the endpoint 03/29/23 09:19:58.085
    Mar 29 09:19:59.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/29/23 09:19:59.087
    STEP: create a pod that should be denied by the webhook 03/29/23 09:19:59.095
    STEP: create a pod that causes the webhook to hang 03/29/23 09:19:59.101
    STEP: create a configmap that should be denied by the webhook 03/29/23 09:20:09.105
    STEP: create a configmap that should be admitted by the webhook 03/29/23 09:20:09.109
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/29/23 09:20:09.114
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/29/23 09:20:09.117
    STEP: create a namespace that bypass the webhook 03/29/23 09:20:09.119
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/29/23 09:20:09.122
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 29 09:20:09.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6888" for this suite. 03/29/23 09:20:09.134
    STEP: Destroying namespace "webhook-6888-markers" for this suite. 03/29/23 09:20:09.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar 29 09:20:09.158: INFO: Running AfterSuite actions on all nodes
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar 29 09:20:09.158: INFO: Running AfterSuite actions on node 1
Mar 29 09:20:09.158: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 29 09:20:09.158: INFO: Running AfterSuite actions on all nodes
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar 29 09:20:09.158: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 29 09:20:09.158: INFO: Running AfterSuite actions on node 1
    Mar 29 09:20:09.158: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.051 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5720.722 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h35m20.898075173s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

